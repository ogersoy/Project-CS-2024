# Foreword
This Technical Report has been produced by the 3^rd^ Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# 1 Scope
The present document investigates various aspects related to the support of
the GTP-C signalling based load / overload control solution as specified in
3GPP TS 23.401 [2] and TS 23.060 [3], as also concluded in TR 23.843 [4], with
the main focus on:
  * Definition of \"Load Control\" and \"Overload Control\" related > information with enough precision to guarantee a common > multi-vendor interpretation of this information allowing > inter-operability between various GTP-C nodes;
  * Mechanisms to address various \"Notes\", which are targeted to the > stage 3, specified in clause 8.2.5, clause 8.2.6 & clause 10.2 of > the 3GPP TR 23.843 v1.0.0 [4].
This technical report addresses the following aspects related to the \"GTP-C
signalling based load and overload control\" feature in detail:
\- Investigation and study of the following aspects related to the \"Load
Control Information\" to fulfill the normative requirements or to produce
recommendations.
\- Definition of the \"Load Control Information\" by evaluating various
parameters which can be exchanged under this information.
\- Inclusion of \"Load Control Information\" in GTP-C messages.
\- Potential enhancements to the existing node selection algorithm to take
information received from \"Load Control Information\" into account.
\- Investigation and study of the following aspects related to the \"Overload
Control Information\" to fulfill the normative requirements or to produce
recommendations.
\- Definition of the \"Overload Control Information\" by evaluating various
parameters which can be exchanged under this information.
\- Inclusion of the \"Overload Control Information\" in GTP-C messages.
\- Message throttling algorithms and message prioritization when congestion
mitigation is applied.
\- Propagation of the MME/SGSN identity to the PGW to ensure that the overload
mitigation is always applied to the currently serving MME/SGSN.
\- Potential interactions with the existing overload control mechanisms when
the overload factor is received within the \"Overload Control Information\".
\- Investigation and study of the following the other deployment related
aspects to fulfill the normative requirements or to produce recommendations.
\- Applicability of this feature to 3GPP and non-3GPP based GTP-C interfaces.
\- Methods to discover the support of this feature by the peer node in the
network.
\- Supporting the feature across the PLMN boundary.
\- Issues within the network with partial support of this feature.
\- Overload mitigation policies when this feature support is not enabled in
the network.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
> [2] 3GPP TS 23.401: \"GPRS Enhancements for E-UTRAN Access\".
>
> [3] 3GPP TS 23.060: \"General Packet Radio Service (GPRS); Service
> description; Stage 2\".
[4] 3GPP TR 23.843: \"Study on Core Network Overload Solutions\".
> [5] 3GPP TS 29.060: \"General Packet Radio Service (GPRS); GPRS Tunnelling
> Protocol (GTP) across the Gn and Gp interface\".
[6] 3GPP TS 29.274: \"3GPP Evolved Packet System (EPS); Evolved General Packet
Radio Service (GPRS) Tunnelling Protocol for Control plane (GTPv2-C); Stage
3\".
[7] 3GPP TS 23.402: \"Architecture enhancements for non-3GPP accesses\".
[8] 3GPP TS 24.302: \"Access to the 3GPP Evolved Packet Core (EPC) via
non-3GPP access networks\".
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
TR 21.905 [1] and the following apply.
A term defined in the present document takes precedence over the definition of
the same term, if any, in TR 21.905 [1].
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
FFS
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in TR 21.905
[1] and the following apply.
An abbreviation defined in the present document takes precedence over the
definition of the same abbreviation, if any, in TR 21.905 [1].
# 4 Introduction to GTP-C overload control
## 4.0 GTP-C overload problem
GTP-C is the GPRS Tunnelling protocol used across many interfaces in the GPRS
and Evolved Packet Core Networks. See 3GPP TS 29.060 [5] and 3GPP TS 29.274
[6] for GTPv1-C and GTPv2-C respectively.
GTP-C entities can communicate with other GTP-C peers in direct contact (e.g.
MME and SGW) or remote GTP-C peers through intermediate GTP-C entities (e.g.
MME and PGW via the SGW). In normal conditions, requests sent by a GTP-C
entity will be processed by the receiving GTP-C entity which will send back a
message indicating the result of the request (success/failure).
Overload situations in a GTP-C signalling network occur when the number of
incoming requests exceeds the maximum request throughput supported by the
receiving GTP-C entity. As a consequence of the overload situation, the
receiving GTP-C entity cannot successfully process the exceeding proportion of
requests. These requests can be either simply dropped or extremely delayed in
the processing. At best, the GTP-C entity may have enough internal resources
to send back to the request initiator a message indicating that the requests
cannot be successfully processed. Whatever the behaviour of the overloaded
GTP-C nodes, the rate of successfully processed requests and consequently the
overall performances of the network decrease.
Given the nature of GTP-C protocol in how it relies on retransmissions of
unacknowledged requests (GTP-C is carried over UDP transport), when a GTP-C
entity experiences overload (or severe overload) the number of unacknowledged
GTP-C messages compounds exponentially and can lead to a node congestion or
even collapse. An overload or failure of a node can lead to an increase of the
load on the other nodes in the network and, in the worst case, turn into a
complete network issue via a snow ball effect.
The impact of GTP-C overload to services can be such as:
\- loss of PDN connectivity (IMS, Internet ...) and associated services;
\- loss of ability to setup and release radio and core network bearers
necessary to support services e.g. GBR bearers for VoLTE;
\- loss of ability to report to the PGW/PCRF user information\'s changes, e.g.
location information for emergency services and lawful intercept, changes in
RAT or QoS;
\- billing errors and loss of revenue.
## 4.1 Scenarios leading to GTP-C overload
Reasons for these temporary overload cases are many and various in an
operational network, such as insufficient internal resource capacity of a
GTP-C entity faced with a sudden burst of requests e.g. after network
failure/restart procedures affecting a large number of users, deficiency of a
GTP-C entity component leading to a drastic reduction of the overall
performances of the GTP-C entity.
Clause 4.9 of 3GPP TR 23.843 [4] provides a description of various scenarios
which can cause GTP-C overload:
\- traffic flood resulting from the failure of a network element, inducing a
signalling spike, e.g. when the network needs to re-establish the PDN
connections affected by the failure of an EPC node;
\- traffic flood resulting from a large number of users performing TAU/RAU or
from frequent transitions between idle and connected mode;
\- an exceptional event locally generating a traffic spike, e.g. a large
amount of calls (and dedicated bearers) being setup almost simultaneously upon
a catastrophic event or an exceptional but predictable event (e.g. Christmas,
New year);
\- overload caused by an overload of a downstream node (on a GTP or non-GTP-c
interface), e.g. due to GTP-C retransmissions.
Other scenarios (not listed in clause 4.9 of 3GPP TR 23.843 [4]) exist that
may also result in GTP-C overload, e.g.:
\- Frequent RAT-reselection due to scattered non 3GPP (e.g. WiFi) coverage or
massive mobility between 3GPP and non 3GPP coverage may potentially cause
frequent or massive intersystem change activities i.e. UEs trying to either
create PDN connections over the new access or moving PDN connections between
3GPP and non 3GPP coverage.
Besides, the current GTP-C load balancing based on semi-static DNS weights may
lead to a load imbalance and likely lead to overload in one or more nodes of a
SGW or PGW cluster while there is still remaining capacity on other nodes of
the same cluster.
## 4.2 GTP-C signalling based Load and Overload Control solution
### 4.2.1 Description
GTP-C load control and overload control are two distinct but complementary
concepts:
\- GTP-C load control enables a GTP-C entity (e.g. SGW/PGW) to send its load
information to a GTP-C peer (e.g. MME/SGSN) to adaptively balance the session
load across entities supporting the same function (e.g. SGWs cluster)
according to their effective load. The load information reflects the operating
status of the resources of the originating GTP-C entity.
\- GTP-C overload control enables a GTP-C entity becoming or being overloaded
to gracefully reduce its load by instructing its GTP-C peers to reduce sending
traffic according to its available signalling capacity to successfully process
the traffic. A GTP-C entity is in overload when it operates over its
signalling capacity resulting in diminished performance (including impacts to
handling of incoming and outgoing traffic).
GTP-C load control allows for better balancing of the session load, so as to
attempt to prevent overload in the first place (preventive action). GTP-C
overload control aims at shedding traffic as close to the traffic source as
possible generally when overload has occurred (reactive action), so to avoid
spreading the problem inside the network and using resources of intermediate
nodes in the network for signalling that would anyhow be discarded by the
overloaded node.
GTP-C load control does not trigger overload mitigation actions even if the
GTP-C entity reports a high load.
### 4.2.2 Principles of Load Control
Stage 2 requirements on GTP-C load control are defined in clause 4.3.7.1a.1 of
3GPP TS 23.401 [2] and clause 5.3.6.1a of 3GPP TS 23.060 [3]. The high level
principles are summarized below:
a) GTP-C Load Control is an optional feature;
b) a GTP-C node sends its Load Control Information, reflecting the operating
status of its resources, in GTP-C signalling allowing the receiving GTP-C peer
node to use the same to augment the existing GW selection procedures;
c) the calculation of the Load Control Information is implementation
dependent;
d) the SGW is allowed to send its Load Control Information to the MME/SGSN.
The PGW is allowed to send its Load Control Information to MME/SGSN via SGW;
e) the Load Control Information is piggybacked in any GTP-C request or
response message such that exchange of Load Control Information does not
trigger extra signalling;
f) a node supporting GTP-C Load Control feature sends Load Control Information
to a peer GTP control node based on local configuration or protocol based
feature negotiation (to be decided in stage 3);
g) the format of the Load Control Information shall be specified with enough
precision to guarantee a common interpretation of this information allowing
interoperability between nodes of different vendors;
h) for inter-PLMN case, local configuration may restrict the exchange and use
of Load Control Information across PLMNs;
i) the GTP-C node may decide to send different values of Load Control
Information on inter-network (roaming) and on intra-network (non-roaming)
interfaces based on local configuration.
NOTE: This is interpreted as allowing a node to send on intra-network
interfaces values that may differ from the values sent on inter-network
interfaces based on local configuration. Additionally, the node should also
send the same values between 3GPP and non-3GPP based interfaces.
j) stage 3 will specify how the Load Control Information received via GTP-C
signalling can be used in conjunction with existing Weight Factors.
See clause 4.2.4 for the applicable interfaces.
### 4.2.3 Principles of Overload Control
Stage 2 requirements on GTP-C overload control are defined in clause
4.3.7.1a.2 of 3GPP TS 23.401 [2] and clause 5.3.6.1a of 3GPP TS 23.060 [3].
The high level principles are summarized below:
a) GTP-C overload control is an optional feature;
b) a GTP-C entity signals its overload to its GTP-C peers by including
Overload control Information in GTP-C signalling which provides guidance to
the receiving GTP-C entity to decide actions which leads to signalling traffic
mitigation towards the sender of the information;
c) the Overload control Information may signal an overload of a GTP-C node
(e.g. PGW) or provide status information about specific APN(s);
d) an MME/S4-SGSN can signal an overload to the PGW via the SGW. An SGW can
signal an overload to the MME/S4-SGSN and to the PGW. A PGW can signal an
overload to the MME/S4-SGSN via the SGW;
NOTE 1: Stage 2 needs to be aligned to reflect that an MME/S4-SGSN will not
signal an overload to the SGW (i.e. SGW will not perform overload control
towards MME/S4-SGSN) as this is redundant with DDN throttling (see subclause
6.6.1), and that an SGW can signal an overload to the PGW (see subclause 6.7).
e) GTP-C overload Control feature should continue to allow for preferential
treatment of priority users (eMPS) and emergency services;
f) the Overload control Information is piggybacked in any GTP control plane
request or response message such that exchange of Overload control Information
does not trigger extra signalling;
g) the computation and transfer of the Overload control Information shall not
add significant additional load to the node itself and to its corresponding
peer nodes. The calculation of Overload control Information should not
severely impact the resource utilization of the node;
h) stage 2 provides examples of various potential overload mitigation actions
based on the reception of the Overload related information exchanged between
GTP-c nodes. However, the exact internal processing logics of a node will not
be standardized.
i) for inter-PLMN case, local configuration may restrict the exchange and use
of Overload Control Information across PLMNs;
j) the GTP-C node may decide to send different values of Overload Control
Information on inter-network (roaming) and on intra-network (non-roaming)
interfaces based on local configuration.
NOTE 2: This is interpreted as allowing a node to send on intra-network
interfaces values that may differ from the values sent on inter-network
interfaces based on local configuration. Additionally, the node should also
send the same values between 3GPP and non-3GPP based interfaces.
See clause 4.2.4 for the applicable interfaces.
### 4.2.4 Applicability to 3GPP and non-3GPP access based interfaces
#### 4.2.4.1 Introduction
While recommending the list of interfaces for which GTP-C load/overload
mechanism should be applied, the decision on the exact list of applicable
interfaces has been left by the stage 2 on the stage 3. Specifically, for 3GPP
access based interfaces it needs to be assessed whether it is beneficial to
support the overload control mechanisms on the interfaces such as Sm/Sn, S10,
S3 or S16. For non-3GPP access based interfaces, the applicability of the
overload control mechanisms over any of the interfaces, i.e. S2a or S2b, needs
to be assessed.
As a reminder, GTPv1-C is used across the Gn/Gp interfaces in the GPRS Core
Network. GTPv2-C is used across all the following EPC signalling interfaces:
\- 3GPP access based interfaces: S3, S4, S5, S8, S10, S11, S16, Sm, Sn, Sv;
\- non-3GPP access based interfaces: S2a, S2b, S101, S121.
#### 4.2.4.2 GTP-C load control
##### 4.2.4.2.1 Description
GTP-C load control may possibly apply to GTP-C interfaces towards GTP-C
entities responsible for network node selection.
Stage 2 (see clause 4.3.7.1a.1 of 3GPP TS 23.401 [2] and clause 5.3.6.1a of
3GPP TS 23.060 [3]) already requires support of load control on the S11/S4 and
S5/S8 interfaces as follows:
\- an SGW can signal its Load Information to the MME/S4-SGSN (for enhanced
load balancing across SGWs);
\- a PGW can signal its Load Information to the MME/S4-SGSN via the SGW (for
enhanced load balancing across PGWs);
Scenarios have also been identified in clause 4.1 which can cause overload at
the PGW over the S2a/S2b interfaces. It is thus proposed to support load
control on the S2a/S2b interface (for WLAN access) as follows:
\- a PGW can signal its Load Information to the TWAN/ePDG (for enhanced load
balancing across PGWs during Attach or new PDN connectivity request scenarios
and thus to try to avoid PGW overload at first place).
GTP-C load control will not be supported in Rel-12 for the following GTP-C
based interfaces:
\- S3, S10, S16 (selection of target MME/S4-SGSN during inter-CN handover,
limited GTP-C traffic, minimize impacts to MME/S4-SGSN);
\- Sm, Sn (MME/S4-SGSN selection by MBMS GW, limited GTP-C traffic, avoid
impacts to MBMS GW);
\- Sv (MSC-S selection in MSC pools by MME/S4-SGSN, avoid impacts to legacy CS
products);
\- S101, S121 (avoid impacts to legacy HRPD products);
\- Gn/Gp (avoid impacts to legacy Gn-SGSN/GGSN products and GTPv1-C protocol).
Table 4.2.4.2.1-1 summarizes the applicable interfaces and nodes for GTP-C
load control.
Table 4.2.4.2.1-1: Applicability of Load Control Information to GTP-C
interfaces and nodes
+------------+----------+--------------------------------------------+ | Originator | Consumer | Applicable Interfaces | +------------+----------+--------------------------------------------+ | PGW | MME | S11, S5/S8 | | | | | | | | SGW relays Load Control Information from | | | | S5/S8 to S11 interface. | +------------+----------+--------------------------------------------+ | PGW | S4-SGSN | S4, S5/S8 | | | | | | | | SGW relays Load Control Information from | | | | S5/S8 to S4 interface. | +------------+----------+--------------------------------------------+ | SGW | MME | S11 | +------------+----------+--------------------------------------------+ | SGW | S4-SGSN | S4 | +------------+----------+--------------------------------------------+ | PGW | ePDG | S2b | +------------+----------+--------------------------------------------+ | PGW | TWAN | S2a | +------------+----------+--------------------------------------------+
##### 4.2.4.2.2 Conclusions
It is concluded that support of GTP-C load control shall be specified in
Release 12 for the GTP-C interfaces and nodes indicated in Table 4.2.4.2.1-1.
#### 4.2.4.3 GTP-C overload control
##### 4.2.4.3.1 General
GTP-C overload control should be designed as a generic mechanism possibly
applicable to any GTP-C based interface and any direction. However, some
interfaces are more prone to experience overload than others, and thus the
applicability of GTP-C overload control needs to be assessed for each
interface in terms of potential benefits but also impacts and complexity.
##### 4.2.4.3.2 Applicability to 3GPP access based interfaces
###### 4.2.4.3.2.1 Description
Scenarios have been identified in clause 4.1 which can cause overload at the
MME/SGSN, SGW and PGW over the S11/S4 and S5/S8 interfaces. Thus stage 2 (see
clause 4.3.7.1a.2 of 3GPP TS 23.401 [2] and clause 5.3.6.1a of 3GPP TS 23.060
[3]) already requires support of overload control on the S11/S4 and S5/S8
interfaces as follows:
\- an MME/S4-SGSN can signal an overload to the PGW;
\- an SGW can signal an overload to the MME/S4-SGSN;
\- a PGW can signal an overload to the MME/S4-SGSN via the SGW;
NOTE: Stage 2 needs to be aligned to reflect that an MME/S4-SGSN will not
signal an overload to the SGW as this is redundant with DDN throttling (see
subclause 6.6.1), and that an SGW can signal an overload to the PGW (see
subclause 6.7).
Traffic flood may possibly occur on the S3, S10 and S16 interfaces, resulting
from a large number of users performing TAU/RAU (e.g. overlaid RATs and
failure of RAN node, MME load re-balancing, train moving across MME pools
boundaries...). Beyond mobility management procedures, RAN Information
procedures may also generate traffic on these interfaces e.g. for SON.
However:
  * most of the S3 traffic should remain internal to the combo node with the deployment of combo MME/S4-SGSN nodes. The traffic over S10/S16 is also much reduced with the deployment of MME and SGSN pools. It is therefore not essential to throttle the traffic on these interfaces when an MME or S4-SGSN experiences overload;
  * throttling signalling on these interfaces resulting from user\'s mobility (inter-MME/S4-SGSN TAU, RAU and Handover) would result in bad end user\'s perception (handover failure, loss of PDN connections) and so should be avoided as far as possible;
  * an MME or S4-SGSN in overload may drop locally incoming RIM messages w/o causing GTP-C retransmissions (although this may cause the RAN to retransmit the message).
Support of overload control over S3/S10/S16 may be beneficial but this is not
critical as for some other interfaces. It is therefore recommended that an
MME/S4-SGSN return a cause indicating a node overload when it cannot process a
request (other than a RAN Information Relay message) but is still able to
answer, as specified in clause 6.8.1. Support of overload control on
S3/S10/S16 may be considered in a later release.
GTP-C overload control will not be supported in Rel-12 for the following GTP-C
based interfaces:
\- S3, S10, S16 (see considerations above, minimize impacts to MME and
S4-SGSN);
\- S11/S4 (from an MME/S4-SGSN to an SGW, with SGW as consumer);
\- S5/S8 (from a PGW to an SGW, with SGW as consumer);
\- Sm, Sn (no overload scenario identified, limited GTP-C traffic, avoid
impacts to MBMS GW);
\- Sv (no overload scenario identified, avoid impacts to legacy CS products);
\- S101, S121 (no overload scenario identified, avoid impacts to legacy HRPD
products);
\- Gn/Gp (avoid impacts to legacy Gn-SGSN/GGSN products and GTPv1-C protocol).
Table 4.2.4.3.2.1-1 summarizes the applicable 3GPP access based interfaces and
nodes for GTP-C overload control.
Table 4.2.4.3.2.1-1: Applicability of Overload Control Information to 3GPP
access based GTP-C interfaces and nodes
+------------+----------+--------------------------------------------+ | Originator | Consumer | Applicable Interfaces | +------------+----------+--------------------------------------------+ | MME | PGW | S11, S5/S8 | | | | | | | | SGW relays Overload Control Information | | | | from S11 to S5/S8 interface. | +------------+----------+--------------------------------------------+ | S4-SGSN | PGW | S4, S5/S8 | | | | | | | | SGW relays Overload Control Information | | | | from S4 to S5/S8 interface. | +------------+----------+--------------------------------------------+ | SGW | MME | S11 | +------------+----------+--------------------------------------------+ | SGW | S4-SGSN | S4 | +------------+----------+--------------------------------------------+ | SGW | PGW | S5/S8 | | | | | | | | (in MME/S4-SGSN originated signalling | | | | towards the PGW) | +------------+----------+--------------------------------------------+ | PGW | MME | S5/S8, S11 | | | | | | | | SGW relays Overload Control Information | | | | from S5/S8 to S11 interface. | +------------+----------+--------------------------------------------+ | PGW | S4-SGSN | S5/S8, S4 | | | | | | | | SGW relays Overload Control Information | | | | from S5/S8 to S4 interface. | +------------+----------+--------------------------------------------+
###### 4.2.4.3.2.2 Conclusions
It is concluded that support of GTP-C overload control shall be specified in
Release 12 for the GTP-C interfaces and nodes indicated in Table
4.2.4.3.2.1-1.
DDN throttling mechanisms (see subclause 6.6.1) are used to reduce the DDN
traffic the SGW may originate towards an MME/S4-SGSN experiencing overload.
SGWs should be allowed by configuration to throttle DDN requests for normal
priority traffic (the SGW will then throttle in priority DDN requests for low
priority traffic).
##### 4.2.4.3.3 Applicability to non-3GPP access based interfaces
###### 4.2.4.3.3.1 General
Scenarios have been identified in clause 4.1 which can cause an overload at
the PGW over the S2a/S2b interfaces, so it is desirable in principle to
support overload control on the S2a/S2b interfaces to reduce the signalling
traffic the TWAN or ePDG send to the PGW according to the PGW\'s available
signalling capacity.
The GTP-C signalling from the TWAN/ePDG to the PGW essentially encompasses:
\- Create Session Request to establish new PDN connections or for handovers
(e.g. when the UE moves from 3GPP access to non-3GPP access or when the UE
moves from one ePDG/TWAN to another ePDG/TWAN);
\- Delete Session Request to release PDN connections;
\- and various other messages to a much smaller extent, e.g.
\- Modify Bearer Command to provide the PGW with updated parameters e.g.
resulting from a change of user subscription information;
\- Trace Session Activation / Trace Session Activation to activate or
deactivate a trace session in the PGW.
Without support of overload control on S2a/S2b, these requests could be simply
dropped or extremely delayed when the PGW experiences overload. In this case,
this would entail GTP-C retransmissions on S2a/S2b that would further increase
the load in the PGW. At best, the PGW may have enough internal resources to
send back to the TWAN/ePDG a message indicating that the requests cannot be
successfully processed. This entails that all kinds of requests (handover, PDN
connection establishment or deletion) would fail.
Support of overload control would allow e.g. to prioritize handover scenarios
over the establishment of new PDN connections, and thus minimize end user
impacts for on-going PS sessions.
Furthermore, without support of overload control on S2a/S2b, uncontrolled rate
of signalling from TWAN/ePDG would require higher throttling of the signalling
received by the PGW over S5/S8 in order control the overall rate of signalling
towards the PGW to avoid its meltdown. This would result into an unfair
advantage for UEs accessing the EPC via a non-3GPP access over those accessing
the EPC via a 3GPP access.
The following subsections address examples of potential mechanisms that may be
used over the untrusted and trusted access when the network cannot process a
request from the UE due to an overload at the PGW, to prevent the UE from re-
trying the whole process to re-establish connectivity via the WLAN access and
cause undesirable signalling towards the 3GPP AAA Server and HSS during the
network overload condition.
NOTE: The following subclauses provide example call flows assuming certain
enhancements to the TWAN/ePDG and UE behaviour. See 3GPP TS 23.402 [7] for the
actual call flows and TWAN/ePDG and UE behaviour retained in Release 12.
###### 4.2.4.3.3.2 Untrusted WLAN access
As per clause 7.2.4 of 3GPP TS 23.402 [7], the GTP-C Create Session
Request/Exchange takes place before the completion of the IPsec tunnel
establishment.
The ePDG can thus reject a new PDN connection or handover request from the UE,
if the network is not able to process it successfully, using the existing
IPsec tunnel establishment procedures specified in clause 7.2.2 of 3GPP TS
24.302 [8], which ensure that the UE will not attempt to re-establish
immediately a PDN connection to the same APN via the same ePDG:
\"If NBM is used and if the ePDG needs to reject a PDN connection due to
conditions as specified in 3GPP TS 29.273 [17] or the network policies or the
ePDG capabilities to indicate that no more PDN connection request of the given
APN can be accepted for the UE, the ePDG shall include, in the IKE_AUTH
response message, a Notify Payload with a Private Notify Message Type
PDN_CONNECTION_REJECTION as specified in clause 8.1.2. Additionally if the
IKE_AUTH request message from the UE indicated Handover Attach as specified in
clause 7.2.2, the Notification Data field of the Notify Payload shall include
the IP address information from the Handover Attach indication. If the UE
indicated Initial Attach, the Notification Data field shall be omitted.
If NBM is used and if the UE receives from the ePDG an IKE_AUTH response
message containing a Notify Payload with a Private Notify Message Type
PDN_CONNECTION_REJECTION as specified in clause 8.1.2 that includes an IP
address information in the Notification Data field, the UE shall not attempt
to re-establish this PDN connection while connected to the current ePDG and
the UE shall close the related IKEv2 security association states.
If NBM is used and if the UE receives from the ePDG an IKE_AUTH response
message containing a Notify Payload with a Private Notify Message Type
PDN_CONNECTION_REJECTION as specified in clause 8.1.2 and no Notification Data
field, the UE shall not attempt to establish additional PDN connections to
this APN while connected to the current ePDG. The UE shall close the related
IKEv2 security association states. Subsequently, the UE can attempt to
establishment additional PDN connections to the given APN if one or more
existing PDN connections to the given APN are released. While connected to the
current ePDG, if this PDN connection is the first PDN connection for the given
APN, the UE shall not attempt to establish PDN connection to the given APN.\"
This approach may however possibly entail the following problem:
\- the UE access to the APN via the untrusted access may be deferred for an
undetermined duration, e.g. if the UE remains connected to the current ePDG.
Note that when a handover request is rejected, resources are still maintained
for the PDN connection at the PGW, and at the source SGW, MME or SGSN for the
case of a 3GPP to non-3GPP handover. The UE remains registered via the 3GPP
access and can continue to use its PDN connection via the 3GPP access.
NOTE: As per clause 5.2.2.1 3GPP TS 24.301 [2], the UE enters the state EMM-
DEREGISTERED \"when an inter-system change from S1 mode to non‑3GPP access is
completed and the non‑3GPP access network provides PDN connectivity to the
same EPC\".
To avoid deferring longer than necessary the UE access to the APN via the
untrusted access, it could be envisaged to enhance the IKEv2 protocol with an
additional back-off mechanism that would allow the ePDG to reject the UE
request with a duration function of the overload level of the PGW, if no other
alternative PGWs can serve the UE request. This would prevent the UE to
initiate any new session or handover request for the congested APN during the
back-off time, be the UE camping over 3GPP or non-3GPP access.
###### 4.2.4.3.3.3 Trusted WLAN access
Some issues may possibly arise for the trusted WLAN access when the network is
not able to process successfully a new PDN connection from the UE due to the
fact that there has been so far no-well defined UE behaviour for handling
network overload, as opposed to 3GPP access or even HRPD access for which
back-off mechanisms have been specified, per APN or system-wide.
NOTE 1: This holds true regardless of whether overload control is supported or
not.
Three communication modes have been defined for Trusted WLAN access (see 3GPP
TS 23.402 [7]):
\- Transparent Single-Connection mode (corresponds to the mode of operation
specified by SaMOG in Rel-11);
\- Single-Connection mode (introduced by eSaMOG in Rel-12);
\- Multi-Connection mode (introduced by eSaMOG in Rel-12).
The Multi-Connection mode requires support of a new WLCP (WLAN Control
Protocol) between the UE and the TWAN to setup or release PDN connection. The
WLCP protocol could thus be leveraged to convey adequate cause codes from the
TWAN to the UE to ensure that the UE does not unnecessarily retry new PDN
connection or handover requests to a PGW that is overloaded. The WLCP could
also advantageously support an APN backoff mechanism, as supported for 3GPP
access or HRPD access, if no other alternative PGWs can serve the UE request.
This would prevent the UE to initiate any new session or handover request for
the congested APN during the back-off time, be the UE camping over 3GPP or
non-3GPP access.
The Transparent Single-Connection mode and Single-Connection mode do not rely
on the WLCP protocol to establish and tear down the PDN connection. This may
possibly entail the following problems:
\- if the UE does not know the reason why its new PDN connection or handover
request fails, the UE may attempt again to whole process to re-establish
connectivity via the WLAN access, including the access authentication and
authorization procedure, causing extra / unnecessary signalling towards the
3GPP AAA Server and HSS.
NOTE 2: For instance, WFA Hotspot 2.0 Specification, Release 2.0 version 4.0.0
(Nov. 2013) specifies in clause 6.4.4:
_\- \"A mobile device may fail to successfully complete EAP authentication to
the hotspot using a particular credential. Failure may be due to a variety of
reasons including invalid credentials, network problems, misconfigured APs,
etc. However, authentication failure does not necessarily mean there is a
problem with a credential or subscription; the credential may still be valid
with other APs. Therefore, in the case of an EAP authentication failure the
mobile device:_
_\- Shall not attempt more than 10 consecutive EAP authentications that result
in EAP authentication failures at the same ESS using a given credential within
a 10-minute interval. The authentication process may restart after the expiry
of the 10-minute time interval._
> _\- Should not disable this credential from being used with other BSSs.\"_
The Single-Connection mode requires support of EAP extensions between the UE
and the 3GPP AAA server to setup or handover a PDN connection. These EAP
extensions could thus be leveraged to convey an \"APN congestion\" cause and
an APN back-off timer from the 3GPP AAA Server to the UE to prevent the UE
from unnecessarily retrying new session or handover requests to a PGW or APN
that is overloaded, when no other alternative PGWs can serve the UE request,
as depicted in the example call flow depicted in Figure 4.2.4.3.3.3.1.
{width="6.690972222222222in" height="3.470138888888889in"}
Figure 4.2.4.3.3.3.1: APN back-off mechanism for UEs in Single-Connection mode
This corresponds to the call flow specified in figure 16.2.1-1 of 3GPP TS
23.402 [7] for the Single-Connection mode with the following additions:
1\. At step 14, when rejecting a UE request (new PDN connection or handover
request in trusted WLAN access) from an authenticated and authorized user
(after step 11 or step 13)), the TWAN may signal an APN congestion cause and a
back-off time to the 3GPP AAA Server for the UE and the requested APN when
congestion control is active for the APN (e.g. PGW has triggered overload
control towards the TWAN for traffic targeting this APN).
2\. At step 15, the 3GPP AAA Server includes an APN congestion cause and the
back-off time (received from the TWAN) in the EAP Failure message it sends to
the UE.
3\. At step 16, upon reception of this cause and back-off time, the UE shall
not initiate any new session or handover request for the congested APN (be it
via 3GPP or non-3GPP access) for the duration of the back-off time. The UE may
initiate new requests for NSWO or other APNs.
For UEs in Transparent Single-Connection mode (for which no extensions are
being nor can be defined between the network and the UE), the TWAN may support
a back-off mechanism on behalf of the UE as depicted in the example call flow
depicted in in Figure 4.2.4.3.3.3.2.
{width="6.683333333333334in" height="4.664583333333334in"}
Figure 4.2.4.3.3.3.2: APN back-off mechanism for UEs in Transparent Single-
connection mode
This corresponds to the call flow specified in figure 16.2.1-1 of 3GPP TS
23.402 [7] for the Transparent Single-Connection mode with the following
additions:
1\. At step 17, when rejecting a UE request (new PDN connection request in
trusted WLAN access) from an authenticated and authorized user (after step 11
or step 13), the TWAN may start a back-off timer per UE and SSID when
congestion control is active for the APN (e.g. PGW has triggered overload
control towards the TWAN for traffic targeting this APN).
NOTE 3: On a given SSID, a UE in Transparent Single-Connection mode can only
access one pre-defined APN for EPC access, thus if an UE attempt has been
rejected on this SSID due to congestion, any further UE connectivity attempt
may be rejected during the back-off timer. The UE may nevertheless be allowed
to access on another SSID where the UE may be granted access to NSWO.
NOTE 4: In this example, the PGW rejects the PDN connection establishment
request due to an overload of the APN or an overload of the PGW. The PGW may
provide overload information in its response to the TWAN, requesting the TWAN
to throttle a certain percentage of the traffic the TWAN sends towards that
PGW. But it is also possible that the TWAN does not initiate the PDN
connection establishment request over S2a if it has beforehand received an
indication of a PGW or APN overload and decides immediately that the UE
connection request is to be rejected.
2\. At step 21, the TWAN may immediately reject (or silently discard) any
subsequent request from the UE targeting this APN, i.e. any subsequent EAP
signalling originated from the same UE\'s MAC address over the same selected
SSID, received before the back-off time is expired. In that case, the TWAN
does not generate any AAA signalling towards the 3GPP AAA Server. The TWAN may
reject the UE request by sending back an EAP Failure message (e.g. with the
code 4 as per IETF RFC 3748 [x]) and/or by releasing the WLAN association
(step 21\' and step 22).
NOTE 5: The UE\'s MAC address is used in the SaMOG connection model for
associating in the TWAN the UE-TWAG point-to-point link and S2a tunnel.
NOTE 6: If the TWAN does not immediately reject the UE request at step 21, the
full sequence (steps 1 to 16) takes place again, and this as many times as the
UE will repeat its request during the overload situation.
###### 4.2.4.3.3.4 Conclusions
It is concluded that support of GTP-C overload control shall be specified in
Release 12 for the GTP-C interfaces and nodes indicated in Table
4.2.4.3.3.4-1.
Table 4.2.4.3.3.4-1: Applicability of Overload Control Information to non-3GPP
access based GTP-C interfaces and nodes
* * *
Originator Consumer Applicable Interfaces PGW TWAN S2a (Trusted WLAN access)
PGW ePDG S2b (Untrusted WLAN access)
* * *
The mitigation actions that a TWAN or ePDG may apply towards PGWs that have
indicated overload, during mobility or session management procedures over a
trusted or untrusted WLAN access, is specified in clause 4.5 of 3GPP TS 23.402
[7].
# 5 Load Control Information
## 5.1 General
In order to guarantee a common interpretation in a multi-vendor network
deployment, it is necessary to define the \"Load Control Information\" with
enough precision such that coherent and homogeneous node selection algorithms
are applied by different nodes of the same network such that an evenly load
balanced network is realized. This clause investigates possible parameters and
their definitions which can be exchanged under the \"Load Control
Information\". Thus in turn, this clause aims at defining the exact format the
\"Load Control Information\".
## 5.1A APN level load control
### 5.1A.1 Description
Node level load control refers to advertising of the load information at node
level -- i.e. load information at node level granularity -- and selection of
the target node based on this information. On the other hand, APN level load
control refers to advertising of the load information with APN level
granularity and selection of the target node based on this information. This
clause studies and highlights various aspects related to the APN level load
control.
### 5.1A.2 Requirements
**Pre-Condition:**
In the given network, when the ratio of the configured APN resource limit to
the overall capacity of the PGW is not the same across all the PGWs in the
network.
Following are the requirements to support the APN level load control in the
network when the above pre-condition is met:
**1) To achieve evenly balanced network with the APN level granularity:** The
PGW may be configured to handle more than one APN in the network. In such a
case, the PGW may be additionally configured to allocate different resources
(e.g. based on the session license) for each of the configured APN, e.g. the
PGW may be configured to handle \"X\" number of sessions for the \"consumer\"
APN while \"Y\" number of session for the \"corporate\" APN. In this case, the
load information with node level granularity is not sufficient to make better
decision of the APN level load condition of the target PGW. And hence, it
could result in a network where one PGW has more sessions for the \"consumer\"
APN while another PGW has more sessions for the \"corporate\" APN. Thus, an
evenly balanced network with APN level load granularity cannot be realized.
**2) To ensure effective overload control in the network:** If the
distribution of sessions at APN level is uneven, then there is a high risk of
overload of some PGWs as compared to other PGWs, e.g. the PGW handling
sessions for \"consumer\" APN may have to handle more messages (e.g. generated
due to mobility events resulting into change of ULI, RAT type, Serving GW,
etc.) as compared to the PGW handling sessions for \"stationary-machine\" APN.
This would result in some PGWs facing overload condition more often while the
resources (e.g. handling of messages) of other PGWs remain underutilized.
Thus, the situation leads to poor overload control of the network.
**3) To ensure efficient node selection algorithm:** Based on the node level
load information, the source node (e.g. MME) may end-up selecting the PGW for
a new session for the given APN. However, the selected PGW may reject the new
session request if it is running at 100% load capacity for the given APN. Or
the new session request may be throttled by the source node based on the
overload information of the APN for the given PGW. Thus, unless the source
node takes the overload information into account while performing the node
selection, the new session request may be denied (i.e. rejected by the
selected PGW or throttled by the source node based on PGW\'s APN level
overload information) while the other PGW may have the capacity to handle the
same. Thus, the lack of APN level load information may result in inefficient
node selection algorithm at the source node.
### 5.1A.3 Example use case
To better understand various requirements identified in clause 5.1A.2 and to
also study other aspects related to APN load control it is better to consider
an example network topology and configuration as follows:
{width="6.6930555555555555in" height="3.6368055555555556in"}
Figure 5.1A.3.1: Example network topology and configuration
As depicted in the figure 5.1A.3.1, there are 3 PGWs in the network configured
with different maximum session capacity and resource limit for APN1. Although
PGW3 has highest session capacity, PGW1 has the highest resources reserved for
APN1. Based on the configured resource limit for APN1, each PGW can calculate
its own APN1\'s resource limit relative to the overall resources at the node
level, i.e. \"PGW1-APN1-res-limit = (PGW1-APN1-max-sess / PGW1-max-sess) *
100\".
### 5.1A.4 Issues when APN load control is not used
#### 5.1A.4.1 Node selection using DNS weight-factor only
Assuming that the node selection is performed based on DNS weight-factor only,
and hence without considering the APN load control, for the example use case
given in clause 5.1A.3 the MME would calculate the relative weight of each PGW
-- i.e. PGW\'s capacity as compared to overall network\'s capacity -- as
follows:
PGWx-relative-weight = (PGWx-weight-factor / sum-of-all-the-PGWs-weight-
factor) * 100.
Accordingly,
PGW1-relative-weight = (20 / (20+20+60)) * 100 = 20%
PGW2-relative-weight = (20 / (20+20+60)) * 100 = 20%
PGW3-relative-weight = (60 / (20+20+60)) * 100 = 60%
**[Session Distribution:]{.underline}**
Based on the above, the MME will select PGW1 for 20% of the new session
requests, PGW2 for 20% of the new session requests, PGW3 for 60% of the new
session requests. Hence following session distribution will take place when
the MME has to distribute 100 new session requests for APN1:
PGW1-APN1-sess = 20.
PGW2-APN1-sess = 20.
PGW3-APN1-sess = 60.
**[Analysis of the network load:]{.underline}**
As it can be observed from the above, PGW3 has 60 sessions active for APN1.
Since PGW3 has been configured with the limit of 60 sessions for APN1, its
resource capacity is exhausted and it cannot handle any more sessions related
to the APN1. However since the MME is not aware about this situation, the MME
will perform the PGW selection based on PGW\'s relative-weight-factor. And
hence for 10 new session requests for APN1, the MME will select PGW3 for 6
sessions. These requests will be rejected by the PGW3 and the MME may try
another PGW for establishing the session for APN1. Thus, MME not performing
APN load control would result in inefficient node selection as highlighted in
requirement no. 3 in clause 5.1A.2.
Besides, it can also be observed that the above selection logic would result
in uneven distribution of load at APN level as highlighted in requirement no.
1 in clause 5.1A.2. The APN1 level resource utilization relative to the APN\'1
resource limit can be calculated as follows:
PGW1-APN1-resource-utilization = (20 / 100) * 100 = 20%
PGW2-APN1-resource-utilization = (20 / 40) * 100 = 50%
PGW3-APN1-resource-utilization = (60 / 60) * 100 = 100%
Further, based on the above, it can be observed that PGW3 and PGW2, which have
higher resource utilization for APN1, have higher risk of resource overload
for APN1 as compared to PGW1. This is highlighted in requirement no. 2 in
clause 5.1A.2.
#### 5.1A.4.2 Node selection using node level load information and DNS weight-
factor
Assuming that the node selection is performed based on current load and DNS
weight-factor, and hence without considering the APN load control, for the
example use case given in clause 5.1A.3, the MME could calculate the effective
load of each PGW -- i.e. PGW\'s load considering the current load and its DNS
weight -- as follows:
PGWx-effective-available-load = (100 -- PGWx-load-metric)% X PGWx\'s-weight-
factor.
Now, assuming that all the PGWs are equally loaded and their load-metric=10%,
PGW1-effective-available-load = (100 -- 10)% X 20 = 18.
PGW2-effective-available-load = (100 -- 10)% X 20 = 18.
PGW3-effective-available-load = (100 -- 10)% X 60 = 54.
Using the above the MME will calculate the PGW-relative-weight as follows:
PGWx-relative-available-load = (PGWx-effective-available-load / sum-of-all-
the-PGWs-effective-available-load) * 100.
Accordingly,
PGW1-relative-available-load = (18 / (18+18+54)) * 100 = 20%
PGW2-relative-available-load = (18 / (18+18+54)) * 100 = 20%
PGW3-relative-available-load = (54 / (18+18+54)) * 100 = 60%
Based on the above, the MME will select PGW1 for 20% of the new session
requests, PGW2 for 20% of the new session requests, PGW3 for 60% of the new
session requests. This results in exactly same selection logic as described in
\"Session Distribution\" of clause 5.1A.4.1. Hence without repeating all the
calculations again, it is clear that the distribution of 100 new session
requests for APN1, without considering the APN load control, would result in
the same set of issues as highlighted in \"Analysis of the network load\" of
clause 5.1A.4.1.
### 5.1A.5 Information needed for APN load control
APN load control is useful only when the PGW is configured with a resource
limit for one or more of its APN(s). Otherwise, when all the resources of the
PGW are available for all the APNs served by that PGW, the node level load
information is exactly the same as APN level load information, for each of its
APNs, and hence performing the node load control is sufficient. When the APN
load control is required, the PGW should advertise the related APN load
information. For allowing the MME to perform the APN load control effectively,
following information are required to be advertised by the PGW, as part of APN
load information:
**APN** : The APN for which the PGW wants to advertise the load information.
**APN-Load-Metric** : It indicates the current resource utilization for a
particular APN, in percentage, as compared to the total resources reserved for
that APN at the target PGW. Its computation is implementation dependant and it
has same characteristics as \"Load Metric\" described in clause 5.2.2.1.2.1,
when applied at APN level.
**APN-res-limit** : It indicates the total resources configured for a given
APN as compared to the total resources available at the target PGW, in
percentage. It is a static limit and does not change unless the configured
limit for the resources reserved for the APN changes. Using APN-res-limit and
the DNS weight-factor of the given PGW, the MME can judge the PGW\'s APN
related resources as compared other PGWs in the network, e.g. for the example
use case given in clause 5.1A.3, the PGW\'s APN-effective-res-limit can be
calculated by multiplying the APN-res-limit and DNS-weight-factor of each APN:
PGW1-APN1-effective-res-limit = 50% X 20 = 10.
PGW2-APN1-effective-res-limit = 20% X 20 = 4.
PGW3-APN1-effective-res-limit = 10% X 60 = 6.
Thus, based on APN-effective-res-limit it can conclude that the PGW1 has
highest APN1 related resources reserved as compared to the other PGWs in the
network. And hence the MME can use this information to favour PGW1 over other
PGWs for APN1 related new session requests.
### 5.1A.6 Impacts on node level load control
For the example use case given in clause 5.1A.3, assuming that the current
APN1-load-metric=0% for each of the PGWs, based on the clause 5.4.2.2, the
APN1-relative-available-load can be calculated for each of the PGW as follows:
PGW1-APN1-effective-available-load = (100 -- 0)% X 50% X 20 = 10
PGW2-APN1-effective-available-load = (100 -- 0)% X 20% X 20 = 4
PGW3-APN1-effective-available-load = (100 -- 0)% X 10% X 60 = 6
Hence,
PGW1-APN1-relative-available-load = 10 / (10 + 4 + 6) = 50%
PGW2-APN1-relative-available-load = 4 / (10 + 4 + 6) = 20%
PGW3-APN1-relative-available-load = 6 / (10 + 4 + 6) = 30%
This means, for 100 new session requests for APN1, PGW1 will be selected for
50 sessions, PGW2 will be selected for 20 sessions and PGW3 will be selected
for 30 sessions. Based on this, the node level load metric and APN1 level load
metric for each of the PGWs can be calculated as follows:
PGW1-load-metric = 50 / 200 = 25%
PGW2-load-metric = 20 / 200 = 10%
PGW3-load-metric = 30 / 600 = 5%
And,
PGW1-APN1-load-metric = 50 / 100 = 50%
PGW2-APN1-load-metric = 20 / 40 = 50%
PGW3-APN1-load-metric = 30 / 60 = 50%
The above indicates that the PGWs are perfectly balanced at APN1 level, i.e.
their APN1-load-metric is exactly the same and hence APN1 resources are
equally utilized at each PGWs, while they are unevenly balanced from node
point of view, i.e. their node level load-metric is very different from each
other and hence their node level resources are not utilized equally. However,
in this network, total resources configured for APN1 are sum-of-
APN1-resources-of-each-PGWs = 100 + 40 + 60 = 200. While the total resources
available = 200 + 200 + 600 = 1000. So, only 20% of the resources (i.e. 200
out of 1000 sessions) are reserved for APN1 while the remaining resources,
i.e. 80%, are reserved for other APNs, i.e. APNx. Moreover, 4 times the
resources are reserved for APNx as compared to APN1 highlighting the fact that
higher number of sessions for APNx is expected as compared to APN1. So
considering the proportion of the resources 20:80 between APN1:APNx, if the
network has received 100 sessions for APN1, then it is expected to receive 400
sessions for APNx. Based on APN1\'s resource limit, APNx\'s resource limit can
be calculated as below:
APNx-res-limit = 100 -- APN1-res-limit.
Hence,
PGW1-APNx-res-limit = 100 -- 50 = 50%
PGW2-APNx-res-limit = 100 -- 20 = 80%
PGW3-APNx-res-limit = 100 -- 10 = 90%
Correspondingly, assuming APNx-load-metric=0% for each PGWs, the effective-
available-load and relative-available-load can be calculated as below:
PGW1-APNx-effective-available-load = (100 -- 0)% X 50% X 20 = 10
PGW2-APNx-effective-available-load = (100 -- 0)% X 80% X 20 = 16
PGW3-APNx-effective-available-load = (100 -- 0)% X 90% X 60 = 54
Hence,
PGW1-APNx-relative-available-load = 10 / (10 + 16 + 54) = 12.5%
PGW2-APNx-relative-available-load = 16 / (10 + 16 + 54) = 20%
PGW3-APNx-relative-available-load = 54 / (10 + 16 + 54) = 67.5%
This means, for 400 new session requests for APNx, PGW1 will be selected for
50 sessions, PGW2 will be selected for 80 sessions and PGW3 will be selected
for 270 sessions. Based on this, the node level load metric for each of the
PGWs can be calculated as follows:
PGW1-load-metric = (APN1-sessions + APNx-sessions) / (Total sessions capacity)
= (50 + 50) / 200 = 50%
PGW2-load-metric = (APN1-sessions + APNx-sessions) / (Total sessions capacity)
= (20 + 80) / 200 = 50%
PGW3-load-metric = (APN1-sessions + APNx-sessions) / (Total sessions capacity)
= (30 + 270) / 600 = 50%
The above indicates that the PGWs are perfectly balanced at the node level,
i.e. their node-load-metric is exactly the same and hence resources are
equally utilized at each of the PGWs.
In summary, it can be observed that for the given APN the selection logic will
favour the PGW which has higher resource reservation for that APN, i.e. for
APN1 PGW1 is preferred over other PGWs since it has higher resources reserved
for APN1. Since PGW1 has higher resources reserved for APN1, it has lower
resources available for other APNs as compared to the other PGWs. And hence
the selection logic automatically ensures that for the sessions related to
other APNs, PGW1 is least preferred as compared to other PGWs in the network.
And hence when the mix of APN1:APNx sessions are equal to the proportion of
the resources reserved for APN1:APNx, the selection logic ensures that the
network remains well balanced at the node level as well as the APN level.
Thus, no special consideration for node level load control is needed when APN
load control is deployed in the network.
### 5.1A.7 Conclusion
With APN level load control feature an evenly balanced network with the APN
level granularity can be realized, besides an evenly balanced network with the
node level granularity. However, the support of this feature is useful in the
network where the pre-condition as specified in clause 5.1A.2 is applicable,
hence CT4 concluded to define the support of the APN level load control
feature as an optional feature to support, as a part of the support of the
GTP-C load control mechanism. Additionally, CT4 concluded to recommend
activating the support of this feature only when all the nodes in the network
support this feature. This will ensure that the node and network resources are
not wasted since the information related to this feature is generated and
provided by the sender and utilized by the receiver only when the feature
support is activated. If the receiver, not supporting this feature, receives
parameters related to this feature, it will ignore the same. The corresponding
behaviour shall be specified in 3GPP release 12.
Additionally, following aspects should be considered during normative work.
Table 5.1A.7: Aspects to be considered during further normative work
* * *
Subclause Comments  
5.1A.2 Requirements Text from corresponding subclause for normative work.
5.1A.5 Description of the information needed for APN load control Text from
corresponding subclause for normative work. 5.1A.4 Issues when APN load
control is not used Potential text from the corresponding subclause for
informative work. 5.1A.6 Impacts on node level load control Potential text
from the corresponding subclause for informative work.
* * *
## 5.2 Definition
The \"Load Control Information\" provides a set of parameters representing the
load condition of the sender. In turn, these parameters provide the assistance
to the receiver to perform the node selection such that an evenly balanced
network is realized. Each alternative below provides a complete definition of
the \"Load Control Information\" with a set of the applicable parameters.
### 5.2.1 Requirements
The definition of the \"Load Control Information\" should be compliant with
the following requirements.
  * The granularity of the load level indicated via \"Load Control Information\" should be fine enough to allow for fine load balancing across the network nodes.
  * Various parameters should be defined clearly (e.g. the intended use at the receiver) to ensure common interpretation and inter-operability between GTP-C nodes in a multi-vendor network environment.
  * For each parameter the applicable source and the consumer node(s) should be clearly identified, e.g. if the parameter is applicable to SGW or not.
  * Optionality of the parameter(s), wherever applicable, should be clearly identified. The sender may include it and the receiver, not supporting the same, may ignore it.
  * The definition should be extendable in future, if needed. In other words, it should be possible to add more parameter(s) under this information in future releases, if required, while ensuring the compatibility with the older releases.
  * For the forward compatibility reason, the behaviour of a node on reception of an unsupported optional parameter(s) should be clearly defined.
  * There shall be clear indication allowing the node to associate the received load control information with the identity of the node originating it.
  * For the requirements identified in clause 5.1A.2, it shall be possible to signal whether the load control information applies to the node or specific APNs:
NOTE 1: Stage 2 needs to be aligned to reflect that the APN level load
information may be advertised by the PGW when the APN level load control
feature is supported and activated in the network.
### 5.2.2 Alternative 1 - New Load Control Information IE piggybacked in
existing GTP-C signalling
#### 5.2.2.1 Description
##### 5.2.2.1.1 General
Load Control Information allows the node to advertise its session related load
information. In turn, it allows the receiving node to use this information
during the node selection function to achieve evenly balanced network.
It is proposed to define a new Load Control Information (LCI) IE that is
piggybacked on existing GTP-C messages and existing signalling, under the
principles that:
\- the computation and transfer of the Load control Information shall not add
significant additional load to the node itself and to its corresponding peer
nodes;
\- The inclusion of Load control information in existing messages means that
the frequency increases as the session load increases, allowing faster
feedback and thus better regulation.
Within a message, one or multiple instances of the \"Load Control
Information\" may be included by the same node. When multiple \"Load Control
Information\" instances are included by the sender, each of them provides the
information about the identity of the sending node and may provide additional
information about the load condition.
Below is a representative definition of \"Load Control Information\". The
actual format/encoding may differ from the below representation, however it
should ensure all the elements mentioned in the definition below.
Load Control Information := \ /* Identifies if the sender is PGW
or SGW */
\
\
...
\
The applicable parameters are further defined below. One or more parameters
may be included within the same instance of the \"Load Control Information\",
as depicted above. When multiple parameters are included within the same
instance of the \"Load Control Information\", the receiver shall consider all
the parameters in conjunction while using this information for the node
selection, e.g. if parameters P1, P2 and P3 are included within the given
\"Load Control Information\" then it represents the load condition when P1 and
P2 and P3, all of them, are valid/applicable. When more than one instance of
the \"Load Control Information\" are included, the receiver shall consider the
parameters included in each instance independently, while using this
information for the node selection, e.g. if one instance of \"Load Control
Information\" includes parameter P1 and the other instance includes parameter
P2 then the receiver shall use it for node selection when P1 is
valid/applicable separately, then when P2 is valid/applicable.
Each parameter will be evaluated against the requirements and its usefulness
towards achieving an evenly balanced network. Finally, a set of parameters
will be identified as part of this alternative.
In the first release introducing the support of the Load Control mechanism, a
set of the parameters will be defined as mandatory parameters to support the
Load Control mechanism.
In future releases, when new parameters are defined, they will be categorized
as:
  * Non-critical optional parameters -- the support of these parameters are _not critical_ for the receiver. The receiver can successfully and correctly comprehend the Load Control Information instance, containing one or more of these parameters, by using the other parameters and ignoring the non-critical optional parameter.
  * Critical optional parameters -- the support of these parameters are _critical_ for the receiver to correctly comprehend the instance of the Load Control Information containing one or more of these parameters.
The sender can include one or more non-critical optional parameter within any
instance of Load Control Information without having the knowledge of the
receiver\'s capability to support the same. However, the sender shall only
include one or more critical optional parameter in any instance of Load
Control Information towards a receiver if the corresponding receiver is known
to support of these parameters. The sender may be aware of this either via
signalling methods or by configuration.
##### 5.2.2.1.2 Parameters
###### 5.2.2.1.2.1 Load Metric
The Load Metric parameter contains the information regarding the current load
level of the originating node. The computation of the Load Metric is left to
the implementation. The node may consider various aspects such as the used
capacity of the node based on activated bearers in relationship to maximum
number of bearers the node can handle, the load that these active bearers
produce in the node (e.g. memory/CPU usage in relationship to the total
memory/CPU available, etc.).
The Load Metric represents the current load level of the sending node in
percentage with the range of 0-100. Where 0 means no or 0% load and 100 means
maximum or 100% load reached (no further load is desirable).
The support of the Load Metric is mandatory for the GTP-C node supporting
GTP-C overload control mechanism. The Load Metric shall always be included in
the \"Load Control Information\".
The range of Load Metric, i.e. 0 to 100, does not mandate the sender to
collect its own load information at every increment/decrement and hence to
advertise the change of Load Metric with granularity 1. Based on various
implementation specific parameters, such as the architecture, session and
signalling capacity, the current load and so on, the sender is free to define
its own logic and periodicity with which its own load information is
collected.
Besides, considering the processing requirement of the receiver of the Load
Control Information, e.g. handling of the new information, tuning the node
selection algorithm to take the new information into account, the sender
should refrain from advertising every small variation, e.g. with the
granularity of 1 or 2, in the Load Metric which does not result in useful
improvement in node selection logic at the receiver. During the typical
operating condition of the sender, a larger variation in the Load Metric, e.g.
5 or more unit, should be considered as reasonable enough for advertising a
new Load Control Information and thus justifying the processing requirement
(to handle the new information) of the receiver.
###### 5.2.2.1.2.2 Load Control sequence number
The GTP-C protocol requires retransmitted messages to have the same contents
as the original message (see sub clause 7.6 of 3GPP TS 29.274 [6]). Due to
GTP-C (re)transmissions, the load control information received by a GTP-C
entity at a given time may be less recent than load control information
already received from the same GTP-C entity. The Load Control Sequence number
helps in sequencing the load control information received from a GTP-C entity.
The Load Control Sequence number contains a value that indicates the sequence
number associated with the Load Control Information IE. This sequence number
is used to differentiate two Load Control Information IEs generated at two
different instants by the same GTP-C entity. The Load Control Sequence number
is a mandatory parameter to be supported (when supporting GTP-C load control)
and shall always be present in the Load Control Information IE. It is
applicable to all nodes / GTP-C interfaces for which GTP-C load control is
defined.
The sender of this information shall increment the Load Control Sequence
number associated to a particular load control information whenever modifying
some information in the Load-Control-Information IE. The Load Control Sequence
number shall not be incremented otherwise.
This parameter shall be used by the receiver of the Load Control Information
IE to properly collate out-of-order GTP-C messages e.g. due to GTP-C
retransmissions. This parameter may also be used by the receiver of the Load
Control Information IE to determine whether the newly received load
information has changed compared to load information previously received from
the same node for the same load control scope earlier.
If the receiving entity has already received and stored load control
information from the peer GTP-C entity, the receiving entity shall update its
load control information only if the Load Control Sequence number received in
the new load control information is larger than the stored value of the Load
Control Sequence number associated with the peer GTP-C entity for the given
scope. However due to overflow the Load Control Sequence number may be reset
to an appropriate base value by the peer GTP-C node. And hence the receiving
entity should be prepared to receive a Load Control Sequence number parameter
whose value is less than the previous value.
NOTE 1: the GTP-C sequence number cannot be used for collating out-of-order
load control information as e.g. load control information may be sent in both
GTP-C requests and responses, using independent sequence numbering.
###### 5.2.2.1.2.3 List-of-APN_&_ResourceLimit
The List-of_APN_&_ResourceLimit contains list of the tuple (APN,
ResourceLimit) and indicates one or more APNs for which the Load Control
Information is applicable. The \"APN\" contains the name of the APN and the
ResourceLimit corresponds to the \"APN-res-limit\" as described in clause
5.1A.5 for the corresponding APN. When present in the Load Control Information
IE, the scope of the load information is the list of APNs for the PGW that
sends the load information. And in that case the \"Load Metric\" should be
interpreted as \"APN-Load-Metric\" as described in clause 5.1A.5. Only one
instance of List-Of-APN_&_ResourceLimit can be included within one Load
Control Information instance.
NOTE 1: The maximum number of (APN, ResourceLimit) in the List-of-
APN_&_ResourceLimit is set to 10. More than 10 occurrences of (APN,
ResourceLimit), within one single instance of List-of-APN_&_ResourceLimit will
be treated as protocol error by the receiver.
If the List-of-APN_&_ResourceLimit has not been transmitted, the scope of the
Load Control Information is the entire PGW node (unless restricted by other
parameters in the Load Control Information). This is an optional parameter to
support (when supporting GTP-C load control mechanism) and its support depends
upon the support of the APN level load control feature. In a network, where
the APN level load control feature is deemed useful, e.g. based on the pre-
condition specified in clause 5.1A.1, the operator may activate the support of
this feature at the PGW when the other nodes of the PLMN, i.e. SGW, MME/SGSN,
ePDG, TWAN, also support this feature. In that case, the PGW may include this
parameter in the Load Control Information IE (depending on the scope of the
reported load control information). The receiver shall handle this parameter
if it supports APN load control feature and if this parameter is received.
The PGW may signal a Load Control Information including this parameter when it
is handling multiple APNs and when the resource limit is configured for one or
more APNs, e.g. when the static session capacity, different than the entire
node\'s sessions capacity, is configured for one or more APNs and/or dynamic
resource utilization of each APN is different. In general, when the APN level
load control feature is activated in the network the PGW should include this
parameter when the inclusion of the same can result in better node selection
and even distribution of the sessions with the APN level granularity.
This parameter can be provided by the PGW only and it is used by the node
performing node selection only (e.g. MME/SGSN).
Maximum number of APNs, for which the PGW is allowed to advertise the Load
Control Information, is limited to 10. In other words, the maximum number of
occurrences of (APN, ResourceLimit), within and across various instances of
the List-of-APN_&_ResourceLimit, is limited to 10 for a given PGW. Hence, if
the PGW supports more than 10 APNs, it can advertise the load control only for
10 of the most important APNs. In future, if needed, this limit may be
increased to allow the PGW to advertise the load information for more number
of APNs. In that case, the receiver not supporting the higher limit, will only
handle the first 10 APNs and ignore the load information for the remaining
APNs.
NOTE 2: Considering various aspects such as, the processing and storage
requirements at the overloaded node and the receiver, numbers of important
APNs for which load control advertisement will be necessary, interoperability
between the nodes of various vendors, etc. it was decided to define a limit on
maximum number of APNs for advertising the load control information. In this
release, it was decided to fix this limit to 10 while also ensuring that the
mechanism exists to extend this limit in future releases, if required.
The PGW shall always provide the node level load information by providing one
instance of the Load Control Information without List-of-APN_&_ResourceLimit
parameter.
NOTE 3: The PGW will encode APN level load information and node level load
information differently so that the receiver will ignore the APN level load
information if it does not support APN level load control feature.
If this parameter is not received for a given APN but has been received for
the other APN(s) from a PGW, then for this given APN, the node performing the
node selection calculates the load metric as described in detail in clause
5.4.2.2 for the target PGW. If the node level load information as well as APN
level load information are received from a PGW (i.e. if multiple instances of
Load Control Information, one without List-of-APN_&_ResourceLimit and others
with List-of-APN_&_ResourceLimit, is received from a PGW), while performing
the node selection for a given APN, the source node applies the APN level node
information
##### 5.2.2.1.3 Parameter Evaluation
The conclusion regarding the support and inclusion of the parameters, within
Load Control Information, is summarized in the table below.
The conclusion regarding the support and inclusion of the parameters, within
Load Control Information, is summarized in the table below.
Table 5.2.2.1.3-1: Support of parameters as part of the support of the Load
Control Information
+-------------+-------------+-------------+-------------+-------------+ | Parameter | Support by | Support by | Inclusion | Handling by | | | the sender | the | by the | the | | | | receiver | sender | receiver | +-------------+-------------+-------------+-------------+-------------+ | Load Metric | Mandatory | Mandatory | Mandatory | Mandatory | | (as defined | | | | | | in clause | | | | | | 5 | | | | | | .2.2.1.2.1) | | | | | +-------------+-------------+-------------+-------------+-------------+ | Load | Mandatory | Mandatory | Mandatory | Mandatory | | Control | | | | | | sequence | | | | | | number (as | | | | | | defined in | | | | | | clause | | | | | | 5 | | | | | | .2.2.1.2.2) | | | | | +-------------+-------------+-------------+-------------+-------------+ | List-of | Optional | Optional | Optional | Mandatory | | -APN_&_Re | | | | (when | | sourceLimit | (NOTE 1) | (NOTE 1) | (NOTE 2) | provided by | | | | | | the sender | | | | | | and the APN | | | | | | load | | | | | | control | | | | | | feature is | | | | | | supported | | | | | | by the | | | | | | receiver) | | | | | | | | | | | | (NOTE 2) | +-------------+-------------+-------------+-------------+-------------+
NOTE 1: This is an optional parameter to support and it depends upon the
support of the APN level load control feature.
NOTE 2: The sender can include this parameter while providing APN level load
information, if the APN level load control feature is supported and enabled.
If this parameter is received, the receiver supporting the APN load control
feature is required to handle and process the same.
### 5.2.3 Conclusion
It is concluded that the definition of the Load Control Information IE, as
described in clause 5.2.2.1.1, and the parameters, as mentioned in clause
5.2.2.1.3, shall be specified in 3GPP Release 12.
Additionally, following aspects should be considered during normative work.
Table 5.2.3-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
5.2.2.1.1 General description of the definition Load Control Information Text
from corresponding subclause for normative work. 5.2.2.1.2.1 Load Metric Text
from corresponding subclause for normative work. 5.2.2.1.2.2 Load Control
sequence number Text from corresponding subclause for normative work.
5.2.2.1.2.3 List-of-APN_&_ResourceLimit Text from corresponding subclause for
normative work. 5.2.1 Requirements for the definition of Load Control
Information Some of the requirements, which are useful for the future
extendibility of the Load Control Information IE, may be considered for
normative work.
* * *
## 5.3 Frequency of inclusion
### 5.3.1 Requirements
This sub clause aims at defining how often/frequently the \"Load Control
Information\" should be transferred, while ensuring the following
requirements:
\- The transfer of the load Information shall not add significant additional
load to each peer node.
\- The calculation of load Information should not severely impact the resource
utilization of the node.
### 5.3.2 Alternative 1 -- Inclusion by piggybacking only when the new/changed
value has not been provided to a peer
#### 5.3.2.1 Description
In this alternative, the node includes the Load Control Information only when
the new/changed value has not already been indicated to the peer node, i.e.
when there is a change in the load condition or if the load information is not
already indicated to the peer node, the sender includes the Load Control
Information by piggybacking it in the very first message (which can carry the
Load Control Information) sent to the peer node. In the subsequent messages,
which can carry the Load Control Information, sent to the same peer, the
sender node does not include the Load Control Information, unless the
information has changed. The receiver shall continue to use the earlier
received Load Control Information until new information is received.
NOTE: The change of load condition simply refers to change of one or more
parameter(s) within Load Control Information such that the sender decides to
update the information at the peer node.
This alternative requires the sender to remember if it has already sent the
Load Control Information to a given peer or not. Additionally, the sender also
needs to remember what value was sent to a peer node so as to decide if the
new value representing the load condition is same or a different from that
peer node point of view, e.g. if Load Metric within Load Control Information
changes from 10 to 15, the sender shall include it to a peer node to which 15
was not advertised earlier. However, to a peer node if 15 was advertised
earlier and 10 was never advertised, (e.g. there was no messaging towards that
peer node while the Load Metric changed from 15 to 10) there is no need to
advertised 15 again.
#### 5.3.2.2 Using a subset of the applicable messages
In this variant of this alternative, the sender only uses subset of the
applicable messages for propagating the Load Control Information. Out of all
the applicable messages, which can carry the Load Control Information, the
sender defines its own subset of messages and applies the principles described
in the clause 5.3.2.1. The intention of using a subset of messages is to avoid
sending of the Load Control Information in some of the messages which are sent
less frequently.
Since the subset of the messages used by a sender is specific to that sender
and not known to the receiver and also since this subset may differ between
two different senders, the receiver shall support the handling of the Load
Control Information in all the applicable messages, which can carry Load
Control Information.
#### 5.3.2.3 Advantages
Following are the advantages of this alternative:
  * Unnecessary information in the message is avoided. The Load Control Information is included only when it has changed from a peer node perspective and hence the peer node does not receive redundant information in any message.
\- Less overhead in the message and less message processing requirement at the
receiver.
#### 5.3.2.4 Drawbacks
Following are the drawbacks of this alternative:
  * The sender has to remember if the Load Control Information was sent and its value towards each peer node, for every peer node.
  * Extra processing and storage requirement on the sender node.
  * The sender has to accurately know the identity of the currently serving (remote) peer node.
### 5.3.3 Alternative 2 -- Inclusion by piggybacking in every message towards
a peer
#### 5.3.3.1 Description
In this alternative, the node includes the Load Control Information in each
and every message, which can carry the Load Control Information, sent to a
peer node by piggybacking it over the existing messages. The sender does not
need to remember what value of the Load Control Information was already sent
towards a peer node. And hence, the peer node may receive the same information
from a sender in multiple messages.
The receiver shall continue to use the earlier received Load Control
Information until new information is received.
NOTE: This is required if the sender does not include the information in every
message (see clause 5.3.3.2) and for forward compatibility if the load
information is extended to additional GTP-C messages in future (e.g. to a
message supported by both the sender and the receiver, but with a sender
implementing an earlier version of the specification and not including the new
load control information in the message).
#### 5.3.3.2 Using a subset of the applicable messages
In this variant of this alternative, the sender only uses subset of the
applicable messages for propagating the Load Control Information. Out of all
the applicable messages, which can carry the Load Control Information, the
sender defines its own subset of messages and applies the principles described
in the clause 5.3.3.1. The intention of using a subset of messages is to avoid
sending of the Load Control Information in some of the messages which are sent
less frequently. The receiver shall continue to use the earlier received Load
Control Information until new information is received.
Since the subset of the messages used by a sender is specific to that sender
and not known to the receiver and also since this subset may differ between
two different senders, the receiver shall support the handling of the Load
Control Information in all the applicable messages, which can carry Load
Control Information.
#### 5.3.3.3 Advantages
Following are the advantages of this alternative:
  * The sender need not remember if the Load Control Information was sent and its value towards any peer node. Thus, less processing and storage requirement on the sender node.
\- Simple to implement at the sender node.
#### 5.3.3.4 Drawbacks
Following are the drawbacks of this alternative:
  * Potentially redundant information will be included in multiple messages.
\- Extra processing at the receiver to compare and discard the new information
if it is same as the old information received from the same node.
NOTE: A parameter within Load Control Information, representing the
newness/freshness of the information (e.g. Load-control-sequence-number as
defined in clause 5.2.2.1.2.2), may help to address the above drawback to some
extent, e.g. the receiver may simply discard the Load Control Information if
the new value of the Load- control-sequence-number is same as earlier received
from the same peer.
### 5.3.4 Alternative 3 -- Hybrid of Alternative 1 and Alternative 2
#### 5.3.4.1 Description
In this alternative, the sender follows the principles of the Alternative 1
towards some peers, i.e. the sender includes the Load Control Information only
when the new/changed value has not already been indicated to the peer node,
while the sender follows the principles of Alternative 2 for the other peers,
i.e. the sender includes the Load Control Information in each and every
message (or a subset of the same), which can carry the Load Control
Information, sent to a peer node by piggybacking the information over the
existing messages. Alternative 1 may be easy to implement in a smaller network
with few nodes or towards the peer nodes of the same PLMN (towards which the
amount of traffic is very high). Alternative 2 may be easy to implement in a
larger network or towards the peer nodes of a different PLMN. Thus, the sender
can leverage the benefits of Alternative 1 (specifically, avoiding of the
redundant information in each and every message) towards some peers and
Alternative 2 (specifically, not remembering what value of overload
information was sent to which peer) towards the other peers.
The receiver shall support the handling of the Load Control Information in all
the applicable messages, which can carry the Load Control Information. The
receiver shall continue to use the earlier received Load Control Information
until the new information is received from the same peer.
#### 5.3.4.2 Advantages
Following are the advantages of this alternative:
  * All the advantages of Alternative 1 (clause 5.3.2.3) when Alternative 1 is followed towards a given peer. All the advantages of Alternative 2 (clause 5.3.3.3) when Alternative 2 is towards a given peer.
#### 5.3.4.3 Drawbacks
Following are the drawbacks of this alternative:
  * All the drawbacks of Alternative 1 (clause 5.3.2.4) when Alternative 1 is followed towards a given peer. All the drawbacks of Alternative 2 (clause 5.3.3.4) when Alternative 2 is towards a given peer.
### 5.3.5 Alternative 4 -- Inclusion by piggybacking the load control
information periodically
#### 5.3.5.1 Description
In this alternative, the sender includes the Load Control Information in GTP-C
signalling periodically. E.g. the sender includes the Load Control Information
in all the messages during a first period (aimed at distributing the
information to all peers) and then ceases to do so for a second period. The
sender does not need to remember whether and what value of the Load Control
Information was already sent towards a peer node. The sender shall include the
information often enough to ensure that all peers receive the information at
an acceptable pace. The duration of the period may be adaptive and function of
the load level e.g. the node may include the information more frequently when
the load increases to ensure a faster reaction from the peers.
NOTE: The period at which to include the Load Control Information may depend
e.g. on the period with which the sender re-assesses its load level, the
capacity of the sending node (a node with a smaller capacity can experience
faster variations of its load level than a node with a larger capacity), the
number of peer nodes.
The peer node may receive the same information from a sender in multiple
messages. The receiver shall continue to use the earlier received Load Control
Information until the new information is received from the same peer.
#### 5.3.5.2 Advantages
Following are the advantages of this alternative:
  * the sender need not remember if the Load Control Information was sent and its value towards any peer node. Thus, less processing and storage requirement on the sender node;
  * simple to implement at the sender node;
  * less overhead in the message and less message processing requirement at the receiver than if the Load Control Information is included in every message.
#### 5.3.5.3 Drawbacks
Following are the drawbacks of this alternative:
  * the sender has to remember when it has sent the information the last time to decide when to send it again;
  * potentially redundant information will be included in multiple messages;
  * extra processing at the receiver to compare and discard the new information if it is same as the old information received from the same node;
NOTE: A parameter within Load Control Information, representing the
newness/freshness of the information (e.g. Overload-Sequence-Number as defined
in clause 6.2.2.1.2.3), may help to address the above drawback to some extent,
e.g. the receiver may simply discard the Load Control Information if the new
value of the Load-Sequence-Number is same as earlier received from the same
peer.
  * the periods at which to include the Load Control Information needs to be carefully defined, e.g. if the first period is too short, there is a risk of not providing the load information towards some peers with which there is no messaging within that period; if the second period is too short; this minimizes the gains vs. including the information in every message;
\- between two periods if the load information changes then it cannot be
advertised and hence it may result in delay in advertising of the latest
information. This in turn it may cause delay in applying the information at
the receiver for load balancing.
### 5.3.6 Conclusion
How often/when the sender includes the load information is implementation
specific, e.g. the sender can include the load information based on any of the
alternatives provided in section 5.3. It is the responsibility of the sender
to ensure that any new/updated load information is propagated to the target
receivers with an acceptable delay, such that the purpose of the information
(i.e. effective load control) is achieved.
The receiver shall be prepared to receive the information in any of the GTP-C
messages extended with a Load Control Information IE and upon such reception,
shall be able to act upon the received information.
Table 5.3.6-1 identifies the messages which will be extended with a Load
Control Information IE, from
\- the PGW to the MME/S4-SGSN via the SGW;
\- the SGW to the MME/S4-SGSN;
\- the PGW to the ePDG/TWAN.
The sender may include the load information in a subset of these messages.
Table 5.3.6-1: GTP-C messages extended with a Load Control Information IE
+------------------+------------------+------------------+----------+ | Message | Direction of | Inclusion of | Comments | | | messages | load information | | +------------------+------------------+------------------+----------+ | Create Session | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Create Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Modify Bearer | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> ePDG | | | +------------------+------------------+------------------+----------+ | Bearer Resource | PGW -> SGW, | No | Note 3 | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Modify Bearer | PGW -> SGW, | No | Note 3 | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete Bearer | PGW -> SGW, | No | Note 3 | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete Session | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Update Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Downlink Data | SGW -> | Yes | | | Notification | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Modify Access | SGW -> | Yes | | | Bearers Response | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Change | PGW -> SGW, | No | Note 1 | | Notification | | | | | Response | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Release Access | SGW -> | Yes | | | Bearers Response | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete Indirect | SGW -> | No | Note 1 | | Data Forwarding | MME/S4-SGSN | | | | Tunnel Response | | | | +------------------+------------------+------------------+----------+ | Create Indirect | SGW -> | No | Note 1 | | Data Forwarding | MME/S4-SGSN | | | | Tunnel Response | | | | +------------------+------------------+------------------+----------+ | Stop Paging | SGW -> | No | Note 1 | | Indication | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Suspend | PGW -> SGW, | No | Note 1 | | Acknowledge | | | | | | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Resume | PGW -> SGW, | No | Note 1 | | Acknowledge | | | | | | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete PDN | PGW \ SGW, | No | Note 2 | | Connection Set | | | | | Request | SGW \ MME | | | | | | | | | | PGW \ | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete PDN | SGW \ PGW, | No | Note 2 | | Connection Set | | | | | Response | MME \ SGW | | | | | | | | | | TWAN/ePDG \ | | | | | PGW | | | +------------------+------------------+------------------+----------+ | PGW Restart | SGW -> | No | Note 2 | | Notification | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | PGW Downlink | PGW -> SGW, | No | Note 2 | | Triggering | | | | | Notification | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Note 1: | | | | | Inclusion of | | | | | load information | | | | | is set to \"No\" | | | | | because the | | | | | message is used | | | | | less frequently. | | | | | | | | | | Note 2: | | | | | Inclusion of | | | | | load information | | | | | is set to \"No\" | | | | | because the | | | | | message is only | | | | | used during | | | | | restoration | | | | | procedures. | | | | | | | | | | Note 3: | | | | | Inclusion of | | | | | load information | | | | | is set to \"No\" | | | | | since load | | | | | information does | | | | | not change very | | | | | frequently and | | | | | hence it was | | | | | decided to not | | | | | impact these | | | | | messages. | | | | +------------------+------------------+------------------+----------+
## 5.4 Interaction with existing mechanisms
### 5.4.1 General
The parameters such as weight factor of the node are an essential input to the
node selection algorithm. These parameters are either returned by the DNS or
configured locally and hence they are mostly static or pseudo-dynamic type of
information. On the other hand, the \"Load Control Information\", transferred
within GTP-C messages, provides the current value of the load level
representing the dynamic load condition of the sending node more accurately.
Hence, the node selection algorithm should take both into account, the
preference related information provided by the DNS or other mechanisms and the
dynamic load level provided by the \"Load Control Information\", to calculate
the effective load of the target node. This clause investigates the
enhancements to the node selection algorithms which take \"Load Control
Information\" and other existing parameters into account.
### 5.4.2 Information received from DNS
#### 5.4.2.1 With node level load information
The node level load information consists of the \"Load Metric\" of the target
node, i.e. PGW/SGW, representing the current utilization of the resources as
compared to overall available resources at the target node. Using the existing
methodology the node performing the node selection, i.e. MME, SGSN, ePDG, TWAN
(or termed as source node, here), prepares the candidate list of the target
nodes satisfying the required criteria to serve the new session request. Lets
call the list \"TNodeList\", i.e. list of candidate target nodes \"TNode\".
Then, the source node uses the \"Load Metric\" and DNS-weight-factor of each
of TNode to perform node selection as given below.
**Effective-available-load:**
Effective-available-load of the target node is calculated considering the
current available load and the DNS weight-factor of the target node as
follows:
TNode-effective-available-load = (100 -- TNode-load-metric)% X TNode-weight-
factor
**Relative-available-load:**
Relative-available-load represents the effective-available-load of the target
node as compared to the effective-available-load of all the target nodes in
the network and calculated as follows:
TNode-relative-available-load = (TNode-effective-available-load / sum-of-
effective-available-load-of-all-the-nodes-in-TNodeList) X 100%
**Node Selection:**
The TNode-relative-available-load of the target node indicates the fraction of
the new session request for which the corresponding node should be selected to
ensure evenly balanced network, e.g. TNode-relative-available-load value of X%
indicates that the corresponding target node should be selected for X% of the
new sessions requests.
**Example:**
For the example use case in clause 5.1A.3, let\'s consider the following
\"Load Metric\":
PGW1-load-metric = 10%
PGW2-load-metric = 20%
PGW3-load-metric= 30%
Based on the above, the effective-available-load can be calculated as below:
PGW1-effective-available-load = (100 -- 10)% X 20 = 18
PGW2-effective-available-load = (100 -- 20)% X 20 = 16
PGW3-effective-available-load = (100 -- 30)% X 60 = 42
Then the relative-available-load can be calculated as below (by rounding-off
to nearest integer value):
PGW1-relative-available-load = 18 / (18 + 16 + 42) =\~ 24%
PGW2-relative-available-load = 16 / (18 + 16 + 42) =\~ 21%
PGW3-relative-available-load = 42 / (18 + 16 + 42) =\~ 55%
Out of the total new session requests, the PGW1 should be selected for 24%,
PGW2 should be selected for 21% and PGW3 should be selected for 55%, e.g. for
100 new sessions requests, the PGW1 should be selected for 24 sessions, PGW2
should be selected for 21 sessions and for remaining 55 new sessions PGW3
should be selected.
When a TNode\'s node level load-metric is not available (e.g. due to the load
control feature is not supported between two PLMNs and the PGW and the MME are
from different PLMNs), the source should use load-metric=0 for the TNode for
various calculations above.
#### 5.4.2.2 With APN level load information
The APN level load information is only applicable to the PGW and hence this
clause is only applicable for the selection of the PGW. The APN level load
information consists of the \"APN Load Metric\", representing the current
utilization of the APN resources as compared to overall available APN
resources at the target node, and \"APN Resource Limit\", representing the
resources reserved for the APN as compared to overall available resources at
the target node. Using the existing methodology the node performing the node
selection, i.e. MME, SGSN, ePDG, TWAN (or termed as source node, here),
prepares the candidate list of the target nodes satisfying the required
criteria to serve the new session request for the given APN. Let\'s call the
list \"TNodeList\", i.e. list of candidate target nodes \"TNode\". Then, the
source node uses the \"APN Load Metric\", \"APN Resource Limit\" and DNS-
weight-factor of each of TNode to perform node selection as given below.
**APN-Effective-available-load:**
APN-Effective-available-load of the target node is calculated considering the
current APN load (i.e. APN-load-metric), APN-resource-limit and the DNS
weight-factor of the target node as follows:
TNode-APN-effective-available-load = (100 -- TNode-APN-load-metric)% X TNode-
APN-resource-limit X TNode-weight-factor
**APN-Relative-available-load:**
APN-Relative-available-load represents the APN-effective-available-load of the
target node as compared to the APN-effective-available-load of all the target
nodes in the network and calculated as follows:
TNode-APN-relative-available-load = (TNode-APN-effective-available-load / sum-
of-APN-effective-available-load-of-all-the-nodes-in-TNodeList) X 100%
**Node Selection:**
The TNode-relative-APN-load of the target node indicates the fraction of the
new session request of the given APN for which the corresponding node should
be selected to ensure evenly balanced network at that APN level, e.g. TNode-
relative-APN-load value of X% indicates that the corresponding target node
should be selected for X% of the new sessions requests for the given APN.
**Example:**
For the example use case in clause 5.1A.3, let\'s consider the following
\"APN1 Load Metric\":
PGW1-APN1-load-metric = 60%
PGW2-APN1-load-metric = 50%
PGW3-APN1-load-metric= 50%
And the following APN1-resource-limit:
PGW1-APN1-resource-limit = 50%
PGW2-APN1-resource-limit = 20%
PGW3-APN1-resource-limit = 10%
Based on the above, the APN1-effective-available-load can be calculated as
below:
PGW1-APN1-effective-available-load = (100 -- 60)% X 50% X 20 = 4
PGW2-APN1-effective-available-load = (100 -- 50)% X 20% X 20 = 2
PGW3-APN1-effective-available-load = (100 -- 50)% X 10% X 60 = 3
Then the APN-relative-available-load can be calculated as below (by rounding-
off to nearest integer value):
PGW1-APN1-relative-available-load = 4 / (4 + 2 + 3) =\~ 45%
PGW2-APN1-relative-available-load = 2 / (4 + 2 + 3) =\~ 22%
PGW3-APN1-relative-available-load = 3 / (4 + 2 + 3) =\~ 33%
Out of the total new session requests for APN1, the PGW1 should be selected
for 45%, PGW2 should be selected for 22% and PGW3 should be selected for 33%,
e.g. for 100 new sessions requests for APN1, the PGW1 should be selected for
45 sessions, PGW2 should be selected for 22 sessions and for remaining 33 new
sessions PGW3 should be selected.
When a PGW\'s APN level load-metric is not available (e.g. due to no APN level
resource limit configured at that PGW), and if the MME is using APN level load
control from other PGWs in the candidate list, then the MME should use APN-
load-metric=(PGW\'s)load-metric and APN-res-limit=100 for that PGW for various
calculations above.
When APN level load-metric is not received for a given APN but has been
received for the other APN(s) from a PGW then for this given APN the MME
should calculate the as described below and APN-res-limit=(100 -- sum of
[resource-limit of other APN(s)]) for the same PGW for various calculations
above.
**Calculating the load-metric when the PGW has not advertised the APN\'s load-
metric:**
Consider, APNn as the list of APNs for which the load-metric and resource-
limit is available. Consider, APNx is the APN for which load-metric is not
available. Then, the APNx\'s load-metric can be calculated as below:
PGW-load-metric X PGW-capacity = sum of [for each APNs in APNn (APN-load-
metric X PGW-capacity X APN-resource-limit / 100)] + APNx-load-metric X PGW-
capacity X (100 -- sum of [ resource-limit of each APNs in APNn) / 100
Hence,
APNx-load-metric = {PGW-load-metric -- sum of [for each APNs in APNn (APN-
load-metric X APN-resource-limit) / 100]} / {100 -- sum of [ resource-limit of
each APNs in APNn]) / 100}
For example, assuming that APN load metric for only APN1 is advertised by the
PGW, the load-metric for APNx (i.e. other APNs served by the same PGW) can be
calculated as:
PGW-load-metric = 64%
APN1-load-metric = 50% and APN1-resource-limit = 30%
APNx-load-metric = {64% - (50% X 30% / 100)} / {(100 -- 30) / 100} = {64% -
15%} / {0.7} = 70%
#### 5.4.2.3 Conclusion
It is concluded that the node responsible for the node selection function
shall use the Load Control Information along with information received from
DNS while performing the node selection function, if GTP-C load control is
supported. Clause 5.4.2.1 specifies the details on how the node level \"Load
Metric\" received within the Load Control Info shall be used together with the
DNS weight factor on top of the existing DNS procedure during a node selection
procedure. Clause 5.4.2.2 specifies the details on how the \"APN Load Metric\"
and \"APN Resource Limit\" received within the Load Control Info shall be used
together with the DNS weight factor on top of the existing DNS procedure
during a node selection procedure, if APN load control is supported. The node
performing the node selection function shall realize an implementation based
on these selection logics while also considering the other information
received from the DNS (i.e. information applicable for node selection such as
order, preference, topological closeness, nodes colocation) for the target
node per existing requirements in 3GPP TS 29.303. The corresponding behaviour
shall be specified in 3GPP Release 12.
## 5.5 Limit on maximum number of instances
### 5.5.1 Description
One or multiple instances of Load Control Info IE -- each providing load
information for a different scope -- can be included by the sender of the IE.
The receiver is required to handle all these instances, from each of the peer
node, by processing, storing and acting upon the same for the node selection.
Higher number of instances in a message would result in larger size of the
message (e.g. potentially resulting in a GTPv2 message size beyond the maximum
payload limit of the UDP protocol) and more processing overhead at the
receiver. Moreover, without any limit, there is a potential risk of a
misbehaving sender providing multiple different instances of the Load Control
Info IE (within one message as well as across different messages) driving up
the resources utilization of the network as well as the receiver and hence
causing the overload of the receiver. And so, it is necessary to limit the
maximum number of instances of the Load Control Info IE, a sender can provide,
at message level (i.e. number of different load information which can be
included in a message) as well at a node level (i.e. number of different load
information which can be provided across multiple messages by a given node).
### 5.5.2 At message level
By limiting the maximum number of instances of Load Control Info IE at message
level, we are limiting the sender\'s ability to provide different load
information within a single message. And hence keeping the need for future
extensibility in mind and allowing enough flexibility for the sender to
provide different load information within a single message (and hence allowing
for better load control in the network), it is proposed to fix the maximum
number of instances of Load Control Info IE, in a message, at 10.
### 5.5.3 At node level
#### 5.5.3.1 Alternative 1 -- Same as at message level & providing full set of
information
##### 5.5.3.1.1 Description
In this alternative, it is proposed to keep the limit of the maximum number of
instances of Load Control IE at node level same as the maximum number of
instances of Load Control Info IE at message level and the sender always
includes the full set of load information in any given message carrying the
load information towards the receiver and sets the same identity for all of
them, e.g. by using the same value of \"Load Control Sequence Number\". The
receiver overwrites the existing information of a peer with the newly received
load information (via one or multiple instances) from the same peer node (when
the new information is different than the old information), e.g. if the
receiver has stored \'X\' instances of the load information for a peer node,
it overwrites those \'X\' instances with new set of \'Y\' instances received
in a message from the same peer node; where X, Y are any integer number. For
providing new value for one or more instances, the sender includes all the
instances of the Load Control IE by providing new identity for all of them,
e.g. by using a new value of \"Load Control Sequence Number\" for all the
included instances.
##### 5.5.3.1.2 Advantages
Following are the advantages of this alternative:
\- Very easy to implement for the sender as well as the receiver. While
providing the new load information (within one or multiple Load Control Info
IE), the sender includes full set of load information and marks all of them as
a new information, e.g. by providing new and common value of \"Load Control
sequence number\" for all the instances of Load Control Info IE. When a node
receives multiple instances of Load Control Info IE in a message, it checks if
any of the instance has new value or not, e.g. by comparing the new value and
old value of the \"Load Control sequence number\" of any one of the instance.
And if that instance does not represent new information then all the instances
can be ignored assuming that no new load information is provided. Otherwise,
all the existing instances are overwritten with the new instances.
\- Easy to manage the Load Control instance identifier since it is same for
different scope provided by the same peer.
\- The receiver need not check if individual instance has changed or not since
if one instance is new then all the instances may contain new value.
##### 5.5.3.1.3 Drawbacks
Following are the drawbacks of this alternative:
\- When a sender wants to update partial set of load information, it has to
include full set of load information in the message. Thus, this alternative
may result in the inclusion of redundant information in a message.
\- The receiver has to replace all the existing instances of the given peer
with the new set of instances even when only one or more instances contain new
value and hence this cause extra processing of information at the receiver for
handling of the information which has not changed.
#### 5.5.3.2 Alternative 2 -- Higher than or same as at message level &
providing partial set of information
##### 5.5.3.2.1 Description
In this alternative, it is proposed to allow higher number for the maximum
number of instances of Load Control Info IE at node level than the maximum
number of instances of Load Control Info IE at message level and the sender
can include partial set of load information in any given message towards the
receiver, e.g. the sender includes load information for APN1 and APN2 in a one
message and for APN3 and APN4 in another message. The receiver has to ensure
to overwrite the newly received load information from a peer node with the
existing load information of the same peer node only when the scope of the new
and old information matches, e.g. if the receiver has stored load information
for APN1, APN2 for a peer node and it receives new load information for APN1,
APN3, APN4 from the same peer node, it overwrites only APN1\'s existing load
information with the new load information while it stores, additionally, new
load information for APN3 and APN4. APN2 related load information remains
unchanged.
##### 5.5.3.2.2 Advantages
Following are the advantages of this alternative:
  * The sender can provide and/or update partial set of load information in a message. Thus, this alternative avoids the inclusion of redundant information in a message. This also minimizes the processing overhead at the receiver.
##### 5.5.3.2.3 Drawbacks
Following are the drawbacks of this alternative:
  * If the sender wants to ensure that full set of load information is provided to a peer node, the sender may need to remember the exact set of partial load information which is provided to the same peer node, i.e. if load information for APN1, APN2 are provided in a given message to a peer node then the sender has to remember this and include load information for the remaining APNs (e.g. APN3, APN4) in a different message sent to the same peer node. Otherwise, the full set of load information may not be available at the given receiver.
  * The receiver has to match the scope of the newly received instance of the load information with all the existing load information of a peer node and then compare, e.g. using \"Load Control sequence number\", to decide if any of the existing instance of load information can be overwritten or not. Hence it is complex for the receiver to implement.
\- Adds some implementation complexity for the sender. The sender has to
manage the identifier for each of the scope separately and shall ensure that
the total number of different instances sent across various messages does not
exceed the maximum number of instances limit at the node level.
### 5.5.4 Conclusion
It is concluded to support the following requirements in 3GPP Release 12.
\- Maximum number of different instances of Load Control Information IE at
message level shall not exceed 10, as described in clause 5.5.2.
\- The sender shall always include the full set of Load Control Information
IE(s) -- i.e. all the instances representing the load information -- in any
given message carrying the load information towards the receiver, as described
in clause 5.5.3.1
Additionally, following aspects should be considered during normative work.
Table 5.5.4-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
5.5.1 General description of the need to have limit on maximum number of
instances of Load Control Info at message level and at node level Text from
corresponding subclause for normative work. 5.5.2 Limit on maximum number of
instances at message level Text from corresponding subclause for normative
work. 5.5.3.1 Limit on maximum number of instances at node level Text from
corresponding subclause for normative work.
* * *
# 6 Overload Control Information
## 6.1 General
In order to guarantee a common interpretation in a multi-vendor network
deployment, it is necessary to define the \"Overload Control Information\"
with enough precision such that coherent and homogeneous mitigation policies
are enforced by different nodes of the same network alleviate the congestion,
effectively. This clause investigates possible parameters and their
definitions which can be exchanged under the \"Overload Control Information\".
Thus in turn, this clause aims at defining the exact format the \"Overload
Control Information\".
## 6.2 Definition
The \"Overload Control Information\" provides a set of parameters representing
the overload condition of the sender. In turn, these parameters provide the
assistance to the receiver to apply various mitigation policies to relieve the
overload of the sender. Each alternative below provides a complete definition
of the \"Overload Control Information\" with a set of the applicable
parameters.
### 6.2.1 Requirements
The definition of the \"Overload Control Information\" should be compliant
with the following requirements.
  * The granularity of the overload indicated via \"Overload Control Information\" should be fine enough to allow for the smooth and graceful overload mitigation actions. In other words, the overload indicated shall allow for a gradual reduction and increase in the traffic such that the oscillations in load shedding are prevented and the system remains stable.
  * Various parameters should be defined clearly (e.g. the intended action at the receiver) to ensure common interpretation and inter-operability between GTP-C nodes in a multi-vendor network environment.
  * For each parameter the applicable source and the consumer node(s) should be clearly identified, e.g. if the parameter is applicable to SGW or not.
  * Optionality of the parameter(s), wherever applicable, should be clearly identified. The sender may include it and the receiver, not supporting the same, may ignore it.
  * The definition should be extendable in future, if needed. In other words, it should be possible to add more parameter(s) under this information in future releases, if required, while ensuring the compatibility with the older releases.
  * For the forward compatibility reason, the behaviour of a node on reception of an unsupported optional parameter(s) should be clearly defined.
  * There shall be clear indication allowing the node to associate the received overload control information with the identity of the overloaded node originating it.
  * It shall be possible to signal whether the overload applies to the node or to specific APNs.
  * Node receiving an overload indication shall be able to make decisions using the most recent indication from the overloaded node.
### 6.2.2 Alternative 1 - New Overload Control Information IE piggybacked in
existing GTP-C signalling
#### 6.2.2.1 Description
##### 6.2.2.1.1 General
When a GTP-C entity becomes overloaded, it needs to be able to gracefully
reduce its load, e.g. by instructing its clients to reduce sending traffic
according to its current capacity to successfully process the traffic.
It is proposed to define a new Overload Control Information (OCI) IE that is
piggybacked on existing GTP-C messages and existing signalling, under the
principles that:
\- the computation and transfer of the Overload control Information shall not
add significant additional load to the node itself and to its corresponding
peer nodes;
\- the inclusion of overload information in existing messages means that the
frequency increases with the system loading, allowing faster feedback and thus
better regulation.
When a GTP-C entity determines that the offered traffic is growing (or is
about to grow) beyond its nominal capacity, it shall signal an Overload
Control Information IE to instruct its GTP-C peers to reduce the offered load
accordingly.
GTP-C overload control is performed independently for each direction between
two GTP-C entities. GTP-C overload control may run concurrently \-- but
independently -- for each direction between two GTP-C entities.
Within a message, one or multiple instances of the \"Overload Control
Information\" may be included by the same node. When multiple \"Overload
Control Information\" instances are included by the sender, each of them
provides the information about the identity of the overloaded node and may
provide additional information about the overload condition. Correspondingly,
this will allow the receiver to apply mitigation actions which will result in
a targeted and efficient alleviation of the overload condition at the sender.
Below is a representative definition of \"Overload Control Information\". The
actual format/encoding may differ from the below representation, however it
should ensure all the elements mentioned in the definition below.
Overload Control Information:= \ /* Identifies if the sender is
e.g. PGW, SGW, MME, S4-SGSN */
Parameter 1 [Type, Length, Value]
Parameter 2 [Type, Length, Value]
Parameter 3 [Type, Length, Value]
...
Parameter N [Type, Length, Value]
The applicable parameters are further defined below. One or more parameters
may be included within the same instance of the \"Overload Control
Information\", as depicted above. When multiple parameters are included within
the same instance of the \"Overload Control Information\", the receiver shall
consider all the parameters in conjunction while applying the overload
mitigation action, e.g. if parameters P1, P2 and P3 are included within the
given \"Overload Control Information\" then the receiver shall apply the
overload control when P1 and P2 and P3, all of them, are valid/applicable.
When more than one instances of the \"Overload Control Information\" are
included, the receiver shall consider the parameters included in each
instances independently, while applying the overload mitigation action, e.g.
if one instance of \"Overload Control Information\" includes parameter P1 and
the other instance includes parameter P2 then the receiver shall apply
overload control when P1 is valid/applicable separately, then when P2 is
valid/applicable.
Each parameter will be evaluated against the requirements and its usefulness
towards achieving an smooth overload control within the network. Finally, a
set of parameters will be identified as part of this alternative.
In the first release introducing the support of the Overload Control
mechanism, a set of the parameters will be defined as mandatory parameters to
support the Overload Control mechanism.
In future releases, when new parameters are defined, they will be categorized
as:
  * Non-critical optional parameters -- the support of these parameters are _not critical_ for the receiver. The receiver can successfully and correctly comprehend the Overload Control Information instance, containing one or more of these parameters, by using the other parameters and ignoring the non-critical optional parameters.
  * Critical optional parameters -- the support of these parameters are _critical_ for the receiver to correctly comprehend the instance of the Overload Control Information containing one or more of these parameters.
The sender can include one or more non-critical optional parameter(s) within
any instance of Overload Control Information without having the knowledge of
the receiver\'s capability to support the same. However, the sender shall only
include one or more critical optional parameter(s) in any instance of Overload
Control Information towards a receiver if the corresponding receiver is known
to support these parameter(s). The sender may be aware of this either via
signalling methods or by configuration.
##### 6.2.2.1.2 Parameters
###### 6.2.2.1.2.1 Overload-Reduction-Metric
The Overload-Reduction-Metric is a value in the range of 0 to 100 (inclusive)
which indicates the percentage of traffic reduction the sender of the overload
control information requests the receiver to apply. An Overload-Reduction-
Metric of \"0\" always indicates that the node is not in overload (that is, no
overload abatement procedures need to be applied) for the indicated scope.
The range of Overload-Reduction-Metric, i.e. 0 to 100, does not mandate the
sender to collect its own overload information at every increment/decrement
and hence to advertise the change of Overload-Reduction-Metric with
granularity 1. Based on various implementation specific parameters, such as
the architecture, session and signalling capacity, the current load/overload
situation and so on, the sender is free to define its own logic and
periodicity with which its own overload information is collected.
Besides, considering the processing requirement of the receiver of the
Overload-Reduction-Metric Information, e.g. to perform overload control based
on the updated Overload-Reduction-metric, the sender should refrain from
advertising every small variation, e.g. with the granularity of 1 or 2, in the
Overload-Reduction-Metric which does not result in useful improvement for
mitigating the overload situation. During the typical operating condition of
the sender, a larger variation in the Overload-Reduction-Metric, e.g. 5 or
more unit, should be considered as reasonable enough for advertising a new
Overload-Reduction-Metric Information and thus justifying the processing
requirement (to handle the new information) of the receiver.
The computation of the exact value for this parameter is left as an
implementation choice at the sending node.
The Overload-Reduction-Metric is a mandatory parameter to support (when
supporting GTP-C overload control) and shall always be present in the Overload
Control Information IE.
It is applicable to all nodes / GTP-C interfaces for which GTP-C overload
control is defined.
The inclusion of the Overload Control Information IE signals an overload,
unless the Overload-Reduction-Metric is set to 0, which signals that the
overload condition has ceased. Conversely, the absence of the Overload Control
Information IE in a message does not mean that the overload has abated.
###### 6.2.2.1.2.2 Period-Of-Validity
The Period-Of-Validity indicates the length of time, in seconds, during which
the overload condition specified by the Overload Control Information IE is to
be considered valid (unless overridden by a subsequent Overload Control
Information IE for the same scope).
An overload condition is said \"valid\" from the time the Overload Control
Information IE is received until the next \"relevant\" Overload Control
Information IE is received from the same GTP-C entity for the same overload
scope, at which point the newly received overload control information data
prevails. The timer corresponding to the period of validity is restarted each
time a relevant Overload Control Information IE is received. When this timer
expires, the last received overload control information data shall be
considered outdated and obsolete and the overload values reset (no overload),
i.e. any associated overload condition is considered to have ceased. The
Period-Of-Validity is a mandatory parameter to support (when supporting GTP-C
overload control). The Period-Of-Validity shall be present in the Overload
Control Information IE if the Overload-Reduction-Metric is not null.
It is applicable to all nodes / GTP-C interfaces for which GTP-C overload
control is defined. The Period-Of-Validity parameter achieves the following:
\- it avoids the need for the overloaded node to include the Overload Control
Information IE in every GTP-C messages it signals to its GTP-C peers when the
overload state does not change; thus it minimizes the processing required at
the overloaded node and its GTP-C peers upon sending/receiving GTP-C
signalling;
\- it allows to reset the overload condition after some time in the GTP-C
peers having received an overload indication from the overloaded GTP-C entity,
e.g. if no signalling traffic takes place between these nodes for some time
due to overload mitigation actions. This also removes the need for the
overloaded node to remember the list of GTP-C nodes to which it has sent a
non-null overload reduction metric and to which it would subsequently need to
signal when the overload condition ceases if the Period-Of-Validity parameter
was not defined.
###### 6.2.2.1.2.3 Overload-Sequence-Number
The GTP-C protocol requires retransmitted messages to have the same contents
as the original message (see clause 7.6 of 3GPP TS 29.274 [6]). Due to GTP-C
retransmissions, the overload information received by a GTP-C entity at a
given time may be less recent than overload information already received from
the same GTP-C entity for the same overload scope. The Overload-Sequence-
Number aids in sequencing the overload information received from an overloaded
GTP-C entity. The Overload-Sequence-Number contains a value that indicates the
sequence number associated with the Overload Control Information IE. This
sequence number is used to differentiate two Overload Control Information IEs
generated at two different instants by the same GTP-C entity for the same
overload scope. The Overload-Sequence-Number is a mandatory parameter to
support (when supporting GTP-C overload control) and shall always be present
in the Overload Control Information IE. It is applicable to all nodes / GTP-C
interfaces for which GTP-C overload control is defined.
The sender of this information shall increment the Overload-Sequence-Number
associated to a particular overload scope whenever modifying some information
in the Overload-Control-Information IE. The Overload-Sequence-Number shall not
be incremented otherwise.
This parameter shall be used by the receiver of the Overload Control
Information IE to properly collate out-of-order GTP-C messages e.g. due to
GTP-C retransmissions. This parameter may also be used by the receiver of the
Overload Control Information IE to determine whether the newly received
overload information has changed compared to overload information previously
received from the same node for the same overload scope. If the newly received
Overload Control Information has the same Overload-Sequence-Number as the
previously received Overload Control Information, from the same GTP-C peer and
for the same scope, then the receiver can simply discard the newly received
Overload Control Information while continuing to apply the overload abatement
procedures as per the old value.
NOTE 1: Likewise, the timer corresponding to the period of validity is not
restarted if the newly received Overload Control Information has the same
Overload-Sequence-Number as the previously received Overload Control
Information. If the overload condition persists and the overloaded node needs
to extend the duration during which the overload information shall apply, the
sender needs to provide a new Overload Control Information with an incremented
Overload-Sequence-Number (even if the parameters within the Overload Control
Information have not changed).
If the value contained in the Overload-Sequence-Number parameter overflows
during the period in which the overload mitigation is in effect, then the
parameter shall be reset to an appropriate base value.
Due to an overflow, GTP-C entities receiving an overload indication should be
prepared to receive an Overload-Sequence-Number parameter whose value is less
than the previous value. GTP-C implementations may handle this by continuing
to perform overload control until the Period-Of-Validity related to the
previous value of Overload-Sequence-Number parameter expires.
If the receiving entity already received and stored (still valid) overload
information from the overloaded GTP-C entity for the same overload scope, the
receiving entity shall update its overload scope entry only if the Overload-
Sequence-Number received in the new overload information is larger than the
value of the Overload-Sequence-Number associated with the stored entry.
NOTE 1: this parameter is equivalent to the \"oc-seq\" parameter defined for
SIP Overload control in IETF draft-ietf-soc-overload-control-13.
NOTE 2: the GTP-C sequence number cannot be used for collating out-of-order
overload information as e.g. overload information may be sent in both GTP-C
requests and responses, using independent sequence numbering.
###### 6.2.2.1.2.4 APN-List
The APN-List indicates one or more APNs for which the Overload Control
Information is applicable. When present in the Overload Control Information
IE, the scope of the overload information is the list of APNs for the PGW that
sends the overload information. Only one instance of APN-List can be included
within one Overload Control Information instance.
NOTE 1: The maximum number of APNs in the APN-List is set to 10. More than 10
occurrences of APN within one single instance of APN-List will be treated as
protocol error by the receiver.
If the APN-List has not been transmitted, the scope of the Overload Control
Information is the entire PGW node (unless restricted by other parameters in
the Overload Control Information). The APN-List is a mandatory parameter to
support (when supporting GTP-C overload control). The APN-List may be present
or absent in the Overload Control Information IE (depending on the scope of
the reported overload control information).
This parameter can be provided by the PGW only and it is used by the MME/SGSN
only.
NOTE 2: This parameter may also be used by TWAN/ePDG if the overload control
is supported on the S2a/S2b interfaces -- see clause 4.2.4.3.3.
The PGW may signal an Overload Control Information including an APN-List when
it detects overload for certain APNs, e.g. based on shortage of internal or
external resources for an APN (IP address pool). This may also allow a PGW to
selectively throttle the traffic for certain (lower priority) APNs when
experiencing overload, based on operator policy.
NOTE 3: Interactions with the APN back-off mechanism are studied in clause
6.6.2.
Maximum number of APNs, for which the PGW is allowed to advertise the Overload
Control Information, is limited to 10. In other words, the maximum number of
occurrences of APNs within and across various instances of the APN-List, is
limited to 10 for a given PGW. Hence, if the PGW supports more than 10 APNs,
it can advertise the overload control only for 10 of the most important APNs.
In future, if needed, this limit may be increased to allow the PGW to
advertise the overload information for more number of APNs. In that case, the
receiver not supporting the higher limit, will only handle the first 10 APNs
and ignore the overload information for the remaining APNs.
NOTE 4: Considering various aspects such as, the processing and storage
requirements at the overloaded node and the receiver, numbers of important
APNs for which overload control advertisement will be necessary,
interoperability between the nodes of various vendors, etc. it was decided to
define a limit on maximum number of APNs for advertising the overload control
information. In this release, it was decided to fix this limit to 10 while
also ensuring that the mechanism exists to extend this limit in future
releases, if required.
##### 6.2.2.1.3 Parameter Evaluation
The conclusion regarding the support and inclusion of the parameters, within
Overload Control Information, is summarized in the table below.
Table 6.2.2.1.3-1: Support of parameters as part of the support of the
Overload Control Information
* * *
Parameter Support by the sender Support by the receiver Inclusion by the
sender Handling by the receiver Overload-Reduction Metric (as defined in
clause 6.2.2.1.2.1) Mandatory Mandatory Mandatory Mandatory Period-of-Validity
(as defined in clause 6.2.2.1.2.2) Mandatory Mandatory Mandatory (if Overload-
Reduction-Metric is not null) Mandatory (if provided by sender) Overload-
Sequence Number (as defined in clause 6.2.2.1.2.3) Mandatory Mandatory
Mandatory Mandatory APN-List (as defined in clause 6.2.2.1.2.4) Mandatory
Mandatory Optional Mandatory (if provided by the sender)
* * *
#### 6.2.3 Conclusion
It is concluded that the support of the definition of Overload Control
Information IE, as described in clause 6.2.2.1.1, and the parameters, as
mentioned in clause 6.2.2.1.3, shall be specified in 3GPP Release 12.
Additionally, the following aspects should be considered during normative
work.
Table 6.2.3-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
6.2.2.1.1 General description of the definition Overload Control Information
Text from corresponding subclause for normative work. 6.2.2.1.2.1 Overload-
Reduction-Metric Text from corresponding subclause for normative work.
6.2.2.1.2.2 Period-of-Validity Text from corresponding subclause for normative
work. 6.2.2.1.2.3 Overload-Sequence-Number Text from corresponding subclause
for normative work. 6.2.2.1.2.4 APN-List Text from corresponding subclause for
normative work. 6.2.1 Requirements for the definition of Load Control
Information Some of the requirements, which are useful for the future
extendibility of the Load Control Information IE, may be considered for
normative work.
* * *
## 6.3 Frequency of inclusion
### 6.3.1 Requirements
This sub clause aims at defining how often/frequently the \"Overload Control
Information\" should be transferred, while ensuring the following
requirements:
\- The transfer of the overload Information shall not add significant
additional load to each peer node.
\- The calculation of overload Information should not severely impact the
resource utilization of the node.
### 6.3.2 Alternative 1 -- Inclusion by piggybacking only when the new/changed
value has not been provided to a peer
#### 6.3.2.1 Description
In this alternative, during the overload condition the node includes the
Overload Control Information only when the new/changed value has not already
been indicated to the peer node, i.e. during the overload condition when there
is a change in the overload condition or if the overload information has not
already been indicated to the peer node, the sender includes the Overload
Control Information by piggybacking it in the very first message (which can
carry the Overload Control Information) sent to the peer node. In the
subsequent messages, which can carry the Overload Control Information, sent to
the same peer, the sender node does not include the Overload Control
Information, unless the information has changed. The receiver shall continue
to use the earlier received Overload Control Information until the old
information is valid (e.g. based on the validity period of the old
information) or until the new information is received.
NOTE: The change of overload condition simply refers to change of one or more
parameter(s) within Overload Control Information such that the sender decides
to update the information at the peer node.
This alternative requires the sender to remember if it has already sent the
Overload Control Information to a given peer or not. Additionally, the sender
also needs to remember what value was sent to a peer node so as to decide if
the new value representing the overload condition is same or a different from
that peer node point of view, e.g. if Overload Metric within Overload Control
Information changes from 10 to 15, the sender shall include it to a peer node
to which 15 was not advertised earlier. However, to a peer node if 15 was
advertised earlier and 10 was never advertised, (e.g. there was no messaging
towards that peer node while the Overload Metric changed from 15 to 10) there
is no need to advertised 15 again.
#### 6.3.2.2 Using a subset of the applicable messages
In this variant of this alternative, the sender only uses subset of the
applicable messages for propagating the Overload Control Information. Out of
all the applicable messages, which can carry the Overload Control Information,
the sender defines its own subset of messages and applies the principles
described in the clause 6.3.2.1. The intention of using a subset of messages
is to avoid sending of the Overload Control Information in some of the
messages which are sent less frequently.
Since the subset of the messages used by a sender is specific to that sender
and not known to the receiver and also since this subset may differ between
two different senders, the receiver shall support the handling of the Overload
Control Information in all the applicable messages, which can carry the
Overload Control Information.
#### 6.3.2.3 Advantages
Following are the advantages of this alternative:
  * Unnecessary information in the message is avoided. The Overload Control Information is included only when it has changed from a peer node perspective and hence the peer node does not receive redundant information in any message.
\- Less overhead in the message and less message processing requirement at the
receiver.
#### 6.3.2.4 Drawbacks
Following are the drawbacks of this alternative:
  * The sender has to remember if the Overload Control Information was sent and its value towards each peer node, for every peer node. If the message is terminated at the next-hop peer the remote peer will not receive the Overload Control Information and hence the sender needs to remember the same to ensure that the Overload Control Information is provided to the remote peer as well.
  * Extra processing and storage requirement on the sender node.
  * The sender has to accurately know the identity of the currently serving (remote) peer node.
### 6.3.3 Alternative 2 -- Inclusion by piggybacking in every message towards
a peer
#### 6.3.3.1 Description
In this alternative, the node includes the Overload Control Information in
each and every message, which can carry the Overload Control Information, sent
to a peer node by piggybacking it over the existing messages. The sender does
not need to remember what value of the Overload Control Information was
already sent towards a peer node. And hence, the peer node may receive the
same information from a sender in multiple messages.
The receiver shall continue to use the earlier received Overload Control
Information until the old information is valid (e.g. based on the validity
period of the old information) or until the new information is received.
NOTE: This is required if the sender does not include the information in every
message (see clause 6.3.3.2) and for forward compatibility if the overload
information is added to additional GTP-C messages in future (e.g. to a message
supported by both the sender and the receiver, but with a sender implementing
an earlier version of the specification and not including the new overload
control information in the message).
#### 6.3.3.2 Using a subset of the applicable messages
In this variant of this alternative, the sender only uses subset of the
applicable messages for propagating the Overload Control Information. Out of
all the applicable messages, which can carry the Overload Control Information,
the sender defines its own subset of messages and applies the principles
described in the clause 6.3.3.1. The intention of using a subset of messages
is to avoid sending of the Overload Control Information in some of the
messages which are sent less frequently. The receiver shall continue to use
the earlier received Overload Control Information until the old information is
valid (e.g. based on the validity period of the old information) or until the
new information is received.
Since the subset of the messages used by a sender is specific to that sender
and not known to the receiver and also since this subset may differ between
two different senders, the receiver shall support the handling of the Overload
Control Information in all the applicable messages, which can carry the
Overload Control Information.
#### 6.3.3.3 Advantages
Following are the advantages of this alternative:
  * The sender need not remember if the Overload Control Information was sent and its value towards any peer node. Thus, less processing and storage requirement on the sender node.
\- Simple to implement at the sender node.
#### 6.3.3.4 Drawbacks
Following are the drawbacks of this alternative:
  * Potentially redundant information will be included in multiple messages.
\- Extra processing at the receiver to compare and discard the new information
if it is same as the old information received from the same node.
NOTE: A parameter within Overload Control Information, representing the
newness/freshness of the information (e.g. Overload-Sequence-Number as defined
in clause 6.2.2.1.2.3), may help to address the above drawback to some extent,
e.g. the receiver may simply discard the Overload Control Information if the
new value of the Overload-Sequence-Number is same as earlier received from the
same peer.
### 6.3.4 Alternative 3 -- Hybrid of Alternative 1 and Alternative 2
#### 6.3.4.1 Description
In this alternative, the sender follows the principles of the Alternative 1
towards some peers, i.e. the sender includes the Overload Control Information
only when the new/changed value has not already been indicated to the peer
node, while the sender follows the principles of Alternative 2 for the other
peers, i.e. the sender includes the Overload Control Information in each and
every message (or a subset of the same) which can carry the Overload Control
Information, sent to a peer node by piggybacking the information over the
existing messages. Alternative 1 may be easy to implement in a smaller network
with few nodes or towards the peer nodes of the same PLMN (towards which the
amount of traffic is very high). Alternative 2 may be easy to implement in a
larger network or towards the peer nodes of a different PLMN. Thus, the sender
can leverage the benefits of Alternative 1 (specifically, avoiding of the
redundant information in each and every message) towards some peers and
Alternative 2 (specifically, not remembering what value of overload
information was sent to which peer) towards the other peers.
The receiver shall support the handling of the Overload Control Information in
all the applicable messages, which can carry the Overload Control Information.
The receiver shall continue to use the earlier received Overload Control
Information until the old information is valid (e.g. based on the validity
period of the old information) or until the new information is received from
the same peer.
#### 6.3.4.2 Advantages
Following are the advantages of this alternative:
  * All the advantages of Alternative 1 (clause 6.3.2.3) when Alternative 1 is followed towards a given peer. All the advantages of Alternative 2 (clause 6.3.3.3) when Alternative 2 is towards a given peer.
#### 6.3.4.3 Drawbacks
Following are the drawbacks of this alternative:
  * All the drawbacks of Alternative 1 (clause 6.3.2.4) when Alternative 1 is followed towards a given peer. All the drawbacks of Alternative 2 (clause 6.3.3.4) when Alternative 2 is towards a given peer.
### 6.3.5 Alternative 4 -- Inclusion by piggybacking the overload control
information periodically
#### 6.3.5.1 Description
In this alternative, the node includes the Overload Control Information in
GTP-C signalling periodically. E.g. the sender includes the Overload Control
Information in all the messages during a first period (aimed at distributing
the information to all peers) and then ceases to do so for a second period.
The sender does not need to remember whether and what value of the Overload
Control Information was already sent towards a peer node. The sender shall
include the information often enough to ensure that all peers receive the
information at an acceptable pace. The duration of the period may be adaptive
and function of the overload level e.g. the node may include the information
more frequently when the overload increases to ensure a faster reaction from
the peers.
NOTE: The period at which to include the Overload Control Information may
depend e.g. on the period with which the sender re-assesses its overload
level, the capacity of the sending node (a node with a smaller capacity can
experience faster variations of its overload level than a node with a larger
capacity), the number of peer nodes.
The peer node may receive the same information from a sender in multiple
messages.
#### 6.3.5.2 Advantages
Following are the advantages of this alternative:
  * the sender need not remember if the Overload Control Information was sent and its value towards any peer node. Thus, less processing and storage requirement on the sender node;
  * simple to implement at the sender node;
  * less overhead in the message and less message processing requirement at the receiver than if the Overload Control Information is included in every message.
#### 6.3.5.3 Drawbacks
Following are the drawbacks of this alternative:
  * the sender has to remember when it has sent the information the last time to decide when to send it again;
  * potentially redundant information will be included in multiple messages;
  * extra processing at the receiver to compare and discard the new information if it is same as the old information received from the same node;
NOTE: A parameter within Overload Control Information, representing the
newness/freshness of the information (e.g. Overload-Sequence-Number as defined
in clause 6.2.2.1.2.3), may help to address the above drawback to some extent,
e.g. the receiver may simply discard the Overload Control Information if the
new value of the Overload-Sequence-Number is same as earlier received from the
same peer.
  * the periods at which to include the Overload Control Information needs to be carefully defined, e.g. if the first period is too short, there is a risk of not providing the overload information towards some peers with which there is no messaging within that period; if the second period is too short; this minimizes the gains vs. including the information in every message;
\- between two periods if the overload information changes then it cannot be
advertised and hence it may result in delay in advertising of the latest
information. This in turn it may cause delay in applying the information at
the receiver for overload mitigation.
### 6.3.6 Conclusion
How often/when the sender includes the overload information is implementation
specific, e.g. the sender can include the overload information based on any of
the alternatives provided in section 6.3. It is the responsibility of the
sender to ensure that any new/updated overload control information is
propagated to the target receivers with an acceptable delay, such that the
purpose of the information (i.e. effective overload control protection) is
achieved.
The receiver shall be prepared to receive the information in any of the GTP-C
messages extended with an Overload Control Information IE and upon such
reception, shall be able act upon the received information.
Table 6.3.6-1 identifies the messages which will be extended with an Overload
Control Information IE, from:
\- the MME/S4-SGSN to the PGW via the SGW;
\- the SGW to the PGW;
\- the PGW to the MME/S4-SGSN via the SGW;
\- the PGW to the ePDG/TWAN.
The sender may include the overload information in a subset of these messages.
Table 6.3.6-1: GTP-C messages extended with an Overload Control Information IE
+------------------+------------------+------------------+----------+ | Message | Direction of | Inclusion of | Comments | | | messages | | | | | | overload | | | | | information | | +------------------+------------------+------------------+----------+ | Create Session | MME/S4-SGSN -> | Yes | | | Request | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Create Session | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Create Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Create Bearer | MME/S4-SGSN -> | Yes | | | Response | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Modify Bearer | MME/S4-SGSN -> | Yes | | | Request | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | ePDG -> PGW | No | Note 4 | +------------------+------------------+------------------+----------+ | Modify Bearer | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> ePDG | | | +------------------+------------------+------------------+----------+ | Bearer Resource | MME -> SGW, | Yes | | | Command | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Bearer Resource | PGW -> SGW, | Yes | | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete Session | MME/S4-SGSN -> | Yes | | | Request | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Delete Session | PGW -> SGW, | Yes | | | Response | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete Bearer | MME/S4-SGSN -> | Yes | | | Response | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Downlink Data | SGW -> | Yes | | | Notification | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Downlink Data | MME/S4-SGSN -> | No | Note 1 | | Notification | SGW | | | | Acknowledge | | | | +------------------+------------------+------------------+----------+ | Downlink Data | MME/S4-SGSN -> | No | Note 1 | | Notification | SGW | | | | Failure | | | | | Indication | | | | +------------------+------------------+------------------+----------+ | Modify Bearer | MME/S4-SGSN -> | Yes | | | Command | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Modify Bearer | PGW -> SGW, | Yes | | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete Bearer | PGW -> SGW, | Yes | | | Failure | | | | | Indication | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Update Bearer | PGW -> SGW, | Yes | | | Request | | | | | | SGW -> | | | | | MME/S4-SGSN | | | | | | | | | | PGW -> | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Update Bearer | MME/S4-SGSN -> | Yes | | | Response | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | | TWAN/ePDG -> | No | Note 4 | | | PGW | | | +------------------+------------------+------------------+----------+ | Delete Bearer | MME/S4-SGSN -> | Yes | | | Command | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Change | MME/S4-SGSN -> | No | Note 2 | | Notification | SGW, | | | | Request | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Change | PGW -> SGW, | No | Note 2 | | Notification | | | | | Response | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Release Access | MME/S4-SGSN -> | No | Note 1 | | Bearers Request | SGW | | | +------------------+------------------+------------------+----------+ | Release Access | SGW -> | Yes | | | Bearers Response | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Modify Access | MME/S4-SGSN -> | No | Note 1 | | Bearers Request | SGW | | | +------------------+------------------+------------------+----------+ | Modify Access | SGW -> | Yes | | | Bearers Response | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Create | MME/S4-SGSN -> | No | Note 1 | | Forwarding | SGW | | | | Tunnel Request | | | | +------------------+------------------+------------------+----------+ | Create | SGW -> | No | Note 2 | | Forwarding | MME/S4-SGSN | | | | Tunnel Response | | | | +------------------+------------------+------------------+----------+ | Delete Indirect | MME/S4-SGSN -> | No | Note 1 | | Data Forwarding | SGW | | | | Tunnel Request | | | | +------------------+------------------+------------------+----------+ | Delete Indirect | SGW -> | No | Note 2 | | Data Forwarding | MME/S4-SGSN | | | | Tunnel Response | | | | +------------------+------------------+------------------+----------+ | Create Indirect | MME/S4-SGSN -> | No | Note 2 | | Data Forwarding | SGW | | | | Tunnel Request | | | | +------------------+------------------+------------------+----------+ | Create Indirect | SGW -> | No | Note 2 | | Data Forwarding | MME/S4-SGSN | | | | Tunnel Response | | | | +------------------+------------------+------------------+----------+ | Stop Paging | SGW -> | No | Note 2 | | Indication | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Suspend | MME/S4-SGSN -> | No | Note 2 | | Notification | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Suspend | PGW -> SGW, | No | Note 2 | | Acknowledge | | | | | | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Resume | MME/S4-SGSN -> | No | Note 2 | | Notification | SGW, | | | | | | | | | | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Resume | PGW -> SGW, | No | Note 2 | | Acknowledge | | | | | | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | Delete PDN | PGW \ SGW, | No | Note 3 | | Connection Set | | | | | Request | SGW \ MME | | | | | | | | | | PGW \ | | | | | TWAN/ePDG | | | +------------------+------------------+------------------+----------+ | Delete PDN | SGW \ PGW, | No | Note 3 | | Connection Set | | | | | Response | MME \ SGW | | | | | | | | | | TWAN/ePDG \ | | | | | PGW | | | +------------------+------------------+------------------+----------+ | Update PDN | SGW -> PGW | No | Note 3 | | Connection Set | | | | | Request | | | | +------------------+------------------+------------------+----------+ | Update PDN | PGW -> SGW | No | Note 3 | | Connection Set | | | | | Response | | | | +------------------+------------------+------------------+----------+ | PGW Restart | SGW -> | No | Note 3 | | Notification | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | PGW Restart | SGW -> | No | Note 3 | | Notification | MME/S4-SGSN | | | | Acknowledge | | | | +------------------+------------------+------------------+----------+ | PGW Downlink | PGW -> SGW, | No | Note 3 | | Triggering | | | | | Notification | SGW -> | | | | | MME/S4-SGSN | | | +------------------+------------------+------------------+----------+ | PGW Downlink | MME/S4-SGSN -> | No | Note 3 | | Triggering | SGW, | | | | Notification | | | | | Acknowledge | SGW -> PGW | | | +------------------+------------------+------------------+----------+ | Trace Session | MME/S4-SGSN -> | No | Note 2 | | Activation | SGW, | | | | | | | | | | SGW -> PGW, | | | | | | | | | | TWAN/ePDG -> | | | | | PGW | | | +------------------+------------------+------------------+----------+ | Trace Session | MME/S4-SGSN -> | No | Note 2 | | Deactivation | SGW, | | | | | | | | | | SGW -> PGW, | | | | | | | | | | TWAN/ePDG -> | | | | | PGW | | | +------------------+------------------+------------------+----------+ | Note 1: | | | | | Inclusion of | | | | | overload | | | | | information is | | | | | set to \"No\" | | | | | because the | | | | | consumer is SGW, | | | | | the message is | | | | | not sent to PGW | | | | | | | | | | Note 2: | | | | | Inclusion of | | | | | overload | | | | | information is | | | | | set to \"No\" | | | | | because the | | | | | message is used | | | | | less frequently. | | | | | | | | | | Note 3: | | | | | Inclusion of | | | | | overload | | | | | information is | | | | | set to \"No\" | | | | | because the | | | | | message is only | | | | | used during | | | | | restoration | | | | | procedures. | | | | | | | | | | Note 4: | | | | | Inclusion of | | | | | overload | | | | | information is | | | | | not supported | | | | | from TWAN/ePDG | | | | | to PGW. | | | | +------------------+------------------+------------------+----------+
## 6.4 Message throttling
### 6.4.0 General
The message throttling is applicable to initial messages only. Triggered
request or response messages should not be throttled since that would result
in the retransmission of the corresponding request message by the sender.
Refer to clause 6.8.1 for the handling of the received initial messages when
the node decides not to process the same during an overload condition.
Before piggybacking the initial message over a response message, the initial
message should be subjected to the message throttling in the similar manner as
any other non-piggybacked initial message. If the node decides to throttle
this initial message then the response message should be sent without any
piggyback message.
### 6.4.1 Throttling algorithms
#### 6.4.1.1 General
As a part of the overload mitigation, the GTP-C node is required to reduce the
total number of messages, which would have been sent otherwise, towards the
overloaded peer based on the information received within \"Overload Control
Information\". This is achieved by discarding a fraction of the messages in
proportion to the overload level of the target peer. This is called message
throttling and there could be multiple ways (i.e. algorithms) to achieve the
same. Correspondingly, this sub clause examines various message throttling
algorithms, and for each algorithm evaluates various aspects such as
effectiveness, efficiency, ease of implementation, etc. Finally, one of the
algorithms will be recommended as the default algorithm which should be
minimally implemented as the part of the support of the \"GTP-C overload
control\" feature.
#### 6.4.1.2 Throttling by \"Loss\" algorithm
##### 6.4.1.2.1 Description
This algorithm allows the GTP-C node to ask its peer to reduce the number of
requests they would ordinarily send by a specified percentage. For example, if
a sender requests another peer that it reduces the traffic it is sending by
10%, then that peer will throttle 10% of the traffic that would have otherwise
been sent to this node.
The sender provides the requested traffic reduction in percentage within the
\"Overload-Reduction-Metric\" as specified in clause 6.2.2.1.2.1. The
recipients of the \"Overload-Reduction-Metric\" shall reduce the number of
requests sent by that percentage, either by redirecting them to an alternate
destination if possible (e.g. the Create Session Request message can be
redirected to an alternate SGW/PGW), or by failing the request and treating it
as if it was rejected by the destination node.
This algorithm does not guarantee that the future traffic towards the
overloaded node will be less than the past traffic. It only ensures that the
total traffic sent towards the overloaded node is less than what would have
been sent without any throttling in place. The overloaded node is expected to
periodically adjust the requested traffic reduction based e.g. on the traffic
reduction factor that is currently in use, the current system utilization
(i.e. overload level) and the desired system utilization (i.e. target load
level), and/or the rate of the current overall received traffic, e.g. after
requesting 10% reduction in traffic, if the overloaded node receives more
traffic than in the past while in overload, leading to worsen rather than
improve the overload level, then the overloaded node may request for 20%
reduction in traffic. Thus, by periodically adjusting the requested traffic
reduction, the overloaded node can ensure that it receives, approximately, the
amount of traffic which it can handle.
Since the reduction is requested in percentage, and not in absolute amount,
this algorithm achieves better useful throughput while ensuring the protection
of the overloaded node. For example, when the overloaded node request 10% of
the reduction in traffic from all its source nodes, some source nodes may send
less traffic than in the past while others may send more traffic than in the
past (depending upon the events generated towards these source nodes by other
entities in the network), while applying 10% reduction in traffic. And the
total traffic received by the overloaded node may still be within its ability
to handle this traffic. Thus, the source node generating more traffic is
balanced by the other source nodes generating less traffic and hence achieving
higher useful throughput towards the overloaded node.
##### 6.4.1.2.2 Implementation
For the implementation of \"Loss\" algorithm, among other possible methods, it
is possible to make use of a statistical loss function (e.g., random
assignment of messages into \"success\" and \"failure\" categories based on
the indicated percentage) to decide if the given message can be sent or need
to be throttled. For example, the source node generates a random number
between (0, 100) for each message which is a potential candidate for
throttling. To realize 10% throttling, the message with a random number 10 or
less is assigned \"failure\" category while remaining messages are assigned
\"success\" category. The message with \"failure\" category is throttled and
hence this achieves approximately a 10% reduction in the overall traffic. The
actual traffic reduction might vary slightly from the requested percentage,
albeit in an insignificant amount. However, this allows very simple and
effective implementation of the \"Loss\" algorithm without having the need for
the source node to implement any queuing/buffering mechanism. Besides, no
special coordination or central control is needed within the source node with
distributed system architecture having multiple processes sending messages
towards the same overloaded node in parallel.6.4.1.2.3 Message priority
related consideration
The algorithm can select certain messages to throttle in priority. For
example, implementations are allowed to distinguish between higher-priority
and lower-priority messages, and drop the lower-priority messages in favour of
dropping the higher priority messages, as long as the total reduction in
traffic conforms to the requested reduction in effect at the time. For
example, in the 50-50 distribution of high priority and low priority messages,
20% reduction to low priority messages and 0% reduction to high priority
messages need to be applied in order to achieve the effective reduction in
traffic by 10% towards the overloaded node. Similarly, 100% reduction to low
priority messages and 20% reduction to higher priority messages need to be
applied to achieve the effective reduction in traffic by 60% towards the
overloaded node. Refer to clause 6.4.2 regarding the guidance on determining
the priority of a message.
##### 6.4.1.2.3 Advantages
Following are the advantages of using the \"Loss\" algorithm for traffic
throttling:
  * Very easy to implement at the source node and causes very little overhead in terms of processing power to realize the throttling. Equally suitable for the distributed as well as monolithic type of architectures.
  * Based on some statistical analysis, the overloaded node can approximately calculate the required reduction in traffic without being bothered about the risk of over/under throttling by the source nodes. Thus, simple to calculate the required reduction in traffic and hence easy to implement at the overloaded node.
  * Since the traffic reduction is requested in percentage, the total capacity of the overloaded node is not literally divided among the source nodes. And hence it ensures higher useful throughput towards the overloaded node. Thus an efficient throttling mechanism which avoids over/excessive throttling.
  * Highly suitable in the network where each node is connected to multiple peer nodes. In other words, suitable for mesh type of network topology with higher node density.
\- The overloaded node does not need to track the set of upstream nodes or the
request rate it receives from each upstream node. It is sufficient to monitor
the overall system utilization.
##### 6.4.1.2.4 Drawbacks
Following are the drawbacks of using the \"Loss\" algorithm for traffic
throttling:
  * May not be very effective in protecting the overloaded node during sudden and heavy peaks in the traffic. The issue may not be very severe unless the pattern of traffic peaks coincide at all the source nodes at the same time.
  * Since the reduction in future traffic, relative to the past traffic, is not guaranteed, the overloaded node may need to adjust the required reduction in traffic more often and periodically.
#### 6.4.1.3 Conclusion
It is concluded that for message throttling the \"Loss\" algorithm as
specified in clause 6.4.1.2, shall be specified in 3GPP Release 12 as
mandatory algorithm to support and default to use.
Additionally, following aspects should be considered during normative work.
Table 6.4.1.3-1: Aspects to be considered during further normative work
+-----------+---------------------------+---------------------------+ | Subclause | Comments | | +-----------+---------------------------+---------------------------+ | 6.4.1.1 | General description of | Text from corresponding | | | throttling algorithm | subclause for normative | | | | work. | +-----------+---------------------------+---------------------------+ | 6.4.1.2.1 | Description of \"Loss\" | Text from corresponding | | | algorithm. | subclause for normative | | | | work. | | | Some of the aspects | | | | mentioned in 6.4.1.2.3 | | | | and 6.4.1.2.4 may also be | | | | considered as part of | | | | description. | | +-----------+---------------------------+---------------------------+ | 6.4.1.2.2 | Implementation of | Text from corresponding | | | \"Loss\" algorithm and | subclause for normative | | | possible method as an | work. | | | implementation example | | +-----------+---------------------------+---------------------------+
### 6.4.2 Message prioritization
#### 6.4.2.1 Description
As part of overload mitigation mechanisms, based on the \"Overload Control
Information\" received, the GTP-C node may start throttling the messages
towards the overloaded peer. In the absence of any guidelines related to
identifying the priority of the messages, the GTP-C node may perform random
throttling and hence start discarding the message without any special
consideration. This type of sub-optimal message throttling would result in the
overall poor congestion mitigation mechanism. Correspondingly, this clause
investigates various criteria for the prioritization of the messages so that
the messages which are considered as low priority are considered for
throttling before the other messages, when the message throttling is to be
applied. These prioritization mechanisms can also be used by the overloaded
node to handle the received initial messages during an overloaded condition,
as a part of the self-protection mechanism.
All the procedures, and hence correspondingly all the messages of those
procedures, should be treated with the highest priority for the emergency and
the priority users (eMPS) related sessions. And hence messages belonging to
those types of sessions should be given the least preference while performing
message throttling. For the other types of sessions, prioritized message
throttling as described below may be applied.
#### 6.4.2.2 Based on procedures
For performing procedure based prioritized message throttling, the procedures
(of the message) are grouped into various categories. Each of these categories
is assigned a priority. A node, that supports procedure based prioritized
message throttling, should evaluate the priority of the message based on the
category of the procedure for which the message is being sent. Additionally,
within a given category of the procedure, messages can be further prioritized
based on session parameters, such as APN, QCI, ARP, LAPI, etc. (e.g. as
described in clause 6.4.2.3). Subsequently, messages with high priority shall
be given lower preference while throttling and messages with low priority
shall be given higher preference while throttling. The grouping of the
procedures is not performed for an individual node but while considering all
the procedures in general. A node should consider the procedures applicable to
it and apply prioritized message throttling based on the category of the
procedure as described below. The categories are listed in the decreasing
order of priority with category 1 having highest priority 1. For each category
non-exhaustive message list is provided. Any existing or newly defined message
in future should be considered based on the category (as specified below) of
the procedure for which the message is sent.
1\. **UE session mobility within and across 3GPP or non-3GPP access:**
Procedures involving active or idle mode UE mobility, such that GTP-C
signalling is involved, are classified under this category. Some of the
examples are X2/S1 based handover with/without SGW change, TAU/RAU with change
of MME/SGSN with/without SGW change, 3GPP access to trusted non-3GPP access
handover, etc. Throttling of these messages, during the procedures related to
UE session mobility, would result in the failure of the corresponding
procedures. This, potentially, could result in the loss of the PDN connection
and/or interruption of the services. Hence, the messages, as identified below,
when sent during the procedures belonging to this category, shall be
considered with the highest priority and hence shall be given the lowest
preference while throttling.
\- Create Session Request
\- Create Session Request with \"handover\" indication bit set
\- Modify Bearer Request
\- Modify Bearer Request with \"handover\" indication bit set
\- Modify Access Bearer Request
2\. **Release of PDN connection or bearer resources:** Procedures resulting in
deactivation of an existing PDN connection, deactivation of bearer(s) and data
forwarding tunnel of an UE leads to freeing up of the resources at the
overloaded node and, hence, can potentially ease the overload situation since
the freed up resources can be used for serving the remaining of the UEs. Thus,
the messages belonging to this category resulting in the deactivation of PDN
connection or bearer(s) or data forwarding tunnel(s), as identified below,
shall be treated with the next lower level of priority and hence shall be
given corresponding preference while throttling.
\- Delete Session Request
\- Delete Bearer Request
\- Delete Bearer Command
\- Delete Indirect Data Forwarding Tunnel Request
3\. **Miscellaneous session management procedures:** This category consists of
the session management procedures, except PDN connection creation and bearer
creation/modification procedures. Some of the examples are location reporting
when it is not combined with other mobility procedures, Service request and S1
release procedure. These procedures do not severely impact the ongoing service
of the UE. Hence, the messages, as identified below when sent during the
procedures identified under this category, shall be treated with the next
lower level of priority and hence shall be given corresponding preference
while throttling.
\- Release Access Bearer Request
\- Modify Bearer Request
\- Change Notification
\- Suspend Notification
\- Resume Notification
4\. **Request for new PDN Connection or bearer resources:** Procedures
requesting for creation of PDN connection, creation or modification of
bearer(s) and creation of data forwarding tunnel are classified in this
category. Throttling of the messages belonging to this category would result
in denial of new services while continuing with the existing services.
However, this is the natural outcome of an overload condition, i.e. the
overloaded node, due to lack of resources, is not able to provision new
services while the trying to maintain the existing services. And hence the
messages, as identified below when sent during the procedures belonging to
this category, shall be considered with the lowest level of priority and hence
shall be given highest preference while throttling.
\- Create Session Request during PDN connection request
\- Create Bearer Request
\- Update Bearer Request
\- Bearer Resource Command
\- Modify Bearer Command
\- Create Indirect Data Forwarding Tunnel Request
#### 6.4.2.3 Based on session parameters
Message prioritization can be performed based on the session parameters such
as APN, QCI, ARP, Low Access Priority Indicator (LAPI) etc., e.g. the PDN
connections are grouped based on the APN, and then based on the configured
value of APN\'s priority, the relative priority of a set of PDN connection is
determined. The procedures and messages associated with the higher priority
sessions are given lesser preference while throttling as compared to the
procedures and messages associated with the lower priority sessions.
Additionally, within each group of the PDN connections, the sessions can be
further prioritized based on the QCI/ARP/LAPI. Finally, within each group of
the session, the messages can be prioritized based on the category of the
procedure for which the message is being sent (e.g. as described in clause
6.4.2.2). With this type of message prioritization scheme, there is potential
risk that messages related to a critical procedure, e.g. UE mobility, for
lower priority sessions are throttled over messages related to less critical
procedure, e.g. location reporting, for higher priority session. However, this
type of prioritization scheme also ensures better handling of all the messages
and procedures related to a higher priority sessions.
NOTE 1: A GTP-C entity can possibly assign TEID-C values based on session
parameter such as APN or LAPI during session establishment. This TEID-C can be
later used by the overloaded node to reject the incoming messages as part of
self-protection mechanism. The sender of the message can use its own TEID-C or
other information from local context to throttle the outgoing traffic.
#### 6.4.2.4 Conclusion
It is concluded that the node supporting the prioritized message throttling
should consider message prioritization based on the procedure as described in
clause 6.4.2.2 or based on session parameter as described in clause 6.4.2.3.
The corresponding behaviour shall be specified in 3GPP Release 12.
Additionally, following aspects should be considered during normative work.
Table 6.4.2.3-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
6.4.2.1 General description of the message prioritization Text from
corresponding subclause for normative work. 6.4.2.2 Procedure based message
prioritization Text from corresponding subclause for normative recommendation.
6.4.2.3 Session parameter based message prioritization Text from corresponding
subclause for normative recommendation.
* * *
## 6.5 Propagation of MME/S4-SGSN identity to PGW
### 6.5.1 Description
The PGW may not be aware about the identity of the currently serving
MME/S4-SGSN since there is no signalling over S5/S8 interface during the
procedures involving inter-MME/S4-SGSN and intra-SGW scenarios, when the
reporting of RAT, ULI, UCI, Serving Network is not needed. In that case, if
the PGW has received the \"Overload Control Information\" from the source
MME/S4-SGSN, it may enforce the mitigation actions assuming that the source
MME/S4-SGSN is the currently serving the UE. This would result in incorrect
enforcement of the overload control and hence it should be avoided while also
ensuring that we do not overload S5/S8 interface with unnecessary signalling.
This sub clause investigates possible methods for propagating the currently
serving MME/S4-SGSN identity to PGW when the overload control for the source
MME/S4-SGSN is to be applied by the PGW.
### 6.5.2 Updating PGW with current serving MME/S4-SGSN identity
#### 6.5.2.1 Introduction
In order to ensure that the PGW can apply the overload control towards the
overloaded MME/S4-SGSN, we need to ensure that the currently serving
MME/S4-SGSN\'s identity is provided to the PGW during normal working
condition. In order to achieve the same, the MME/S4-SGSN supporting the GTP-C
overload control feature shall provide the MME/S4-SGSN identifier IE over the
S11/S4 interfaces in all the messages that result in establishment of an
S11/S4 session, e.g. during PDN connection establishment, during HO procedures
with MME/S4-SGSN change and/or SGW change.
Additionally we must also ensure that serving MME/S4-SGSN\'s identifier is
provided to the PGW, specifically during the scenarios when there is no S5/S8
signalling, currently, e.g. during MME/S4-SGSN change with/without SGW change.
The following alternatives propose a means to do that.
#### 6.5.2.2 Alternative 1: Always send MBReq over S5/S8
During MME/S4-SGSN change with/without SGW change, the SGW forwards the Modify
Bearer Request message over S5/S8 and includes the target MME/S4-SGSN
identity.
Drawback of this solution:
\- will generate one extra S5/S8 Modify Bearer Request/Response signalling
exchange during an inter-MME or an inter-S4-SGSN TAU/RAU/Handover without an
SGW change in scenarios where no other information such as ULI, CGI needs to
be reported to the PGW.
#### 6.5.2.3 Alternative 2: Only send MBReq over S5/S8 normally
The SGW always includes the currently serving MME/S4-SGSN's identity in the
Modify Bearer Request message over S5/S8 interface, whenever there is
signalling over S5/S8 interface. This will ensure that the currently serving
MME/S4-SGSN's identity is informed to the PGW without generating extra
signalling over S5/S8 interface.
Drawback of this proposal:
\- the PGW may not have up-to-date information about the currently serving
MME/S4-SGSN's identity for all the UEs. Until next S5/S8 signalling updates
the MME/SGSN ID for the PDN connection, the PGW may still throttle the PGW
initiated dedicated bearer signalling for that PDN connection if the old
MME/SGSN is overloaded.
\- requires the PGW to check at every incoming S5/S8 signalling message
whether the MME/S4-SGSN\'s identity has changed or not.
#### 6.5.2.4 Alternative 3: Opt Alt 2 -- send over S5/S8 if not provided
previously
Optimization of the alternative 2: The SGW includes currently serving
MME/S4-SGSN's identity in Modify Bearer Request message over S5/S8 interface
only if the same is not already provided to the PGW.
The drawbacks of this solution are:
  * the PGW may not have up-to-date information about the currently serving MME/S4-SGSN's identity for all the UEs. Until next S5/S8 signalling updates the MME/SGSN ID for the PDN connection, the PGW may still throttle the PGW initiated dedicated bearer signalling for that PDN connection if the old MME/SGSN is overloaded.
\- requires the SGW to remember if the currently serving MME/S4-SGSN's
identity is provided to the PGW or not. However it would avoid unnecessary
information over S5/S8 interface.
#### 6.5.2.5 Alternative 4: Send MBReq over S5/S8 when source in overload
During an MME/S4-SGSN change w/o SGW change, the SGW sends immediately a
Modify Bearer Request message over S5/S8 interface including the new serving
MME/S4-SGSN's identity only if the old MME/SGSN had initiated overload control
towards the PGW. Otherwise, it sends the new serving MME/S4-SGSN\'s identity
at the next existing S5/S8 signalling.
Drawbacks of this solution:
\- if the old MME/S4-SGSN starts notifying overload after the mobility
scenario (i.e. no overload had been activated before), the PGW may wrongly
throttle dedicated bearer signalling towards the MME/SGSN for that PDN
connection until S5/S8 signalling occurs.
\- if the source MME/SGSN is in overload it will generate one extra S5/S8
Modify Bearer Request/Response signalling exchange during inter-MME/SGSN intra
SGW relocation mobility scenarios where no other information such as ULI, CGI
needs to be reported to the PGW.
\- SGW needs to store whether an MME/SGSN has activated overload control or
not;
\- requires the PGW to check at incoming S5/S8 signalling message whether the
MME/S4-SGSN\'s identity has changed or not.
#### 6.5.2.6 Conclusion
It is concluded that the identity of the currently serving MME/SGSN shall be
provided to the PGW only during the existing S5/S8 signalling, as specified in
clause 6.5.2.3. This may result in PGW not updated with the currently serving
MME/SGSN\'s identity, for a given subscriber, between the time when the inter
MME/SGSN and intra SGW mobility scenario such that there is no S5/S8
signalling take place (i.e. when no other information such as ULI, CGI,
Serving Network, etc. needs to be reported to the PGW) and the time when there
is subsequent S5/S8 signalling for the same subscriber. However, considering
the features deployed in the network leading to S5/S8 signalling even during
not so frequent inter MME/SGSN and intra SGW mobility, the chance of PGW not
having the identity of the currently serving MME/SGSN is generally very low in
the network. And hence the risk the S5/S8 signalling message, for the given
subscriber, being throttled by the PGW in order to enforce the overload
control for the older MME/SGSN is even lower. And hence it was concluded to
not define new S5/S8 signalling to update the PGW with the currently serving
MME/SGSN\'s identity. Correspondingly, it is concluded that the approach
mentioned in clause 6.5.2.3 shall be specified in 3GPP Release 12.
### 6.5.3 Updating PGW with overload control information of target MME/S4-SGSN
#### 6.5.3.1 Introduction
The Overload control information of the new serving MME/S4-SGSN is passed to
the PGW during inter-MME/S4-SGSN inter-SGW TAU/RAU/Handover via the existing
S5/S8 Modify Bearer Request. There is no problem for this scenario.
Overload control information of the new serving MME/S4-SGSN may be sent to the
PGW during inter-MME/S4-SGSN intra-SGW TAU/RAU/handover during overload of the
target MME/SGSN. The following alternatives address this issue.
It should be noted that for the alternatives below, the target MME/S4-SGSN
shall not use Modify Access Bearer Request message if it is including overload
control information, since this may generate S5/S8 signalling.
#### 6.5.3.2 Alternative 1: Forward MBReq over S5/S8 if target sends OCI
During MME/SGSN change without SGW change, the SGW forwards the Modify Bearer
Request message over S5/S8 if the SGW receives Overload Control Information
from the target MME/S4-SGSN.
Drawbacks of this solution:
\- will generate extra signalling over S5/S8 interface for the scenarios when
there is no S5/S8 signalling currently. However, it also ensures that the
latest Overload Control Information is provided to the PGW so that PGW applies
overload control immediately.
#### 6.5.3.3 Alternative 2: Opt Alt 1 -- Forward only if target OCI not
previously sent
Optimization of the alternative 1: During MME/S4-SGSN change with/without SGW
change, the SGW forwards the MBReq message containing overload control
information over S5/S8 signalling only if the target MME/S4-SGSN's overload
control information has not already been propagated to the PGW earlier. The
SGW can check the Overload-Sequence-Number to decide if the information
provided by the target MME/S4-SGSN is new or not. If it is new, the SGW shall
ensure that it is propagated to PGW via S5/S8 signalling.
Drawbacks of this solution:
\- SGW needs to store whether an MME/SGSN has activated overload control and
to determine whether the overload control information provided by the
MME/S4-SGSN has changed or not.
#### 6.5.3.4 Alternative 3: Only send MBReq over S5/S8 normally
During MME/SGSN change without SGW change, the SGW will forward the overload
control information over the S5/S8 interface only when the Modify Bearer
Request message is normally forwarded over S5/S8. This will ensure that the
currently serving MME/S4-SGSN's overload control information is passed on to
the PGW without generating extra signalling over S5/S8 interface.
Drawback of this proposal:
\- Before receiving the Modify Bearer Request message containing the
MME/SGSN\'s current overload information, the PGW does not have up-to-date
information regarding the MME/SGSN\'s overload information. And hence the PGW
may apply stale/old overload control for the first message initiated towards
the MME/SGSN. However, at the first PGW initiated signalling towards that
MME/S4-SGSN, the PGW will be made aware about the MME/S4-SGSN's overload
control information and will then be able to perform overload control as
appropriate.
#### 6.5.3.5 Conclusion
It is concluded that the overload control information of the target MME/SGSN
shall be sent to the PGW only during the existing S5/S8 signalling, as
specified in clause 6.5.3.4. If the MME/SGSN provides overload control
information during the scenario which does not result in S5/S8 signalling,
e.g. during inter MME/SGSN and intra SGW mobility when no other information
such as ULI, CGI, Serving Network, etc. needs to be reported to the PGW, the
overload information will not be delivered to the PGW. Hence, the MME/SGSN
should consider the same while including overload control information. The
corresponding behaviour shall be specified in 3GPP Release 12.
## 6.6 Interaction with existing mechanisms
It shall be possible to run the existing congestion control mechanisms in
parallel and concurrently with the new congestion control mechanisms defined
as part of \"GTP-C Overload Control Mechanisms\". However, there could be
potential impact to these existing mechanisms due to the support of GTP-C
overload control mechanism, e.g. potential interaction with the DDN throttling
mechanism when the MME/SGSN sends an \"Overload Control Information\" to
SGW/PGW. Correspondingly, the analysis of these impacts and possible
interaction between the existing and new mechanisms are investigated in this
clause.
### 6.6.1 DDN throttling
#### 6.6.1.1 Description
The following procedures for throttling Downlink Data Notification (DDN)
Requests have been specified in earlier releases:
1) throttling of DDN requests for low priority traffic (see clause 4.3.7.4.1a
of 3GPP TS 23.401 [2] and clause 5.3.6.5 of 3GPP TS 23.060 [3]) -- from
Release 10 onwards:
\- when the MME/SGSN load exceeds an operator configured threshold, the
MME/SGSN may request the SGWs to selectively reduce the number of DDN requests
it sends for downlink low priority traffic received for UEs in idle mode
according to a throttling factor and for a throttling delay specified in the
Downlink Data Notification Ack message;
\- the SGW determines whether a bearer is for low priority traffic or not on
the basis of the bearer\'s ARP priority level and operator policy (i.e.
operator\'s configuration in the SGW of the ARP priority levels to be
considered as priority or non- priority traffic).
\- the SGW resumes normal operations at the expiry of the throttling delay.
The last received value of the throttling factor and throttling delay
supersedes any previous values received from that MME. The reception of a
throttling delay restarts the SGW timer associated with that MME.
2) throttling of unnecessary DDN requests during UE triggered service requests
(see clause 5.3.4.2 of 3GPP TS 23.401 [2]) -- from Release 8 onwards:
\- the MME/SGSN monitors the rate of unnecessary DDN requests it receives
during UE triggered service request procedures, and if the rate becomes
significant and the MME/SGSN\'s load exceeds an operator configured value, the
MME/SGSN indicates in Modify Bearer Request (or Modify Access Bearers Request
for an MME only) a \"Delay Downlink Packet Notification Request\" with
parameter D to the SGW, where D is the requested delay given as an integer
multiple of 50 ms, or zero. The SGW then uses this delay in between receiving
downlink data and sending the DDN message;
\- normally, upon receipt of a downlink data packet for which there is no DL-
TEID of the S1/S4/S12 user plane tunnel, the SGW shall send the DDN message to
the MME/SGSN without delay.
An MME/SGSN receives over S11/S4 signalling traffic that originates from an
SGW or PGW:
\- DDN requests are the essential part of the signalling traffic an SGW
originates to the MME/SGSN (the rest is negligible);
\- given that an MME/SGSN can signal overload information to a PGW to throttle
the traffic that the PGW may originate towards the MME/SGSN, and that overload
control should be homogeneously supported within a PLMN (see clause 8), there
is no need for an SGW to throttle PGW originated traffic towards an MME/SGSN.
This could only be useful for roaming scenarios with home routed traffic when
overload control is not supported or used across the VPLMN and HPLMN
boundaries, but this represents moderate traffic (LBO is used for IMS) and
thus not proposed to be retained.
So signalling overload information from an MME/SGSN to an SGW would only serve
to trigger throttling of DDN requests, which would be largely redundant with
the existing DDN throttling mechanisms. One potential limit of the existing
mechanisms is that an SGW may possibly continue sending DDN requests for
normal priority traffic to an MME/SGSN in overload. This can be solved by
allowing by configuration the SGW to also throttle DDN requests for normal
priority traffic (the SGW would then throttle in priority DDN requests for low
priority traffic).
Consequently, no real need has been identified at this stage for introducing a
new overload control procedure from the MME/SGSN to the SGW (with SGW as
consumer of the overload information).
NOTE: No need has been identified either for introducing a similar procedure
from the PGW to the SGW (with SGW as consumer), see clause 4.2.4.3.2.
Therefore GTP-C overload control procedure will have minimum impacts on SGW.
#### 6.6.1.2 Conclusion
See conclusions in subclause 4.2.4.3.2.2.
### 6.6.2 Congestion control using APN back-off timer
#### 6.6.2.1 Description
Following are the principles of the APN back-off timer based congestion
control mechanism as defined in 3GPP TS 23.401 [2] clause 4.3.7.5 from Rel-10
onwards:
\- When performing overload control the PDN GW may reject the PDN connection
requests including the \"PDN-GW back-off time\" for a specific APN.
\- For this APN, during the time identified by \"PDN-GW back-off time\", the
MME shall not select the corresponding PDN-GW.
\- During this time, the MME may select another PDN-GW for that APN.
Otherwise, the MME should reject the PDN connection request towards the UE.
Since we are also introducing the overload control mechanism, it is necessary
to study the interaction between the above mechanism and the overload control
mechanism, using the Overload Control Info IE, and the possibility of their
co-existence.
#### 6.6.2.2 Alternative 1 -- To allow simultaneous usage
##### 6.6.2.2.1 Description
In this alternative, it is proposed to allow simultaneous use of APN back-off
timer mechanism as well as APN level overload control mechanism (i.e.
providing Overload Control Info IE with APN-List included) for a given APN for
performing congestion control at APN level, e.g. the sender can provide
Overload Control Info IE with APN-List=\"APN1\" and while this Overload
Control Info IE is valid (i.e. its \"Period-of-Validity\" has not expired), it
can also reject the Create Session Request message for \"APN1\" by providing
APN back-off timer.
##### 6.6.2.2.2 Advantages
Following are the advantages of this alternative:
\- For a sender, it is possible to achieve the use case of completely blocking
of Create Session Request messages while throttling (and hence not completely
blocking) of the other messages at the same time for the given APN.
##### 6.6.2.2.3 Drawbacks
Following are the drawbacks of this alternative:
\- Complex for the receiver to implement since two different mechanisms have
to be applied for the messages related to the same APN. The receiver will
receive different set of parameters via two mechanisms and hence has to ensure
interworking of these mechanisms.
\- Once the APN back-off timer is provided, the overloaded node cannot update
the value of the timer when the overload condition changes (i.e. improves or
deteriorates) for that APN.
#### 6.6.2.3 Alterative 2 -- To only allow exclusive usage
##### 6.6.2.3.1 Description
In this alternative, for a given APN, it is proposed to either use APN back-
off timer mechanism or APN level overload control mechanism (i.e. providing
Overload Control Info IE with APN-List included), but not together, at any
point of time for performing the congestion control at APN level, e.g. if the
sender provides Overload Control Info IE with APN-List=\"APN1\" and while this
Overload Control Info IE is valid (i.e. its \"Period-of-Validity\" has not
expired), it shall not reject the Create Session Request message for \"APN1\"
by providing APN back-off timer; however, the sender is allowed to reject the
Create Session Request message for \"APN2\" by providing APN back-off timer.
And the sender is also allowed to reject the Create Session Request message
with APN-Congestion cause code.
In summary, in this alternative, only one of the two mechanisms, APN back-off
timer or APN level overload control, is activated by the sender at any point
of time for a given APN. For a peer node, if the receiver has one mechanism
active (e.g. APN back-off timer is active) while it receives information for
another mechanism (e.g. Overload Control Info IE with APN-List included) then
the receiver shall deactivate/stop the earlier mechanism and considers only
the information received in latter mechanism.
##### 6.6.2.3.2 Advantages
Following are the advantages of this alternative:
\- Simple for the receiver to implement since two different mechanisms will
not be activated at the same time for the messages related to the given APN.
##### 6.6.2.3.3 Drawbacks
Following are the drawbacks of this alternative:
  * For a sender, it may be possible to achieve the use case of completely blocking of Create Session Request messages while throttling (and hence not completely blocking) of the other messages at the same time for the given APN, via other mechanism (e.g. by setting APN-Load-Metric as 100%). However, there is no absolute guarantee of realizing the same.
  * Once the APN back-off timer is provided, the overloaded node cannot update the value of the timer when the overload condition changes (i.e. improves or deteriorates) for that APN.
#### 6.6.2.4 Conclusion
It is concluded that for a given APN, at any point of time, either the APN
back-off timer based congestion control or the overload control information
based congestion control may be activated. However, both shall not be
activated at the same time, as specified in clause 6.6.2.3. The corresponding
behavior shall be specified in 3GPP Release 12 and the text in specified in
clause 6.6.2.3 will be considered for normative work.
## 6.7 Enforcement of overload control
### 6.7.1 General
When the GTP-C node receives Overload Control Information from its peer, it
applies the overload abatement algorithm, based on the received information,
for the messages sent towards the peer node. This is called the enforcement of
the overload control and it involves throttling of the messages targeted for
the overloaded peer node.
The enforcement of the overload is straight forward in a network topology
where the source node is directly connected to the overloaded peer node, e.g.
the MME applying the overload control for the messages targeted for the SGW
based on the overload information of the SGW. However, in a topology where an
intermediate node is involved, the enforcement of the overload control may
require additional consideration regarding the overload condition of the
intermediate node, e.g. besides the overload condition of the PGW, the MME may
need to consider the overload condition of the SGW (which is an intermediate
node) for the messages targeted for the PGW, while performing the overload
control. This clause investigates various approaches for the enforcement of
the overload control which can be applied by the source node for the messages
targeted for the overloaded peer node, specifically for the network topology
involving the source node and the overloaded target connected via an
intermediate node.
### 6.7.2 Aspects related to enforcement of the overload control
#### 6.7.2.1 Good throughput of the network
When overload control is applied towards the overloaded peer, the messages
destined for the overloaded peer are throttled by the source node. The
throttling simply means that the corresponding procedure is postponed and may
be retried again in future, hopefully when the overload condition eases. The
indication that the procedure cannot be handled now and postponed for future
should be propagated all the way towards the original source of the procedure.
In most of the cases, the original source of the procedure is UE and hence the
indication has to be delivered back to the UE. When the UE retries the
postponed procedure, it involves new radio and core network signalling.
Besides, since the user request is postponed, the subscriber experience may be
impacted as well. Thus, the throttling of the message has a cost associated
with it and hence throttling should be done carefully. The source node should
avoid any mechanism resulting in over throttling of the messages. Enforcement
of the overload control while ensuring that good throughput (i.e. measured in
terms of the rate of total number of messages the overloaded node can
successfully process) of the network remains consistent to that when no
overload control is applied, should be one of the prime objective of the
source node.
#### 6.7.2.2 Message processing efficiency at the source node
Enforcement of overload control requires extra logic and extra processing at
the source node. This is an overhead since the source node has to spend its
resources in an activity other than processing of the messages. Hence, the
implementation as well as the processing complexity of the enforcement of the
overload control should not result in poorer efficiency (i.e. the ratio of
message related processing cycles v/s total processing cycles) of the source
node.
#### 6.7.2.3 Self-protection by the target node
The source node enforcing the overload control cannot ensure that the
overloaded target will not receive more messages than what it can handle
during the overload condition, e.g. the \"loss\" algorithm does not guarantee
that the future traffic reaches perfectly that requested by the overloaded
node. Hence the target node must protect itself from the risk of meltdown even
in a network where all the source nodes support the overload control
mechanism. As a part of this self-protection, the target node may reject or
discard the messages which it cannot handle during an overloaded state.
### 6.7.3 Enforcement approaches
#### 6.7.3.1 Hop-by-hop only
##### 6.7.3.1.1 Description
In this approach, the source node enforces the overload control for the next-
hop node only and assumes that the next-hop node does the same towards its
next-hop peer node.
{width="6.689583333333333in" height="1.2840277777777778in"}
Figure 6.7.3.1.1-1: Hop-by-hop enforcement of the overload control
As depicted in the figure 6.7.3.1.1-1, for the messages terminated at the
Target node, the Source node enforces the overload control based on the
overload information of the Intermediate node. For the received messages, the
Intermediate node enforces the overload control based on the overload
information of the Target node. And thus, in this hop-by-hop approach of the
enforcement of the overload control each node applies overload control only
based on the overload condition of its next-hop peer node.
##### 6.7.3.1.2 Example scenario
###### 6.7.3.1.2.1 Overload condition
Consider the following overload condition for our example:
Target_Overload-Metric = 30%
Intermediate_Overload-Metric = 10%
Target_Terminated-Messages = 100
###### 6.7.3.1.2.2 Outcome of the overload control enforcement
Following is the outcome of the hop-by-hop overload control enforcement:
Source-Intermediate_Messages = 90.
Intermediate-Target_Messages = 63; Out of 90 messages, 27 messages are
rejected by the Intermediate node towards the Source node.
##### 6.7.3.1.3 Advantages
Following are the advantages of this approach:
  * It is very simple to implement since each node has to perform the overload control only for its next-hop peer node. Additionally, each node needs to know the overload condition of its next-hop peer only.
\- The intermediate node may never receive more messages than it can handle
during the overloaded condition.
##### 6.7.3.1.4 Drawbacks
Following are the drawbacks of this approach:
\- As depicted in the example scenario in clause 6.7.3.1.2.2, the Intermediate
node rejects 30% of the received messages in order to apply the overload
control towards the Target node. In the EPC architecture, in which the Source
node knows when the message is meant for the Target node, this approach proves
to be very inefficient since it generates extra traffic and rejected responses
between the Source and the Intermediate node and thus extra load for both
nodes.
#### 6.7.3.2 End-to-end only
##### 6.7.3.2.1 Description
In this approach, the source node enforces the overload control based on the
overload information of the target of the message without considering the
overload information of the intermediate node. Since, the source node
considers the overload information of the final target of the message only,
the approach is called \"End-to-end\" enforcement of the overload control.
{width="6.689583333333333in" height="1.2840277777777778in"}
Figure 6.7.3.2.1-1: End-to-end enforcement of the overload control
As depicted in the figure 6.7.3.2.1-1, for the messages terminated at the
Target node, the Source node enforces the overload control based on the
overload information of the Target node, without considering the overload
condition of the Intermediate node. For the received messages, the
Intermediate node does not further enforce any the overload control and hence
does not reject any messages towards the Source node. However, since the
overload condition of the Intermediate nodes are overlooked by the Source
node, there is a risk that the Intermediate node may receive more messages,
terminated at the Target node, than what it can handle during the overload
situation.
##### 6.7.3.2.2 Example scenario
###### 6.7.3.2.2.1 Overload condition
Consider the following overload condition for our example:
Target_Overload-Metric = 30%
Intermediate_Overload-Metric = 40%
Target_Terminated-Messages = 100
###### 6.7.3.2.2.2 Outcome of the overload control enforcement
Following is the outcome of the end-to-end overload control enforcement:
Source-Intermediate_Messages = 70; The Intermediate node indicated that it
could only handle 60 out of 100 messages.
Intermediate-Target_Messages = 60.
##### 6.7.3.2.3 Advantages
Following are the advantages of this approach:
  * It is very simple to implement since each node has to perform the overload control only for the end node (which is the target of the message) while ignoring the overload condition of the intermediate nodes.
##### 6.7.3.2.4 Drawbacks
Following are the drawbacks of this approach:
  * As depicted in the example scenario in clause 6.7.3.2.2.2, the intermediate node may receive more messages than what it could handle since the source node does not consider the overload condition of the intermediate nodes. If the intermediate node cannot handle all of those messages, this approach may end-up rejecting or discarding some of them, which proves to be very inefficient since it generates extra traffic and rejected responses between the Source and the Intermediate node and thus extra load for both nodes.
  * this approach may result in the same or lower throughput than the \"End-to-end and intermediate approach\":
\- it achieves the same throughput when no SGW experiences an higher overload
than the PGW;
it achieves a lower throughput when an SGW experiences an higher overload than
the PGW since it leads to send more traffic than the SGW\'s signalling
capacity.
#### 6.7.3.3 End-to-end and intermediate node
##### 6.7.3.3.1 Description
In this approach, the source node enforces the overload control based on the
overload information of the target of the message as well as the overload
information of the intermediate node. Since, the source node considers the
overload information of the final target as well as the intermediate nodes,
the approach is called \"End-to-end and intermediate node\" enforcement of the
overload control.
{width="6.689583333333333in" height="1.2840277777777778in"}
Figure 6.7.3.3.1-1: End-to-end and intermediate node enforcement of the
overload control
As depicted in the figure 6.7.3.3.1-1, for the messages terminated at the
Target node, the Source node enforces the overload control based on the
overload information of the Target node as well as the overload information of
the Intermediate node. For the received messages, the Intermediate node does
not further enforce any the overload control and hence does not reject any
messages towards the Source node. However, since the overload condition of the
Intermediate nodes are already taken care of, the Intermediate node may not
receive more messages than what it can handle during the overload situation.
Thus, this approach ensures the overload protection of the Target as well as
Intermediate nodes. The Source node may be connected to the same Target node
via multiple different Intermediate nodes. In that case, enforcing of the
overload control, while ensuring the overload condition of all the involved
nodes and also while ensuring higher good throughput of the network by
avoiding the unnecessary throttling of the messages, may put an extra burden
on the Source node to implement a relatively complex algorithm.
##### 6.7.3.3.2 Example scenario
###### 6.7.3.3.2.1 Overload condition
Consider the following network topology for illustrating the enforcement of
the overload control:
{width="6.689583333333333in" height="2.4902777777777776in"}
Figure 6.7.3.3.2.1-1: Example network topology
Consider the following overload condition for the topology above:
Target_Overload-Metric = 30%
IntermediateA_Overload-Metric = 10%
IntermediateB_Overload-Metric = 20%
IntermediateC_Overload-Metric = 40%
Target_Terminated-Messages = 100
IntermediateA-Target_Messages = 20
IntermediateB-Target_Messages = 50
IntermediateC-Target_Messages = 30
###### 6.7.3.3.2.2 Outcome of the overload control enforcement --
Implementation 1
In this implementation, the Source node applies the message throttling based
on the max (Intermediate_Overload-Metric, Target_Overload-Metric) for each of
the path between the Source and the Target node. Each path is considered
independently while performing the message throttling. Following is the
outcome of the overload control enforcement:
Source-IntermediateA_Messages = 14; Message throttling of 30% applied.
Source-IntermediateB_Messages = 35; Message throttling of 30% applied.
Source-IntermediateC_Messages = 18; Message throttling of 40% applied.
Source-Target_Messages = 67; The Target node received only 67 messages while
it could have handled 70 messages in the overloaded condition.
###### 6.7.3.3.2.3 Outcome of the overload control enforcement --
Implementation 2
In this implementation, the Source node first applies the message throttling
based on the Overload-Metric of the Target node, without considering the
individual path between the Source and the Target node. Then, for the messages
that survived the throttling, for each path, the Source node applies
additional throttling only if the Overload-Metric of the corresponding
Intermediate node is higher than that of the Target node. Thus the overload
control requirements of the Target as well as Intermediate nodes are ensured
while also achieving higher good throughput. Following is the outcome of the
overload control enforcement:
Source-IntermediateA_Messages = 17; Message throttling of less than 30% but
more than 10% applied.
Source-IntermediateB_Messages = 35; Message throttling of 30% applied.
Source-IntermediateC_Messages = 18; Message throttling of 40% applied.
Source-Target_Messages = 70.
##### 6.7.3.3.3 Advantages
Following are the advantages of this approach:
  * It ensures the overload control of the target as well as the intermediate nodes. Thus, it ensures that any node does not receive more messages than what it can handle during the overload condition. And hence the possibility of rejection/discard of the received message is minimized in this approach.
  * this approach may result in the same or higher throughput than the \"End-to-end only approach\":
\- it achieves the same throughput when no SGW experiences an higher overload
than the PGW;
\- it achieves a higher throughput when an SGW experiences an higher overload
than the PGW since it adapts the signalling load to the SGW\'s signalling
capacity.
##### 6.7.3.3.4 Drawbacks
Following are the drawbacks of this approach:
  * As depicted in the example scenario in clause 6.7.3.3.2.2 and 6.7.3.3.2.3, this approach may result in multiple implementations for the enforcement of the overload control. Option depicted in clause 6.7.3.3.2.2 may result in simpler implementation while lower good throughput of the network. Option depicted in clause 6.7.3.3.2.2 may result in higher good throughput (when an SGW experiences an higher overload than a PGW while the load/overload status of alternative SGWs is lower such that it allows the MME/SGSN to send more traffic to the PGW via these SGWs to compensate for the traffic not send to the PGW via the first SGW) at the cost of extra complexity in the implementation.
### 6.7.4 Conclusion
Enforcement of overload control between remote GTP-C nodes (i.e. with an
intermediate GTP-C node) shall be performed taking into account the overload
information of both the remote and the intermediate nodes, as specified in
subclause 6.7.3.3.1 (\"End-to-end and intermediate node\").
How to implement the throttling of messages at the source node using the
overload information of both the remote and the intermediate nodes is
implementation specific and will not be standardized. Clauses 6.7.3.3.2.2 and
6.7.3.3.2.3 provide examples of possible implementations.
This applies to the following GTP-C signalling flows:
\- signalling originated by an MME or SGSN to the PGW via the SGW;
\- signalling originated by the PGW to an MME or SGSN via the SGW.
The aspects listed in Table 6.7.4.-1 should also be considered during further
normative work.
Table 6.7.4-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
6.7.1 General Informative 6.7.2.1 Good throughput of the network Normative,
recommendation for the source node 6.7.2.2 Message processing efficiency at
the source node Normative, recommendation for the source node 6.7.2.3 Self-
protection by the target node Normative, recommendation for the target node
6.7.3.3.1 Description of End-to-end and intermediate node enforcement
Normative, requirement for the source node 6.7.3.3.2.2 Outcome of the overload
control enforcement -- Implementation 1 Informative, example algorithm for the
source node
* * *
## 6.8 Behaviours of GTP-C entities
### 6.8.1 Sender of overload information
The mechanism to detect that a node enters overload is implementation
specific. The computation can e.g. include any resource that is limited and
consumed by GTP-C signalling such as CPU utilization, processor interrupts,
I/O throughput, internal message queue depths.
Please refer to clause 6.9.3.1 and 6.9.3.2 for the potential alternatives
depicting the detailed behaviour of the sender while providing the overload
information. Refer to clause 6.9.4 for the exact behaviour of the sender while
providing the overload information.
If a receiving GTP-C entity decides to not process a request, but is still
able to answer, it should answer possibly with a new error cause indicating a
node or an APN overload in addition to signalling overload information. This
avoids the retransmissions of the GTP-C messages; these causes can also be
used e.g. for dedicated metrics and to assess the efficiency of the overload
mechanism.
### 6.8.2 Receiver of overload information
Upon receipt of an overload information in a GTP-C message from a GTP-C entity
with which the support for the overload control mechanism has been enabled,
the receiving GTP-C entity shall update the overload information for this
GTP-C entity, with the parameters received in the overload information. The
receiver shall overwrite the existing information with the newly received
information for the given peer, when the newly provided information is more
recent than the earlier provided information from the same peer. Please refer
to clause 6.9.3.1 and 6.9.3.2 for the potential alternatives depicting the
detailed behaviour of the receiver (based on the corresponding behaviour of
the sender while providing the overload information). Refer to clause 6.9.4
for the exact behaviour of the receiver while handling of the overload
information.
## 6.9 Limit on maximum number of instances
### 6.9.1 Description
One or multiple instances of Overload Control Info IE -- each providing
overload information for a different scope -- can be included by the sender of
the IE. The receiver is required to handle all these instances, from each of
the peer node, by processing, storing and acting upon the same for the
overload control. Higher number of instances in a message would result in
larger size of the message and more processing overhead at the receiver.
Moreover, without any limit, there is a potential risk of a misbehaving sender
providing multiple different instances of the Overload Control Info IE (within
one message as well as across different messages) driving up the resources
utilization of the network as well as the receiver and hence causing the
overload of the receiver. And so, it is necessary to limit the maximum number
of instances of the Overload Control Info IE, a sender can provide, at message
level (i.e. number of different overload information which can be included in
a message) as well at a node level (i.e. number of different overload
information which can be provided across multiple messages by a given node).
### 6.9.2 At message level
By limiting the maximum number of instances of Overload Control Info IE at
message level, we are limiting the sender\'s ability to provide different
overload information within a single message. And hence keeping the need for
future extensibility in mind and allowing enough flexibility for the sender to
provide different overload information within a single message (and hence
allowing for better overload control in the network), it is proposed to fix
the maximum number of instances of Overload Control Info IE, in a message, at
10.
### 6.9.3 At node level
#### 6.9.3.1 Alternative 1 -- Same as at message level & providing full set of
information
##### 6.9.3.1.1 Description
In this alternative, it is proposed to keep the limit of the maximum number of
instances of Overload Control IE at node level same as the maximum number of
instances of Overload Control Info IE at message level and the sender always
includes full set of overload information in any given message towards the
receiver and sets the same identity for all of them, e.g. by using the same
value of \"Overload-Sequence-Number\". The receiver overwrites existing
information of a peer with the newly received overload information (via one or
multiple instances) from the same peer node (when the new information is
different than the old information), e.g. if the receiver has stored \'X\'
instances of the overload information for a peer node, it overwrites those
\'X\' instances with new set of \'Y\' instances received in a message from the
same peer node; where X, Y are any integer number. For providing new value for
one or more instances, the sender includes all the instances of the Overload
Control IE by providing new identity for all of them, e.g. by using a new
value of \"Overload-Sequence-Number\" for all the included instances.
##### 6.9.3.1.2 Advantages
Following are the advantages of this alternative:
\- Very easy to implement for the sender as well as the receiver. While
providing a new overload information (within one or multiple Overload Control
Info IE), the sender includes full set of overload information and marks all
of them as a new information, e.g. by providing new and common value of
\"Overload-Sequence-Number\" for all the instances of Overload Control Info
IE. When a node receives multiple instances of Overload Control Info IE in a
message, it checks if any of the instance has new value or not, e.g. by
comparing the new value and old value of the \"Overload-Sequence-Number\" of
any one of the instance. And if that instance does not represent new
information then all the instances of Overload Control Info IE can be ignored
assuming that no new overload information is provided. Otherwise, all the
existing instances are overwritten with the new instances.
\- Easy to manage the Overload Control instance identifier since it is same
for different scope provided by the same peer.
\- The receiver need not check if individual instance has changed or not since
if one instance is new then all the instances may contain new value.
##### 6.9.3.1.3 Drawbacks
Following are the drawbacks of this alternative:
\- When a sender wants to update partial set of overload information, it has
to include full set of load information in the message. Thus, this alternative
may result in the inclusion of redundant information in a message.
\- The receiver has to replace all the existing instances of the given peer
with the new set of instances even when only one or more instances contain new
value and hence this cause extra processing of information at the receiver for
handling of the information which has not changed.
#### 6.9.3.2 Alternative 2 -- Higher than or same as at message level &
providing partial set of information
##### 6.9.3.2.1 Description
In this alternative, it is proposed to allow higher number for the maximum
number of instances of Overload Control Info IE at node level than the maximum
number of instances of Overload Control Info IE at message level and the
sender can include partial set of overload information in any given message
towards the receiver, e.g. the sender includes overload information for APN1
and APN2 in a one message and for APN3 and APN4 in another message. The
receiver has to ensure to overwrite the newly received overload information
from a peer node with the existing overload information of the same peer node
only when the scope of the new and old information matches, e.g. if the
receiver has stored overload information for APN1, APN2 for a peer node and it
receives new overload information for APN1, APN3, APN4 from the same peer
node, it overwrites only APN1\'s existing overload information with the new
overload information while it stores, additionally, new overload information
for APN3 and APN4. APN2 related load information remains unchanged.
##### 6.9.3.2.2 Advantages
Following are the advantages of this alternative:
  * The sender can provide and/or update partial set of load information in a message. Thus, this alternative avoids the inclusion of redundant information in a message. This also minimizes the processing overhead at the receiver.
##### 6.9.3.2.3 Drawbacks
Following are the drawbacks of this alternative:
  * If the sender wants to ensure that full set of overload information is provided to a peer node, the sender may need to remember the exact set of partial overload information which is provided to the same peer node, i.e. if overload information for APN1, APN2 are provided in a given message to a peer node then the sender has to remember this and include overload information for the remaining APNs (e.g. APN3, APN4) in a different message sent to the same peer node. Otherwise, the full set of overload information may not be available at the given receiver.
  * The receiver has to match the scope of the newly received instance of the overload information with all the existing overload information of a peer node and then compare, e.g. using \"Overload-Sequence-Number\", to decide if any of the existing instance of overload information can be overwritten or not. Hence it is complex for the receiver to implement.
\- Adds some implementation complexity for the sender. The sender has to
manage the identifier for each of the scope separately and shall ensure that
the total number of different instances sent across various messages does not
exceed the maximum number of instances limit at the node level.
### 6.9.4 Conclusion
It is concluded to support the following requirements in 3GPP Release 12.
\- Maximum number of different instances of Overload Control Information IE at
message level shall not exceed 10, as described in clause 6.9.2.
\- The sender shall always include the full set of Overload Control
Information IE(s) -- i.e. all the instances representing the overload
information -- in any given message carrying the overload information towards
the receiver, as described in clause 6.9.3.1.
Additionally, following aspects should be considered during normative work.
Table 6.9.4-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
6.9.1 General description of the need to have limit on maximum number of
instances of Overload Control Info at message level and at node level Text
from corresponding subclause for normative work. 6.9.2 Limit on maximum number
of instances at message level Text from corresponding subclause for normative
work. 6.9.3.1 Limit on maximum number of instances at node level Text from
corresponding subclause for normative work.
* * *
# 7 Deployment related considerations
## 7.1 General
In this clause, various deployment related considerations, for the support of
load/overload control mechanism, are investigated.
## 7.2 Discovery of the support of the feature by the peer node
### 7.2.1 Description
In order to apply the overload control mechanisms and exchange the
load/overload control related information, the node may need to be made aware
about the support of the \"GTP-C signalling based Load & Overload Control\"
feature of the peer node. Methods to realize the support of this feature by
the peer node are investigated within this sub clause, keeping in mind the
inter-PLMN and the intra-PLMN related considerations. . Under \"GTP-C
signalling based Load & Overload Control\" mechanism, the support and
advertisement of Load Control and Overload Control may be realized
independently. However, since the PGW\'s information is relayed by the SGW,
the PGW\'s support of Load Control and Overload Control feature depends upon
the corresponding support from the SGW in the network.
### 7.2.2 Across the PLMN boundary
From 3GPP TS 23.401 clause 4.3.7.1a.1
> _Local configuration may allow the VPLMN to decide whether or not to act
> upon Load Information sent from a peer GTP control plane node in the HPLMN_
From 3GPP TS 23.401 clause 4.3.7.1a.2
> _[Based on local policies/configuration]{.underline}, a GTP-C node may
> support Overload Control feature and act upon or ignore Overload control
> Information in the VPLMN when received from HPLMN and in the HPLMN when
> received from VPLMN[.]{.underline}_
Based on the above, it is clear that stage 2 has decided to use local
policies/configuration for the use of the load control feature and overload
control feature across the PLMN boundary.
### 7.2.3 Within the PLMN boundary
#### 7.2.3.1 Alternative 1 -- Using protocol based indicator
In this alternative, it is proposed to use protocol based indicator to
advertise the support of the load/overload control feature to the peer node.
Over the existing signalling, a newly defined indicator is set if the
load/overload control feature is supported by the node. On reception of this
indication, the node can start providing the Load Control Info as well as
Overload Control Info IE to the corresponding peer which has indicated the
support of this feature.
##### 7.2.3.1.1 Advantages
Following are the advantages of this alternative:
\- Feature support is discovered dynamically, as part of the signalling
between the two nodes. Hence, no new local configuration required for
supporting this feature.
##### 7.2.3.1.2 Drawbacks
Following are the drawbacks of this alternative:
  * The node has to remember the support of this feature by each peer node in the network. Even if some peer nodes in the network do not support this feature, the node has to calculate its Load/Overload Control Info IE for the other peer nodes which support this feature. And hence, the only saving is in avoiding the inclusion of these IEs towards the non-supporting peer nodes.
  * During the inter-MME and intra-SGW change related procedures (which does not generate S5 signalling), new signalling may be required to propagate the identity and the feature support of the new MME to the PGW.
  * Based on the issues identified due to the partial support of the feature in the network in clause 8.3, the operator may decide to ensure homogeneous support of the load/overload control feature in the network. In that case, the protocol based indicator to discover the support of the feature by the peer node becomes redundant.
#### 7.2.3.2 Alternative 2 -- Using PLMN-wide local configuration
In this alternative, it is proposed to use PLMN-wide configuration to
activate/disable the support of the feature. The feature is activated when all
or some of the nodes in the PLMN support the feature and hence there is no
need for the node to remember the support of the feature for each peer node in
the network. Since the PLMN-wide configuration is used, the nodes assumed that
all the peer nodes support this feature when the configuration is activated.
##### 7.2.3.2.1 Advantages
Following are the advantages of this alternative:
\- Simple to implement as well as to deploy.
##### 7.2.3.2.2 Drawbacks
Following are the drawbacks of this alternative:
\- If the feature is activated in the network without homogenous support of
this feature, there may be issues as identified in clause 8.3.
### 7.2.4 Conclusion
It is concluded to support PLMN-wide local configuration to discover if the
Load Control and/or Overload Control features are supported by the peer nodes
in the network. The nodes may support the Load Control and/or the Overload
Control feature independently. The corresponding mechanism for discovering the
support of these features across the PLMN boundary and within the PLMN
boundary, as specified in clause 7.2.2 and 7.2.3.2, respectively, shall be
specified in 3GPP Release 12.
Additionally, following aspects should be considered during normative work.
Table 7.2.4-1: Aspects to be considered during further normative work
* * *
Subclause Comments  
7.2.1 General description of the discovery of the support of the feature Text
from corresponding subclause for normative work. 7.2.2 Support of the feature
across the PLMN boundary Text from corresponding subclause for normative work.
7.2.3.2 Support of the feature within the PLMN boundary Text from
corresponding subclause for normative work.
* * *
# 8 Heterogeneous network related considerations
## 8.1 General
The network with non-homogenous support of the \"GTP-C overload control
mechanisms\", such that for a particular interface, some nodes are upgraded
with the support for the feature while the others are not, are termed as
heterogeneous network for this study, irrespective of the PLMN boundary. If
the HPLMN or VPLMN operators do not support or activate the support of this
feature in the whole network then potentially, other mechanisms which do not
rely on the explicit exchange of load/overload control information could be
considered to identify a possible overload condition of the GTP-C peer node.
These are termed as \"Implicit overload control mechanisms\". On the other
hand, if the operator enables the support of this feature in the heterogeneous
network then there could be potential issues related to the handling of the
overload mitigation, e.g. uncontrolled rate of signalling from the nodes which
do not support this feature would require higher throttling from the nodes
which support this feature in order control the overall rate of signalling
towards the target node to avoid its meltdown. And this result into unfair
advantage to the nodes not supporting the feature in the heterogeneous network
compared to the nodes not supporting this feature in the homogeneous network.
These and other related aspects are investigated in this clause.
## 8.2 Implicit overload control mechanisms
When two nodes, interfacing with each other, are not supporting the explicit
exchange of the overload information between them, e.g. nodes across the PLMN
boundary, where the operators have decided to not support the overload control
feature between their PLMNs, explicit overload control cannot be performed
between them. However, this does not prevent the possibility of these nodes
getting into the overload condition. In such a case, supporting and
implementing some simple and basic overload control mechanisms, termed as
\"Implicit overload control\" mechanisms, would be very useful and may prevent
the severe overload of the target network. Following are some principles of
\"Implicit overload control\" mechanisms:
\- Even during the overloaded condition, the node should not drop/discard the
request message without sending the response message. Without the response
message the source node cannot distinguish between the case of request not
reaching the target node due to the network error v/s the case of the
overloaded target node not able to process the received request and send the
response message. This will cause the source node to retransmit the request
message and hence will cause the further overload of the target node. Hence,
when the node decides to not process the message during the overload
condition, it should at least send the reject response message indicating the
cause \"temporary unavailability of the resources\".
\- During the overload condition, the overloaded node may follow the
principles message prioritization, as described in clause 6.4.2, while
selecting the messages to be throttled (and send reject responses) for the
received request messages.
\- Based on the number of reject response and the rate of the reject response
indicating the \"temporary unavailability of the resources\", the source node
should try to judge the overload level of the target node. Additionally, the
source node should apply the principles of message throttling, as described in
clause 6.4, and reduce the overall traffic towards the overloaded target node
and thus allowing the target node to recover from the overload condition.
NOTE: \"Temporary unavailability of the resources\" may be indicated using one
of the existing GTPv2 cause code or via a newly defined cause code. This can
be decided while finalizing the stage 3 details.
## 8.3 Issues in the network with partial support of the feature
### 8.3.1 Partial support of the load control
#### 8.3.1.1 Among the nodes performing the node selection function
Assuming that the support of the load control feature is homogenously deployed
among the nodes advertising the load control, i.e. PGW, SGW, following are the
issues when the support of this feature is not homogenously deployed among the
nodes performing the node selection function, i.e. MME, SGSN, ePDG, TWAN:
\- Non-supporting nodes (i.e. nodes not supporting load control feature) will
use the existing mechanism (i.e. DNS based semi-static information) for the
selection of the PGW/SGW. Since this does not take into account the dynamic
load of the PGW/SGW, it may result in uneven distribution of the sessions,
causing some PGW/SGW(s) to be heavily loaded as compared to other PGW/SGW(s)
in the network, and hence poorly balanced network.
Since the PGW/SGW are supporting the load control feature, the dynamic load
condition of these nodes are advertised towards the nodes performing the node
selection and which are supporting the load control feature. These supporting-
nodes will perform the node selection based on the current load condition of
the PGW/SGW and hence will give preference to the node which is less loaded
over the other node which is comparatively heavily loaded. Thus, the network
imbalance, caused due to non-supporting node, may get rectified to some extent
due to the supporting-node making use of dynamic load information of the
PGW/SGW for the selection of PGW/SGW. And hence resulting in an overall
balanced network.
#### 8.3.1.2 Among the nodes advertising the load control
If any PGW/SGW in the network does not support the load control feature then
its dynamic load information will not be available at the nodes performing the
node selection function. In that case, the selection of that node is performed
purely based on its DNS based semi-static information. And hence the dynamic
load of this node and other nodes in the network cannot be guaranteed to be at
the same level. Thus, this would result in poorly balanced network where some
nodes (e.g. for which the dynamic load information is not available) may be
heavily loaded as compared to the other nodes (e.g. for which the dynamic load
information is available) in the network, resulting in a higher risk of
overload for some nodes in the network.
#### 8.3.1.3 Conclusion
It is concluded to strongly recommend homogenous support of the Load Control
feature across SGWs and PGWs in the network. The consequence of not supporting
this feature homogeneously across SGWs and PGWs would be poor load balancing
of the whole network such that the nodes not supporting the feature may
operate near their maximum capacity while the other nodes have free capacity.
And hence resulting in a network where those nodes, not supporting the
feature, are more vulnerable to be overloaded. The corresponding
recommendation shall be captured in 3GPP Release 12.
It is concluded to recommend homogenous support of the Load Control feature
across MMEs, SGSNs, ePDGs, TWANs. However, the non-homogenous network (i.e.
with only some nodes supporting the feature) may not create any problem and
the network may still be fairly balanced assuming that the network imbalance
caused due to non-supporting node may get rectified due to the supporting-node
making use of dynamic load information while performing the node selection
function. The corresponding recommendation shall be captured in 3GPP Release
12.
### 8.3.2 Partial support of the overload control
#### 8.3.2.1 General
Following are the issues when the support of the overload control feature is
not homogenously deployed in the network:
  * An overloaded node getting messages beyond its acceptable rate of processing even after announcing its overload level. This may cause severe overload of the overloaded node resulting into its breakdown.
  * A non-supporting node getting unfair advantage in sending all the messages to an overloaded node whereas a supporting node is requested to throttle more messages. In summary, the non-supporting node(s) gets unfairly benefited at the expense of the supporting node(s) in the network.
#### 8.3.2.2 Conclusion
It is concluded to strongly recommend homogenous support of the Overload
Control feature across the nodes in the network. The consequences of not
supporting this feature homogeneously in the network are identified in clause
8.3.2.1. The corresponding recommendation and the consequences of not
following the recommendation shall be captured in 3GPP Release 12.
# 9 Conclusions and recommendations
It is agreed to consider the standardization work in 3GPP Rel-12 on various
aspects related to the GTP-C overload control solution based on the following
table of summary:
Table 9-1: Summary of conclusion related to various aspects of GTP-C overload
control solution
* * *
Title Clause Conclusion Impacted Specification GTP-C overload problem 4.0
Corresponding text is to be considered for normative work. 3GPP TS 29.274
Scenarios leading to GTP-C overload 4.1 Corresponding text is to be considered
for normative work. 3GPP TS 29.274 GTP-C signalling based Load and Overload
Control solution 4.2.1, 4.2.2, 4.2.3 Corresponding text is to be considered
for normative work. 3GPP TS 29.274 Applicability of GTP-C load control to 3GPP
and non-3GPP based interfaces 4.2.4.2 Based on clause 4.2.4.2.2. 3GPP TS
29.274 Applicability of GTP-C overload control to 3GPP based interfaces
4.2.4.3.2 Based on clause 4.2.4.3.2.2 3GPP TS 29.274 Applicability of GTP-C
overload control to non-3GPP based interfaces 4.2.4.3.3 Based on clause
4.2.4.3.3.4 3GPP TS 29.274 APN level load control 5.1A Based on clause 5.1A.7
3GPP TS 29.274, 3GPP TS 29.303 Definition of Load Control Information 5.2
Based on clause 5.2.3 3GPP TS 29.274 Frequency of inclusion of Load Control
Information 5.3 Based on clause 5.3.6 3GPP TS 29.274 Interaction between Load
Control Information and the information received from DNS 5.4.2 Based on
clause 5.4.2.3 3GPP TS 29.303 Maximum number of instances of Load Control
Information at message level and at node level 5.5 Based on clause 5.5.4 3GPP
TS 29.274 Definition of Overload Control Information 6.2 Based on clause 6.2.3
3GPP TS 29.274 Frequency of inclusion of Overload Control Information 6.3
Based on clause 6.3.6 3GPP TS 29.274 Message throttling algorithms 6.4.1 Based
on 6.4.1.3 3GPP TS 29.274 Message prioritization 6.4.2 Based on 6.4.3 3GPP TS
29.274 Propagating MME/SGSN identity to the PGW 6.5.2 Based on 6.5.2.6 3GPP TS
29.274 Propagating target overload information of the MME/SGSN to the PGW
6.5.3 Based on 6.5.3.5 3GPP TS 29.274 Interaction between DDN throttling and
overload control information 6.6.1 Based on 6.6.1.2 3GPP TS 29.274 Interaction
between APN back-off timer and overload control information 6.6.2 Based on
6.6.2.4 3GPP TS 29.274 Enforcement of overload control 6.7 Based on 6.7.4 3GPP
TS 29.274 Behaviour of the sender of overload information 6.8.1 A New
rejection cause code, which indicates that the message is rejected due to the
overload condition, is required to be defined. This cause code may be used in
the network where the GTP-C overload control mechanism is activated and the
overloaded node cannot process the message due to the overload condition. 3GPP
TS 29.274 Maximum number of instances of Overload Control Information at
message level and at node level 6.9 Based on clause 6.9.4 3GPP TS 29.274
Discovery of the support of the feature 7.2 Based on clause 7.2.4 3GPP TS
29.274 Implicit overload control mechanism 8.2 Corresponding text is to be
considered for normative work. It was additionally decided to use one of the
existing rejection cause code for indicating \"Temporary unavailability of the
resources\". 3GPP TS 29.274 Partial support of the Load Control feature in the
network 8.3.1 Based on 8.3.1.3 3GPP TS 29.274 Partial support of the Overload
Control feature in the network 8.3.2 Based on 8.3.2.2 3GPP TS 29.274
Associating the Load/Overload Control Info with the identity of the sender
N.A. The mechanism for associating the received Load/Overload Control Info IE
and the identity of the sender needs to be defined. 3GPP TS 29.274
* * *
#