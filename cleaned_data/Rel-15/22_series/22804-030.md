# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
5G systems will extend mobile communication services beyond mobile telephony,
mobile broadband, and massive machine-type communication into new application
domains, so-called vertical domains, with special requirements toward
communication services. Communication for automation in vertical domains comes
with demanding requirements―high availability, high reliability, low latency,
and, in some cases, high-accuracy positioning.
Communication for automation in vertical domains has to support the
applications for production in the corresponding vertical domain, for
instance, industrial automation and energy automation, but also
transportation. This focus―together with regulations specific for vertical
domains―have led to tailored communication concepts in vertical domains such
as dependable communication as well as specific security standards and
mechanisms. The present document provides an overview on these concepts, in
order to foster a common understanding used in communication for automation in
vertical domains and 5G communication services, as well as the inference of a
common terminology.
Many vertical use cases have been analysed by 3GPP and resulted in several
vertical communication requirements that are already part of TS 22.261 [3].
Besides the already specified KPIs for latency, jitter, reliability,
communication service availability, and data rate, other general vertical
communication requirements need to be transposed into potential service
requirements for 5G systems.
Communication for automation in vertical domains may take place in private
networks. Network monitoring interfaces are necessary in order to assure
network operation (SLAs). Multiple verticals and users might be using the same
5G communication network (multi-tenancy). Furthermore, vertical domains have
their own security standards, implementations, and vertical-domain specific
regulations, leading to stage-1 potential security requirements. Finally,
integration between 5G communication systems and already existing
communication networks of vertical domains is required.
Several missing representative vertical use cases for communication in
automation in vertical domains are described in the present document and used
for the derivation of further stage-1 potential requirements. These use cases
focus on low latency, high reliability, and high communication service
availability. Examples are automated guided vehicles and rail-bound mass
transit (subways and suburban rail).
The present document provides new use cases and stage 1 potential requirements
that have not yet been covered in previous stage 1 specifications. It is based
on input from relevant stakeholders of the respective vertical domains.
# 1 Scope
The present document focuses on 5G communication for automation in vertical
domains. This is communication that is involved in the production of and
working on work pieces and goods, and/or the delivery of services in the
physical world. Such communication often necessitates low latency, high
reliability, and high communication service availability. Nevertheless, other
types of communication are also possible in this area. Moreover,
communications with low latency, high reliability, and high communication
service availability, and other, not so demanding communication services, may
run in parallel on the same 5G infrastructure.
The present document identifies stage 1 potential requirements for 5G
communication for automation in vertical domains. The potential requirements
are derived from different sources:
  * existing work on dependable communication as used in vertical domains; see, for instance, IEC 61907 [2].
  * use cases describing network operation in vertical domains with, for instance, common usage of the network (multi-tenancy) and network monitoring for assurance of service level agreements.
  * security mechanisms already used in vertical domains; supporting the specific security requirements of vertical domains.
  * new (additional to already existing stage-1 work), representative use cases in different vertical domains based on input from relevant vertical interest organisations and other stakeholders.
Furthermore, the present document provides an overview of relevant
communication concepts for automation in vertical domains from the point of
view of 5G systems. This overview is provided in order to facilitate the
mapping between communication for automation in vertical domains and
communication in 5G systems.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
> [2] IEC 61907, \"Communication network dependability engineering\", 2009.
>
> [3] 3GPP TS 22.261: \"Service requirements for the 5G system\".
[4] **IEC 62290-1: \"** Railway applications - Urban guided transport
management and command/control systems - Part 1: System principles and
fundamental concepts\".
[5] TTA TTAK KO-06.0-369: \"Functional Requirements for LTE-Based
Communication System\", Oct. 2014.
[6] J. Kim, S. W. Choi, Y.-S. Song, and Y.-K. Kim, \"Automatic train control
over LTE: Design and performance evaluation\", IEEE Comm. Mag., Oct. 2015.
[7] IEEE 1474.1-2004: \"IEEE Standard for Communications-Based Train Control
(CBTC) Performance and Functional Requirements\".
[8] (Void)
[9] Deterministic Networking, bas-usecase-detnet, IETF, October 2015
[10] Richard C. Dorf and Robert H. Bishop, \"Modern Control Systems\",
Pearson, 12th Edition , 2011.
[11] Ernie Hayden, Michael Assante, and Tim Conway, \"An Abbreviated History
of Automation & Industrial Controls Systems and Cybersecurity\", SANS
Institute, https://ics.sans.org/media/An-Abbreviated-History-of-Automation-
and-ICS-Cybersecurity.pdf {accessed: 2017-05-23}, 2014.
[12] Gartner, \"Gartner IT
[13] ITU Study Group Q.22/1, \"Report on Best Practices for a National
Approach to Cybersecurity: A Management Framework for Organizing National
Cybersecurity Efforts\", 2008.
[14] Gartner, \"Gartner Says the Worlds of IT and Operational Technology Are
Converging\", http://www.gartner.com/newsroom/id/1590814 {accessed:
2017-08-01}, 2011.
[15] ITU-T X.200, \"REFERENCE MODEL OF OPEN SYSTEMS INTERCONNECTION FOR CCITT
APPLICATIONS\", 1988.
[16] Lutz Rauchhaupt, Elke Hintze, André Gnad, \"Über die Bewertung der
Zuverlässigkeit industrieller Funklösungen - Teil 1 Theoretische Grundlagen\",
atp 49 Heft 3, S. 38, 2007.
[17] BZKI, \"Aspects of dependability assessment in RWCI\" (German Version),
June 2017.
[18] BZKI, \"Requirement Profiles in RWCI\" (German Version), 2016 .
[19] IEC 62657-1: \"Industrial communication networks -- Wireless
communication networks -- Part 1: Wireless communication requirements and
spectrum considerations\".
[20] **ISO 10795: \"** Space systems --- Programme management and quality ---
Vocabulary\", 2011.
[21] 20/53/EU: \"Directive 2014/53/EU of the European Parliament and of the
Council of 16 April 2014 on the harmonisation of the laws of the Member States
relating to the making available on the market of radio equipment and
repealing Directive 1999/5/EC\", http://eur-lex.europa.eu/legal-
content/EN/TXT/HTML/?uri=CELEX:32014L0053&from=DE, {accessed: 2017-08-03}.
[22] ITU-T G.1000: \"Communications quality of service: A framework and
definitions\", 2001.
[23] NIST, \"Framework for Cyber-Physical Systems\", Release 1.0,
https://s3.amazonaws.com/nist-
sgcps/cpspwg/files/pwgglobal/CPS_PWG_Framework_for_Cyber_Physical_Systems_Release_1_0Final.pdf
{accessed: 2017-08-04}, 2016.
[24] IEC 62673: \"Methodology for communication network dependability
assessment and assurance\".
[25] IEC 61784-3: \"Industrial communication networks -- profiles -- part 3:
functional fieldbuses -- general rules and profile definitions\", 2016.
[26] IEC 62439-1: \"Industrial communication networks -- High availability
automation networks - Part 1: General concepts and calculation methods\".
[27] H. Kagermann, W. Wahlster, and J. Helbig, \"Recommendations for
implementing the strategic initiative INDUSTRIE 4.0\", Final report of the
Industrie 4.0 working group, acatech -- National Academy of Science and
Engineering, Munich, April 2013
[28] IEC 61158: \"Industrial communication networks -- fieldbus
specification\", 2014.
[29] IEC 61784: \"Industrial communication networks -- profiles\", 2014.
[30] R. Zurawski, \"Industrial communication technology handbook\", second
edition, CRC Press, September 2017.
[31] IEC 61508: \"Functional safety of electrical/electronic/programmable
electronic safety-related systems\", 2010.
[32] IEC 62061: \"Safety of machinery - Functional safety of safety-related
electrical, electronic and programmable electronic control systems\" (IEC
62061:2005 + A1:2012).
[33] A. Manjeshwar and D.P. Agrawal, \"TEEN: a routing protocol for enhanced
efficiency in wireless sensor networks.\", In Proceedings 15th International
Parallel and Distributed Processing Symposium. IPDPS 2001. IEEE Comput. Soc.
https://doi.org/10.1109/ipdps.2001.925197.
[34] M. A. Mahmood, W. K. G. Seah, and I. Welch, I., \"Reliability in wireless
sensor networks: A survey and challenges ahead.\", Computer Networks, 79, 166
--187, 2015. https://doi.org/10.1016/j.comnet.2014.12.016
[35] Bosch Connected Devices and Solutions GmbH, \"Cross Domain Development
Kit \| XDK, Bosch XDK110 datasheet\", April 2017. https://xdk.bosch-
connectivity.com/hardware [Revised July 2017].
[36] B. E. Keiser and E. Strange, \"Pulse Code Modulation.\", in Digital
Telephony and Network Integration (pp. 19--34). Springer Netherlands.
https://doi.org/10.1007/978-94-015-7177-7_3, 1985.
[37] ISO/IED 13818-3: \"Information technology -- Generic coding of moving
pictures and associated audio information -- Part 3: Audio\", 1998.
[38] Zhao, G. (2011). Wireless sensor networks for industrial process
monitoring and control: A survey. _Network Protocols and Algorithms_ , _3_(1),
46-63.
[39] G. Ullrich, \"Automated Guided Vehicle Systems - A Primer with Practical
Applications.\", ISBN 978-3-662-44813-7, Springer Berlin Heidelberg, 2015.
[40] IEC 62443: \"Industrial communication networks - Network and system
security\", https://webstore.iec.ch/searchform&q=IEC%2062443.
[41] IEC 62443-3-2: \"Security for industrial automation and control systems -
Part 3-2: Security risk assessment and system design\", in progress.
[42] IEC 62443-3-3: \"Industrial communication networks - Network and system
security - Part 3-3: System security requirements and security levels\", 2013.
[43] ZVEI, \"Working ZVEI Whitepaper - Security Interessen für die 5G
Standardisierung\", (in German),
https://www.zvei.org/fileadmin/user_upload/Themen/Cybersicherheit/5G/Working_ZVEI_Whitepaper_Security_Interessen_bei_5G_OEV.pdf
(accessed: 2017-07-25), 2017.
[44] Avizienis, Algirdas, et al. \"Basic concepts and taxonomy of dependable
and secure computing.\" _Dependable and Secure Computing, IEEE Transactions
on_ 1.1, pp. 11-33, 2004.
[45] DKE-IEV, German online edition of the International Electrotechnical
Vocabulary (IEV), DKE - Deutsche Kommission Elektrotechnik Elektronik
Informationstechnik in DIN und VDE, (German Commission Electrical Technology
Electronics Information Technology), https://www2.dke.de/de/Online-
Service/DKE-IEV/Seiten/IEV-Woerterbuch.aspx
[46] RESERVE project, Deliverable D1.3, ICT Requirements, September 2017\
http://www.re-serve.eu/files/reserve/Content/Deliverables/D1.3.pdf
[47] RESERVE project, Deliverable D1.2, Energy System Requirements, September
2017\ http://www.re-serve.eu/files/reserve/Content/Deliverables/D1.2.pdf
[48] Feuchtinger, Ulrich, Frank, Reinhard, Riedl, Johannes, and Eger, Kolja,
**_Smart Communications for Smart Grids_** , Siemens White Paper, 2012.
[49] DKE-IEV, German online edition of the International Electrotechnical
Vocabulary (IEV), DKE - Deutsche Kommission Elektrotechnik Elektronik
Informationstechnik in DIN und VDE, (German Commission Electrical Technology
Electronics Information Technology), [https://www2.dke.de/de/Online-
Service/DKE-IEV/Seiten/IEV-Woerterbuch.aspx]{.underline}
...
[x] \ \[ ([up to and
including]{yyyy[-mm]\|V\}[onwards])]: \"\\".
# 3 Definitions, symbols and abbreviations
Editor's note: Delete from the heading below those words which are not
applicable. Clause numbering depends on applicability and should be renumbered
accordingly.
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**aggregator:** Service provider managing a system of electric generation
units, storage systems, and load (consumers), with independent control and
customer support in its own coverage area
**automation:** the automatic operation or control of a process, device, or
system
NOTE 1: This definition is based on [10].
**communication service availability** : percentage value of the amount of
time the end-to-end communication service is delivered according to an agreed
QoS, divided by the amount of time the system is expected to deliver the end-
to-end service according to the specification in a specific area.
NOTE 2: The end point in \"end-to-end\" is assumed to be the communication
service interface.
NOTE 3: The communication service is considered unavailable if it does not
meet the pertinent QoS requirements. If availability is one of these
requirements, the following rule applies: the system is considered unavailable
in case an expected message is not received within a specified time, which, at
minimum, is the sum of end-to-end latency, jitter, and survival time.
NOTE 4: This definition was taken from subclause 3.1 in [3].
**communication service reliability:** ability of the communication service to
perform as required for a given time interval, under given conditions
NOTE 5: Given conditions would include aspects that affect reliability, such
as: mode of operation, stress levels, and environmental conditions.
NOTE 6: Reliability may be quantified using appropriate measures such as
meantime to failure, or the probability of no failure within a specified
period of time.
NOTE 7: This definition is based on [2].
**end-to-end latency:** the time that takes to transfer a given piece of
information from a source to a destination, measured at the communication
interface, from the moment it is transmitted by the source to the moment it is
successfully received at the destination
NOTE 8: This definition was taken from subclause 3.1 in [3].
**IoT device:** a type of UE which is dedicated for a set of specific use
cases or services and which is allowed to make use of certain features
restricted to this type of UEs.
NOTE 9: An IoT device may be optimised for the specific needs of services and
application being executed (e.g., smart home/city, smart utilities, e-Health
and smart wearables). Some IoT devices are not intended for human type
communications.
NOTE 10: This definition was taken from subclause 3.1 in [3].
**jitter:** the variation of a time parameter relative to a reference or
target value
NOTE 11: In this document, jitter is used for describing the variation of end-
to-end latency, and update time.
**microgrid:** Local grid, with own energy generation and power consumption;
limited geographical area, typical example: power network for a university
campus
**renewable generators:** photovoltaic panels or wind turbines; energy
generation unit
**transmission time:** the interval from a start event at the reference
interface of a source until a stop event of the same transmission at the
reference interface of a target
NOTE 12: Depending on the type of reference interface, the start event can be
the transfer of the first bit of user data, the first byte, or a trigger event
at a process interface. Respectively, the stop event can be the last bit of
user data, the last byte or a trigger event of a process interface.
NOTE 13: This definition is based on [19].
**update time:** the interval from a start event at the reference interface of
a target until a following stop event at the same reference interface
NOTE 14: Depending on the type of reference interface, the start event can be
the transfer of the last bit of user data, the last byte, or a trigger event
at the process interface of a consumer.
NOTE 15: The stop event can be the last bit of user data, the last byte, or a
trigger event of a process interface that can be referred to the following
successful transmission of the same source
NOTE 16: This definition is based on [19].
**up state:** state of being able to perform as required
NOTE 17: This definition is based on entry IEV 192-02-01 in [49].
**up time:** time interval for which the item is in an up state
NOTE 18: This definition is based on entry IEV 192-02-02 in [49].
**user equipment:** An equipment that allows a user access to network services
via 3GPP and/or non-3GPP accesses.
NOTE 19: This definition was taken from subclause 3.1 in [3].
**vertical domain:** a particular industry or group of enterprises in which
similar products or services are developed, produced, and provided
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
_T_ ~cycle~ Cycle time of a cyclic data communication service
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
5G Fifth Generation
AGV Automated Guided Vehicle
AR Augmented Reality
A/V Audio/Video
C2C Control-to-Control
CAN Controller Area Network
CCTV Closed Circuit Television
DMS Distribution Management System
DSO Distribution System Operator
ECU Engine Control Unit
EMS Energy Management System
ERP Enterprise Resource Planning
FR Foundational Requirement
GoA Grade of Automation
HD High definition
HGV Heavy Good Vehicle
HMI Human-Machine Interface
IEEE Institute of Electrical and Electronics Engineers
IEM In-Ear Monitor
INV Inverter, electronic power converter, with co-located communications and
data processing unit
IoT Internet of Things
IPsec IP Security
IT Information Technology
µDC Micro Data Center
MEC Multi-Access Edge Computing
MES Manufacturing Execution System
ML Machine Learning
MNO Mobile Network Operator
MPSC Manufactured Product as a Smart Client
MTTC Mass Transit Train Control
OT Operational Technology
PA Public Address
PER Packet Error Ratio
PLC Programmable Logic Controller
PMSE Programme Making and Special Events
PTP Precision Time Protocol
RE Requirement Enhancement
RES Renewable Energy Sources
SL Security Level
SR Security Requirement
TSO Transmission System Operator
VLAN Virtual LAN
VR Virtual Reality
WSN Wireless Sensor Network
# 4 Overview
## 4.1 Background
Editor's Note: Overall purpose of this chapter. Brief discussion of how
wireless communication is used in verticals.
Clause 4 serves two main purposes. One, in Subclause 4.2, it discusses the
concept of a vertical domain and it provides an overview of the vertical
domains addressed in the present document. Two, in Subclause, 4.3, it provides
a short overview of salient concepts developed in automation research and
standardisation. In Subclause 4.3.1 automation control paradigms and related
data flows are discussed. In Subclause 4.3.2, models for describing
communication in automation are introduced and the introduced concepts and
parameters are mapped on 5G concepts. In Subclause 4.3.3, the paramount
automation system attribute---i.e dependability---is discussed and
implications for 5G communication systems are identified. In Subclause 4.3.4
these dependability attributes are described from a 5G communication service
perspective.
## 4.2 Vertical Domains
Editor's Note: Short text on what is a vertical domain. Text to be proposed by
rapporteur. Overview of the vertical domains addressed in this study (to be
written once Clause 5 stable).
## 4.3 Automation
### 4.3.1 Data flows in automation
#### 4.3.1.1 Introduction
The term automation stands for the control of processes, devices, or systems
in vertical domains by automatic means [10]. Note that a process always
includes physical entities and their attributes. By providing particular input
to a process, one tries to generate a particular output (see Figure
4.3.1.1-1).
{width="3.5868055555555554in" height="0.8958333333333334in"}
Figure 4.3.1.1-1: Process to be controlled [10]. Note that the process always
has a physical component. Example: input = heat, process = chemical reaction
in a gas. In this example, the output is the chemical reaction products.
Examples for such processes are chemical processes in the chemical industry,
the control of subways, and factory automation with industrial robots. Example
(1): the process in question is a chemical reaction and the input is heat; the
yield of the chemical reaction, i.e. the output, varies with said heat.
Example (2): for a subway, the input can be electrical energy, the process the
acceleration of the subway, and the output reaching the cruising velocity of
the subway.
The technology related to automation is referred to as operational technology,
which \"is hardware and software that detects or causes a change through the
direct monitoring and/or control of physical devices, processes and events in
the enterprise.\" [12]. An overview of operational-technology devices can be
found elsewhere in the literature **[11]. The last decade has seen an
increased integration of operational and information technology [14].**
Automation technology can be used in private settings such as factories, but
it is also used in critical infrastructure such as the electricity grid, civil
aviation, public transport, etc. [13].
In Subclause 4.3.1.2 we introduce the main type of systems used in automation,
i.e. control systems. In Subclause 4.3.1.3, we introduce the most common
patterns of control systems. In Sucblause 4.3.1.4, we discuss the
communication attributes of an automation system. Finally, in Subclause
4.3.1.5, we present the communication patterns entailed by automation systems.
#### 4.3.1.2 Control systems
As outlined in Subclause 4.3.1.1, automation is about controlling processes by
aid of automated means. This objective is accomplished by the use of control
systems. \"A **control system** is an interconnection of components forming a
system configuration that will provide a desired [process] response.\" [10].
Four main control functions can be distinguished [11]:
  * Measure: obtain values from sensors and feed these values as input to a process and/or provide these values as output, for instance to a human user;
  * Compare: evaluate measured values and compare to process design values;
  * Compute: calculate, for instance, current error, historic error, future error etc.;
  * Correct or control: adjust the process.
The four functions above are typically performed by four elements [11]:
  * Sensor: device capable of measuring various physical properties;
  * Transmitter: device that converts measurements from a sensor and sends the signal;
  * Controller: provides the logic and control instructions for the process;
  * Actuator: changes the state of the environment; here the process.
NOTE: Frequently, the combination of sensor and transmitter is referred to as
a sensor. This is the style we are adhering to in the remainder of the present
document.
There are three common patterns of automation. One is open-loop control, the
second is feedback or closed-loop control, and the third is sequence control
[10] [11]. We discuss all three patterns in more detail in Subclause 4.3.1.3.
#### 4.3.1.3 Activity patterns in automation
##### 4.3.1.3.1 Open-loop control
The salient aspect of open-loop control is the lack of output control; when
providing desired output responses to an actuator, it is assumed that the
output of the influenced process is predetermined and within an acceptable
range. Figure 4.3.1.3.1-1 depicts an open-loop control system.
Figure 4.3.1.3.1-1: Open-loop control system [10].
This kind of control loop works if the influences of the environment on
process and actuator are negligible. Also, this kind of control is applied in
case unwanted output can be tolerated. For instance, the damage done by an
electric toaster (open-loop control system), i.e. slightly burning a slice of
bread, is usually assumed to be negligible. In this case, the desired output
response is the crispiness of the toasted bread. The response can, for
instance, be chosen by turning a dial on the toaster. The actuator is the
heater in the toaster. The dial sets a certain energy level and/or toasting
time. When activated, the heater generates heat, which increases the
crispiness of the inserted slice of bread over time. This is the process. The
output is the toast itself.
##### 4.3.1.3.2 Closed-Loop Control
Closed-loop control enables the manipulation of processes even if the
environment influences the process or the performance of the actuator changes
over time. This type of control is realised by sensing the process output and
by feeding these measurements back into a controller. **Figure 4.3.1.3.2-1
depicts such a system.**
Figure 4.3.1.3.2-1: Closed-loop control system [10].
\"In contrast to an open-loop control system, a closed-loop control system
utilizes [measurements] of the actual output to compare the actual output with
the desired output response.\" [10]. \"An example of a closed-loop control
system is a person steering an automobile (assuming his or her eyes are open)
by looking at the auto's location on the road and making the appropriate
adjustments.\" [10].
##### 4.3.1.3.3 Sequence Control
Sequence control may either step through a fixed sequence or it employs logic
that performs different actions based on various system states and system
input [10]. Sequence control can be seen as an extension of both open-loop and
closed-loop control, but instead of achieving only one output instance, an
entire sequence of output instances can be produced. An example of sequence
control is controlling an elevator. Based at what floor it currently resides,
to what floor it is summoned, and to what floor it is directed, different
kinds of control actions and actuations are taken. Note that although the name
\"sequence control\" seems to imply a pre-programmed sequence of desired
output responses, the elevator shows that event-based control can also be
realised with sequence control.
#### 4.3.1.4 Communication attributes
##### 4.3.1.4.1 Introduction
**Communication in automation can be characterised by two main attributes:
periodicity and determinism.**
##### 4.3.1.4.2 Periodicity
**In terms of periodicity, it is possible to send messages periodically or
aperiodically.**
**Periodically means that a transmission interval is repeated. For example, a
transmission occurs every 15 ms. Reasons for a periodical transmission can be
the periodic update of a position or the repeated monitoring of a
characteristic parameter. Note that a transmission of a temperature every 15
minutes is a periodical transmission, too. However, most periodic intervals in
communication for automation are rather short. The transmission is started
once and continuous unless a stop command is provided.**
**An aperiodical transmission is, for example, a transmission which is
triggered instantaneously by an event, i.e. events are the trigger of the
transmission. Events are defined by the control system or by the user. Example
events are:**
  * **Process events: events that come from the process when thresholds are exceeded or fallen below, e.g., temperature, pressure, level, etc.**
  * **Diagnostic events: Events that indicate malfunctions of an automation device or module, e.g. power supply defective; short circuit; over temperature; etc.**
  * **Maintenance events: Events based on information that indicates necessary maintenance work to prevent the failure of an automation device.**
**Most events, and especially alarms, are confirmed. In this context, alarms
are messages that inform a controller or operator that an event has occurred,
e.g. an equipment malfunction, process deviation, or other abnormal condition
requiring a response. The receipt of the alarm is acknowledged by the
application that received the alarm. The receipt of the event is usually
confirmed within a short time period. If no acknowledgment is received from
the target application after a pre-set time, the so-called monitoring time,
has elapsed, the alarm is sent again after a preset time.**
##### 4.3.1.4.3 Determinism
**Determinism refers to whether the delay between transmission of a message
and receipt of the message at the destination address is stable (within
bounds). Usually, communication is called deterministic if it is bounded by a
given threshold for the latency/transmission time.**
##### 4.3.1.4.4 Control systems and related traffic patterns
**There is no straight-forward, one-to-one mapping between the type of control
and the communication pattern. However, there are preferences. Open-loop
control is characterised by one or many messages sent to the actuator. These
can be sent in a periodic or an aperiodic pattern. However, the communication
means used need to be deterministic since typically an activity response from
the receiver and/or the receiving application is expected. For instance, for
the toaster example in Subclause 4.3.1.3.1, the waiting time between
activating the toaster and commencement of heating the slice of bread should
not be arbitrary.**
**Closed-loop control produces both periodic and aperiodic communication
patterns. If, for instance, sensor output is only generated when a threshold,
e.g. a preset room temperature is exceeded, the timing of the message
transmitted to the controller is regulated by the process and not a preset
timer. Closed-loop control is often used for the control of continuous
processes with tight time-control limits, e.g. the control of a printing
press. In this case, one typically relies on periodic communication patterns.
Note that in both the aperiodic and periodic case, the communication needs to
be deterministic. For instance, for periodic communication, measurements from
adjacent measurement cycles could otherwise arrive at the controller out of
order and not within the time needed to guarantee a stable operation of the
controller.**
**The communication attributes required by sequence control generally depend
on whether the underlying control paradigm is open or closed.**
**Logging of device states, measurements, etc. for maintenance purposes and
such typically entails aperiodic communication patterns. In case the
transmitted logging information can be time stamped by the respective
function, determinism is often not mandatory.**
### 4.3.2 Communication in automation
#### 4.3.2.1 Modelling of communication in automation
##### 4.3.2.1.1 Area of consideration
For our discussion of the communication in automation we apply a definition of
the area of consideration for industrial radio communication that is found
elsewhere in the literature [16]. This definition is depicted in Figure
4.3.2.1.1-1.
Figure 4.3.2.1.1-1: Abstract diagram of the area of consideration for
industrial radio communication. Blue objects: communication system; other
objects: automation application system.
Here, a distributed automation application system is depicted. This system
includes a distributed automation application, which is the aggregation of a
number of automation functions. These can be functions in sensors, measurement
devices, drives, switches, I/O devices, encoders etc. Field bus systems,
industrial Ethernet systems, or wireless communication systems can be used for
connecting the distributed functions. The essential function of these
communication systems is the distribution of messages among the distributed
automation functions. Depending on the objectives, the dependability of the
entire communication system and/or of its devices or its links may be of
interest (more on dependability in Subclause 4.3.3). Communication functions
are realised by the respective hardware and software implementation.
In order for the automation application system to operate, messages need to be
exchanged between spatially distributed application functions. For that
process, messages are exchanged at an interface between the automation
application system and the communication system. This interface is termed the
reference interface. Required and guaranteed values for characteristic
parameters which describe the behavioural properties of the radio
communication system refer to that interface (see Subclause ??).
Note to Editor: Replace \"??\" with a reference to the Subclause \"influencing
parameters\".
These characteristic parameters include dependability parameters of industrial
radio communication, which are defined in [17].
The conditions that influence the behaviour of wireless communication are
framed by the communication requirements of the application (e.g. length of
the message), the characteristics of the communication system (e.g. output
power of a transmitter), and the transmission conditions of the media (e.g.
signal fluctuations caused by multipath propagation).
If a dependability assessment is to be performed, it is necessary---in
accordance with the definition of the concept of dependability---to specify an
asset, its function, and the conditions under which the function is to be
performed. In this context, an asset is for instance a logical link (see
Subclause 4.3.2.1.2.1).
General requirements from the application point of view for the time and
failure behaviour of a communication system are mostly related to an end-to-
end link. It is assumed in this connection that the behaviour of the link is
representative of the communication system as a whole and of the entire scope
of the application.
##### 4.3.2.1.2 Logical link
###### 4.3.2.1.2.1 Nature and function
Starting with the general approach mentioned above, the logical link can be
regarded as a possible asset within the area of consideration (see Figure
4.3.2.1.2.1-1). The condition under which its functions are to be performed
are vital for the dependability of the automation application system.
Figure 4.3.2.1.2.1-1: The concept of a logical link.
This is the link between a logical end point in a source device and the
logical end point in a target device. Logical end points are elements of the
reference interface, which may group several logical end points together.
The intended function of the logical link is the transmission of a sequence of
messages from a logical source end point to the correct logical target end
point. This is achieved by transforming each message into a form that fosters
error-free transmission. The transmission process includes certain processes,
e.g. repetitions, in order to fulfil the intended function. After
transmission, the message is converted back into a form which is usable by the
application. The message is to be available and correct at the target within a
defined time. The sequence of messages at the target is to be the same as the
sequence at the source.
The functional units which are necessary to fulfil this function are shown in
Figure 4.3.2.1.2.1-2.
**Error! Objects cannot be created from editing field codes.Error! Objects
cannot be created from editing field codes.**
Figure 4.3.2.1.2.1-2: The asset \"logical link\".
The required function can be impaired by various influences, which can lead to
communication errors. Such errors are described elsewhere in the literature
[16][18]. A summary of these errors is provided in Annex B. The occurrence of
one of these errors influences the values of the relevant dependability
parameters of the logical link.
###### 4.3.2.1.2.2 Message transformation
From an implementation point of view, it is hardly possible to identify
communication layers and interfaces in devices in a unified manner, e.g. with
reference to the Open System Interconnection (OSI) model [15]. However, the
implementation of communication functions is mostly split between a higher
communication layer (HCL) and a lower communication layer (LCL), which may
contain different parts of the OSI reference model from implementation to
implementation. Our further discussion is therefore based on a generic
implementation view with HCL and LCL.
The messages to be transmitted for the intended function of a logical link are
defined by strings of characters with a certain semantic. Such a character
string is handed over as user data at the reference interface for
transmission. If the number of characters in a message is too great for it to
be transmitted as a unit, the message can be divided for transmission into
several packets (fragmentation). Figure 4.3.2.1.2.2-2 uses repeated sending as
a hedging method for packet loss (example of an unconfirmed service). The
packets are then passed from a higher communication layer (HCL) to a lower
communication layer (LCL) [Figure 4.3.2.1.2.2-1]. There, a bit steam is
created and handed over to the physical layer (PL). A signal stream
corresponding to the bit stream is transmitted from the physical layer of the
source device to the target device. In the target device, the signal stream
received is converted by physical layer into a bit stream, which is passed to
the lower communication layer. There, packets are formed, handed over by the
lower communication layer to the higher communication layer and grouped
together into a message. Suitable mechanisms (acknowledgement, parallel
transmission through different communication channels/media, multiple
transmissions of identical packets, etc.) can increase the probability of the
message reaching the application correctly when a packet is lost. The loss of
a packet is therefore not to be equated in all cases with the loss of a
message.
Figure 4.3.2.1.2.2-1 shows the transmission of a message that is broken into
two packets. The transmission includes acknowledgements. If no acknowledgement
is received within the required period (packet 2), the packet is transmitted
again (bit stream 2). This is the main difference to, for example, Figure
4.3.2.1.2.2-2, where the packets are repeated from the beginning to protect
against loss or error directly. In Figure 4.3.2.1.2.2-2, a confirmation is not
sent.
{width="6.541666666666667in" height="6.465972222222222in"}
Figure 4.3.2.1.2.2-1: Illustration of message, packet and bit stream
transmission for the example of repeated packet transmission. HCL: higher
communication layer; LCL: lower communication layer; PL: physical layer.
{width="6.43125in" height="5.916666666666667in"}
Figure 4.3.2.1.2.2-2: Illustration of message, packet and bit stream for the
example of unacknowledged repeated transmission. HCL: higher communication
layer; LCL: lower communication layer; PL: physical layer.
##### 4.3.2.1.3 Communication device
The communication devices---together with the physical link---determine the
function and thus the dependability of the logical link (see Figure
4.3.2.1.3-1). The function of the communication devices is the correct sending
and correct receipt of sequences of messages. The methods and algorithms
implemented in the communication devices should take the best possible account
of the transmission conditions during message transmission, and fulfil the
requirements for message transmission as well as possible.
Figure 4.3.2.1.3-1: Asset \"communication device\".
Apart from the methods and algorithms themselves, their implementation in
hardware and software is also of importance. The errors listed in Annex C can
have an impact on dependability.
##### 4.3.2.1.4 Communication system
The communication system as an asset represents a quantity of logical links
whose message transmissions are implemented by a number of wireless devices
via one or more media. The communication system function to be provided
consists in transmitting messages for all the logical links in the distributed
application. This function is to be performed for a defined period, the
operating time of the automation application.
In an automation application system it is paramount that requirements
pertaining to logical links are fulfilled. These requirements and the
conditions can be very different from one case and implementation to the
other. The functions (services and protocols) for individual logical links can
therefore also be different. In spite of these differences, some of the
logical links share communication devices and media. Consequently, the
communication system as a whole is an asset for dependability assessment in
the examination of system and application aspects.
### 4.3.3 Dependable communication
### 4.3.3.1 Introduction
According to ISO, dependability (of an item) is the \"ability to perform as
and when required\" [20]. This is a paramount property of any automation
system. Automation systems that are not dependable can, for instance, be
unsafe or they can exhibit low productivity. Subclause 4.3.3.2 discusses
system dependability in further detail and this information is used to analyse
communication dependability and its implication for 5G systems in Subclause
4.3.3.3.
#### 4.3.3.2 System dependability
Dependability can be broken down into five system properties: reliability,
availability, maintainability, safety, and integrity (see Figure 4.3.3.2-1)
[44].
Figure 4.3.3.2-1: The five facets of system dependability: reliability,
availability, maintainability, safety, and integrity [44].
Definitions for each system property are provided in Table 4.3.3.2-1.
Table 4.3.3.2-1: Definitions of the five system properties into which system
dependability can be broken down (see Figure 4.3.3.2-1) [44].
* * *
**System property** **Defnition** Reliability Continuity of correct operation
Availability Readiness for correct operation Maintainability Ability to
undergo modifications and repairs Safety Absence of catastrophic consequences
on user(s) and environment Integrity Absence of improper system alterations
* * *
Availability indicates whether the system is ready for use at a given time.
This system property is typically quantified by the percentage of time during
which a system operates correctly. Reliability indicates how long correct
operation continues. This system property is typically defined as the (mean)
time between failures. Let us illustrate both properties with and example. In
this example the system has an availability of 99,99%. This implies that its
unavailability is 0,001%, or 53 min on average per year. If the system fails
on average thrice a year then reliability, quantified as the mean time between
failures, is four months.
Availability and reliability are closely related to the productivity of a
system. A system featuring a low availability is rarely ready for operation
and is thus characterised by low productivity. If the system reliability is
low, i.e. the time between failures is short, the system comes often to a
halt, which contravenes continuous productivity.
#### 4.3.3.3 Definition of communication dependability and its implications
##### 4.3.3.3.1 Introduction
A composite system, where every subsystem is instrumental to the operation of
the composite system, is not dependable if any of the subsystems is
undependable. This has the following implications for the use of communication
systems in general, and 5G systems in particular. Figure 4.3.3.3.1-1 depicts a
generic, distributed automation system, where the automation functions
interact via a communication system.
Figure 4.3.3.3.1-1: Example of a distributed automation system consisting of
automation functions and a communication network.
In this example, all three subsystems, i.e. the automation functions **and**
the communication network need to be dependable for the automation system to
be dependable.
Communication dependability is the property of a dependable communication
system. According to IEC 61907, network dependability is the \"ability to
perform as and when required to meet specified communication and operational
requirements\" [2]. This definition largely agrees with 3GPP's own definition:
\"A performance criterion that describes the degree of certainty (or surety)
with which a function is performed regardless of speed or accuracy, but within
a given observational interval\" [1]. What does communication dependability
imply in praxis from the vantage point of the automation functions? We address
this for each of the five system properties in Figure 4.3.3.2-1.
##### 4.3.3.3.2 Reliability
According to IEC 61907, network reliability is the \"ability to perform as
required for a given time interval, under given conditions\" [2].
NOTE: Given conditions \"would include aspects that affect reliability, such
as: mode of operation, stress levels, environmental conditions\" [2].
Automation functions need highly reliable communication. As a rule of thumb
the more infrequent communication is unavailable the better.
Note that reliability in the context of dependability has a different meaning
than employed in TS 22.261, which defines it as the \"percentage value of the
amount of sent network layer packets successfully delivered to a given node
within the time constraint required by the targeted service, divided by the
total number of sent network layer packets\" [3]. This definition is more akin
to the definition of network availability (see Subclause 4.3.3.3.3) and it
focuses on the inner working of the network rather than the end-to-end
experience of functions consuming the network\'s communication capabilities.
In order to avoid confusion, reliability of a communication system is
henceforth referred to as communication service reliability. We discuss this
in more detail in Subclause 4.3.4.
##### 4.3.3.3.3 Availability
According to IEC 61907, network availability is the \"ability to be in a state
to perform as and when required, under given conditions, assuming that the
necessary external resources are provided\" [2]. Note that given conditions
\"would include aspects that affect reliability, maintainability and
maintenance support performance\" [2]. It is important to point out that a
communication network that does not meet the communication requirements of the
automation functions, e.g. a maximum end-to-end latency, are considered to be
unavailable.
##### 4.3.3.3.4 Maintainability
According to IEC 61907, network maintainability is the \"ability to be
retained in, or restored to, a state in which it can perform as required under
given conditions of use and maintenance\" [2]. Note that given conditions of
maintenance \"include the procedures and resources to be used\" [2].
\"Maintainability may be quantified using such measures as, mean time to
restoration, or the probability of restoration within a specified period of
time\" [2]. Subclause 4.3.4 discusses what maintainability implies for a
dependable communication service.
##### 4.3.3.3.5 Safety
As introduced in Subclause 4.3.3.2, safety stands for the absence of
catastrophic consequences on user(s) and environment. For a distributed
automation system this implies that neither the automation functions including
their physical embodiment, nor the processes, nor the environment should be
damaged by the communication system. This communication system property is---
for instance---addressed through regulations such as directive 2014/53/EU
[21]. Note that in most automation implementations the safety of the
communication systems can be treated separately and that the overall safety of
the distributed automation system is addressed by automation functions such as
functional-safety mechanisms.
##### 4.3.3.3.6 Integrity
According to IEC 61907, network integrity is the \"ability to ensure that the
data throughput contents are not contaminated, corrupted, lost or altered
between transmission and reception\" [2]. Note that this communication system
property is---in the communication network community---seen as an atomic
property of information security in communication systems. More on this in
Subclause 6.1.
##### 4.3.3.3.7 Implications for 5G systems
In order to be suitable for automation in vertical domains, 5G systems need to
be dependable, i.e. they need to come with the system properties in Subclause
4.3.3.3.2 to Subclause 4.3.3.3.6. What particular requirements each property
needs to meet depends on the particularities of the domain and the use case.
More on this in Clause 5. Subclause 4.3.4 addresses what the request for
communication dependability implies for communication services provided by 5G
systems.
### 4.3.4 Dependable communication service
#### 4.3.4.1 Introduction
This section discusses important criteria that are used for evaluating
dependable 5G communication services from an end-to-end perspective.
Dependability and its attributes are addressed in Subclause 4.3.3.
#### 4.3.4.2 Network dependability
Network dependability can be classified as follows [2]:
  * Needed dependability: The end-users' network dependability requirements;
  * Offered dependability: The service provider's offerings of network dependability (or planned/targeted network dependability);
  * Achieved dependability: The dependability achieved or delivered by the service provider;
  * Perceived dependability: The dependability perceived/experienced by the end-users.
The end-users' \"dependability needs are the primary source of information for
establishing dependability requirements." [2]. Note that in this framework one
differentiates between needed, offered, achieved, and perceived/experienced
dependability. This is in line with concepts developed by the ITU-T for
quality in of service [22]. The ITU-T differentiates between the customer\'s
QoS requirements and the offered, achieved and perceived QoS.
#### 4.3.4.3 Network serviceability
When communication functionalities are offered as services, dependability is
contingent on what is referred to as serviceability. \"Serviceability reflects
the delivery of network dependability of service to the end-users. Higher
serviceability improves availability, provides integrity of service without
excessive impairments, and reduces service costs.
Serviceability can be described using the following performance criteria.
a) Service accessibility
Service accessibility is the ability of a network service to be accessed by
the user, under given conditions, for a given period of time. For connection-
oriented services, it refers to the ability to establish connection.
Accessibility can be measured in terms of service access delay, network access
capability, and service access control capability. [...]
b) Service retainability
Service retainability is the ability of a network service, once obtained, to
continue to be provided under given conditions for a requested duration. It
reflects the reliability of [the] network. [...] Retainability requires
network dependability support to maintain stable operation. [...]
c) Service integrity
Service integrity is the delivery of information and data by the network
without excessive impairment. Service integrity relates to the transfer of
information and data known as throughput. [...]
d) Disengagement
Disengagement concerns the network devices and links involved in the end-to-
end communication of a user as well as network resources (including bandwidth,
channel or resources related to upper-layer protocols) to be released when the
communication connection or session is closed. [...] Disengagement is a
characteristic affecting service accessibility and service retainability in
network serviceability.\" [2].
NOTE: disengagement is not so much a concern of the end user, rather of the
network operator. This aspect fosters service accessibility and retainability
by keeping the share of committed but unused communication resources low.
Serviceability has an additional flavour, which is operability. \"From the
user's perspective, operability refers to the ability of a service to be
successfully and easily operated by a user.\" [2]. This flavour is especially
important in dynamic communication scenarios and for machine-to-machine
communication.
NOTE: Operability is used to characterise the four serviceability criteria
above. When applying the operability criterion one can, for instance, ask how
easy it is to gain access to the communication service, while service
accessibility focuses on how access is gained.
Table 4.3.4.3-1 provides a mapping between serviceability criteria and
dependability attributes. For the latter see Subclause 4.3.3.2.
Table 4.3.4.3-1: Serviceability criteria and corresponding dependability
attributes.
* * *
Serviceability criterion Corresponding dependability attribute Accessibility
Availability, reliability Retainability Availability, reliability,
maintainability, safety Integrity Integrity, safety Disengagement
Availability, reliability
* * *
A comparison of serviceability criteria stipulated by IEC 61907 (see Table
4.3.4.3-1) and those stipulated by 3GPP (see the definition of QoS in [1]) is
provided in Table 4.3.4.3-2.
Table 4.3.4.3-2: Servicability criteria according to IEC 61907 [2] and 3GPP
[1].
* * *
Serviceability criterion IEC 61907 3GPP Accessibility X X Retainability X X
Integrity X X Dissengagment X --
* * *
#### 4.3.4.4 Describing dependable communication services
In order to deliver a dependable communication service, one needs to assure
the \"continuity of service against failures and denial of service access and
disengagement, and the transfer of user information against loss or
interruption." [2]. The dependability of the service from an end-user
perspective is ensured at the service access point (interfaces in Figure
4.3.3.3.1-1). The end-user dependability requirements determine the required
network performance, which in turn determine the required network parameters.
As outlined in Subclause 4.3.4.3, dependability requirements can be sorted
under four serviceability criteria. However, as discussed in Subclause
4.3.4.2, the dependability mind set also necessitates the differentiation of
requested, offered, achieved and experienced dependability. The implied
assurance is another paramount facet of a communication service. Assurance is
a praxis that produces assurance judgments, i.e. statements that inspire
confidence of, for instance, the user of the communication service. Ideally,
assurance judgments are based on evidence. The evidence on which an assurance
judgement is based can be obtained by the means of service monitoring. More on
assurance, assurance methodology, and assurance frameworks can be found in
appendix A.3 of [23]. More details on communication assurance and monitoring
for assurance can be found elsewhere in the literature [24]. Such monitoring
could, for instance, include the communication service availability. If the
service monitoring shows that the service has been available for more than
99,92% over a year, the following assurance statement can be made: \"The
continuous monitoring of communication service availability over a year shows
that the unavailability of the communication service is less than 0,08%. This
implies that the communication service availability requested by the user,
i.e. 99,9%, is met.\" Assurance takes place at the service interface, which is
the single point of interaction between the communication network and the
users, i.e. the automation functions. The set of facets of a dependable
communication service is summarised in Figure 4.3.4.4-1.
Figure 4.3.4.4-1: Facets of a dependable communication service.
## 4.4 Deployment scenarios for automation in vertical domains
### 4.4.1 Description
3GPP supports a range of deployment scenarios based around macro cells and
small cells and 5G is expected to extend the supported topologies whilst
supporting both macro and small cell deployments. Specifically, short
wavelength spectrum such as mm-wave is likely to trigger new types of cell
deployment due to its radio characteristics. 3GPP also has a host of other
deployment tools such as LAA, Dedicated Core Networks, IOPS, etc. which can
offer a great number of deployment options. What is key, is matching the
appropriate network deployment to the required use case or scenario.
Editor\'s Note: \"LAA\" and \"IOPS\" to be added to the list of abbreviations
(Subclause 3.3).
As discussed in the present document, 5G communication for automation in
vertical domains requires high levels of availability, reliability, and
maintainability in order to ensure high levels of service accessibility and
productivity. To meet specific end user requirements, specific network
deployments and parameters may be utilised. For example, a sub-surface mining
operation and associated surface support have very different levels of
existing deployment & coverage, propagation environment, existing deployed
communication infrastructure, and required services in comparison to a
nationwide electricity grid with communication services deployed via public
WWAN.
Editor\'s Note: \"WWAN\" to be added to the list of abbreviations (Subclause
3.3).
# 5 Use cases
Editor's Note: This Clause describes new vertical use cases from a system's
perspective (at a summary level). It also provides clarification on
\"vertical\" use cases in TS 22.261 (where needed).
Editor's Note: External references for claims made and requirements proposed
in contributions are highly appreciated.
## 5.1 Rail-bound mass transit
### 5.1 _._ 1 Description of vertical
In order to keep the attractiveness of public transport high, some of the key
challenges mass transit operators are facing are
  * Growing traffic, both in terms of passenger flow and the number of and frequency of mass transit vehicles;
  * The need to ensure passenger safety and security;
  * Improvement of travel comfort, including delivery of real-time multimedia information and access to the internet (social networks, etc.), both in stations and on trains.
To reach this goal, investments are not only needed in rolling stock and
infrastructure, but also in communication networks, communication
technologies, and communication end devices.
#### 5.1 _._ 1.1 Communication services in rail-bound mass transit
There are two main drivers behind the growing importance of communication
services in mass transit: (1) passenger information and internet access and
(2) train automation. The latter can be divided into control and operations.
Both types of train automation consist of distributed applications that rely
on dependable communication. Typically, control applications are of higher
priority than operational applications, and the latter are typically of higher
priority than passenger services. One of the main challenges is to guarantee
the premium priority of control-related communication over other types of
communication, especially since the data bandwidth consumed for control is
typically dwarfed by data traffic stemming from the other two application
areas. Another challenge is to guarantee the super priority of operational
data communication over passenger-related communication. Automation in public
transport, especially in mass transit, has reached one of the highest levels
of automation among infrastructure applications. Nowadays, driverless metro
systems are not science fiction, rather they become increasingly pervasive.
The number of driverless metro lines worldwide already exceeds 100, and this
number is expected to grow. Note that driverless trains are not the only
source of automation in mass transit, and thus not the only source of
dependable machine-type communication. For instance, mass transit train
control assisted by rail-to-rail-side wireless communication (MTTC), which is
at the core of driverless trains, is also used for trains exhibiting lower
grades of automation. Examples for MTTC are communication-based train control
(CBTC) [7] and Korea Radio-Based Train Control System (KRTCS) [5][6]. The
grades of automation of trains are specified in IEC 62290-1 [4] and are
summarised in Figure 5.1.1.1-1.
{width="6.050694444444445in" height="3.1881944444444446in"}
**Figure 5.1.1.1.-1: The different grades of automation (GoA) in rail
systems.**
Caused by increased safety awareness, onboard video surveillance/CCTV and
emergency calls play an increasing role in rolling-stock equipment. In GoA 4
systems (see Figure 5.1.1.1-1), both applications are mandatory in order to
setup communication with passenger during emergencies, e.g. when trains have
to stop inside tunnels. Onboard video surveillance/CCTV system generates also
excessive amount of video recordings, which need to be archived and ultimately
offloaded/ transferred from onboard into the ground storage. Additional
operational applications are train diagnostics and voice communication for
operational, service and maintenance purposes. Other common applications for
increasing passenger satisfaction are passenger information (PIS), online
advertisement, and online internet access. Figure 5.1.1.1-2 provides an
overview of the common data services in contemporary mass transit trains.
{width="6.291666666666667in" height="4.564583333333333in"}
**Figure 5.1.1.1.-2: Common communication-based services in rail-bound mass
transit. Not all services listed are shown in the diagram.**
#### 5.1 _._ 1.2 Characteristics of main mass-transit data services
In a first level of consideration, mass-transit data services differ in
directionality and bandwidth. For instance, MTTC is a bidirectional data
service, while CCTV traffic mainly flows from train to track-side destinations
(uplink). In contrast, the PIS communication pattern is almost reverse
(dominance of downlink traffic).
In Table 5.1.1.2-1, a selection of mass-transit data services is characterised
by different attributes such as data rate, directionality, and priority. The
latter, which are not standardised, are provided for guidance.
Table 5.1.1.2-1: Characteristics of the data services in rail-bound mass
transit. DL: downlink; UL: uplink
* * *
Service Main Direction Data rate in Mbit/s per application Priority End-to-end
latency Communication service availability Security Data Integrity MTTC DL, UL
\ 99,999 % Highest Mandatory Real-time CCTV UL >
4 High (= 2) \ 99,99 % High Recommended PIS DL \ 99,99 % High Not required Passenger internet access DL ≥ 0,2 Low (= 4) \ 0,1 Medium (= 3) \ 99,99 % High Recommended
* * *
As Table 5.1.1.2-1 shows, MTTC is a very demanding data service, since every
related attribute―except data rate―is at the highest requirement level. Other
important services, from an operational point of view, are CCTV and emergency
voice connections. Internet access, on the other hand, requires high aggregate
data rates while it represents the least important operation-related service
of all. The maximum bit-error ratio for control messages, e.g. MTTC messages,
is 10^-6^.
Table 5.1.1.2-2 translates the above qualitative services into typical data
rates for a commuter train.
Table 5.1.1.2-2: Typical data-rate requirements per commuter train and
service. The overall data rate is calculated under the assumption of one
application per device.
* * *
Service # of networked devices\ Data rate per device\ Overall data rate per
train\ (typically) [Mbit/s] [Mbit/s]
MTTC 2 0,1 0,2
Real-time CCTV 20 4 80
PIS 6 0,5 3
Emergency voice 8 0,2 1,6
Passenger internet access 500 0,2 100
Train diagnostics 50 0,1 5
CCTV offload / archiving 1 ≥ 1000 ≥ 1000
* * *
MTTC only consumes some hundred kbit/s in contrast to around 100 Mbit/s for
passenger internet access. Also, the number of networked devices per train
varies considerably from one service to the other. MTTC is at the very low end
of the list. The big number for passenger internet access stems from the fact
that passenger end devices, e.g. smart phones, significantly outnumber
automation data sources and data sinks and that many automation devices
consume comparably low data rates. Due to constantly increasing minimum
resolutions for CCTV cameras, currently reaching up to approximately 4 Mbit/s
per camera, the data rate requirements of live real-time CCTV reaches close to
that of overall data rate of Passenger internet Access. Furthermore, the CCTV
offload, which means the transfer of CCTV archives from train to ground when
the train stops at the stations or at the depot., requires relatively high
overall data rate, similar to that of required in of long haul trains, i.e.,
min. 1 Gb/s. The transfer CCTV offload is even more challenging in commuter
trains than in long haul trains due to shorter stop times.
Note that communication service reliability is also an important parameter for
mass-transit communication. However, commonly agreed upon values do not exist.
As a rule of thumb, the time between communication service failures needs to
be maximised.
Also note that some mass-transit applications are comparably immune to
intermittent communication service dropouts. In case the unavailability of the
communication service is shorter than a pre-defined time (for MTTC typically
around 500 ms), the communicating applications do not experience the service
as unavailable. However, if the dropout exceeds this time limit, it counts as
a communication service failure. As a general rule, the time to repair the
communication service should be kept as short as feasible.
### 5.1.2 Coexistence of MTTC service and CCTV
#### 5.1.2 _._ 1 Description
This use case considers the behaviour of a high-priority MTTC service in
presence of a CCTV high-priority data service. The former is characterised by
rather low data rates and the latter by rather high data rates (see Table
5.1.1.2-1). In this use case CCTV is streamed in real time to the rail-side,
for instance to a traffic control centre.
5.1.2.2 Preconditions
The high-priority service MTTC is switched on and communication between train
and ground-based train control units ensues. The priority level of MTTC is 1
(highest level). Train-borne and ground MTTC instances are exchanging data.
The exchange fulfils the required data rate and other relevant service
parameters (see Table 5.1.1.2-1). All other data services are switched off.
5.1.2.3 Service flows
The CCTV service is switched on. The priority of this data service is 2 (one
level lower than MTTC). All other data services are switched off.
The CCTV service is switched on. The desired data rate of the CCTV service is
negotiated (typically at least 100 Mbit/s). CCTV data connections between
track and train are established (CCTV cameras are switched on and video
streams relayed to a track-side video management system, a.k.a. the CCTV data
sink).
5.1.2.4 Post-conditions
  * The MTTC QoS parameters stay in the desired range while the CCTV service is running;
  * The CCTV QoS parameters stay in the desired range.
5.1.2.5 Challenges to the 5G system
```{=html}
``` \- Isolation of communication flows;
  * Guarantee of user experienced data rates;
  * High communication service availability;
  * High communication service reliability.
5.1.2.6 Potential requirements
+----------------+----------------+----------------+----------------+ | Reference | Requirement | Application / | Comments | | Number | text | Transport | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The MTTC | A | CCTV data | | 1.1_ | service has | | rates may | | | the highest | | reach 500 | | | priority and | | Mbit/s per | | | shall not be | | train. | | | affected by | | | | | the CCTV | | [This | | | service. | | requirement is | | | End-to-end | | not covered | | | latency and | | yet by | | | availability | | existing 3GPP | | | of MTTC are | | requirements] | | | not affected | | | | | when running | | | | | the CCTV | | | | | service in | | | | | parallel. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The CCTV | A | [This | | 1.2_ | service shall | | requirement is | | | not be | | not covered | | | affected by | | yet by | | | the MTTC | | existing 3GPP | | | service, which | | requirements] | | | has a higher | | | | | priority but | | | | | lower data | | | | | rate. | | | | | Especially, | | | | | delay and | | | | | packet loss of | | | | | CCTV is not | | | | | affected by | | | | | MTTC service | | | | | that runs in | | | | | parallel. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The user | T | | | 1.3_ | experienced | | | | | data rate for | | | | | MTTC services | | | | | shall be at | | | | | least 200 | | | | | kbit/s. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The end-to-end | T | | | 1.4_ | latency for | | | | | MTTC services | | | | | shall be below | | | | | 100 ms. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The | T | | | 1.5_ | communication | | | | | service | | | | | availability | | | | | for MTTC | | | | | services shall | | | | | be higher than | | | | | 99,999%. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The use | T | | | 1.6_ | experienced | | | | | data rate for | | | | | CCTV services | | | | | shall not be | | | | | lower than 2 | | | | | Mbit/s per | | | | | CCTV | | | | | application. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit_ | The end-to-end | T | | | | latency for | | | | | CCTV | | | | | applications | | | | | shall be below | | | | | 500 ms. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The | T | | | 1.7_ | communication | | | | | service | | | | | availability | | | | | for CCTV | | | | | services shall | | | | | be higher than | | | | | 99,99%. | | | +----------------+----------------+----------------+----------------+
### 5.1.3 Coexistence of MTTC service and a high data rate service with low
priority
#### 5.1.3 _._ 1 Description
This use case considers the behaviour of MTTC in presence of another data
service with low priority but very high data rates. An example for this other
service is passenger internet access.
#### 5.1.3.2 Preconditions
MTTC is switched on and communication between train and ground is maintained.
The priority of the MTTC service is set to the highest value. Train-borne and
ground MTTC instances are exchanging data with required data rate and other
specified service parameters. All other data services are switched off.
#### 5.1.3.3 Service flows
Another data service with different service parameters is switched on. The
priority of this data service is the lowest. An example service is passenger
internet access service.
The additional service is switched on. The desired data rate of the additional
service is set to, e.g., 100 Mbit/s. A pursuant data connection between track
and train is established. After successful initialisation of the additional
service, the QoS of the MTTC is unaffected, even if the data rate of the
additional service is increased up to 500 Mbit/s.
#### 5.1.3.4 Post-conditions
The MTTC QoS parameters are in the specified range, even when the other data
service is running at the highest permissible data rate.
#### 5.1.3.5 Challenges to the 5G system
  * Isolation of communication flows.
#### 5.1.3.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Mass
Transit 2.1_ The MTTC service shall not be affected by a low priority data
service. End-to-end latency, packet loss, and availability of MTTC are not
affected by the other data service. A [This requirement is not covered yet by
existing 3GPP requirements]
* * *
### 5.1.4 Coexistence of MTTC service and high data rate service with low
priority
#### 5.1.4 _._ 1 Description
This use case considers the behaviour of the start up of the MTTC service in
case of other already running data services.
#### 5.1.4.2 Preconditions
MTTC is switched off. Data services that consume significant bandwidth are
running. Examples for such services are
  * CCTV;
  * PIS;
  * Emergency Voice;
  * Passenger internet Access;
  * Train diagnostics.
The communication service for each application is characterised by an
individual set of QoS parameters. The resulting data rate per train is 500
Mbit/s and lower.
#### 5.1.4.3 Service flows
The MTTC service is switched on.
Trackside and train-borne MTTC instances boot and communication between both
commences.
After the MTTC start-up procedure, the MTTC service is entering the data
exchange mode.
#### 5.1.4.4 Post-conditions
The MTTC service is running properly, fulfilling specified QoS parameters.
#### 5.1.4.5 Challenges to the 5G system
  * Isolation of communication flows.
#### 5.1.4.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Mass
Transit 3.1_ The MTTC service start-up shall not be affected by already
running services with different priorities. A [This requirement is not covered
yet by existing 3GPP requirements] _Mass Transit 3.2_ Already running
communication services shall not be affected by booting the MTTC service. A
[This requirement is not covered yet by existing 3GPP requirements]
* * *
### 5.1.5 Set-up of emergency call
#### 5.1.5 _._ 1 Description
This use case considers setting up emergency calls while other data services
are running.
NOTE: This use case deals with emergency calls using the train and rail
infrastructure and not public emergency services. An example for a train
emergency system is microphone/speaker boxes that are integrated into the
walls of the passenger area.
#### 5.1.5.2 Preconditions
The high priority emergency voice service is in standby mode. The
corresponding voice call devices are switched on.
Examples for such services are
  * Real-time CCTV;
  * PIS;
  * Emergency voice;
  * Passenger internet access;
  * Diagnostics.
The communication service, for each application, is characterised by an
individual set of QoS parameters. The resulting data rate per train is 500
Mbit/s and lower.
#### 5.1.5.3 Service flows
An emergency voice call is initiated. Trackside and train-borne voice
communication between them commences.
#### 5.1.5.4 Post-conditions
A successful voice call between train and track side devices is established.
#### 5.1.5.5 Challenges to the 5G system
  * Isolation of communication flows;
  * Guarantee of user experienced data rates;
  * High communication service availability;
  * High communication service reliability.
#### 5.1.5.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Mass
Transit 4.1_ The set-up of an emergency voice call shall not be affected by
already running services with different priorities. A The compound data rate
of other services is ≤ 500 Mpbs per train. [This requirement is not covered
yet by existing 3GPP requirements] _Mass Transit 4.2_ The user experienced
data rate for emergency voice calls shall be at least 200 kbit/s. A  
_Mass Transit 4.3_ The end-to-end latency for emergency voice calls shall be
below 200 ms. T  
_Mass Transit 4.4_ Them communication service availability for emergency voice
calls shall be higher than 99,99%. T
* * *
### 5.1.6 Emergency call during a sudden rise of CCTV data rate
#### 5.1.6 _._ 1 Description
This use case considers setting up emergency calls while the data rate of
other data services rises.
#### 5.1.6.2 Preconditions
An emergency voice call is established. The MTTC service is running.
#### 5.1.6.3 Service flows
Train-borne CCTV devices start streaming videos to the trackside video system
at an overall data rate of up to 500 Mbit/s.
#### 5.1.6.4 Post-conditions
The voice call between train and track side devices is not interrupted.
#### 5.1.6.5 Challenges to the 5G system
  * Isolation of communication flows.
#### 5.1.6.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Mass
Transit 5.1_ An emergency voice call shall not be interrupted, even when a
sudden rise of data rate of other lower-priority service such as CCTV occurs.
A The maximum compound data rate of the lower-priority services is 500 Mbit/s.
[This requirement is not covered yet by existing 3GPP requirements]
* * *
### 5.1.7 Use Case: CCTV offload / transfer of CCTV archives from commuter
train to ground
#### 5.1.7.1 Description
This use case describes the CCTV offload, i.e., the transfer of CCTV archives
from the on-board system to the ground system. This use case assumes the
following:
  * The retention time for the recordings in the on-board system is seven days.
  * The minimum retention time for the CCTV recordings in the ground system is 31 days.
  * CCTV offload/transfer of CCTV archives is performed only when train approaches stations/stops in order to stop and at the depot.
{width="6.0625in" height="4.136805555555555in"}
Figure.5.1.7.1-1. Onboard CCTV storage sizes with different offload rates.
#### 5.1.7.2 Pre-conditions
  * CCTV offload/transfer of CCTV archives is performed only when the commuter train stops at the stations.
  * Mobile communication infrastructure between commuter train and ground system enables CCTV offload/ transfer of CCTV archives from commuter train to the ground system while the train stops at the stations.
  * The ground system supports sufficient archiving system for the transferred recordings.
#### 5.1.7.3 Service flows
1\. The commuter train approaches the station/stop.
2\. Mobile communication system in commuter train establishes connection
dedicated for the CCTV offload/ transfer of CCTV archives with the ground
system at a priority level allowing critical communication to continue in
parallel
3\. The CCTV offload/ transfer of CCTV archives is started upon successful
connection with the ground system.
4\. The CCTV offload/transfer of CCTV archives is stopped when the connection
is no longer available.
#### 5.1.7.4 Post-conditions
  * The on-board CCTV system may re-write over the seven days and older recordings that have been transferred.
  * The on-board mobile communication system remains monitoring the next approach of station/stop.
#### 5.1.7.5 Challenges to the 5G system
Editor's Note: For Further Study
#### 5.1.7.6 Potential requirements and gap analysis
+-------------+-------------+-------------+-------------+-------------+ | Reference | Requirement | Application | SA1 spec | Comments | | Number | text | / Transport | covering | | +-------------+-------------+-------------+-------------+-------------+ | _Mass | The onboard | A/T | A: Not | See: | | Transit | System | | covered T: | http://www. | | 6.1_ | shall be | | Data rate | 3gpp.org/te | | | able to | | covered by | chnologies/ | | | support | | LTE and NR | keywords-ac | | | that CCTV | | transport | ronyms/97-l | | | archives | | | te-advanced | | | can be | | | | | | transferred | | | | | | into the | | | | | | ground | | | | | | system in a | | | | | | time and | | | | | | resource | | | | | | efficient | | | | | | way with a | | | | | | minimum of | | | | | | 1 Gb/s in | | | | | | dedicated | | | | | | places such | | | | | | as | | | | | | sta | | | | | | tions/stops | | | | | | or train | | | | | | depots. | | | | +-------------+-------------+-------------+-------------+-------------+ | _Mass | CCTV | A/T | A: Not | | | Transit | offload/T | | covered | | | 6.2_ | ransferring | | | | | | CCTV | | T: Covered | | | | archives | | by basic | | | | shall not | | 3GPP and | | | | affect | | suitable | | | | mission | | QoS | | | | critical | | | | | | com | | | | | | munication. | | | | | | | | | | | | NOTE: | | | | | | T | | | | | | ransferring | | | | | | CCTV | | | | | | archives is | | | | | | not | | | | | | considered | | | | | | to be a | | | | | | mission | | | | | | critical | | | | | | service. | | | | +-------------+-------------+-------------+-------------+-------------+
### 5.1.8 Wireless communication between mechanically coupled train segments
#### 5.1.8 _._ 1 Description
Two mass transit coaches or trains are mechanically coupled together. The
communication between both (control, operational, passenger services) is going
to be provided via a 5G RAN link. For the sake of simplicity we henceforth
refer to coaches as trains.
#### 5.1.8.2 Preconditions
Services are running in both trains. Typically, these services are of an
operational nature, but they can also include control and passenger services.
The two trains are coupled together mechanically, but communication between
both trains is not established yet.
#### 5.1.8.3 Service flows
A wireless 5G link is established between the two trains.
#### 5.1.8.4 Post-conditions
Communication services between both trains are established via the 5G wireless
link and run dependably.
#### 5.1.8.5 Challenges to the 5G system
  * Guarantee of very high user experienced data rates over a short air gap.
  * Very high communication service availability;
  * Very high communication service reliability.
#### 5.1.8.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Mass
Transit 7.1_ The user experienced data rate between the two trains shall be 1
Gbit/s. T  
_Mass Transit 7.2_ The end-to-end latency for MTTC services in both trains
shall be below 100 ms. T  
_Mass Transit 7.3_ The communication service availability between applications
situated in different trains shall be higher than 99,999%. T  
_Mass Transit 7.4_ Communication services between shall be possible for angles
of up to 0,52 rad between the two trains. T
* * *
### 5.1.9 Wireless communication between virtually coupled trains
#### 5.1.9.1 Description
One of the important missions that the future railway service should achieve
is to increase its transport capacity and operational efficiency. A straight-
forward solution is to minimise the distance between successive mass transit
trains so that mass transit interval is reduced. It is difficult to do so in a
legacy mass transit system, because the distance between two successive mass
transit trains should be larger than the safety braking distance.
The distance between successive trains can be shortened if the following mass
transit train immediately triggers braking as soon as the leading mass transit
train starts braking. This is the fundamental principle of the virtual
coupling. Figure 5.9.1 shows the basic concept of the virtual coupling.
Multiple mass transit trains in proximity move together _as if they are
mechanically coupled_. As two trains get closer, trains need to communicate
each other more frequently through a very-low-latency off-network
communication link for control, operational, and passenger services. The
communication between the virtually coupled trains is provided through both
on-network and off-network communication links simultaneously for
complementary use of both links.
{width="6.752777777777778in" height="2.0118055555555556in"}
Figure 5.1.9.1 The concept of virtual coupling.
#### 5.1.9.2 Pre-conditions
1\. Control, operational, and passenger services are running in both leading
and following mass transit trains. Both trains are configured to be coupled
virtually.
2\. Two mass transit trains are connected through an on-network (e.g., Uu
interface). Each mass transit train is capable of off-network communications
(e.g., PC5 interface).
#### 5.1.9.3 Service flows
As the following train approaches the leading train, a very-low-latency off-
network communication link is established between two trains. Two mass transit
trains are still connected through the on-network communication link for
complementary use of both links.
#### 5.1.9.4 Post-conditions
Control, operational, and passenger communication services between virtually
coupled trains are established through both on-network and off-network
communication links simultaneously.
#### 5.1.9.5 Challenges to the 5G system
  * Guarantee of very long communication range for off-network communication link
  * Very low end-to-end latency for off-network communication link
  * Service continuity in the application layer between on-network and off-network
#### 5.1.9.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport **Comments** _Mass
Transit 8.1_ The 3GPP system shall support off-network communication up to 3
km in the line of sight (LOS) channel environment. T  
_Mass Transit 8.2_ The end-to-end latency through off-network communication
shall be less than or equal to 10 ms. T  
_Mass Transit 8.3_ The 3GPP system shall support off-network communication,
where the UEs' relative speed is less than 50 km/h. T  
_Mass Transit 8.4_ The 3GPP system shall support simultaneous use of on-
network and off-network communication when distance between the virtually
coupled trains is less than or equal to 3 km in the LOS channel. T  
_Mass Transit 8.5_ The 3GPP system shall support service continuity in the
application layer between on-network based connection and off-network based
connection when distance between the virtually coupled trains is less than or
equal to 3km in the LOS channel. A/T
* * *
## 5.2 Building automation
### 5.2 _._ 1 Description of vertical
Building automation [9] refers to the management of equipment in buildings
such as heaters, coolers, and ventilators. Automation of such systems brings
several benefits, including the reduction of energy consumption, the
improvement of comfort level for people using the building, and the handling
of failure and emergency situations. Sensors installed in the building perform
measurements of the environment and report these measurements to Local
Controllers. Local Controllers (LC), in turn, report these results to a
Building Management System.
A Building Management System (BMS) may then execute different operations:
  * Store the information into a database (e.g., for histogram purpose);
  * Send an alarm to a (third-party) Building Management System;
  * The Building Management System sends a command to an actuator (e.g., command to increase room temperature, turn on a light).
Figure 5.2.1-1: Building automation system - Local Controller in Mobile Edge
System and Building Management System outside 3GPP domain.
Figure 5.2.1.-2: Building automation system with Building Management System as
part of Mobile Edge Computing system
Editor's Note: In existing building automation systems there are typically two
layers of network: the upper layer is called management network and the lower
one is called field network. In management networks, an IP-based communication
protocol is used. In field network, non-IP based communication protocols
(a.k.a., field protocols) are mainly used. There are many field protocols used
in today\'s deployment in which some medium access control and physical layers
protocols are standards-based and others are proprietary based. The impact of
such protocols in 3GPP requirements is FFS.
### 5.2.2 Environmental monitoring
#### 5.2.2 _._ 1 Description
In this use case, several sensors are installed in a building and each sensor
performs measurements following a pre-defined measurement interval. The
measurement data might be used for drawing a histogram with as detailed as 1 s
granularity and a 10 times sampling rate, i.e. 10 times per second. A Local
Controller collects the measurement data from its sensors and may transmit it
to the Building Management System at a certain interval. The latency in this
use case is not a concern, but it is important that the transmission is
reliable and all sensor values are collected within the measurement interval.
#### 5.2.2.2 Preconditions
There are several Local Controllers installed in the building, each connected
with many sensors (up to 100 sensors).
#### 5.2.2.3 Service flows
At the measurement interval, which might be as low as 1 second, and with the
needed sampling rate (e.g. 10/s), the Local Controller sends a request to all
its sensors in the building to report their measurements.
#### 5.2.2.4 Post-conditions
Every sensor reports their measurements and measurements are received with
99,999 % reliability. The Local Controller collects these measurements and may
transmit them to the building management System.
#### 5.2.2.5 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Building
Automation 1.1_ The 3GPP system shall support 99,999 % communication service
availability for data transmission every second. Transport
* * *
### 5.2.3 Fire detection
#### 5.2.3 _._ 1 Description
In this use case, when fire is detected, the system triggers several actions,
such as closing fire shutters and turning on fire sprinklers.
#### 5.2.3.2 Preconditions
There are 10 connected sensors and one Local Controller installed in the
building.
#### 5.2.3.3 Service flows
  1. Fire is detected by the building sensors.
  2. Building sensors send an alarm to the Local Controller.
  3. Local controller sends information to Building Management System
  4. Building Management System sends commands to the actuators in the building.
#### 5.2.3.4 Post-conditions
Fire shutters are closed and fire sprinklers are turned on within 1 to 2
seconds from the time the fire is detected.
#### 5.2.3.5 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Building
Automation 2.1_ The 3GPP system shall support an end-to-end latency of 10 ms
with a [99,9999 %] communication service availability for data transmission.
Transport
* * *
Editor\'s Note: It is FFS how 10 ms is calculated. Communication service
availability needs to be confirmed.
#### 5.2.4 Feedback control
#### 5.2.4 _._ 1 Description
In this use case, a (device) state is controlled. For example, a room
temperature is kept at a certain value. Low latency and jitter are required in
this use case in order to provide high quality of feedback control.
#### 5.2.4.2 Preconditions
There are 10 sensors and one Local Controller installed in the building. The
Local Controllers is configured with a target temperature for a connected
sensor and thus the room in which the sensor is installed.
#### 5.2.4.3 Service flows
1\. The Local Controller requests measurements from a target sensor to
establish the state of the sensor.
2\. The Local Controller calculates a control value based on the measured
target sensor state.
3\. The Local Controller sends the control value to a target actuator.
#### 5.2.4.4 Post-conditions
The target actuator receives the command and adjusts the temperature based on
the control value and the temperature reaches the target temperature.
#### 5.2.4.5 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Building
Automation 3.1_ The 3GPP system shall support and end-to-end latency of 10 ms
with a communication service availability of [99,9999 %] for data
transmission. Transport  
_Building Automation 3.2_ The 3GPP system shall support a jitter of up to 1
ms. Transport
* * *
Editor\'s Note: Communication service availability needs to be confirmed.
## 5.3 Factories of the Future
### 5.3.1 Description of vertical
#### 5.3.1.1 Overview
The manufacturing industry is currently subject to a fundamental change, which
is often referred to as the \"Fourth Industrial Revolution\" or simply
\"Industry 4.0\" [27]. The main goals of Industry 4.0 are―among others―the
improvement of flexibility, versatility, resource efficiency, cost efficiency,
worker support, and quality of industrial production and logistics. These
improvements are important for addressing the needs of increasingly volatile
and globalised markets. A major enabler for all this are cyber-physical
production systems based on a ubiquitous and powerful connectivity and
computing infrastructure, which interconnects people, machines, products, and
all kinds of other devices in a flexible, secure and consistent manner.
Instead of static sequential production systems, future smart factories will
be characterised by flexible, modular production systems. This includes more
mobile and versatile production assets, which require powerful and efficient
wireless communication and localisation services.
Today, the vast majority of communication technologies used in industry is
still wire-bound. This includes a variety of dedicated Industrial Ethernet
technologies (e.g., Sercos®, PROFINET® and EtherCAT®) and fieldbuses (e.g.,
PROFIBUS®, CC-Link® and CAN®) [28][29][30]. These communication technologies
are used, for example, for interconnecting sensors, actuators and controllers
in an automation system. Nowadays, wireless communication is primarily used
for special applications and scenarios, for example in the process industry,
or for connecting standard IT hardware to a production network and similar
rather non-critical applications. On the one hand, this is because there was
no need for wireless connectivity in the past, due to relatively static and
long-lasting production facilities. On the other hand, this is because most
existing wireless technologies fall short of the demanding requirements of
industrial applications, especially with respect to end-to-end latency,
communication service availability, jitter, and determinism. With the advent
of Industry 4.0 and 5G, however, this may change fundamentally, since only
wireless connectivity can provide the degree of flexibility, mobility,
versatility, and ergonomics that is required for the Factories of the Future.
Thus, 5G may significantly contribute to revolutionising the way how goods are
produced, shipped, and serviced throughout their whole lifecycle.
In this respect, several different application areas can be distinguished, as
shown in Figure 5.3.1.1-1.
{width="3.775in" height="2.578472222222222in"}
Figure 5.3.1.1-1: Overview of the different application areas of the vertical
\"Factories of the Future\".
These areas can be briefly characterised as follows:
(1) **Factory automation:** Factory automation deals with the automated
control, monitoring and optimisation of processes and workflows within a
factory. This includes aspects like closed-loop control applications (e.g.,
based on programmable logic or motion controllers), robotics, as well as
aspects of computer-integrated manufacturing. Factory automation generally
represents a key enabler for industrial mass production with high quality and
cost-efficiency and corresponding applications are often characterised by
highest requirements on the underlying connectivity infrastructure, especially
in terms of latency, communication service availability and determinism. In
the Factories of the Future, static sequential production systems will be more
and more replaced by novel modular production systems offering a high
flexibility and versatility. This involves a large number of increasingly
mobile production assets, for which powerful wireless communication and
localisation services are required.
(2) **Process automation:** Process automation refers to the control of
production and handling of substances like chemicals, food & beverage, etc.
Process automation improves the efficiency of production processes, energy
consumption and safety of the facilities. Sensors measuring process values,
such as pressures or temperatures, are working in a closed loop via
centralised and decentralised controllers with actuators, e.g., valves, pumps,
heaters. Also monitoring of attributes such as the filling levels of tanks,
quality of material or environmental data are important, as well as safety
warnings or plant shut downs. Workers in the plant are supported by mobile
devices. A process automation facility may range from a few 100 m² to km² or
may be geographically distributed over a certain geographic region. Depending
on the size, a production plant may have several 10 000 measurement points and
actuators. Autarkic device power supply for years is needed in order to stay
flexible and to keep the total costs of ownership low.
(3) **HMIs and Production IT:** Human-machine interfaces (HMIs) include all
sorts of devices for the interaction between people and production facilities,
such as panels attached to a machine or production line, but also standard IT
devices, such as laptops, tablet PCs, smartphones, etc. In addition to that,
also augmented and virtual reality (AR/VR) applications are expected to play
an increasingly important role in future, which may be enabled by special
AR/VR glasses, but also by more standard devices, such as tablet PCs or the
like.
Production IT, on the other hand, encompasses IT-based applications, such as
manufacturing execution systems (MES) as well as enterprise resource planning
(ERP) systems. The overall goal of an MES system, for example, is to monitor
and document how raw materials and/or basic components are transformed into
finished goods, whereas an ERP system generally provide an integrated and
continuously updated view of important business processes. Both systems rely
on the timely availability of large amounts of data from the production
process.\ \ Since both HMIs and Production IT are more related to traditional
IT systems than to factory-specific operational technology (OT) systems, they
are bundled in one application area.
  * **Logistics and warehousing:** Logistics and warehousing refers to the organisation and control of the flow and storage of materials and goods in the context of industrial production. In this respect, intra-logistics is dealing with logistics within a certain property (e.g., within a factory), for example by ensuring the uninterrupted supply of raw materials on the shopfloor level using automated guided vehicles (AGVs), fork lifts, etc. This is to be seen in contrast to logistics between different sites, for example for the transport of goods from a supplier to a factory or from a factory to the end customer. Warehousing particularly refers to the storage of materials and goods, which is also getting more and more automated, for example based on conveyors, cranes and automated storage and retrieval systems. For all kinds of logistics applications, generally also the localisation, tracking and monitoring of assets is of high importance.
(1) **Monitoring and maintenance:** Monitoring and maintenance refers to the
monitoring of certain processes and/or assets without an immediate impact on
the processes themselves (in contrast to a typical closed-loop control system
in factory automation, for example). This particularly includes applications
such as condition monitoring and predictive maintenance based on sensor data,
but also big data analytics for optimising future parameter sets of a certain
process, for instance. For these use cases, the data acquisition process is
typically not latency-critical, but a large number of sensors may have to be
efficiently interconnected, especially since many of these sensors may only be
battery-driven.
For each of these application areas, a multitude of potential use cases
exists, some of which are outlined in the following Subclauses. These use
cases can be mapped to the given application areas as shown in Table 5.4.1-1.
Table 5.3.1-1: Mapping of the considered use cases (columns)\ to application
areas (rows).
* * *
                               Motion control   Control-to-control   Mobile control panels with safety   Mobile robots   Massive wireless sensor networks   Remote access and maintenance   Augmented reality   Closed-loop process control   Process monitoring   Plant asset management
Factory automation X X X X  
Process automation X X X X X HMIs and Production IT X X  
Logistics and warehousing X X  
Monitoring and maintenance X X
* * *
#### 5.3.1.2 Major challenges and particularities
Major general challenges and particularities of the Factories of the Future
include the following aspects:
(1) Industrial-grade quality of service is required for many applications,
with stringent requirements in terms of end-to-end latency, communication
service availability, jitter, and determinism.
(2) There is not only a single class of use cases, but there are many
different use cases with a wide variety of different requirements, thus
resulting in the need for a high adaptability and scalability of the 5G
system.
(3) Many applications have stringent requirements on safety, security (esp.
availability, data integrity, and confidentiality), and privacy.
(4) The 5G system has to support a seamless integration into the existing
(primarily wire-bound) connectivity infrastructure. For example, the 5G shall
allow to flexibly combine the 5G system with other (wire-bound) technologies
in the same machine or production line.
(5) Production facilities usually have a rather long lifetime, which may be 20
years or even longer. Therefore, long-term availability of 5G communication
services and components are essential.
(6) 5G systems shall support private operation within a factory or plant,
which are isolated from PLMNs. This is required by many factory/plant owners
for security, liability, availability and business reasons. Nevertheless,
standardised and flexible interfaces shall be supported for seamless
interoperability and seamless handovers between 5G PLMNs and private 5G
systems.
(7) The radio propagation environment in a factory or plant can be quite
different from the situation in other application areas of the 5G system. It
is typically characterised by very rich multipath, caused by a large number of
---often metallic---objects in the immediate surroundings of transmitter and
receiver, as well as potentially high interference caused by electric
machines, arc welding, and the like.
(8) The 5G system shall be able to support continuous monitoring of the
current network state in real-time, to take quick and automated actions in
case of problems and to do efficient root-cause analyses in order to avoid any
undesired interruption of the production processes, which may incur huge
financial damage. Particularly if a third-party network operator is involved,
accurate SLA monitoring is needed as the basis for possible liability disputes
in case of SLA violations.
#### 5.3.1.3 Deployment aspects
Separate communication services may need to be provided for different
application areas. Note that \"separate\" communication service can be
physically, logically or virtually separate.
The application area \"factory automation\" consists of the use cases \"motion
control\", \"control-to-control\", \"mobile robots\" and \"massive wireless
sensor networks\". Communication services for factory automation meet
stringent requirements; and operation is limited to a relatively small service
area where no interworking with the public network (e.g., mobility, roaming)
is required.
The application area \"process automation\" consists of the use cases \"mobile
robots\", \"massive wireless sensor networks\", \"closed-loop process
control\", \"process monitoring\" and \"plant asset management\".
Communication services for process automation meet stringent requirements.
Interworking with the public network (e.g., mobility, roaming) is required.
The application area \"HMIs and production IT\" consists of the use cases
\"mobile control panels with safety\" and \"augmented reality\". Communication
services for HMIs and Production IT meet stringent requirements; and are
limited to a local service area where no interworking with the public network
(e.g., mobility, roaming) is required.
The application area \"logistics and warehousing\" consists of the use cases
\"control-to-control\" and \"mobile robots\". Communication service for
logistics and warehousing meet very stringent requirements; and are limited to
a local service area (both indoor and outdoor). Interworking with the public
network (e.g., mobility, roaming) is required.
The application area \"monitoring and maintenance\" consists of the use cases
\"massive wireless sensor networks\" and \"remote access and maintenance\".
Communication services for monitoring and maintenance meet stringent
requirements; and are limited to a local service area (both indoor and
outdoor). Interworking with the public network (e.g., mobility, roaming) is
required.
### 5.3.2 Motion control
#### 5.3.2 _._ 1 Description
Motion control is among the most challenging and demanding closed-loop control
applications in industry. A motion control system is responsible for
controlling moving and/or rotating parts of machines in a well-defined manner,
for example in printing machines, machine tools or packaging machines. Due to
the movements/rotations of components, wireless communications based on
powerful 5G systems constitutes a promising approach. On the one hand this is
because with wirelessly connected devices, slip rings, cable carriers,
etc.―which are typically used for these applications today―can be avoided,
thus reducing abrasion, maintenance effort and costs. On the other hand, this
is because machines and production lines may be built with less restrictions,
allowing for novel (and potentially much more compact and modular) setups.
A schematic representation of a motion control system is depicted in Figure
5.3.2.1-1. A motion controller periodically sends desired set points to one or
several actuators (e.g., a linear actuator or a servo drive) which thereupon
perform a corresponding action on one or several processes (in this case
usually a movement or rotation of a certain component). At the same time,
sensors determine the current state of the process(es) (in this case for
example the current position and/or rotation of one or multiple components)
and send the actual values back to the motion controller. This is done in a
strictly cyclic and deterministic manner, such that during one communication
cycle time _T_ ~cycle~ the motion controller sends updated set points to all
actuators, and all sensors send their actual values back to the motion
controller. Nowadays, typically Industrial Ethernet technologies are used for
motion control systems. Examples for such technologies are Sercos®, PROFINET®
IRT or EtherCAT®, which support cycle times below 50 µs. In general, lower
cycle times allow for faster and more accurate movements/rotations.
{width="6.070138888888889in" height="1.3506944444444444in"}
Figure 5.3.2.1-1: Schematic representation of a motion control system.
While it might be possible to move away from the strictly cyclic communication
pattern for motion control systems in the long-term, it is hard to do so in
the short-term since the whole ecosystem (tools, machines, communication
technologies, servo drives, etc.) is based on the cyclic communication
paradigm. In order to support a seamless migration path, the 5G system
therefore should support such a highly deterministic cyclic data communication
service.
Furthermore, there are many scenarios where some devices (e.g., sensors or
actuators) are added / activated or removed / deactivated while the overall
control system keeps on running. In order to support such cases, hot-plugging
support is required without any (observable) impact on the rest of the system.
Table 5.3.2.1-1 shows some typical values for the number of nodes, cycle times
and payload sizes for some of the most important application areas of motion
control systems. However, it should be noted that these values may vary widely
in practice and that not all sensors and/or actuators in a motion control
system may have to be connected using a 5G system. Instead, it is expected
that there will be a seamless coexistence between Industrial Ethernet and the
5G system in the future.
Table 5.3.2.1-1: Typical characteristics of motion control systems for three
major applications
* * *
Application # of sensors / actuators Typical message size Cycle time _T_
~cycle~ Service area Printing Machine > 100 20 B \ 5 | | | | | Mbit/s | | | | | | | | | | Average | | | | | end-to-end | | | | | latency \ A number of mobile robots (up to 100, with a potential enrolment of up to
> 1000) are commonly in use, always guided by a guidance control system. The
> mostly centralised guidance control system communicates bidirectional with
> each mobile robot. In this respect, the following data is typically
> exchanged:
  * Communication direction from guidance control system to mobile robot:
    * Process data for control and management of mobile robots
    * Emergency stop
  * Communication direction from mobile robot to guidance control system:
    * Process data (control and management data)
    * Video or image data
(b) Communication between mobile robots
> The mobile robots can exchange real-time control data with each other and
> provide a collision-free operation of autonomous mobile robots and
> synchronised actions between multiple mobile robots. For this purpose, the
> mobile robots exchange real-time control data.
(c) Communication between mobile robots and peripheral facilities
> The mobile robots communicate with the peripheral facilities. For example,
> mobile robots are able to open and close doors or gates. For this purpose,
> the mobile robots transmit the control data to the door or gate control.
> Furthermore, mobile robots can be working together with fixed installations
> like cranes or manufacturing machines. To this end, the mobile robots
> exchange real-time control data with cranes or manufacturing machines.
#### 5.3.7.4 Post-conditions
Mobile robots and AGVs can be controlled in a safe way while satisfying the
requirements.
#### 5.3.7.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Very high requirements on latency, communication service availability, and determinism.
  * Very high requirements on clock synchronicity between different mobile robots.
  * Simultaneous transmission of non-real time data, real-time streaming data (video) and highly-critical, real-time control data with highest requirements in terms of latency and communication service availability over the same link and to the same mobile robot.
  * Potentially high density of mobile robots
  * Good 5G coverage in indoor (from basement to roof), outdoor (plant/factory wide) and indoor/ outdoor environment is needed due to mobility of the robots.
  * Seamless mobility support such that there is no impairment of the application in case of movements of a mobile robot within a factory or plant.
#### 5.3.7.6 Potential requirements
+----------------+----------------+----------------+----------------+ | Reference | Requirement | Application / | Comments | | Number | text | Transport | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | The size of | | the Future | shall support | | messages at | | 7.1_ | a cyclic data | | the | | | communication | | application | | | service, | | layer are | | | characterised | | 15kbytes to | | | by at least | | 150 kbytes for | | | the following | | video frames | | | parameters: | | in | | | | | video-operated | | | Cycle time of | | remote | | | | | control. The | | | 1 ms for | | size of all | | | precise | | other messages | | | cooperative | | in all use | | | robotic motion | | cases, e.g. | | | control | | control | | | | | messages to an | | | 1--10 ms for | | actuator, is | | | machine | | 40 bytes to | | | control | | 250 bytes. | | | | | | | | 10--50 ms for | | | | | cooperative | | | | | driving | | | | | | | | | | 10--100 ms for | | | | | video operated | | | | | remote control | | | | | | | | | | 40 ms to500 ms | | | | | for standard | | | | | mobile robot | | | | | operation and | | | | | traffic | | | | | management | | | | | | | | | | Jitter \ 99,9999% | | | | | | | | | | Max. number of | | | | | mobile robots: | | | | | 100 | | | +----------------+----------------+----------------+----------------+ | _Factories of | For certain | A | | | the Future | applications, | | | | 7.2_ | the 5G system | | | | | shall support | | | | | real-time | | | | | streaming data | | | | | transmission | | | | | (video data) | | | | | from each | | | | | mobile robot | | | | | to the | | | | | guidance | | | | | control system | | | | | by at least | | | | | the following | | | | | parameter: | | | | | | | | | | Data | | | | | transmission | | | | | rate per | | | | | mobile robot: | | | | | > 10 Mb/s | | | | | | | | | | Number of | | | | | mobile robots: | | | | | 100 | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 7.3_ | seamless | | | | | mobility such | | | | | that there is | | | | | no impairment | | | | | of the | | | | | application in | | | | | case of | | | | | movements of a | | | | | mobile robot | | | | | within a | | | | | factory or | | | | | plant. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 7.4_ | user equipment | | | | | ground speeds | | | | | of up to 50 | | | | | km/h. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 7.5_ | uniform and | | | | | unequivocal | | | | | parameters for | | | | | interfaces to | | | | | allow | | | | | dependability | | | | | monitoring | | | | | (see Subclause | | | | | 4.3.4). | | | +----------------+----------------+----------------+----------------+ | _Factories of | Communication | T | | | the Future | complying with | | | | 7.6_ | the above | | | | | requirements | | | | | shall be | | | | | available over | | | | | a service area | | | | | of 1 km^2^ and | | | | | less. | | | +----------------+----------------+----------------+----------------+
### 5.3.8 Massive wireless sensor networks
#### 5.3.8 _._ 1 Description
Sensor networks aim at monitoring the state or behaviour of a particular
environment. In the context of the Factory of the Future, wireless sensor
networks (WSN) are targeting the monitoring of a process and the corresponding
parameters in an industrial environment. This environment is typically
monitored using various types of sensors such as microphones, CO~2~ sensors,
pressure sensors, humidity sensors, and thermometers. In particular, these
sensors usually form a distributed monitoring system. The monitored data, from
such a system, is used to detect anomalies in the data, i.e., by leveraging
machine learning (ML) algorithms. These algorithms usually require a training
phase before a trained ML algorithm can later work on a subset of the
available measured data. However, the training as well as the analysis of the
data may be realised in a centralised or distributed manner.
The placement of the monitoring function can be dynamic and thus, may vary
over time to enable dynamic up- and down-scaling of computing resources. In
particular, the placement may also be constrained by the available WSN
hardware. Given rather simple sensing devices, the functionality needs to be
placed into a centralised computing infrastructure such as a mobile data or
data centre cloud. Opposed to that, functionality may be placed inside the
sensor network, i.e., the sensing devices, with additional external
computational resources. The computation is referred to as fog computing,
multi-access edge computing (MEC), and cloud computing, see Figure 5.3.8.1-1,
when sensor devices and gateways, gateways and edge cloud, and edge cloud and
data centre resources are involved, respectively. A more local approach, e.g.,
fog computing or multi-access edge computing, is preferred over a more
centralised approach in order to keep sensitive data in a fabrication site and
keep the automated process independent of an internet connection.
{width="6.729166666666667in" height="3.2604166666666665in"}
Figure 5.3.8.1-1: High level component view of a scalable massive sensor
network. It may comprise a set of heterogeneous measurement-units, wirelessly
connected to gateways, which in turn are connected to a computing
infrastructure such as a micro data center (µDC). Other setups that contain
grouped sensor devices are possible and may assist in reducing load on central
instances.
Sensor networks facilitate the complex task of monitoring an industrial
environment to detect malfunctioning and broken elements in the surrounding
environment. An appropriate detection approach along with a classification of
the anomaly can help choosing a countermeasure or proper action to take in
case of predictive maintenance. Such actions can aid in improving the safety
by automatically triggering a machine's emergency stop in case of the
detection of a critical problem. At the same time, production efficiency can
be increased as machines can continue running in case the detected problem is
not safety relevant and only disrupts service of some elements.
Measuring the environment and propagating events may be realised in different
scenarios. In the simplest scenario, which is the least scalable, the sensors
propagate each newly measured value without any pre-processing, i.e., in a
solely proactive scenario. A more advanced approach is to solely react to the
environmental changes to reduce traffic, e.g., by only propagating events
under certain circumstances whenever a value exceeds a certain threshold [33].
In such a case, each sensor keeps measuring data but only propagates it
whenever it detects a relevant change in the environment. Depending on the
hardware, the sensing device may also be able to pause any active handling
(i.e., polling data from sensor) as long as the given threshold is not
exceeded, e.g., by receiving an interrupt when the sensor itself measures a
sudden significant change of the environment. Usually, active handling in such
a case is triggered by a call back from the sensor or a (remote) control unit.
This optimisation enhances the sensing devices durability by reducing its
power consumption. The power consumption reduction is an important task in
WSNs since sensors are often just equipped with a battery. Thus, the power
consumption reduction has gained a significant momentum in the WSN research.
Moreover, a lot of effort is put on specifying new messaging protocols to
reduce overhead of messaging protocols while still maintaining a high
reliability and low latency [34]. Additionally, the reduction of message
generation, e.g., by analysing measurements in local groups has thoroughly
been investigated.
The traffic patterns generated by the sensor network vary with the type of
measurement and the aforementioned setup. Traffic patterns may arise in the
form of self-similar and/or periodic patterns, i.e., the latter is usually the
case in proactive setups. Moreover, low-bandwidth and high-bandwidth streams
might be transmitted. Depending on the computational resources of the
gateway(s), some pre-processing of the sensor data may reduce the network
load, and with that, the uplink requirements.
{width="4.3694444444444445in" height="3.020138888888889in"}
Figure 5.3.8.1-2: Sensor devices in star topology are connected to a local
gateway (i.e., small cell), which provides connectivity to a base station.
Figure 5.3.8-2 depicts a massive sensor network deployment. A number of sensor
devices are connected to a local gateway (small cell) which connects to a base
station at the edge to the cloud network. Hence, the local gateway aggregates
and forwards monitoring data. Also, while aggregating, the local gateway may
pre-process the incoming data to reduce traffic load to the cloud and
computational resource requirements on the cloud. A local gateway needs to
dynamically handle the attachment requests and detachment events of sensor
devices without disruption of the monitoring service.
{width="4.447916666666667in" height="3.0833333333333335in"}
Figure 5.3.8.1-3: Sensor devices in a mesh topology realising multi-hop
connectivity to gateway (via edge devices).
Another topology for a massive sensor network is shown in Figure 5.3.8.1-3.
Here, a set of sensor nodes is directly interconnected as a mesh, where one
sensor nodes provides an uplink to the serving gateway (small cell). This
reduces the number of locally deployed cells. In such a topology the sensor
nodes may communicate just locally to reduce the load on more central
instances such as gateways and cloud resources.
In both topologies, star and mesh, a sensor node needs to perform a proper
bootstrapping to connect to the network. It needs to be able to attach itself
to the network automatically by attaching itself to a local cell or
neighbouring mesh devices. Additionally, time synchronisation of sensor nodes,
base stations, and gateways can enhance and ease monitoring. Time
synchronisation in a massive sensor network may be realised in local groups,
using the gateways or base stations. However, the bootstrapping of sensor
nodes and the time synchronisation is out of the scope of this document.
Table 5.3.8.1-1: Typical monitoring service requirements
+-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Sce | End-t | Pri | Data | Com | C | Ne | Node | Com | | nario | o-end | ority | U | munic | onnec | twork | de | munic | | | la | | pdate | ation | tions | s | nsity | ation | | | tency | | Time | se | per | calab | | range | | | (note | | | rvice | ga | ility | | per | | | 1) | | | av | teway | | | node | | | | | | ailab | | | | | | | | | | ility | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Cond | 5 -- | Hi | Up to | > | 10 -- | > | 0,05 | \ | 10 -- | > | 0,05 | \ | 10 -- | > | 0,05 | \ 99,9999%) | | | | | for critical | | | | | messages | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.11_ | a very high | | | | | connection | | | | | density of up | | | | | to 10^6^ | | | | | connections | | | | | per km^2^ | | | | | (i.e. 1 per | | | | | m^2^). | | | | | | | | | | NOTE: | | | | | Connection | | | | | density is the | | | | | total number | | | | | of connected | | | | | and/or | | | | | accessible | | | | | devices per | | | | | unit area (per | | | | | km^2^). | | | | | Normally, all | | | | | connected | | | | | devices are | | | | | not sending or | | | | | receiving | | | | | messages at | | | | | the same time. | | | +----------------+----------------+----------------+----------------+
### 5.3.9 Remote access and maintenance
#### 5.3.9 _._ 1 Description
Remote Access and Maintenance is a key motivation for looking beyond
conventionally wired device networks in automation. Typical industrial
networks are isolated from the internet and often based on very specific
protocols. In such industrial networks (peer-to-peer communication links
between just two devices, fieldbus with multiple devices and controllers, LAN
or WLANs), a remote access is often possible already today, but requires
gateway functionality at any transition of the automation pyramid between bus
systems, as shown in Figure 5.3.9.1-1. Mapping of data formats, addresses,
coding, units, and status is required for a remote access to device data going
down the automation pyramid, which comes along with a huge engineering effort.
This data mapping implemented in the gateway(s) is quite static and is not
suitable for a flexible access on event-based conditions rather than a
permanent connection and data exchange, e.g. reading the device revision in
case of a diagnostic event.
{width="6.704861111111111in" height="3.248611111111111in"}
Figure 5.3.9.1-1: Remote access to field devices in existing systems through
controllers\ via gateways at each transition point of the automation pyramid.
Remote Access and Maintenance scenarios apply to
  * Devices which already have a \"cyclic\" connection through a communication service for transmitting data regularly. This ad-hoc communication might be requested by the same end device and just runs in parallel from time to time.
  * Devices which act almost autonomously, i.e., they have local computing power to run algorithms like measurement and data analytics, but they do not have a regular, cyclic connected communication service.
  * Devices which have local personnel to interact with, such as a machine or even a car. A remote connection could be requested during a service case or if the local operator needs remote assistance.
  * Devices which sleep most of the time and which should be woken up by establishing a connection via a dedicated wireless network.
A special sub-set of remote accesses is when the partner is a mobile device
(geographically) near the device instead of a service far away from the
device. The device might not differentiate if it is accessed remotely or
\"locally\" via the 5G network. Use cases described here would be the same
from the device's point of view. A remote user would expect to parse a list
with devices and have them organised in trees. A local operator has the
intention to \"talk with the device in front of her/him\" instead of walking
through a list of 10 000 devices installed at this plant. There should be
means to automatically discover which 5G devices belonging to a certain group
(e.g., all 5G devices installed in a process plant) are in the immediate
vicinity of a local operator and to provide the user with a list of all those
5G devices.
Another use case is the inventory of devices and periodic readouts of
configuration data, event logs, revision data, and predictive maintenance
information. This is often called \"asset management\", and tools for
collecting and displaying data from many connected devices are called \"asset
monitors\". Such a system may work autonomously (set of configured periodical
checks) or it is interacting with a user (\"show me status of this device\").
A remote diagnostic system might be connected to many or all devices of a
certain plant or location. A remote diagnostic system might be connected to
many or all devices the operator is responsible for. A remote diagnostic
system could also run as a device vendor's service to maintain all devices
independent of the device owner (plant, location).
There exist many protocols for reading and writing data to smart devices. Some
of these protocols are standardised and many others proprietary. Assuming an
IP-capability of a 5G system, it should be possible to either use
  * Standardised IP-based protocols (such as OPC UA®, ModbusTCP®, HARTip®)
  * Pack non-IP-based protocols into IP-frames in a proprietary way. This can be inside standardised protocols such as http (port 80) or proprietary IP-based protocols (device specific ports)
  * Proprietary IP-based protocols (device specific ports)
Communicated data could be of any kind from single bytes, longer telegrams of
a few kilobytes to continuous streams. Volume applications are expected with
power and memory limited devices. The full range from power-optimised
applications on the one end to high performance real-time data on the other
end should be covered.
Remote access to a device may happen at any time (in case the user does
authorise). So the remote access shall be non-reactive to other communication
in the 5G network and operation and performance to other devices. Remote
access should have impact only on the contacted device. Prioritised traffic
mechanisms may solve the problem to not violate configured real time
conditions when upgrading a firmware of a device in the same network.
The trigger to remotely access a device may come from the device itself, based
on a certain event or condition. In this case, a device initiates a connection
to another (known) device and submits data which alerts a service to read /
write specific data from / to that device.
A major concern associated with remote access and maintenance is the potential
vulnerability of devices in terms of cyber security. Most classical wired
communication protocols in industry do not consider any cyber security
relevant scenarios. Physical access to devices and networks is almost
restricted to skilled and authorised personnel. Furthermore, protocols are not
widely used outside these specific industries and so the knowledge is kept to
a limited community of specialists.
Users might want to block a device for any remote access, others might
restrict access to only read data or just parts of data, such as operating
hour meter, but no configuration data. Some restriction levels might be
specific to the device and as such not standardised. Those restrictions should
be fully transparent to the lower communication layers (5G in this case). A
basic set of restriction levels are expected from 5G, what could be an
adoption of existing mobile communication standards,
Electronic industrial devices do have a typical life cycle between 5 and 25
years. Customers expect old devices to remain accessible during this time with
(at least) the same functionality that was provided when they were installed.
Installations in process industry and factory automation are continuously
extended and changed, so new devices are operated in parallel to old ones.
The operation for this use case can be in a wide service area, and
interworking with the public network (e.g., mobility, roaming) may be
required.
#### 5.3.9.2 Preconditions
  * Device is connected to the 5G network
  * Service tool (asset management system or asset monitor) is connected to the 5G network via a gateway
#### 5.3.9.3 Service flows
An example service flow may look as follows:
  1. Device detects a condition that \"call for maintenance\" is required and sends a message to a predefined address, i.e., that of the asset management tool OR an asset management tool opens a connection to the device.
  2. The tool checks device type and identity (starting with 5G information elements, going further down to device, customer, location specific information)
  3. The tool reads / writes data from / to the device. This could be a few bytes, new firmware, or read real-time monitoring data. This is done for a limited time.
  4. The tool closes the connection to the device
#### 5.3.9.4 Post-conditions
The maintenance case has been successfully completed.
#### 5.3.9.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * \"Un-configured\", sporadic network load, i.e., remote access injects traffic at a time not known during network setup.
  * Sporadic network traffic with a particular device must not disturb configured and already running communication
  * Low power operation of devices, i.e., long time periods without any communication but always ready to receive a request (or to open a session, link etc.)
  * Compatibility for > 25 years. Compatible means here that a device can be used in a 5G network for 25 years or more with the same or more functionality as during its initial setup (installation).
#### 5.3.9.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Factories
of the Future 9.1_ Spontaneous connection to a device in a 5G network
initiated by a remote service shall be supported. T  
_Factories of the Future 9.2_ Spontaneous connection to a service connected to
a 5G network initiated by a 5G device shall be supported. T  
_Factories of the Future 9.3_ Spontaneous connection and associated data
traffic must not disturb other communication running through the 5G network T  
_Factories of the Future 9.4_ The 5G system shall be able to classify data as
RT (real-time) and non-RT. T RT data transmission (either regular / cyclic or
sporadic/burst) might need prior configuration. _Factories of the Future 9.5_
The 5G system should support the automated discovery of 5G devices belonging
to a certain group (for example all 5G devices installed in a process plant)
in the immediate vicinity of a user (radius of about 20 m around the user) and
provide the user with a list of all detected 5G devices. A This is required in
case a person (e.g., a service technician) without prior knowledge of the
plant wants to connect to a certain device he/she is seeing (e.g., a valve).
One could think of using a label as a simple alternative, but since the device
to connect to may not be directly accessible due to pipes, etc. around it and
since it may not be possible to read a label anymore after some time due to
dirt, de-saturation, etc., the described automated discovery service would be
a nice feature for the given use case. _Factories of the Future 9.6_ A 5G
system shall support IP addressable devices. T  
_Factories of the Future 9.7_ A 5G system shall allow to use standardised and
proprietary IP-based protocols A/T Not supported standardised protocols shall
be explicitly listed. _Factories of the Future 9.8_ 5G networks shall provide
backward compatibility for > 25 years at the user equipment level. T Installed
devices shall be accessible through the 3GPP network for a long time, even
when the network standard further evolves.
* * *
### 5.3.10 Augmented reality
#### 5.3.10 _._ 1 Description
It is envisioned that in future smart factories and production facilities,
people will continue to play an important and substantial role. However, due
to the envisaged high flexibility and versatility of the Factories of the
Future, shop floor workers should be optimally supported in getting quickly
prepared for new tasks and activities and in ensuring smooth operations in an
efficient and ergonomic manner. To this end, augmented reality (AR) may play a
crucial role, for example for the following applications:
  * Monitoring of processes and production flows
  * Step-by-step instructions for specific tasks, for example in manual assembly workplaces
  * Ad-hoc support from a remote expert, for example for maintenance or service tasks
In this respect, especially head-mounted AR devices are very attractive since
they allow for a maximum degree of ergonomics, flexibility and mobility and
leave the hands of workers free for other tasks. However, if such AR devices
are worn for a longer period of time (e.g., one work shift), these devices
have to be lightweight and highly energy-efficient and they should not become
very warm. A very promising approach therefore is to offload complex (video)
processing tasks to the network (e.g., an edge cloud) and to reduce the AR
device essentially to a connected camera and display. This has the additional
benefit that the AR application may have easy access to different context
information (e.g., information about the environment, production machinery,
the current link state, etc.) if executed in the network. A possible
processing chain for such a setup is depicted in Figure 5.3.10.1-1.
{width="6.145138888888889in" height="2.254166666666667in"}
Figure 5.3.10.1-1: Possible processing chain for an augmented reality\ system
with offloaded tracking and rendering
Here, the AR tracking algorithm determines the current viewpoint of the AR
device and places the desired augmentations at the right positions in the
current image. One of the main challenges with such a setup is that the
displayed augmentations have to timely follow any movements of the camera in
the AR device (which may be caused by any movements of the person wearing the
AR device) since otherwise the AR user may get sick after some time and a
reasonable usage would not be possible. Therefore, also a compression of the
video stream from the AR device to the image processing server and back should
be avoided if possible in order to reduce the overall processing latency and
requirements.
This use case has relatively moderate latency requirements. The required
service area is usually bigger than for \"motion control\" (see Subclause
5.3.2). Interworking with the public network (e.g., mobility, roaming) is
likely not required.
#### 5.3.10.2 Preconditions
A user is wearing a head-mounted AR device, which is connected to an image
processing server in a (local) edge cloud via a 5G system. Some augmentations
have already been registered in the field of view of the user and shall be
tracked and rendered by the image processing server.
#### 5.3.10.3 Service flows
A possible service flow is as follows:
  1. A camera integrated into the AR device permanently takes new images, with a frame rate ≥ 60 Hz and at least HD (1280 x 720) or Full HD (1920 x 1080) resolution.
  2. The AR device continuously transmits all images via the 5G system to the image processing server, which may run in a (local) edge cloud, for example.
  3. The image processing server determines the current field of view of the camera integrated into the AR device based on the received image(s).
  4. The image processing server determines the optimal placements of the previously registered augmentations in the current image based on the updated viewpoint and potentially places additional augmentations in the current image.
  5. The image processing server renders the augmented image and sends it back to the AR device via the 5G system.
  6. The AR device displays the augmented image.
#### 5.3.10.4 Post-conditions
The augmentations in the view field of the user smoothly follow any movements
of the AR device in an appropriate way, offering an excellent user experience
and preventing the user to get sick after some time.
#### 5.3.10.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Very high data rate requirements along with low latency requirements.
  * The need for seamless mobility support.
#### 5.3.10.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Factories
of the Future 10.1_ The 5G system shall support the bi-directional
transmission of uncompressed video streams with a frame rate ≥ 60 Hz, HD (1280
x 720) or Full HD (1920 x 1080) resolution and at least 24-bit color depth,
resulting in user experienced data rates of about 1,33 and 3 Gb/s,
respectively. T  
_Factories of the Future 10.2_ The end-to-end latency between capturing a new
image and displaying the augmented image based on the newly captured image
shall be smaller than 50 ms, in order to avoid cyber-sickness. The one-way
end-to-end latency of the 5G system shall therefore be (significantly) smaller
than 10 ms. A  
_Factories of the Future 10.3_ The communication service availability shall be
higher than 99,9% with respect to successfully delivered video frames. That
means 99,9% of all video frames shall be successfully delivered within the
given latency constraints. A  
_Factories of the Future 10.4_ The 5G system shall support seamless mobility
in such a way that a handover from one base station to another one does not
have any observable impact on the application. T  
_Factories of the Future 10.5_ The 5G system shall support the simultaneous
usage of at least 3 AR devices per base station. T  
_Factories of the Future 10.6_ The (bi-directional) video stream between the
AR device and the image processing server shall be encrypted and authenticated
by the 5G system. T
* * *
### 5.3.11 Process automation -- closed-loop control
#### 5.3.11 _._ 1 Description
In this use case, several sensors are installed in a plant and each sensor
performs continuous measurements. The measurement data are transported to a
controller, which takes decision to set actuators. The latency and determinism
in this use case are crucial.
This use case has very stringent requirements in terms of latency and service
availability. The required service area is usually bigger than for \"motion
control\" (see Subclause 5.3.2).Interworking with the public network (e.g.,
mobility, roaming) is not required.
#### 5.3.11.2 Preconditions
There are several control loops running in parallel at the same time in the
plant. All sensors, actuators, and controllers are connected to the 5G
network.
#### 5.3.11.3 Service flows
The sensors continuously send data to the controller and the controller sends
control data to the actuators. Actuators may send back acknowledgements of
their status.
#### 5.3.11.4 Post-conditions
The production plant is running as expected.
#### 5.3.11.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * High requirements on latency, communication service availability, and determinism (small jitter).
  * Potentially high density of UEs in future
#### 5.3.11.6 Potential requirements
* * *
Reference\ Requirement text Application / Transport Comments Number
_Factories of the Future 11.1_ The 5G system shall support strictly
deterministic cyclic traffic with cycle times down to 10 ms and a maximum
jitter of less than 10% of the cycle time. T
_Factories of the Future 11.2_ The 5G system shall support communication
service availability exceeding at least 99,9999%, ideally even 99,999999%. T
* * *
### 5.3.12 Process automation -- process monitoring
#### 5.3.12 _._ 1 Description
Several sensors are installed in the plant to give insight into process or
environmental conditions or inventory of material. The data are transported to
displays for observation and/or to databases for registration and trending.
The operation for this use case can be in a wide service area, and
interworking with the public network (e.g., mobility, roaming) may be
required.
#### 5.3.12.2 Preconditions
Multiple sensors and observation points are distributed over the plant. All of
them are connected to the 5G system.
#### 5.3.12.3 Service flows
The sensors measure in defined time intervals and send the measurement data to
storage. Intermediate data logging within the sensor and acyclic data
transmission is sometimes used in order to reduce power consumption.
#### 5.3.12.4 Post-conditions
Measurement data are available at the places where needed and can be
processed.
#### 5.3.12.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Potentially harsh propagation environments with many metallic parts (pipes, tanks, supports)
  * Potentially high density of UEs
  * Potentially large communication distance over multiple kilometres
  * High energy-efficiency required in case of battery-driven sensors
#### 5.3.12.6 Potential requirements
* * *
Reference\ Requirement text Application / Transport Comments Number
_Factories of the Future 12.1_ The 5G system shall support a communication
service availability of about 99,99 % with a data transmission in intervals
between 50 ms up to several seconds. T
_Factories of the Future 12.2_ The 5G system shall support a very high user
equipment density with up to 10 000 UEs per km². T
* * *
### 5.3.13 Process automation -- plant asset management
#### 5.3.13 _._ 1 Description
To keep a plant running, it is essential that the assets, such as pumps,
valves, heaters, instruments, etc., are maintained. Timely recognition of any
degradation and continuous self-diagnosis of components are used to support
and plan maintenance. Remote software updates enhance and adapt the components
to changing conditions and advances in technology.
The operation for this use case can be in a wide service area, and
interworking with the public network (e.g., mobility, roaming) may be
required.
#### 5.3.13.2 Preconditions
Smart assets including self-diagnosis and sensors providing relevant data for
asset condition are distributed over the plant. All nodes are connected to the
5G system.
#### 5.3.13.3 Service flows
Data from smart assets and sensors are transmitted to storage within a defined
time interval. In the case of an actual failure, an event is transmitted
immediately.
In the case of a software update of smart devices, block data are transferred
to the devices. Multiple devices may be updated at the same time.
#### 5.3.13.4 Post-conditions
Data and event information are available where needed for processing and
displaying. Assets are maintained in an optimal manner.
#### 5.3.13.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Potentially harsh propagation environments with many metallic parts (pipes, tanks, supports)
  * Potentially high density of UEs
  * Potentially large communication distance over multiple kilometres
  * High energy-efficiency required in case of battery-driven sensors
#### 5.3.13.6 Potential requirements
* * *
Reference\ Requirement text Application / Transport Comments Number
_Factories of the Future 13.1_ The 5G system shall support a communication
service availability of about 99.99% with a data transmission in intervals in
the order of several seconds. T
_Factories of the Future 13.2_ The 5G system shall support a very high user
equipment density with up to 10 000 UEs per km². T
* * *
### 5.3.14 Connectivity for the factory floor
#### 5.3.14.1 Description
A factory floor has adopted 5G networks for wireless automation, where a
variety of sensors, devices, machines, robots, actuators, and terminals are
communicating to coordinate and share data. Some of these devices may be
directly connected to a local private network and some may be connected via
gateway(s).
An example of the deployment scenario is in Figure 5.3.14.1-1.
Figure 5.3.14.1-1: Factory of the future using a dedicated local private
network for industrial automation.
#### 5.3.14.2 Pre-conditions
An industrial factory has been provisioned with a dedicated RAN based on local
dedicated cells and a local dedicated core network.
Factory floor equipment like sensors, controllers, operator terminals, and
actuators have been provisioned with 5G connectivity modules. These modules
have subscription information to access the local network.
Operators, technicians, and engineers have 5G enabled devices. These devices
have subscription information to access the local network. These devices may
also have subscription information for other networks.
#### 5.3.14.3 Service flows
Factory equipment and human operators and technicians have sufficient
connectivity and credentials to connect to the local network, authenticating
with the local core network. Typical closed-loop control applications run over
this network with extremely low latency and high reliability. Due to the
dedicated nature of the network there is also high availability and
consistency of latency and throughput.
In the case of a device which does not have subscription information for the
network, the local core network will reject the attempt resulting in the local
RAN refusing access to the device.
Technicians can access the network on site and ensure high availability due to
pre-emptive maintenance for the local network. Network optimisation can also
be performed with a higher level of aptitude due to tighter integration with
the process control. In the case of catastrophic failure, technicians can
repair the network on-site.
Devices can be on-boarded directly by the factory owner.
#### 5.3.14.4 Post-conditions
Typical closed-loop control applications operate with consistent and
appropriate performance.
#### 5.3.14.5 Challenges to the 5G system
  * Integration and connectivity with factory LANs, in particular real-time Ethernet.
  * Support of local private deployment with KPIs that meet specific industrial requirements.
  * Providing isolation between machines involved in specific production processes and other parts of a factory network. For example, in Figure 5.3.14.1-1, there could be some firewall functions in the dedicated local core to allow for isolation.
#### 5.3.14.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Factories
of the Future 14.1_ The 5G system shall support private network deployments,
e.g. within a factory or plant. A  
_Factories of the Future 14.2_ The 5G system shall support isolation of
private network with public network e.g. within a factory or plant. A  
_Factories of the Future 14.3_ The 5G system shall support interworking with
wired devices and legacy end-user equipment (e.g. devices supporting
Ethernet). A
* * *
### 5.3.15 Inbound logistics for manufacturing
#### 5.3.15.1 Description
In this example, a supply-chain company moves products between multiple
different independent manufacturing, distribution, and retail centres. The
container traverses the networks of various port operators, liner ships,
trucking companies, and warehouses. The heavy good vehicles (HGVs) operated by
the supply-chain company are wirelessly connected to the PLMN to enable real
time tracking and telematics. In these cases, the use case often changes when
the actor is in range of one or another network. What these use cases have in
common is that the actor needs to be able to connect to the correct network at
any given time to operate correctly. The pallets carrying the materials for
delivery have also been connected via 5G UEs or IoT devices for tracking and
inventory control purposes.
This use case describes the scenario where a HGV arrives at the receiving area
of a factory and delivers a pallet of materials which is subsequently
incorporated into the local factory inventory management system in an
automated manner.
#### 5.3.15.2 Pre-conditions
A factory has been provisioned with a private local 5G network. A supply-chain
company which delivers palletised goods supplies this factory (amongst
others).
The distribution vehicle has subscription credentials for the public macro
network.
The pallet(s) for delivery to the factory have subscription credentials for
the public macro network and either (a) subscription credentials for the
factory local private networks or (b) facility to obtain and use subscription
credentials for the factory local private network.
The supply-chain company tracks the pallets and vehicles via MNO network when
in transit.
#### 5.3.15.3 Service Flows
PLMN as each item has a separate subscription. This connection is used to
report the HGVs location and telematics, and the location of the pallets.
The HGV arrives at the industrial factory. The HGV remains connected to the
macro network but the pallet is expected to connect to the local private
network to track its arrival and integrate with the stock control systems of
the factory. The pallet detects the presence of the local network of the
destination factory, obtains the credentials to attach to the network (if not
already provisioned), and connects to the local private network. For example,
the home operator may remotely provision the credentials via a secure
mechanism, or the local private network may provision the credentials via a
secure mechanism.
No other pallets on the HGV (which are destined for different locations)
attempt to attach to the local private network.
The pallet identifies itself to the factory's stock control system and is now
tracked by the factory processes.
#### 5.3.15.4 Post-conditions
The HGV leaves the industrial premises whilst still connected to the macro MNO
network and the pallet remains, connected to the private network of the
factory.
#### 5.3.15.5 Challenges to the 5G system
5.3.15.5.1 Identification of private networks and method of connection
As the use cases for local private networks are required to be scalable for
massive numbers of deployments, including by non-traditional MNO actors,
identification of the local private network need to avoid bottleneck into PLMN
ID allocation ( the PLMN IDs with limited number space might quickly exhausted
in those countries with high uptake of local private networks).
In this scenario, when the 5G or IoT device attached to the pallet detects the
presence of a local private network, there are some options on how the 5G UE
or IoT device connects to the local private network.
  * Dual subscription: the 5G device has independent subscriptions for public and private 5G networks.
  * Dual registration: the 5G UE or IoT Device remains registered on and connected to the PLMN and establishes a second registration and connection to the local private network.
  * Manual PLMN selection: where the 5G UE or IoT Device performs a manual PLMN selection procedure (although this process may be initiated by an automatic procedure outside of 3GPP scope). As a consequence, the network should have a human readable name to enable manual selection of the private network.
#### 5.3.15.6 Potential requirements
+------------------+------------------+------------------+----------+ | Reference Number | Requirement text | Application / | Comments | | | | Transport | | +------------------+------------------+------------------+----------+ | _Factories of |_ The 5G system | _T_ | | | the Future 15.1 _| shall support | | | | | unique | | | | | identifiers for | | | | | private | | | | | networks._ | | | | | | | | | | _See note 1_ | | | +------------------+------------------+------------------+----------+ | _Factories of |_ A UE shall be | _T_ | | | the Future 15.2 _| able to detect | | | | | the identity of | | | | | a private | | | | | network before | | | | | attempting to | | | | | attach._ | | | +------------------+------------------+------------------+----------+ | _Facories of the |_ A UE shall be | | | | Future15.3 _| required to have | | | | | a subscription | | | | | for each | | | | | particular | | | | | private network | | | | | from which it | | | | | can receive | | | | | communication | | | | | services._ | | | +------------------+------------------+------------------+----------+ | _Factories of |_ The 5G system | _A_ | | | the Future15.4 _| shall support | | | | | devices that can | | | | | access | | | | | independently | | | | | both public 5G | | | | | network and | | | | | private 5G | | | | | network, | | | | | potentially at | | | | | the same time._ | | | +------------------+------------------+------------------+----------+ | Note 1: | | | | | Identification | | | | | of a private | | | | | network needs to | | | | | be possible, | | | | | even for a | | | | | potentially very | | | | | large number of | | | | | private networks | | | | | that are needed | | | | | for numerous | | | | | enterprises. | | | | | This might | | | | | require | | | | | identification | | | | | schemes that | | | | | extend or | | | | | replace current | | | | | network | | | | | identification | | | | | based on PLMN | | | | | IDs. | | | | +------------------+------------------+------------------+----------+
### 5.3.16 Wide-area connectivity for fleet maintenance
#### 5.3.16.1 Description
A scenario where this form of deployment scenario would be applicable is in
the automatic wide-area data collection and tuning of an automotive fleet. In
the following example, a Heavy Goods Vehicle (HGV) manufacturer has an ongoing
contract with a haulage company to constantly track the performance of a fleet
of vehicles and automatically remap their Electronic Control Unit over-the-air
to ensure efficient performance based on haulage load. In this scenario, wide
area coverage is essential and the use case is highly latency tolerant.
An example of the deployment scenario is in Figure 5.3.16.1-1.
{width="1.1006944444444444in" height="0.73125in"}
{width="1.1006944444444444in" height="0.73125in"}
{width="1.1006944444444444in" height="0.73125in"}
{width="1.1006944444444444in" height="0.73125in"}
#### 5.3.16.2 Pre-conditions
A HGV fleet manager has a contract for the HGV manufacturer to provide ongoing
engine control unit (ECU) updates to ensure optimal fuel efficiency.
The HGV fleet has been provisioned with connectivity modules connected to the
ECU of their engines. These modules have a subscription to a nationwide MNO
for data connectivity.
#### 5.3.16.3 Service flows
The fleet of HGVs periodically upload telematics data via the MNO-connected
connectivity modules. This upload is very delay tolerant (>30 min). The data
is routed via MNO network to the HGV manufacturer's enterprise server for
analytics and storage.
After analysis, a new ECU remapping is generated for an individual HGV and is
transmitted down to the vehicle for installation at a convenient time (e.g.
when the vehicle is parked). This installation may also be scheduled to happen
live with an OTA connection (again, at a convenient time).
The vehicle continues periodic telematic upload throughout.
#### 5.3.16.4 Post-conditions
Data upload and download operates with use-case appropriate performance.
#### 5.3.16.5 Challenges to the 5G system
None.
#### 5.3.16.6 Potential requirements
None.
### 5.3.17 Variable message reliability
#### 5.3.17 _._ 1 Description
Many control systems operate synchronously based on a periodic control cycle.
Measurements (sent by the sensors) are usually periodic. This allows the
controller to request the actuators to perform small adjustments in order to
maintain the desired output response and a stable system. A stable system will
be operating within a desired output response during a predefined time window.
The reliability of the transmissions has to be very high: the measurements
need to be received successfully and any commands sent to the actuator must
also be received successfully, all within tight latency bounds.
There are some processes (or plants) that may already be in a stable
situation. As an example, a robotic arm which performs a repetitive task (e.g.
pick up a package), will require the arm to be positioned within a target area
A at a given time window T. As long as the arm is within the target area A
within window T, the system is considered stable. When the system is stable,
commands to adjust the position of the arm may not be necessary. On the other
hand, if the arm is outside the area at the target time window, the controller
will request the actuator to move the arm toward the target area A, and such
actions will have a high priority or urgency, and therefore the command sent
requires a higher reliability from the network.
Therefore, the information regarding system state or the \"urgency\" of the
message, or the desired reliability of the message transmission, can be used
by the network to manage resources more efficiently by scheduling resources
for each of the transmissions. Similarly, if the process or plant under
control is stable, the data may not be as urgent, and the network can give
fewer resources to the sensor device to send the measurements to the
controller or to a command to keep the actuator at the same position. That
means that the desired reliability for the message can vary dynamically, and
if this information can be provided from the controller to the network, the
network can use that information to optimise resource allocation.
The alternative to adjusting the reliability dynamically is provisioning the
system for the highest possible reliability required, which would require more
resources and therefore reduce the network efficiency.
#### 5.3.17.2 Preconditions
A robotic arm performs a repetitive task to pick up a package.
The system is considered stable if the robot's arm is positioned within a
target area A at a given time window T.
The control system operates synchronously, based on a periodic control cycle.
#### 5.3.17.3 Service flows
The robot's sensors report measurements to the application function
(controller) in an application server outside the 3GPP domain.
The application function recognises that the system is stable and the robot's
arm is within the target area A within time window T.
The application function sends a command to the actuator in the next cycle. As
the system is stable the application function informs the 3GPP network that
the message reliability can be relaxed.
#### 5.3.17.4 Post-conditions
The message is sent with lower reliability.
#### 5.3.17.5 Challenges to the 5G system
New interface definition needed between 3GPP network and application function.
#### 5.3.17.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Factories
of the Future 17.1_ The 3GPP system shall provide means for an application
function outside the 3GPP domain to request specific reliability for each (set
of) message(s) transmitted  
_Factories of the Future 17.2_ The 3GPP system shall provide means for 3^rd^
party application servers to provide information about the required
transmission QoS desired for different granularity of data transmitted by the
3^rd^ party application server. The data granularity can be each packet or a
set of packets. This information may be used by the 3GPP system to optimise
resource usage.
* * *
### 5.3.18 Flexible, modular assembly area
#### 5.3 _._ 18 _._ 1 Description
In the Factories of the Future, static sequential production systems will
increasingly be replaced by novel modular production systems offering high
production flexibility and versatility. This concept encompasses a large
number of increasingly mobile production assets, for which powerful wireless
communication and localisation services are required.
NOTE: The communication streams in this use case will usually have to coexist
with those for other applications, e.g., massive wireless sensor networks
(Subclause 5.3.8) and/or remote access and maintenance (Subclause 5.3.9).
#### 5.3 _._ 18 _._ 2 Preconditions
Life cycle chain status: A modular production environment adaptable to the
different variants of orders is up and running. Modular assembly stations can
be added or moved inside the production area.
Communication infrastructure: A private, local network inside the
manufacturing hall is installed, and the deployment is adapted to the flexible
production layout. The local, private network is realised by a 5G system.
Mobile assets that are in a ready state, i.e. they are ready for factory
production:
  * Lift and lowering AGV (moveable assembly platforms);
  * Convey and lift AGV;
  * Mobile robots with video support for unstructured goods in stock;
  * Root trains;
  * Workers assistance support by aid of video up and download;
  * Portable assembly tools (power screwdriver, riveting tools, staple gun ...);
  * Portal cranes.
#### 5\. 3 _._ 18 _._ 3 Service flows
Communication flows in the assembly area:
  * Vertical: ERP -- MES (order and resource management);
  * Vertical (MES): centralised orchestration of assets, material and quality status;
  * MES (decentralised production): in many cases, communication will be initiated by the assembled part itself. In this document this unit is called MPSC (manufactured product as a smart client). This MPSC is often moved by an AGV. Associated communication patterns
    * MPSC -- MES;
    * MPSC -- asset, worker (e.g. request for processing);
    * Asset -- MES (e.g. status);
    * Asset -- asset (e.g. collaborating robots);
    * Asset -- material (e.g. Ident, localisation);
    * Worker -- MES (e.g. request for assistance);
    * Worker -- asset (e.g. request for localisation);
    * Worker -- material (e.g. Ident, localisation).
  * ERP, MES and MPSC communication:
    * Status information exchange between the MPSC, MES and ERP;
    * MES or MPSC resource request for mobile assets, workers, and tools (identification, localisation, status, commands);
    * Material request based on order management: mobile robots, AGVs, and root trains are instructed to bring parts and material to the assembly station just in time. Sending of status information;
    * At an assembly station:
      * Workers get the task of assembling parts according instructions with real-time video assistance;
      * Mobile robots get the task to assemble parts according to instructions from the MES or MPSC. Particular collaboration with a second robot or workers is requested;
      * Data are reported: during and after the assembly process; status and quality information---combined with position info---are contributed.
#### 5\. 3 _._ 18 _._ 4 Post-conditions
The MPSC is moved to the next process step & assembly station by an AGV.
#### 5\. 3 _._ 18 _._ 5 Challenges to the 5G system
  * Ultra-reliable wireless communication of a variety communication services adhering to negotiated and guaranteed QoS parameters.
  * Communication bursts when several parallel actions occur (e.g. parallel actions supported by real-time video assistance).
  * High density of mobile assets.
  * Changing the 5G network configuration in the production when the layout of the assembly area is altered. The 5G network configuration change is carried out by the production staff, supported by self optimising algorithm. In some cases this only involves the UEs. In other cases, base stations might have to be relocated or more base stations would have to be added to the 5G network.
  * 5G network maintenance and assurance:
    * Monitoring the production environment and processes related to communication; detect potential communication bottlenecks in the production area.
    * The diagnosis of network error, faults, underperformance, etc. contains recommendations for what to do for fulfilling the requested QoS.
#### 5\. 3 _._ 18.6 Potential requirements
Editor\'s note: FFS
### 5.3.19 Plug and produce for field devices
#### 5.3.19 _._ 1 Description
This use case covers the realisation of plug-and-produce for intelligent field
devices that utilise 5G communication services. This use case is based on the
plug-and-produce use case described in [45].
Plug-and-produce addresses the automated integration and configuration of a
(new) field device into an existing production system. The plug-and-produce
use case is applicable to discrete manufacturing as well as continuous and
batch processing. The goal of plug-and-produce is to increase the flexibility
and adaptability of production systems and to speed up the commissioning
process of field devices by reducing manual overhead. The field device in
question may be an individual sensor or actuator or a more complex production
unit. After being connected and being discovered by the production system, the
field device automatically obtains the configuration required to participate
in the production process.
The plug-and-produce use case is divided into 6 steps/scenarios, starting from
the physical connection of the device until the device is fully integrated
into the production process (see Figure 5.3.19.1-1).
Figure 5.3.19.1-1: Integration of field devices into a production network
(based on [45]).
Connection: the commissioning engineer establishes physical network
connection: device is switched on, wired network connection is plugged in or
device is in wireless reach of the 5G base station.
  1. Discovery: the device obtains an IP address and advertises its services in the network.
  2. Establishment of basic communication: the device certificate is validated and the connection towards the production system device management server is authenticated (e.g., OPC UA[^1] connectivity).
  3. Capabilities assessment: the device management server knows the production services offered by the device and is able to configure the devices\' production services.
  4. Configuration: the device is configured and set up for performing the desired task within the production system.
  5. Integration into production: the device contributes to the production process.
In addition to the six steps above, two alternative scenarios concerning the
identification (validation of device certificate failed) and the replacement
of a device are considered.
Plug-and-produce may be realised using either wired or wireless networking
infrastructure. The following sections consider wireless network connectivity
using a 5G communication infrastructure for plug-and-produce.
Note that the communication network and the application of the production
system need to be distinguished. On the one hand, there is the communication
network providing connectivity between the individual network nodes (OSI
network layers 1-3/4, setup by steps 1 and 2 of the use case). The topology of
this network is determined by the wiring of the nodes and the wireless
connectivity of the devices. This functionality is assumed to be provided by
the wireless 5G network. On the other hand, there is the automation
application of the production system (OSI application layers 5 to7, setup by
steps 3 to 6). The communication topology of the automation application is
determined by the tasks to be performed by the production system, e.g., the
collaborations required between individual automation devices in order to
complete a manufacturing step.
In classic automation systems (i.e., before Industrie 4.0), the topologies of
the communication network and the automation application are typically closely
aligned. The topology of the physical communication network is derived from
the requirements of the automation process in the engineering phase.
Automation devices that need to cooperate are placed in the same network zone
(e.g., a LAN, a network segment, or a fieldbus section).
Classic automation systems are structured hierarchically. Exchange of data
between hierarchies or zones is only allowed between well-defined groups of
devices (zones and conduits).[^2] This approach is an important building block
for the security concept for industrial control and automation systems. 5G
needs to provide an equivalent way to realise restricted data flow and
implement (network) access control, etc. for industrial control and automation
systems.
#### 5.3.19.2 Preconditions
The field device possesses a set of authentication credentials. The
authentication credentials were issued by either the plant operator or the
device manufacturer. Different types of credentials are possible, e.g.
  * Challenge response (e.g., CHAP);
  * Generic token cards;
  * One-time passwords;
  * X.509 credentials (e.g., TLS Authentication);
  * SIM/AKA.
The selection of credentials depends on the automation solution in use. X.509
certificates are a popular choice for automation systems relying on OPC UA. It
is likely that an automation solution uses a variety of communication
protocols. However, TCP/IP technologies like OPC UA are usually deployed
throughout the entire plant and are therefore an important driver for the
selection of the authentication mechanisms to be used.
The following services have been set up:
  * Network configuration service: used to provide network (e.g., DHCP) and service access (e.g., OPC UA discovery) configuration.
  * Validation authority: this service provides information about the validity of a digital certificate (here: X.509) within the scope of the automation system.
  * Authentication server: verifies the authentication information sent by the clients. An authentication server may support different types of authentication, depending on the authentication mechanisms and credentials available on the field device. The Authentication server can provide additional information on user/client access rights.
#### 5.3.19.3 Service flows
The following tables describe the individual service flows (see [45]).
Modifications and additions specific to 5G (compared to the original wired
scenario in [45]) are set in _italics_.
**A. Connection - connect device physically**
This step includes the preparation and physical installation of the field
device in the automation system.
* * *
Step Event Name of process/activity Description of process/activity 1 Device
mounting request Prepare device for connection Bring the device to the
location it shall be plugged in, unpack it, _~~plug-in network cable into the
device~~_ , plug-in power cable into power supply (_if not battery-powered_) 2
Device prepared Plug-in device _Bring the device into the proximity of the 5G
radio access point_ 3 Device plugged Turn on device Switch power button on.
* * *
Note, the device may have multiple, different communication ports, both
wireless and wired.
**B. Discover - discover device**
This step includes the establishment of a network connection to the 5G
network.
+------+-------------------+-------------------+-------------------+ | Step | Event | Name of | Description of | | | | process/activity | process/activity | +------+-------------------+-------------------+-------------------+ | 1 | Deviced switched | Recognise newly | Device detects | | | on | connected device | lower network | | | | | layer | +------+-------------------+-------------------+-------------------+ | 2 | Device recognised | Assign network | _Mutual | | | | address | authentication of | | | | | field device and | | | | | network using the | | | | | available, | | | | | persistent device | | | | | credential._ | | | | | | | | | | _The type of | | | | | credential is | | | | | determined by the | | | | | automation | | | | | application (see | | | | | step C)._ | | | | | | | | | | _The choice of | | | | | the persistent | | | | | device credential | | | | | is not dependent | | | | | on the port. This | | | | | includes the 5G | | | | | port._ | | | | | | | | | | _Field device and | | | | | network establish | | | | | temporary session | | | | | credentials to | | | | | secure the radio | | | | | link._ | | | | | | | | | | Determine an | | | | | available network | | | | | address, assign | | | | | it to the device, | | | | | send it to the | | | | | device | | | | | | | | | | _Using a wired | | | | | network, the | | | | | communication is | | | | | initially | | | | | restricted to the | | | | | physical network | | | | | segment to which | | | | | the device is | | | | | connected. 5G | | | | | could be used to | | | | | realise a similar | | | | | restriction._ | | | | | | | | | | _Restrict access | | | | | to logical | | | | | zones/services._ | +------+-------------------+-------------------+-------------------+ | 3 | Device is | Notify device | Announce the | | | addressable via | management | device to the | | | the network | | device management | | | | | server to start | | | | | configuring it. | +------+-------------------+-------------------+-------------------+
**C. Establishment of basic communication of the automation application**
This step includes the establishment of a connection to the automation network
(e.g., via OPC UA). These sub-steps use the communication network but are
independent of the underlying communication technology (5G, Bluetooth LE,
WiFi, etc.).
* * *
Step Event Name of process/activity Description of process/activity 1
Notification of device availability Connect to the device Contact the device
and create a new session context on the device for the following interaction.
2 Connected to device Get device certificate Retrieve device certificate from
the device. 3 Device certificate available Validate certificate Contact
validation authority and validate the certificate to ensure valid identity of
the device. 4 Device certificate validated Authorise connection Contact
authentication service to authorise the device management server for reading
and writing device parameters. NOTE: The field device possesses a set of
different authentication credentials. The selection of the actually used types
of credentials depends on the automation solution in use (see Section
5.3.X.2). The Extensible Authentication Protocol (EAP) is used as a common
authentication framework in order to provide the necessary flexibility,
extensibility, and for being future-proof.
* * *
D. Integrate device into production
+------+-------+-------------------------+-------------------------+ | Step | Event | Name of | Description of | | | | Process/Activity | process/activity | +------+-------+-------------------------+-------------------------+ | _1_ | | _Refine network |_ Configure network in | | | | access/visibility _| order to allow | | | | | communication with | | | | | other devices required | | | | | to complete the current | | | | | production task. | | | | | Restrict communication | | | | | with other field | | | | | devices accordingly._ | | | | | | | | | | _Adapt or establish | | | | | virtual zones and | | | | | conduits._ | +------+-------+-------------------------+-------------------------+
**E. Identity validation failed (alternative service flow)**
This is an alternative scenario to scenario C (Establishment of basic
communication). In contrast to scenario C, the outcome of the validation of
the certificate is negative.
Steps 1-3 are the same as in scenario C (Establishment of basic
communication).
* * *
Step Event Name of Process/Activity Description of process/activity _1-3_ see
C see C see C _4_ Device identity validation failed Disconnect and discard the
device Disconnect from the device or quarantine the device; mark it as not
reliable, notify other systems. NOTE: Step 4 corresponds to the action of the
automation application. Basic network access, as established in step B, might
not be affected by these actions. It is advantageous if the 5G network can
support the quarantining of unreliable devices.
* * *
#### 5.3.19.4 Post-conditions
The intelligent field device is able to communicate in the 5G network. The
communication might be constrained according to the needs of the actual
production scenario (e.g., QoS settings, limited subset of communication
partners (automation system zone), virtual zones and conduits established).
#### 5.3.19.5 Challenges to the 5G system
  * Authentication for automation systems in networks with different technology stacks, e.g. Ethernet, Bluetooth, WLAN, 5G, etc., needs to be possible with the same type of credentials.
Editor\'s Note: FFS; this challenge needs further clarification. Interested
organisations will work on this use cases including this challenge before
3GPPSA1#81.
  * Support of different types of authentication credentials depending on the credentials available in a specific industrial site/application.
Editor\'s Note: FFS whether the same credentials can be used for step B and
step C, and what challenges that would pose for the 5G system.
  * Allow automation system functions (e.g., MES/SCADA) to control network access properties.
NOTE: Field device communication is typically not an all-to-all or many-to-
many communication scenario. Wireless field networking needs to support the
enforcement of predefined communication paths based on engineering and
resource planning information (\"virtual zones and conduits\").
  * Integration of 5G communication links within a local automation network zone, i.e. enforcement of network isolation
  * Assurance of high availability; the communication services required to realise the plug-and-produce use-case have to be available locally even if back-end connectivity should not be available (island mode).
#### 5.3.19.6 Potential requirements
Editor\'s Note: FFS
### 5.3.20 Private-public interaction
#### 5.3.20.1 Description
An enterprise deploys a 5G private network within its factory complex. The
network supports robotic controls for the manufacturing equipment;
communications between the equipment and the UEs of the responsible employees;
and communications between employee UEs. The manufacturing equipment is
limited to receiving service only on the private network. The employee UEs are
capable of being used both on the private network---for communicating with
other devices on that network---and on the PLMN, for communicating to other
UEs outside of the private network.
The robotic controls require URLLC capabilities that ensure appropriate
actions are taken by the robots. As these requirements can be quite different
from those for supporting the employee UEs, the network resources providing
URLLC capabilities may be reserved for use by the robotic controllers. This
separation of resources is needed to prevent an employee UE from using the
URLLC radio or network resources, and potentially interfering with a robotic
controller's ability to precisely control a factory robot.
UEs that do not belong to the factory are not allowed to access the private
network, to avoid any use of private network resources by non-authorised UEs.
Since the private network may overlap the coverage are of one or more PLMNs,
there is a risk of excessive resource usage and churn if a UE that does not
belong the factory finds a stronger signal from the private network and
attempts to attach to the private network, only to be rejected later based on
authentication or authorisation validation. This is of particular concern for
a URLLC type capability where the churn may impact the ability of a time
sensitive factory UE (e.g., robotic controller) in receiving access to the
network.
Since some UEs also need to be able to communicate over the PLMN as well as
within the private network, the enterprise provides employees with a selection
of UEs and service providers/MNOs for their PLMN usage. These UEs need to be
able to support simultaneous service on both the private network and PLMN, as
in the case where the employee is receiving communication from a piece of
manufacturing equipment regarding a processing problem and simultaneously
consulting with a remote colleague for the appropriate corrective action.
The system needs to support service continuity for UEs that cross between the
PLMN and the private network while actively engaged in a communication. For
example, an employee may call a colleague while driving to work. When the
employee reaches work and enters the building, the UE is handed over to the
private network, at which time the communication continues with no disruption
in service. The private network may provide better coverage within the
building as well as support additional functionality related to the factory
business that is not available in the PLMN.
#### 5.3.20.2 Pre-conditions
The factory equipment is only able to access the private network.
The employee UEs may access both the private network and a PLMN.
The employee UEs may simultaneously be active on both the private network and
a PLMN.
The employee UEs may handover between the private network and PLMN when
entering or leaving the factory complex.
Non-authorised UEs are not allowed access to the private network.
#### 5.3.20.3 Service Flows
A piece of factory equipment detects an error condition and reports the
problem to the human supervisor. This communication may take place in a
variety of data formats, from a short text message to streaming video.
A robotic controller responds in a timely and efficient manner to an alert
from one of the factory robots, providing corrective instructions that prevent
an accident.
The supervisor needs to consult a colleague to address the issue. The
colleague is on the way to the factory, but currently outside the range of the
private network, so the remote colleague is currently receiving service from
the PLMN. If dual coverage is available by both the private network and PLMN,
the supervisor can select which to use for the external call.
As the two colleagues are talking, the remote colleague reaches the factory
and the UE is handed over to receive service from the private network where
better coverage is available.
A person walking past the factory attempts to make a call using their personal
UE. The UE is only able to access the PLMN.
#### 5.3.20.4 Post-conditions
There is no disruption of service when the supervisor uses the private network
for communication with the factory equipment and the PLMN for communication
with the remote colleague.
There is no resource conflict between the robotic controller and the employee
UEs using the private network.
There is no disruption of service when the remote colleague reaches the
factory and is switched from PLMN to private network service.
There is no use of private network resources by the non-authorised UE.
#### 5.3.20.5 Potential Requirements
+----------------+----------------+----------------+----------------+ | Reference | Requirement | Application / | Comments | | Number | text | Transport | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | | A private | | the Future | shall support | | network may be | | 20.1_ | the deployment | | realised as | | | of private | | e.g., private | | | networks. | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | network slice | | | | | | | | | | This is a | | | | | deployment | | | | | requirement | | | | | rather than a | | | | | technical | | | | | requirement. | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | | This | | the | shall support | | requirement | | Future20.2_ | a mechanism | | may be met in | | | for a UE to | | different | | | identify a | | ways, | | | private | | depending on | | | network. | | how the | | | | | private | | | | | network is | | | | | realised | | | | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | network slice) | +----------------+----------------+----------------+----------------+ | _Factories of | The 3GPP | | This | | the Future | system shall | | requirement | | 20.3_ | support a | | may be met in | | | mechanism to | | different | | | allow a UE to | | ways, | | | select a | | depending on | | | private | | how the | | | network it is | | private | | | authorised to | | network is | | | access. | | realised | | | | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | network slice) | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall be | | This | | the Future | able to detect | | requirement | | 20.4_ | the | | may be met in | | | availability | | different | | | of a private | | ways, | | | network | | depending on | | | supported by a | | how the | | | cell before | | private | | | attempting to | | network is | | | access the | | realised | | | cell. | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | network slice) | +----------------+----------------+----------------+----------------+ | _Factories of | The 3GPP | | This | | the Future | system shall | | requirement | | 20.5_ | support a | | may be met in | | | mechanism to | | different | | | prevent a UE | | ways, | | | from accessing | | depending on | | | a private | | how the | | | network it is | | private | | | not authorised | | network is | | | to select. | | realised | | | | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | network slice) | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall | | | | the | support | | | | Future20.6_ | multiple | | | | | simultaneously | | | | | active | | | | | subscriptions. | | | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall | | | | the Future | support a | | | | 20.7_ | mechanism to | | | | | simultaneously | | | | | receive | | | | | services using | | | | | multiple | | | | | subscriptions | | | | | and | | | | | connections to | | | | | multiple | | | | | private and/or | | | | | public | | | | | networks. | | | +----------------+----------------+----------------+----------------+ | _Factories of | Subject to an | | Supporting | | the Future | agreement | | intersystem | | 20.8_ | between the | | mobility | | | ope | | between a | | | rators/service | | private and | | | providers, | | public network | | | operator | | depends on | | | policies and | | several | | | the regional | | factors | | | or national | | e.g.,having | | | regulatory | | the | | | requirements, | | appropriate | | | the 5G system | | business | | | shall support | | relationship | | | intersystem | | in place | | | mobility | | between the | | | between a | | network | | | private | | operators. | | | network and a | | | | | PLMN. | | Using common | | | | | identifiers | | | | | | | | | | Using common | | | | | authentication | +----------------+----------------+----------------+----------------+ | _Factories of | A private | | This | | the Future | network shall | | requirement | | 20.9_ | be able to | | allows the | | | provide | | case where the | | | service for | | private | | | UEs with | | network | | | subscriptions | | provides | | | to different | | service for | | | private and/or | | both UE1 and | | | public | | UE2 where UE1 | | | _network_ | | also has a | | | operators. | | subscription | | | | | with MNO-A and | | | | | UE2 also has a | | | | | subscription | | | | | with MNO-B. | +----------------+----------------+----------------+----------------+ | _Factories of | A private | | | | the Future | network shall | | | | 20.10_ | be able to | | | | | operate in | | | | | either | | | | | licensed or | | | | | unlicensed | | | | | bands. | | | +----------------+----------------+----------------+----------------+
## 5.4 Smart Living - Health Care
### 5.4 _._ 1 Description of vertical
Smart living is one of the verticals that is focused on transforming
healthcare through mobile health delivery, personalised medicine, and social
media e-health applications. Medical data is very sensitive and private and
requires a high degree of reliability in transporting the data. There is
already a lot of work done in this area, but 5G mobile will play a significant
part in advancing this area of study. Some of the information transferred is
low data readings and if they are consistent then they can be transferred with
low priority until there is exceptional data that will generate an alarm to be
raised. The use case described here can be likened to any other use cases for
monitoring data. The uniqueness here is the sensitivity and privacy that is
required.
### 5.4.2 Telecare data traffic between home and remote monitoring centre
#### 5.4.2 _._ 1 Description
eHealth provides the capability for remote monitoring and care, this
eliminates the need for frequent visit to the doctors and it allows for
efficient management of chronic diseases for both patient and the medics. This
use case is about the automated monitoring of data and suggests that such
sensitive information should be managed in a secure way and in some cases can
allow for different level of authorisation of this information. In some cases
regular monitoring of patient data can trigger an alarm to the patient
depending on the information received or in other cases it can trigger
authorisation to some other parts of the patient information received by other
medical care bodies and finally the same information with different level of
authorisation can trigger a warming from a consultant which requires a
different but maybe higher level of authorisation.
#### 5.4.2.2 Preconditions
Rules used to categorise different authorisation levels and criticality of
information are set in place using a policy server.
#### 5.4.2.3 Service flows
Most telecare data will need to be transferred to support real-time critical
alarm situations. Alarm situations normally initiate a voice call. In such
alarm situations, link availability and reliability are major considerations.
In this use case it is assumed that the patient has a number of monitoring
device (wearables) which could also be part of or connected to a 3GPP 5G
device. In this scenario there are different types of information that are
collected and transferred via the 3GPP network to a medical centre (data
measuring and policy decision centre) which could be on a 3GPP server
belonging to a service provider or network operator.
The medical centre will determine the alarm level based on a policy and based
on this level will automatically request a call or text to be initiated to
either the patient (level1 decision maker), a medical advisor (a nurse: Level2
decision maker) and or to a consultant (level 3 decision maker) informing them
that the critical level has been reached and the necessary contingency plan
should be taken depending on the severity of the alarm.
The 3GPP system will automatically set up a dedicated line of communication in
a secure manner, depending of the level of the alarm and this will allow for
the right level of confidentiality for the information recipient warning them
they need to either take their medication or prepare to come to the hospital
for further investigation.
{width="5.461111111111111in" height="3.707638888888889in"}
Figure 5.4.2.3 Remote Monitoring and authorisation in Tele Care Management.
#### 5.4.2.4 Post-conditions
The system resets itself to the default state and measuring and monitoring of
data resumes.
#### 5.4.2.5 Challenges to the 5G system
Medical devices are often wearables. Sometimes the medical devices are even
implantable (e.g., an insulin pump). Wearable devices have a small form
factor. This implies that also the battery has to be small. A small battery
has two implications:
  * the battery standby time will be limited unless power saving is used;
  * the maximum output power is limited, which limits the uplink range.
Companies that supply medical devices generally want to be as much as possible
independent of local conditions. E.g., they prefer not to be dependent on a
specific subscription or application that the patient has on his/her mobile
phone (if the patient has a mobile phone) or not dependent on specific fixed
network subscriptions for in-home coverage. Therefore, an independent mobile
connection is generally preferred. This can be implemented with power-
efficient mobile access technology (e.g. eMTC).
In cases where a direct connection to the network is not feasible, a
transparent Relay UE may be used. This combines the advantage of the
subscription for the medical device being independent from the Relay UE with
battery and power efficient connectivity.
#### 5.4.2.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments Smart
Living 1.1 The 3GPP system shall support mechanisms to differentiate between
levels of authorisation required for decision Transport  
Smart Living 1.2 The 5G system shall support battery- and power-efficient
communication for constraint IoT devices. Transport This requirement is
already addressed by the existing CIoT optimisations Smart Living 1.3 The 5G
system shall support the remote UE to access to the same services whether
using indirect communication or using direct communications. Transport This
requirement is already covered by the existing REAR feature as specified in
22.278 (Rel-15) Smart Living 1.4 The 5G system shall be able to ensure the
confidentiality and integrity of data for/from the remote UE data in indirect
communications. Transport This requirement is already covered by the existing
REAR feature as specified in 22.278 (Rel-15) Smart Living 1.5 The 5G system
shall be able to support remote UE and relays with different subscriptions
from different PLMNs. Transport This requirement is **not yet** covered by the
existing REAR feature as specified in 22.278 (Rel-15)
* * *
## 5.5 Smart city
### 5.5.1 Description of vertical
The smart city vertical covers data collection and processing to more
efficiently monitor and control city resources, and to provide services to
city residents. Domains include road traffic, electric and water systems,
waste management, public safety, schools, and other services.
### 5.5.2 Remote CCTV analysis
#### 5.5.2.1 Description
A video analytics company is contracted to analyse CCTV streams for enforcing
restricted zones at various related retail locations across a country. The
local cameras implement motion detection, and on detection motion in their
field of view, they live stream to the video analytics company for analysis.
The remote analysis is based on object & facial recognition and provides
informational alerts to employees at the retail location guiding security
responses.
The purpose of using a dedicated network slice in this scenario is to ensure
that the video streams have sufficient guaranteed quality of service to remain
at a high quality (i.e. no UE rate adaptation), consistent latency &
throughput to prevent buffering, and -- very importantly -- that the video
stream is routed in such a way as to avoid network video optimisation
functions which would otherwise compress the feed, making analysis more
difficult.
An example of the deployment scenario is in Figure 5.5.2.1-1.
{width="0.37569444444444444in" height="0.37569444444444444in"}
Figure 5.5.2.1-1: Dedicated, sliced public wide-area network for industrial
automation
#### 5.5.2.2 Pre-conditions
A video analytics company is contracted to analyse CCTV streams for enforcing
restricted zones at various related retail locations across a country.
#### 5.5.2.3 Service flows
A camera is positioned and configured to monitor an emergency fire escape and
door and the camera is provisioned with network credentials either prior to
installation or remotely by (a) the MNO operating the public network, (b) the
CCTV operator via a platform offered by the MNO operating the public network.
The CCTV camera is also authorised to use the \"CCTV\" slice of the public
network.
The CCTV camera detects motion and the CCTV camera is triggered by this
activity to stream its feed to a video analytics server via the 5G MNO network
covering the location.
The CCTV establishes a connection to the 5G MNO network to us the \"CCTV\"
slice which offers sufficient GBR for 1080p\@30f/s with low jitter & traffic
routeing without a video optimisation server. The CCTV camera sets up a stream
to the configured remote server at the video analytics company and the 5G MNO
routes the traffic appropriately with the requisite QoS.
The video analytics server performs object recognition (\"person\") and then
facial recognition (\"active security employee\") to determine that this is
not a threat and alerts the on-site operators of the retail venue of the
situation, including advice to cancel any ongoing alarms in that area.
#### 5.5.2.4 Post-conditions
The remote server terminates the stream and the CCTV camera returns to motion
detection.
#### 5.5.2.5 Challenges to the 5G system
This use reuses several of the existing features of the 5G system such as
dedicated network slices supporting scalability, minimum reserved capacity,
and data isolation.
Special challenges to the 5G system associated with this use case are the
protection of the integrity of the user data.
#### 5.5.2.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Smart
Cities 1.1_ The 5G system shall enable the network operator to protect the
integrity of user data for services provided by a network slice Transport
* * *
## 5.6 Electric-power distribution
### 5.6.1 Description of Vertical
#### 5.6.1.1 Overview
The energy sector is currently subject to a fundamental change, which is
caused by the evolution towards renewable energy, i.e. a very large number of
power plants based on solar and wind power. These changes lead to bi-
directional electricity flows and increasing dynamics of the power system. New
sensors and actuators are being deployed in the power system to efficiently
monitor and control the volatile conditions of the grid, requiring real-time
information exchange [46] [47].
#### 5.6.1.2 Technical challenges of future energy grids
  * Energy generation by a huge number of decentralised local units. In larger markets, hundreds of thousands of devices need to be connected via 5G.
  * Many businesses and private homes connected to the energy network become prosumers, i.e. customers and producers of energy. They may also operate local energy storage systems. Their power system will be monitored and controlled by inverters, or electronic power converters, which communicate with other parts of the electrical grid.
  * Up to 100% of the energy will be produced from highly volatile, renewable resources, mainly solar, wind and---where available---hydro power. Solar and wind energy do not inject mechanical inertia into the local power grid, which will make keeping the frequency at a constant value much more difficult.
  * Control of voltage and frequency in distribution and transmission grids are the key challenges of future energy networks. These new procedures are still under development, and will require many economic, legal and ICT changes (apart from the modifications in the electrical grid).
  * Especially frequency control has tight requirements in terms of reaction time.
The main goals of future energy networks include―among others―the reduction of
CO~2~ emissions by relying on renewable energy sources (RES), decentralisation
of energy production, continuous matching of injected and outgoing energy
levels, resource efficiency, cost efficiency, maximum security, and reliable
provisioning of services to consumers.
These improvements are important for addressing the needs of increasingly
volatile and decentralised markets. A major enabler for all this are inter-
connected communication systems and computing infrastructure, which
interconnects control centres, substation automation units, energy storage
systems, and power plants of all sizes in a flexible, secure and consistent
manner.
Today, the vast majority of communication technologies used in the energy
sector are still wire-bound. This includes a variety of dedicated Industrial
Ethernet and power line solutions. These communication technologies are used,
for example, for interconnecting sensors, actuators and controllers in an
electrical network automation system.
Nowadays, wireless communication is primarily used for connecting smart meters
for customers of the power network. These meters only monitor the energy
consumption of the connected facility. There was no need for wireless
connectivity in the past, due to relatively static and long-lasting
installation of the power grid equipment. In addition, this is because most
existing wireless technologies fall short of the demanding requirements of
industrial applications, especially with respect to end-to-end latency,
communication service availability, jitter, and reliability.
With the advent of future smart grids and 5G, however, this may change
fundamentally, since only wireless connectivity can provide the degree of
flexibility, mobility, versatility, and ergonomics that is required for the
energy networks of the future. Thus, 5G may significantly contribute to
revolutionising the way how electrical energy is monitored, stored, and
controlled for the entire industry sector.
{width="2.102777777777778in" height="2.3319444444444444in"}
Figure 5.6.1.2 -- 1: Overview of different application areas in electric-power
distribution.
In this respect, three different application areas can be distinguished, as
shown in Figure 5.6.1.2-1.
#### 5.6.1.3 Application Areas
These areas can be briefly characterised as follows:
**Primary Frequency Control with up to 100% RES:** The focus of this
application area is on the instant monitoring and control of the frequency in
the grid. In frequency control, the grid can be a long-distance transmission
network covering countries or large parts there-of, or short-distance
distribution networks connecting local consumers and distributed producers of
energy. Primary frequency control will ensure that a swift response on
frequency variations is provided, while it may not lead to returning the
measured frequency to the nominal, exact target value, e.g. 50 Hz in Europe.
Typically, primary frequency control will use decentralised or distributed
control architectures allowing taking corrective actions swiftly on a local
level.
**Secondary Frequency Control with up to 100% RES:** The focus of this
application area is the second, less time-critical correction of the frequency
in the grid. Again, the grid can be a long-distance transmission network
covering countries or large parts there-of, or short-distance distribution
networks connecting local consumers and distributed producers of energy.
Secondary frequency control will ensure that an accurate and lasting response
on frequency variations is provided, and it shall return the measured
frequency to the nominal, exact target value, e.g. 50 Hz in Europe. Typically,
secondary frequency control will use centralised control architectures
allowing frequency control units to take corrective actions across all parts
of the controlled power network.
**Distributed Voltage Control with up to 100% RES:** The focus of this
application area is monitoring and control of the voltage levels in
distribution networks. Sensors located close to the electronic inverters in
the local grid measure the impedance on the grid, and forward these values to
a voltage control unit co-located with the secondary substation automation
unit. The control unit analyses the impedance values and determines the need
for corrective actions. The correction action is a target impedance value that
is sent to the electronic inverters so additional energy can be injected into
the grid, or electronic inverters may throttle the energy added by power
plants or storage systems.
**A high-** level overview of the communication links is provided in Figure
5.6.1.3‑1.
{width="5.634722222222222in" height="3.321527777777778in"}
Figure 5.6.1.3‑1: Communication Links in Future Energy Networks with up to
100% RES,
#### 5.6.1.4 Major challenges and particularities
Major general challenges and particularities include the following aspects:
  1. Utility-grade quality of service is required for many applications, with stringent requirements in terms of end-to-end latency, communication service availability, jitter, and determinism.
  2. There is not only a single class of use cases, but there are several different use cases with a wide variety of different requirements, thus resulting in the need for a high adaptability and scalability of the 5G system.
  3. Many applications have stringent requirements on safety, security (esp. availability, data integrity, and confidentiality), and privacy.
  4. The 5G system shall support a seamless integration into the existing (primarily wire-bound) connectivity infrastructure. For example, the 5G-based solution shall allow to flexibly combine the 5G system with other (wire-bound) technologies in the same power network.
  5. Most types of electrical equipment usually have a rather long lifetime, which may be 20 years or even longer. Therefore, long-term availability of 5G communication services and components is essential.
  6. 5G systems shall support private operation within a local distribution grid, which are isolated from PLMNs. This is required by many distribution system owners (DSO) for security, liability, availability and business reasons. Nevertheless, standardised and flexible interfaces shall be supported for seamless interoperability and seamless handovers between 5G PLMNs and dedicated 5G systems.
  7. The radio propagation environment in a building with energy generation or storage equipment can be quite different from the situation in other application areas of the 5G system. Inverters may be located in the basement of industrial buildings, where radio connectivity can be challenging. There is a significant risk that these buildings host a large number of---often metallic---objects in the immediate surroundings of transmitter and receiver, as well as potentially high interference caused by electric machines, power transformers, and the like.
  8. The 5G system shall be able to support continuous monitoring of the current network state in real-time, to take quick and automated actions in case of problems and to do efficient root-cause analyses in order to avoid any undesired interruption of the production processes, which may incur huge financial damage. Particularly if a third-party network operator is involved, accurate SLA monitoring is needed as the basis for possible liability disputes in case of SLA violations.
#### 5.6.1.5 Description of this vertical's communication architecture
The smart grid communication architecture, visualised in Fgure 5.6.1.5-1, is
mapped to the power network it serves. The smart grid communication
architecture covers multiple network domains, where each domain relates to a
specific grid voltage and geographic area. Every network domain instance
clearly belongs to exactly one smart grid stakeholder.
{width="6.429861111111111in" height="3.8020833333333335in"}
Figure 5.6.1.5-1: Smart grid communication architecture [48]. CC: Control
centre; DER: distributed energy resource; DSO: distribution system operator,
HV: High voltage; LV: low voltage; MV: medium voltage; NMS: network management
system; TSO: transmission system operator,
The following communication network domains can be distinguished:
  * **Backbone network:** Communication network which connects the primary-substation LANs amongst each other and with regional control centres (often co-located) and central control centres.
  * **Primary-substation LAN:** A primary substation LAN is quite complex and requires its own communication infrastructure that distinguishes between a process bus and a station bus. It is mainly based on a Gigabit Ethernet infrastructure.
  * **Backhaul network:** Communication network which connects secondary-substation LANs with each other and with a control centre. This network domain might also connect to the respective primary-substation LAN in case the DSO and TSO roles are linked.
  * **Secondary-substation LAN:** Network inside the secondary substation (today this network is quite trivial and may consist of just one single Ethernet switch / IP router). The secondary-substation LAN is implemented in US-Style regions more in a distributed manner whereas in Europe the secondary-substation LAN is very often located in an encapsulated enclosure.
  * **Access network:** Communication network which connects the customer premises or e.g. low-voltage sensors to a specific secondary substation.
  * **Customer premises LAN:** In-building communication network whereas a customer is characterised by consumption and production of energy (prosumers) and the customer can be residential, public or industrial prosumer.
  * **Intra-DER Network:** For medium-sized DERs like wind/solar parks a dedicated LAN is required for control, management and supervision purposes.
  * **Intra-Control-Centre Network:** LAN within a DSO's or TSO's control centre.
  * Public network: Fixed or mobile-network-operator owned communication network which offers different connectivity services either via dedicated services or via the open Internet.
Remarks:
  * Each of the described communication network domains usually belongs to one Smart Grid stakeholder.
  * Some stakeholders like aggregator or metering operator do not own private communication infrastructure. Usually they connect via a public network to their assets / customers. Thus, public networks are an important part of the overall Smart Grid architecture.
  * Not all communication network domains are mandatory to exist: For example the secondary-substation LAN can alternatively connect to prosumers via a public network service. In this case no DSO-owned access network is required.
Mobile networks are used mainly for connecting secondary substation LANs and
DERs to control centres and primary substations and for connecting devices
like smart meters and sensors at customer premises to control and data
centers. The requirements of this communication are relatively low regarding
per device bit rate, latency and availability / reliability.
Mobile networks are not used in most cases for mission critical and real time
communication (like tele-protection communication) due to the relatively high
latency, missing reliability/ availability guarantees and Quality-of-Service
capabilities of 2G, 3G and 4G networks. It is our expectation that URLLC-
enabled 5G systems will change this, and that Smart Grid control functions
might also be connected via mobile networks.
### 5.6.2 Primary Frequency Control
#### 5.6.2.1 Description
Primary frequency control is among the most challenging and demanding control
applications in the utility sector. A primary frequency control system is
responsible for controlling the energy supply injected and withheld to ensure
that the frequency is not deviating more than 0,02% from the nominal value,
e.g. 50 Hz in Europe.
Frequency control is based on having sensors for measuring the features in all
parts of the network at all points where energy generation or storage units
are connected to the grid. At these points, electronic power converters, also
known as inverters, will be equipped with communication units to send
measurement values to other points in the grid such as a frequency control
unit, or receive control commands to inject more, or less, energy into the
local network.
With the widespread deployment of local generation units, ie solar power
units, or wind turbines, hundreds of thousands of such units, and their
inverters, may have to be connected in a larger power distribution network.
Therefore, wireless communications based on high-performance 5G systems
constitutes a promising approach. There are two key benefits of this solution:
  1. Wirelessly connected devices, such as sensors, inverters, and control units, do not need cables and wirelines connections, thus reducing installation costs and maintenance effort.
  2. Devices can be connected and disconnected from the communications network without any effort or restrictions, or lengthy preparations, allowing for novel and swift set-ups.
A schematic representation of a communications network for frequency control
is provided in the figure below. The operation and maintenance staff in the
distribution management system (DMS) will define rules and guidelines for
frequency control which are forwarded to the automated frequency control
units. In turn, the frequency control units return alerts, escalations and
statistics to the DMS. Frequency sensors continuously monitor the current
frequency values at their local points, they can share their values with the
frequency control unit, which analyses and compares the incoming measurements,
and takes corrective actions by instructing the inverters in the local grid to
inject more or less energy into the local network. Note that frequency sensors
and actuators (inverters) can be located on the same hardware device, and in
this case, would only require one communication point.
The format and contents of these messages will be defined for standardisation
by regulatory bodies.
{width="6.3in" height="3.0208333333333335in"}
Figure 5.6.2.1 - 1 Schematic Representation of Frequency Control System in a
Distribution Network.
Primary frequency control shall be carried out in one of three available
architecture options:
  1. Centralised control, all data analysis and corrective actions are determined by a central frequency control unit
  2. Decentralised control, the automatic routine frequency control shall be performed by the individual local inverter based on local frequency values. Statistics and other information shall be communicated to the frequency control unit, though.
  3. Distributed control, the automatic routine frequency control shall be performed by the individual local inverter based on local and neighbouring frequency values. Statistics and other information shall be communicated to the frequency control unit, though.
Furthermore, there are many scenarios where specific devices (e.g., sensors or
actuators) are added, activated, reconfigured, removed or deactivated while
the overall control system keeps on running. The overall availability of the
communications network, from an end-to-end point of view must be at least
_five Nines_ , and utility companies will demand that the total amount of
planned and unplanned downtime is not more than **5,26 minutes** per year.
In order to increase the availability level of the system, elements of the
communications network must have full redundancy and back-up units during
times of upgrade or update procedures.
#### 5.6.2.2 Preconditions
All frequency sensors, actuators/inverters, the frequency control units, and
the DMS are switched on and connected to the 5G system. Frequency control
units can be implemented locally depending upon the power system network
topology. The number of sensors, actuators/inverter, and other devices depends
upon the power system network topology.
#### 5.6.2.3 Service flows
At regular, frequent intervals, the following steps are performed. For primary
frequency control, the measurements are taken several times per second.
  1. The frequency control unit requests measurements from all sensors in its areas.
  2. All sensors send the measurement values to the frequency control unit.
  3. The frequency control unit analyses and compares the incoming measurements, and determines if corrective actions are necessary.
  4. In that case, the frequency control unit instructs the inverters in the local grid to inject more or less energy into the local network.
  5. The inverters will return an acknowledgement signal after executing the changes required by the frequency control unit.
All messages exchanged have to be properly secured (especially in terms of
data integrity and authenticity) and the probability of two consecutive packet
errors shall be negligible. This is because a single packet error may be
tolerable, but two consecutive packet errors may lead to erroneous frequency
correction commands, which may cause outages of parts of the local grid. This
may cause significant financial damage.
#### 5.6.2.4 Post-conditions
The components controlled by the primary frequency control system have ensured
that the frequency value is back to acceptable values. Note that this value
may not yet be the exact nominal value yet. _Secondary frequency control_ is
used to ensure that the entire grid is back to this nominal value.
#### 5.6.2.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Very stringent requirements on latency, communication service availability, and reliability.
  * Very stringent requirements on clock synchronicity between different nodes.
  * Transmission of rather small chunks of data, resulting in potentially significant relative overhead due to signalling, security, etc.
  * Potentially high density of end devices, including sensors and actuators.
#### 5.6.2.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Electric-
Power Distribution 1.1_ The 5G system shall support highly performant traffic
with frequency measurement intervals in the order of 50 ms for a communication
group of up to 100,000 UEs and payload sizes of approximately \~100 Bytes. T  
_Electric-Power Distribution 1.2_ The 5G system shall support highly
performant traffic with transmitting measurements and control commands with an
end-to-end latency in the order of \~50ms. T  
_Electric-Power Distribution 1.3_ The 5G system shall support data processing
and frequency control procedures for local, decentralised grids with minimum
end-to-end latency, and maximum reliability and privacy. T  
_Electric-Power Distribution 1.4_ The 5G system shall ensure that critical
data traffic for power utilities is not disturbed by other traffic even at
peak times of load on the user plane. T  
_Electric-Power Distribution 1.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
ultra-low latency and ultra-high reliability requirements T  
_Electric-Power Distribution 1.6_ The 5G system shall support hot-plugging in
the sense that new devices may be dynamically added to and removed from a
frequency control application, without any observable impact on the other
nodes. T  
_Electric-Power Distribution 1.7_ The 5G system shall not violate valid,
general privacy principles applicable for electrical networks in general, and
support data minimisation and user consent if any data collection such as
frequency and impedance measurements in the local smart grid are unavoidable
for providing the required services.
* * *
### 5.6.3 Distributed Voltage Control with up to 100% RES
#### 5.6.3.1 Description
In the evolution towards 100% RES, the objective of voltage control is to
balance the voltage in future low voltage distribution grids connecting local
loads and prosumers as well as energy storage facilities. The aim is to
stabilise the voltage as local as possible, so that decisions and control
commands can be issued as quickly as possible.
The inverters, or electronic power converters, will measure the voltage and
power, and will also change the amount of power injected into the grid, and
connect and disconnect end-points from the distribution network. The
communication flow from the regional voltage control centre of the DSO
terminates at the inverter.
Figure 5.6.3.1-1 shows the communications network overview of a distribution
grid, where the distributed voltage control is performed by _voltage
controllers_ co-located with the regional secondary substation units. These
controllers receive impedance and voltage measurements from the inverters,
analyse the data and take corrective actions by sending commands to the
inverter end-points.
The controllers will also communicate with the Distribution Management System
(DMS), sending statistics, reports, and alerts to the operations centre, and
receiving regulations and other updates from the centre.
In the future, microgrids and aggregators may operate parts of the
distribution grids, with independent voltage control. In this scenario, these
new sector actors will exchange measurements, control commands, and other
information with the supervising Distribution System Operator (DSO).
{width="5.145833333333333in" height="2.9895833333333335in"}
Figure 5.6.3.1-1: Decentralised Architecture for Voltage Control
Distributed voltage control is a challenging and demanding control application
in the utility sector. Consumer devices rely on having stable voltage levels
to operate successfully. When future energy networks rely on thousands of
local energy generation units relying mostly on solar and wind power, then it
is crucial to stabilise the voltage levels in all segments of the distribution
grid.
With the widespread deployment of local generation units, i.e. solar power
units, or wind turbines, hundreds of thousands of such units, and their
inverters, may have to be connected in a larger power distribution network.
Therefore, wireless communications based on high-performance 5G systems
constitutes a promising approach. There are two key benefits of this solution:
  1. Wirelessly connected devices, such as sensors, inverters, and control units, do not need cables and wire lines connections, thus reducing installation costs and maintenance effort.
  2. Devices can be connected and disconnected from the communications network without any effort or restrictions, or lengthy preparations, allowing for novel and swift set-ups.
Distributed control means that the automated voltage control shall be
performed by the local voltage control units based on local _and neighbouring_
voltage and impedance values. Statistics and other information shall be
communicated to the central DMS, though.
Furthermore, there are many scenarios where specific devices (e.g., sensors or
actuators) are added, activated, reconfigured, removed or deactivated while
the overall control system keeps on running. The overall availability of the
communications network, from an end-to-end point of view must be at least
_five Nines_ , and utility companies will demand that the total amount of
planned and unplanned downtime is not more than **5,26 minutes** per year.
In order to increase the availability level of the system, elements of the
communications network must have full redundancy and back-up units during
times of upgrade or update procedures.
#### 5.6.3.2 Preconditions
All voltage sensors, actuators/inverters, the voltage control units, and the
DMS are switched on and connected to the 5G system. Voltage control units can
be implemented locally depending upon the power system network topology. The
number of sensors, actuators/inverter, and other devices depends upon the
power system network topology.
5.6.3.3 Service flows
At regular, frequent intervals, the following steps are performed. For primary
voltage control, the measurements are taken several times per second.
  1. The voltage control unit requests measurements from all sensors in its areas.
  2. All sensors send the measurement values to the voltage control unit.
  3. The voltage control unit analyses and compares the incoming measurements, and determines if corrective actions are necessary.
  4. In that case, the voltage control unit instructs the inverters in the local grid to inject more or less energy into the local network.
  5. The inverters will return an acknowledgement signal after executing the changes required by the voltage control unit.
All messages exchanged have to be properly secured (especially in terms of
data integrity and authenticity) and the probability of two consecutive packet
errors shall be negligible. This is because a single packet error may be
tolerable, but two consecutive packet errors may lead to erroneous voltage
correction commands, which may increase the risk for outages of parts of the
local grid. This may cause significant financial damage.
#### 5.6.3.4 Post-conditions
The components controlled by the distributed voltage control system have
ensured that the voltage value is back to acceptable values.
#### 5.6.3.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Stringent requirements on latency, communication service availability, and reliability.
  * Very stringent requirements on clock synchronicity between different nodes.
  * Transmission of rather small chunks of data, resulting in potentially significant relative overhead due to signalling, security, etc.
  * Potentially high density of end devices, including sensors and actuators.
#### 5.6.3.6 Potential requirements
* * *
Reference Number Requirement text Application / Transport Comments _Electric
Power Distribution 2.1_ The 5G system shall support highly performant traffic
with voltage measurement intervals in the order of 200 ms for a communication
group of up to 100.000 UEs and payload sizes of approximately \~ 100 B. T  
_Electric Power Distribution 2.2_ The 5G system shall support highly
performant traffic with transmitting measurements and control commands with an
end-to-end latency in the order of \~100 ms. T  
_Electric Power Distribution 2.3_ voltage control procedures for local,
decentralised grids with minimum end-to-end latency, and maximum reliability
and privacy T  
_Electric Power Distribution 2.4_ The 5G system shall ensure that critical
data traffic for power utilities is not disturbed by other traffic even at
peak times of load on the user plane. T  
_Electric Power Distribution 2.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
ultra-low latency and ultra-high reliability requirements T  
_Electrical Power Distribution 2.6_ The 5G system shall support hot-plugging
in the sense that new devices may be dynamically added to and removed from a
voltage control application, without any observable impact on the other nodes.
T  
_Voltage Control 2.7_ The 5G system shall not violate valid, general privacy
principles applicable for electrical networks in general, and support data
minimisation and user consent if any data collection such as frequency and
impedance measurements in the local smartgrid are unavoidable for providing
the required services.
* * *
### 5.6.4 Power distribution grid fault and outage management: distributed
automated switching for isolation and service restoration for overhead lines
#### 5.6.4.1 Description
A power distribution grid fault is a stressful situation for operators in the
control center. During the fault period, they have to perform a lot of tasks
in order to restore power to the \"healthy\" section of the distribution grid
as quick as possible. Among other activities, they will:
  * Collect and analyse grid status information;
  * Identify faulty grid sections;
  * Infer appropriate measures;
  * Isolate the fault;
  * Restore service;
  * Inform service crews;
  * Coordinate service crew operation;
  * Restore normal grid configuration after the fault has been repaired.
In case disruptions affect a larger area―for example, during bad weather―the
level of stress further increases, because several outages may occur at the
same time. There are self-healing solutions for automated switching, fault
isolation and, service restoration. These solutions help avoiding these
stressful situations and allow operating personnel to concentrate on repair
work and service crew coordination. Furthermore, these solutions are ideally
suited to handle outages that affect critical power consumers, such as
industrial plants or data centres. In these cases, supply interruptions must
be fixed within less than a minute and manual outage handling in a control
centre usually fails to achieve such short restoration time. Automated
solutions are able to restore power supply within a few hundred milliseconds.
{width="6.5in" height="4.027777777777778in"}
Figure 5.6.4.1-1: Depiction of a distribution ring and a failure (flash of
lighting). GOOSE: Generic Object Oriented Substation Event; NOP: normal open
point.
The FLISR (Fault Location, Isolation & Service Restoration) solution consists
of switch controller devices which are especially designed for feeder
automation applications that support the self-healing of power distribution
grids with overhead lines. They serve as control units for reclosers and
disconnectors in overhead line distribution grids.
The system is designed for using fully distributed, independent automated
devices. The self-healing logic resides in each individual feeder automation
controller located at the poles in the feeder level. Each feeder section has a
controller device with a programmable logic controller (PLC). The PLC can be
configured by the utility by help of a graphical engineering tool. Using peer-
to-peer communication among the controller devices, the system operates
autonomously without the need of a regional controller or control centre.
However, all self-healing steps carried out will be reported immediately to
the control centre to keep the grid status up-to-date.
Modern communication systems primarily use the international IEC 61850
standard to support this distributed application. The IEC 61850 standard
provides the required flexibility and interactivity for the implementation of
self-healing functions. Peer-to-peer communication via IEC 61850 GOOSE
(Generic Object Oriented Substation Event) message provides analogue and
binary data as fast as possible. Each controller unit has its own
comprehensive programmable logic designed by the sequence editor of the
graphical engineering tool to configure the automation functionality according
to the feeder layout. The controllers conduct self-healing of the distribution
line in typically 500 ms by isolating the faults.
A distributed solution offers several benefits:
  * Cost-efficient and future-proof solution for automatic and high speed fault location and service restoration;
  * Flexible solution supporting central and distributed configurations;
  * Easy configuration and maintenance with graphical tools;
  * Automated switching procedure to return to normal operation -- no manual intervention required;
  * SCADA system connectivity to self-healing solution for monitoring and control purposes;
  * Improvement of distribution grid reliability indicators by reduction of outages;
  * Prevention of penalties and secure power supply for critical loads like hospitals and data centres.
There are 5 ... 20 controller units per feeder ring. The geographical
dimension of feeders is usually up to several km^2^.
The peer-to-peer protection communication between controller units is done via
IEC 61850 GOOSE messages. GOOSE is a Layer 2 multicast message (IP traffic is
not used here !). GOOSE messages are sent periodically (with changing interval
time) and are not acknowledged. The GOOSE messages are sent by each controller
to all other controllers or a number of controllers of the same feeder. The
end-to-end latency requirement is less than 5 ms. The bit rate is low (\ Editor's note: Service area for this use case is FFS.
### 5.8.3 Low-latency audio streaming for local conference systems
#### 5.8.3.1 Description
In a conference system the voice of several speakers is captured by a
conference unit, transmitted to the base station of the Local High Quality
Network (see Figure 5.8.3.1-1), where the mixing of the different audio
streams is done, and distributed to all other conference units connected to
the network, which replay the received audio stream. Low latency is an issue
because the speakers always hear themselves speaking.
The main task of a conference system is to distribute voice from one
participant to other participants either within the same location, e.g. a room
or the conference venue, or to a remote location. The conference units can be
centrally configured and controlled, and can start a poll among the
participants. As a concrete example, the _local conference system_ is
described in more details and its typical system parameters are listed in
Table 5.8.3.1-1.
Figure 5.8.3.1-1describes the logical topology of a local conference system.
Each conference unit can act as a transmitter or receiver of an audio stream.
The speakers' voices are transmitted to a conference service application via a
central base station (red arrow) from where the processed audio streams are
distributed again to the other conference units (blue arrows). The receiving
conference units can select between different channels, e.g., each one
containing a different mixture of the speakers' voices or their respective
language translations. The conference system can be deployed either in a fixed
installation setup or in a moving/portable setup. In the fixed installation
case the surrounding conditions, e.g. the presence of cellular network
coverage, stay constant over a longer period in time. In the portable setup
case, the conference system is deployed in an ad-hoc manner. The location and
time of deployment are only known at short-term and indeed can change even
across country-borders.
{width="3.4569444444444444in" height="3.2430555555555554in"}Figure 5.8.3.1-1:
Logical topology of the local conference system use case
Table 5.8.3.1: System parameters of the local conference system use case
* * *
                                           Local Conference System   Comments
**Application latency** \< 20 ms **End-to-end maximum allowable latency of the
use case in focus (i.e.** between the microphone and the speaker/headphone)**,
includes application and application interfacing.** **End-to-end latency** \<
4 ms Latency that is introduced per link of the 5G wireless communication
system excluding application and application interfacing. **User experienced
data rate** 150kb/s to768 kb/s 4x in UL, 42x in DL **Control data rate** ≤ 50
kb/s Data rate per control link **Communication service availability**
99,9999%  
**Packet error ratio** 99,99% The audio frames (with a packet duration of 4
ms) provided to the application layer shall have a packet error ratio (PER)
below 10-4. **# of audio links** Up to 500 The system shall support the
control of 500 user terminals per cell **Service area** ≤ 2500 m^2^ Conference
area, typically indoor. **Typical heights: 3 m to 5 m** **Synchronicity** ≤ 20
µs All wireless mobile devices shall be synchronised inside one local high
quality network, for instance with respect to a reference system clock
provided by one master device, e.g. the 5G base station. **System setup time**
≤ 10 ms  
**Internet access** optional The system shall be able to work without access
to the Internet
* * *
#### 5.8.3.2 Pre-conditions
All wireless audio devices on-site (Figure 5.8.3.1-1) are switched on and
connected to a PMSE production network through a local 5G base station.
#### 5.8.3.3 Service Flows
A typical service flow in a local conference network may look as follows:
  1. The voice of several speakers is captured by a conference unit and wirelessly transmitted to a conference system where the mixing of the different audio streams is done and transferred into the floor channel.
  2. The floor channel is wirelessly transmitted as a multicast stream to all other conference units connected to the network.
  3. Additional audio mixes (e.g. interpretation channels) are distributed wirelessly to dedicated conference units (unicast transmission) connected to the network.
#### 5.8.3.4 Post-conditions
The local conferences devices run as required by the application.
#### 5.8.3.5 Challenges to the 5G System
Special challenges to the 5G system associated with this use case include the
following aspects:
  1. Very stringent requirements on end-to-end latency, PER and communication service availability.
  2. Very stringent requirements on clock synchronicity between different nodes at application level. This implies that the 5G system must support deterministic packet transfer and ultra-precise time synchronisation, even in the user equipment. Time-sensitive networking features may be revealed to the application or equivalent interfaces implemented.
  3. Transmission of rather small chunks of data, resulting in potentially significant overhead due to signalling, routing, security, etc.
  4. Multichannel operation and _multicast_ support.
#### 5.8.3.6 Potential Requirements
* * *
Reference Number Requirement text Application / Transport Comments _PMSE 2.1_
The 5G system shall support a clock synchronicity at application level between
a communication group of 50 to 500 UEs in the order20 µs or below. See note 1.
T  
_PMSE 2.2_ The 5G system shall support data integrity and confidentiality
protection, even for communication services with ultra-low latency and ultra-
high reliability requirements T  
_PMSE 2.3_ The 5G system shall support communication service availability
exceeding at least 99,9999% T  
_PMSE 2.4_ The 5G system shall support hot-plugging in the sense that new
devices may be dynamically added to and removed from a local conference
application, without any observable impact on the other devices. T  
_PMSE 2.5_ The 5G system shall support industry standards for precision clock
synchronisation (e.g., IEEE 1588) for IP-based A/V systems in a way that the
synchronisation requirements outlined in Table 5.X.3.1-1 can be met. T  
Note 1: In multicast operation.
* * *
### 5.8.4 High data rate video streaming / professional video production
#### 5.8.4.1 Description
The use case High data rate video streaming / professional video production
addresses demanding applications that partially require extensive post-
processing of the material supplied by cameras. The post processing increases
the quality requirements on the source material and thus also on the bandwidth
required for transmission. In particular, scenically produced productions,
which due to the time pressure can be considered \"quasi-live\" productions,
place high demands.
In this use case, high quality video data is streamed wirelessly from one or
several cameras to the base station of the Local High Quality Network (see
Figure 5.8.4.1-1), where it can be recorded or edited. Moreover, the system
should allow remote operation of the cameras (e.g. focus control) by sending
and receiving control data to and from the cameras.
The video stream sent from a camera is used for two purposes:
  * As the useful camera signal for e.g. \"quasi live\" editing and recording,
  * For viewing by remote camera operators, e.g. crane operators and focus pullers (i.e. to maintain image sharpness on whatever subject or action is being filmed).
Currently, several dedicated wireless systems for video and control need to be
used in parallel. This is disadvantageous and potentially can be improved by
the use of 5G wireless systems.
The professional video production use case is quite demanding, as high data
rate is required in parallel with low latency. Especially, for the remote lens
control application, performed by the focus pullers, low latency is a critical
issue. This is because focus tracking of moving objects based on delayed
monitoring inevitably would result in focus faults. Further demanding is the
fact that the focus control data are sent back wirelessly to the filming
camera again. Two scenarios are defined for this use case:
  * _Portable high data rate video production system_ : The location and time of deployment are not fixed and can change from one occasion to the next. The system is in worldwide use. It is typically shipped in flight cases from one location to another. This scenario places major requirements on the wireless system, especially to delay, data rate and error behaviour.
  * _Fix installation high data rate video production system_ : The camera part is the same as for the portable scenario. However, the base station(s) including antennas and system wiring can be permanently installed and can make use of the characteristics and infrastructure of a typical production studio.
The separation in two scenarios should mainly be considered for a later
product definition with respect to the base station side. The camera part and
the typical requirements for the performance of the wireless system should be
the same for both scenarios. The latter are summarised in Table 5.8.4.1-1.
Professional video systems rely on ultra-precise time synchronisation at
application level. Each professional video terminal shall independently refer
to an accurate reference signal that reaches all terminal devices
simultaneously, i.e. within a phase jitter much smaller than the video
sampling period.
{width="6.277777777777778in" height="2.4270833333333335in"}
Figure 5.8.4.1-1 Logical topology of high data rate video streaming use case
Table 5.8.4.1-1: System parameters of the high data rate video streaming use
case / Professional Video Production
+----------------------+----------------------+----------------------+ | | Local Conference | Comments | | | System | | +----------------------+----------------------+----------------------+ | **Application | \ < 50 ms | End-to-end delay | | latency** | | from the camera | | | | image sensor back to | | | | the camera lens | | | | motors including | | | | sensor integration | | | | time and delay of | | | | all components | | | | in-between | +----------------------+----------------------+----------------------+ | **End-to-end | \ < 2 ms (no | Latency that is | | latency** | transmission errors) | introduced per link | | | | of the wireless | | | \< 10 ms (including | communication system | | | retransmissions) | excluding | | | | application and | | | | application | | | | interfacing. | | | | | | | | The end-to-end | | | | latency of the | | | | wireless system | | | | shall be below 2 ms | | | | for a nominal | | | | transmission without | | | | transmission errors. | | | | | | | | In case of | | | | transmission errors | | | | the packet shall be | | | | resent autonomously | | | | by the wireless | | | | system. The system | | | | delay including | | | | retransmission(s) | | | | shall be below 10 ms | +----------------------+----------------------+----------------------+ | **User experienced | UL (Camera to base | | | data rate** | station): | | | | | | | | 100 Mb/s to 200 Mb/s | | | | | | | | DL (Base station to | | | | Camera) | | | | | | | | 20 Mb/s | | +----------------------+----------------------+----------------------+ | **Reliability** | 99,999% | The number | | | | represents one | | | | defect video frame | | | | per one hour of | | | | operation. | | | | | | | | One video frame | | | | comprises about 300 | | | | packets, each 1280 | | | | bytes in length. A | | | | video frame is | | | | defect if one or | | | | more bits or packets | | | | representing this | | | | frame are corrupted | | | | or missing | +----------------------+----------------------+----------------------+ | **# of video | 3 | The system shall | | links** | | support 3 video | | | | cameras per cell | +----------------------+----------------------+----------------------+ | **Range** | 1 m to 50 m | Distance from the | | | | camera to the base | | | | station | +----------------------+----------------------+----------------------+ | **Service area** | ≤ 2500 m^2^ | Indoor and Outdoor | +----------------------+----------------------+----------------------+ | **Synchronicity** | ≤ 1 ms | All wireless video | | | | devices shall be | | | | synchronised inside | | | | one local high | | | | quality network | +----------------------+----------------------+----------------------+ | **Mobility** | **≤ 50 km/h** , | The wireless system | | | rotation \< 0,52 | shall allow camera | | | rad/s. | movements of at | | | | least pedestrian | | | | speed and camera | | | | rotations of 0,52 | | | | rad/s | +----------------------+----------------------+----------------------+ | **Security** | Secured data | | | | transmission is | | | | required | | +----------------------+----------------------+----------------------+
#### 5.8.4.2 Pre-conditions
All wireless video devices on-site (see Figure 5.8.4.1-1) are switched on and
connected to a PMSE production network through a local 5G base station.
#### 5.8.4.3 Service Flows
A typical service flow in a professional video production network may look as
follows:
  1. High quality video data is streamed wirelessly from one or several cameras to the professional video production network, where it can be recorded or edited.
  2. Control data to allow the remote operation of the cameras (e.g. focus control) is sent from the professional video production network to the wireless cameras and vice versa.
#### 5.8.4.4 Post-conditions
The video production devices run as required by the application.
#### 5.8.4.5 Challenges to the 5G System
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Very stringent requirements on communication service availability, PER and end-to-end latency throughout the whole operation time.
  * Very stringent requirements on clock synchronicity between different nodes at application level. This implies that the 5G system must support deterministic packet transfer and ultra-precise time synchronisation, even in the user equipment. Time-sensitive networking features may be revealed to the application or equivalent interfaces implemented.
  * Professional video systems rely on ultra-precise time synchronisation at application level and would benefit from IEEE 1588 PTP and Time Sensitive Networking-like features.
#### 5.8.4.6 Potential Requirements
* * *
Reference Number Requirement text Application / Transport Comments _PMSE 3.1_
The 5G system shall support a clock synchronicity between a communication
group of 3 to 10 UEs in the order of 1 ms or below T  
_PMSE 3.2_ The 5G system shall support data integrity and confidentiality
protection, even for communication services with high data rate, low latency
and high reliability requirements T  
_PMSE 3.3_ The 5G system shall support communication service availability
exceeding at least 99,999% T  
_PMSE 3.4_ The 5G system shall support hot-plugging in the sense that new
devices may be dynamically added to and removed from a live performance
application, without any observable impact on the other devices T  
_PMSE 3.5_ The 5G system shall support UE speeds of up to 14 m/s and UE
rotations of 0,52 rad/s, even for communication services with high data rate,
low latency and high reliability T  
_PMSE 3.6_ The 5G system shall support industry standards for precision clock
synchronisation (e.g., IEEE 1588) for IP based A/V systems in a way that the
synchronisation requirements outlined in Table 5.8..4-1 can be met
* * *
# 6 Security
Editor's note: Address how to enable existing and emerging industrial security
solutions. Established security solutions are described for information. List
of potential service requirements.
## 6.1 Particularities of security for automation in vertical domains
### 6.1.1 Introduction
Security concepts and solutions have mostly been developed for office IT
systems and applications. Security for automation in vertical domains not only
comes with different security priorities, it also comes with different
management & operational characteristics and requirements. Before discussing
the different security requirements in detail, the following three Subclauses
present a bird-eyes view of the salient differences between office IT systems
and automation systems.
### 6.1.2 Security priorities
Security has the main attributes confidentiality, integrity, and availability
(availability as in access to the system in question). While typical office IT
prioritises confidentiality over integrity, and integrity over availability,
the complete opposite is true for automation in many vertical domains (see
Figure 6.1.1-1).
Figure 6.1.1-1: Difference in security priorities between Office IT and
automation.
In other words, availability is the main concern of automation security, next
comes integrity, and confidentiality has generally the lowest priority.
### 6.1.3 Management and operation characteristics
As shown in Table 6.1.3-1, the management and operation characteristics of
automation and office systems are quite different.
Table 6.1.3-1: Automation and office-IT systems have different management &
operation characteristics.
* * *
                                      Automation systems                                                      Office IT systems
_Target for security protection_ Automation resources incl. logistics IT
infrastructure including personal, computers, etc. _Required availability_
Very high Medium; delays acceptable _Component lifetime_ Up to 20 years 3-5
years _Required real-time behaviour_ Can be critical Delays acceptable
_Physical security_ Varying High for data centres _Application of security
patches_ Slow; restricted by regulation Regular/scheduled _Anti-virus
programs_ Uncommon; hard to deploy; application white listing as an
alternative Common and widely used _Security testing and auditing_ Increasing
Scheduled and mandated for critical infrastructure such as IT service centres
* * *
Automation systems put not only an emphasis on availability, but this
availability has to be guaranteed over typically 5-7 times longer component
lifetimes. Real-time behaviour of automation systems can be critical
(especially for control applications). Another big difference is the security
\"culture\". Security patches are applied rather slowly in automation, in
particular due to regulatory limitations. Also, in many installations, patches
can be installed only during scheduled maintenance windows of the automation
system. Anti-virus programmes are rather uncommon in automation as the virus
patterns would have to be updated regularly. Instead, application white
listing may be used to ensure that only authorised, unmodified applications
can be executed. While security testing and auditing is the norm for critical
IT infrastructure such as service centres, these assurance approaches are
still evolving in automation in vertical domains. A reason is that the
reliable operation of the automation system must not be endangered by
penetration tests.
### 6.1.4 Security requirements -- an overview
A high-level comparison of security requirements of automation and office IT
systems is shown in Table 6.1.4-1.
Table 6.1.4-1: Automation and office-IT systems have different security
requirements.
* * *
                                                                                         Automation systems                                                                                                                                                       Office IT systems
_Security standards_ Under development; subject to regulation Existing
_Confidentiality (information)_ Low to medium for production floor; high for
business-relevant know-how (know-how protection), high for operation in the
public space (electricity distribution, etc.) High _Integrity_ High Medium
_System availability and reliability (note 1)_ 24 x 365 x ... Medium; delays
acceptable _Non-repudiation_ Medium to high (depending on vertical) Medium
NOTE 1: For a definition of system availability and reliability see Subclause
4.3.3.
* * *
Security standards are well developed for office IT systems, but have been
under development for automation systems in vertical domains. In particular,
the industrial security standard IEC62443 is adopted increasingly in many
vertical domains.
While the importance on confidentiality is in general high in office
environments, it is rather low to medium for physically access-restricted
automation system such as production cells in a factory. However,
confidentiality has to be protected for business-relevant know-how, e.g., for
engineering data or for process parameters of a chemical production process.
Integrity is paramount and, as already eluded to in Subclause 6.1.2,
availability and reliability are of paramount importance. Non-repudiation may
be high for automation systems, where the correct operation of the production
has to be asserted, e.g., in pharmaceutical production, while it is typically
medium for office environments.
## 6.2 Industrial security responsibilities
In vertical use cases, security is in general a shared responsibility between
a product supplier (typically the product manufacturer), the integrator, and
the operator of the infrastructure in question. Hereafter the latter is also
referred to as infrastructure operator.
Let us illustrate these roles for a digital grid use case, i.e. the automation
of an electric-power distribution network (see Figure 6.2-1). In this case,
the infrastructure operator in question is a utility, and the integrator is
typically the company installing the hardware. The OT infrastructure in
question is the energy automation equipment for operating the digital grid.
Note that the infrastructure operator is responsible for both the IT and OT
installation. In the case of the digital grid, an example for OT is an
adjustable transformer, and an example for IT is consumer and prosumer billing
software. The integrator can be the utility itself or a sub-contracted
company. We emphasise the role as an OT infrastructure operator in Figure
6.2-1, since it is controlling the physical world which makes use cases in
this study different from pure IT use cases such as multi-player gaming. In
the case of automation equipment utilising a 5G network, both the supplier of
the digital grid automation equipment and the suppliers of the 5G equipment
are the suppliers.
To address cyber security needs of the infrastructure operator, these actors
rely on adequate processes for handling IT security as well as on technical
and procedural security means. These actors thus require sufficiently secure
products but also a secure integration of products into systems and solutions.
{width="6.688194444444444in" height="3.7020833333333334in"}
Figure 6.X-1: Distribution of security responsibilities for the example of a
digital grid.
Starting from the left side of Figure 6.2-1, all participants, i.e.
infrastructure operator, system integrator, and product supplier operate an
infrastructure that needs to be managed according to the associated risks. As
noted for the infrastructure operator in Figure 6.2-1, this is specifically
enforced through directives, e.g. the NIS directive for critical
infrastructures in Europe.
Risks regarding the operation of an IT infrastructure are typically addressed
with an information security management system. The ISO 27000 series defines a
generalised information security management system by specifying operational
processes and procedures addressing security. This framework also provides
specific guidance on certain vertical domains. Figure 6.2-1shows the example
of a digital grid (a.k.a. Smart Grid), for which ISO 27019 provides an
augmentation of the general security controls described in ISO 27002 for the
power system domain.
Many of the aforementioned processes require interfaces, which come with
requirements for the integrator and the device or system manufacturer. The
integrator defines security requirements based on the intended operational use
cases and determines security levels to be achieved. One standard supporting
this is IEC 62443-3-3 [42] (see Subclause 6.3). Thanks to the definition of a
target security level, which leaves the actual technical realisation open, the
manufacturer can choose among various options for the implementation. As shown
in Figure 6.X-1, the product supplier typically builds products according to
established security standards. For instance, for communication security, the
product supplier can employ IEC 62351, which is a domain-specific security
standard series for the power system domain. This series, beyond other
specifications, profiles already existing security standards like TLS for
securing TCP/IP based communication. 5G security features can also be
leveraged here for supporting secure network access.
In praxis, different deployments (based on equipment from different automation
suppliers) often use different kinds of credentials. Also, it is noteworthy
that automation devices often have different communication ports (e.g., WLAN
and Ethernet), and that authentication of the device is done with the same
credential, irrespective of the port used. In this context it is important to
remember that the security mechanisms used need to be supported for at least
the life expectancy of the installed devices. This life expectancy is
typically ten years or more. So, if 5G UEs etc. are installed in, for
instance, a factory, the 5G system and future updates of the system have to
support the initially used authentication mechanism for the same time span.
Furthermore, it is worth mentioning that there is a pervasive trend toward EAP
as a common authentication framework in non-3GPP communication technologies
for verticals.
## 6.3 Background: the industrial security standard IEC 62443
5G security is essential for low latency high reliability communication
applied to automation, e.g. in industrial automation, Industrie 4.0,
industrial IoT, building automation, and the digital electricity grid. The
industrial security standard IEC 62443 [40] is applied in various vertical
domains, including factory automation, process automation, building
automation, transportation systems, and energy management. Compatibility of 5G
security solutions with IEC62443 is important so that 5G offerings can be used
flexibly in different industrial domains.
The international industrial security standard IEC 62443 is a security
framework defined by the International Electrotechnical Commission (IEC). It
covers both organisational and technical aspects of security, without being
prescriptive regarding the technical solution. In the set of corresponding
documents, security requirements are defined, which target the solution
operator and the integrator, but also the product vendor.
As shown in Figure 6.3-1, the parts of the standard are grouped into four
clusters covering
  * **Common definitions and metrics;**
  * **Requirements concerning setup of a security organisation (related to information security management systems), solution suppliers, and service provider processes;**
  * **Technical requirements and methodology for security at a system-wide level;**
  * Requirements concerning the security development lifecycle of system components, and security requirements for such components at a technical level.
{width="4.677777777777778in" height="3.8069444444444445in"}
Figure 6.3-1: IEC 62443 Overview and Status. Proc.: process; Func.: function;
IACS: industrial automation and control system.
According to the methodology described in IEC 62443-3-2 [41], a complex
automation system is structured into zones that are connected by and
communicate through so-called \"conduits\" that map for example to the logical
network communication between two zones. Moreover, IEC 62443-3-3 defines
security levels (SL) that correlate with the strength of a potential adversary
as shown in Figure 6.3-2 below [42].
{width="3.2416666666666667in" height="2.1555555555555554in"}
Figure 6.3-2: Security levels according to IEC 62443-3-3 [42].
In order to reach a dedicated SL, the specified requirements have to be
fulfilled. For each security level, IEC 62443-3-3 defines a set of security
requirements for an automation system. Seven foundational requirements group
specific requirements:
  * FR 1: identification and authentication control;
  * FR 2: use control;
  * FR 3: system integrity;
  * FR 4: data confidentiality;
  * FR 5: restricted data flow;
  * FR 6 timely response to events;
  * FR 7: resource availability.
For each of the foundational requirements, there exist several concrete
technical security requirements (SR) and further requirement enhancements (RE)
that have to be fulfilled for a specific security level. In the context of
communication security, the security levels are specifically interesting for
the conduits connecting different zones. The following examples are taken from
IEC 62443-3-3 [42] to illustrate some of the requirements.
  * **FR1 SR 1.6 -- Wireless access management: The control system shall provide the capability to identify and authenticate all users (humans, software processes or devices) engaged in wireless communication.**
  * **FR3, SR3.1 -- Communication integrity: \"The control system shall provide the capability to protect the integrity of transmitted information\".**
  * **FR3, SR3.8 -- Session integrity: \"The control system shall provide the capability to protect the integrity of sessions. The control system shall reject any usage of invalid session IDs.\"**
  * **FR4, SR 4.1, RE 1 -- Protection of confidentiality at rest or in transit via untrusted networks: \"The control system shall provide the capability to protect the confidentiality of information at rest and remote access sessions traversing an untrusted network.\"**
  * **FR5, SR 5.2 -- Zone boundary protection: \"The control system shall provide the capability to monitor and control communications at zone boundaries to enforce the compartmentalisation defined in the risk -based zones and conduits model.\"**
As IEC 62443 gains more acceptance in different industry domains, different
technical security solutions suitable for the application domain and coping
with the security requirements are needed.
## 6.4 5G security for automation applications
### 6.4.1 Introduction
Depending on how a 5G network and 5G technologies are used Depending on how a
5G network and 5G technologies are used by a vertical automation application,
different requirements have to be met by the underlying 5G security solutions.
In Subclause 6.4.2, we address pertinent quality properties of the security
solution, and in Subclause 6.4.3, we provide requirements that were inferred
from IEC 62443 [42] and from an application-centric study [43].
### 6.4.2 Quality properties of 5G security solutions
_Authentication of communication peers and the 5G system:_
Editor\'s Note: FFS.
_Flexible subscriber access management:_ Efficient management of 5G
subscriptions/permissions is important for small, medium and huge numbers of
5G UEs. In this context small is on the order of 10 and huge on the order of
10 000 or larger. In this context, management stands for adding UEs to a 5G
subscription so that they can use 5G communication services, but also for
removing UEs from the subscription base. One of the challenges here is
particularly medium-sized installations, where manual management is not
practical, while fully automated solutions that require a complex
infrastructure may be too cumbersome.
_Long-term security:_ Devices in many verticals operate over long usage
periods (in industrial environments typically 10 to 20 years), which makes
long-term security an important topic. Note that in many vertical environments
updates may be installed only during planned service windows. Note that such
updates may imply that a laborious safety recertification has to be repeated.
It is important that an automation application system can be kept in service
over a long usage period without requiring regular upgrades (e.g., replacing
hardware components; redesigning the technical solution).
It is important that an automation application system can be kept in service
over a long usage period without requiring regular upgrades (e.g., replacing
hardware components; redesigning the technical solution).
_5G as communication infrastructure:_ When the security provided by the
communication system is deemed to be insufficient for a vertical automation
application, security of the industrial solution is realised on top (e.g.,
using IPsec or TLS). A non-automation example for this is online banking. In
many deployments, the 5G network is expected to provide certain dependability
guarantees independently of security. Communication dependability is discussed
in detail in Subclause 4.3.3. Note: Since the 5G system is not in control of
the automation application\'s security-related data flows, the aforementioned
required communication service dependability and QoS can perhaps not be met
due to high communication resource consumption. In this case, the automation
application, including its security functions, would need to be optimised in
order to lower the communication resource consumption and to thus increase the
communication service dependability.
_Reliance on 5G security alone:_ 5G systems may be used for connecting IoT
devices. In such deployments, realising security on top of 5G could reduce the
life time of battery-powered IoT devices, or deteriorate experienced
dependability parameters (see above). Therefore, it may be decided to rely on
5G communication security alone. If that is the case, the vertical automation
applications need to verify that the required 5G security mechanisms are
actually active.
### 6.4.3 Potential security requirements
The following security requirements are considered to be essential.
  * Authentication of communication peers and of the 5G system---plus the integrity of transmitted messages---shall always be ensured. This shall also be the case when communication confidentiality is not available, for instance due to regulatory limitations (requirements SR1.6 and SR3.8 in [42]).
  * An automation application that uses a 5G communication service shall be able to log and audit the 5G security mechanisms used by the communication service [43].
# 7 Merged potential service requirements
Editor's note: This Clause proposes service requirements based on a
comprehensive view of the vertical use cases in Clause 5, Clause 6, and TS
22.261.
# 8 Conclusions
# Annex A: Characteristic parameters
## A.1 Transmission time
The transmission time is a fundamental characteristic parameter which can be
used for the assessment of the availability and real-time capability of a
wireless system. In this context, it is of interest to know how long the
transmission of user data from the source (e.g. a sensor) to the target (e.g.
a controller) takes. A consistent understanding of this period of time
requires precise stipulation of the start and end of measurement. The
transmission time is the period from transferring the first discrete component
of the application message (e.g., bit or octet) to the interface between
application and wireless communication of a source and the handover of the
last discrete component of the same user data from the interface between the
wireless communication and application of a target (see Figure A.1-1). The
type of the interface between wireless communication and application are
always to be stated together with the characteristic parameter values.
Figure A.1-1: Definition of the transmission time.
As shown in Figure A.1-1, the values for transmission time fluctuate, for
instance because retransmission is needed. They cannot fall below a certain
minimum, but typically scatter around a value close to that minimum. The value
which occurs most frequent, i.e. the modal value, is usually better suited
than the mean value as a measure of tendency for the transmission time. What
is a sensible spread parameter for the transmission time depends on the use
case and implementation. An example spread parameter is percentile P95, i.e.
the maximum value for 95% of all transmissions.
## A.2 Up time, down time and up state, down state
The assessment of communication is referring to messages. Message ratings can
be _correctly received_ , _incorrectly received_ , or _lost_. A lost message
is a message which left the source application and never reached the target
application.
Up time and down time can be derived from the introduced types of messages. As
far as received messages are correct, the communication is in _up state_. The
time interval of this state experienced by the target device is called _up
time_ or _up time interval_. If a message loss or an incorrectly received
message is detected the communication is in _down state_. The time interval of
this state experienced by the target device is called _down time_ or _down
time interval_. This behaviour is illustrated in Figure A.2-1.
{width="6.560416666666667in" height="5.009722222222222in"}
Figure A.2-1: Definition of up time, down time and up state, and down state.
The time between the begin of the down time interval and last received message
is referred to as the survival time.
The flow of events in Figure A.X1-1 is as follows. The network is up and
running (blue line) and a source device starts sending messages (brown arrows)
to a target device on which there runs an automation function (application).
The communication connection is, from the point of view of the receiving
application in an up state (green line). Later, the network transitions into a
down state, i.e. it cannot longer support end-to-end transmission of the
source device\'s messages to the target device. Usually, the target device
will wait a pre-set period before it declares the connection to be lost
(\"dead line for expected message\"). This is the so-called survival time [3].
Once the network again is in an up state, the connection is declared to be in
an up state (green line).
Up state and down state and the corresponding up time interval and down time
interval are also available at the source device. However, they are
experienced a little differently with respect to the points in time as
illustrated in Figure A.X1-1.
## A.3 Jitter - characterisation of timeliness
Time requirements are typically specified with two values: characteristic time
and jitter.
**Characteristic time:** is the target value of the time parameter in
question, e.g. end-to-end latency.
**Jitter** : The jitter is the variation of a (characteristic) time parameter.
An example is the variation of the end-to-end to latency. If not stated
otherwise, jitter specifies the symmetric value range around the target value
(target value ± jitter/2). If the actual time value is outside this interval,
the transmission was not successful. Figure A.3-1 shows an example of
transmissions with jitter. Note that the end-to-end latency may scatter even
for successful transmissions.
{width="6.694444444444445in" height="6.738888888888889in"}
Figure A.3-1: End-to-end latency and end-to-end latency jitter
Typical characteristic parameters to which jitter values are ascribed are
  * transfer interval;
  * end-to-end latency;
  * update time.
Note, that jitter is not always explicitly stated in technical documents. In
such a case, the maximum value of the characteristic time parameter needs to
be known. Sometimes, also a minimum value may be given. This time should not
be undershot. A minimum value is only used in particular use cases, for
instance, when putting labels at a specific location on moving objects.
## A.4 Periodicity - cycle time, transfer interval, and update time
In industrial automation applications, sensor data is gathered periodically in
many cases. Accordingly, actuator data is provided periodically, too.
Therefore, the transmission of this data via communication networks is also
periodically in most cases.
NOTE: Periodic communication is usually continuous. After the related
automation functions are activated---e.g. controlling, sensing, and actuating
---periodic data transmission continuous until these functions are stopped. In
extreme cases, continuous operation can extend over a calendar year or more.
Event driven data transmission might be also used. In this case, additional
periodic control messages are introduced for dependability reasons. These are
called heart beat messages or keep alive messages.
The periodic processes within industrial automation applications are often
called _cycle_ and the related interval _cycle time_. Unfortunately, the term
cycle has different meanings depending on the point of view. With respect to
the periodic communication, it is better to use the communication-related
terms transfer interval and update time instead of cycle time.
Periodic communication or periodically means that a _transfer interval_ is
repeated. For example, a transmission occurs every 15 ms. Reasons for a
periodical transmission can be the periodic update of a position or the
repeated sensing or monitoring of a characteristic parameter. Although a
transmission of a temperature every 15 minutes is a periodical transmission,
too, most periodic transfer intervals in communication for automation are
rather short. The user experienced data rate of periodic communication of
fixed message sizes is the message size divided by the transfer interval. For
instance, for a message size of 40 B and a transfer interval of 1 ms, the user
experienced data rate is 40 B/1 ms = 320 kb/s.
The _update time_ is the equivalent of the transfer interval at the receiver.
A periodic transmission is started once and continuous unless a stop command
is provided. Figure A.4-1 illustrates transfer interval and update time.
{width="5.944444444444445in" height="7.429861111111111in"}
Figure A.4-1 -- Transfer interval and update time of periodic communication
Note, the update time may experience jitter even for an isochronous transfer
interval. This can be caused by varying waiting times until the actual
transmission, for instance, varying waiting times for the assigned timeslots
in TDMA superframes. This has to be taken into account when requirements are
specified and for the assurance of quality of service.
# Annex B: Communication errors
### B.1 Introduction
IEC 61784-3-3 describes fundamental communication errors which can be
identified for applications with functional safety requirements [25]. The
description of these communication errors refers to field buses. These errors
may however also occur in other communication systems.
### B.2 Corruption
Messages may be corrupted due to errors within a bus participant, due to
errors on the transmission medium, or due to message interference.
NOTE 1: Message error during transfer is a normal event for any standard
communication system, such events are detected at receivers with high
probability by use of a hash function and the message is ignored.
NOTE 2: Most communication systems include protocols for recovery from message
errors, so these messages will not be classed as \'Loss\' until recovery or
repetition procedures have failed or are not used.
NOTE 3: If the recovery or repetition procedures take longer than a specified
deadline, a message is classed as \'unacceptable delay\'.
NOTE 4: In the very low probability event that multiple errors result in a new
message with correct message structure (for example addressing, length, hash
function such as CRC, etc.), the message will be accepted and processed
further. Evaluations based on a message sequence number or a time stamp can
result in fault classifications such as Unintended repetition, Incorrect
sequence, Unacceptable delay, Insertion [25]
### B.3 Unintended repetition
Due to an error, fault or interference, old not updated messages are repeated
at an incorrect point in time.
NOTE 1 Repetition by the sender is a normal procedure when an expected
acknowledgment/response is not received from a target station, or when a
receiver station detects a missing message and asks for it to be resent.
In some cases, the lack of response can be detected and the message repeated
with minimal delay and no loss of sequence, in other cases the repetition
occurs at a later time and arrives out of sequence with other messages.
NOTE 2 Some field buses use redundancy to send the same message multiple times
or via multiple alternate routes to increase the probability of good reception
[25].
### B.4 Incorrect sequence
Due to an error, fault or interference, the predefined sequence (for example
natural numbers, time references) associated with messages from a particular
source is incorrect.
NOTE 1 Field bus systems can contain elements that store messages (for example
FIFOs in switches, bridges, routers) or use protocols that can alter the
sequence (for example by allowing messages with high priority to overtake
those with lower priority).
NOTE 2 When multiple sequences are active, such as messages from different
source entities or reports relating to different object types, these sequences
are monitored separately and errors can be reported for each sequence [25].
### B.5 Loss
Due to an error, fault or interference, a message or acknowledgment is not
received [25].
### B.6 Unacceptable delay
Messages may be delayed beyond their permitted arrival time window, for
example due to errors in the transmission medium, congested transmission
lines, interference, or due to bus participants sending messages in such a
manner that services are delayed or denied (for example FIFOs in switches,
bridges, routers).
NOTE: In underlying field buses using scheduled or cyclic scans, message
errors can be recovered in the following several ways:
a) immediate repetition;
b) repetition using spare time at the end of the cycle;
c) treating the message as lost and waiting for the next cycle to receive the
next value.
In case a) all the following messages in that cycle are slightly delayed,
while in case b) only the resent message gets a delay.
Cases a) and b) are not normally classed as an anacceptable delay.
Case c) would be classed as an unacceptable delay unless the cycle repetition
interval is short enough to ensure that delays between cycles are not
significant and the next cyclic value can be accepted as a replacement for the
missed previous value [25].
### B.7 Masquerade
Due to a fault or interference, a message is inserted that relates to an
apparently valid source entity, so a non-safety related message may be
received by a safety related participant, which then treats it as safety
related.
NOTE Communication systems used for safety-related applications can use
additional checks to detect Masquerade, such as authorised source identities
and pass-phrases or cryptography [25].
### B.8 Insertion
Due to a fault or interference, a message is received that relates to an
unexpected or unknown source entity.
NOTE These messages are additional to the expected message stream, and because
they do not have expected sources, they cannot be classified as Correct,
Unintended repetition, or Incorrect sequence [25].
### B.9 Addressing
Due to a fault or interference, a safety related message is delivered to the
incorrect safety related participant, which then treats reception as correct
[25].
# Annex C: Communication system errors
## C.1 Hardware errors
Hardware errors encompass the failure or disturbance of the function of
electrical, electronic and programmable components. They are caused by
physical and chemical processes which take place in the environment or in the
system. If the function of a component no longer meets the specification, it
is---as a rule---assessed as unusable and therefore as failed.
Depending on the cause, the scope and the speed of occurrence, distinctions
are made between:
  * Random failure and deterministic failure;
  * Total failure and partial failure;
  * Sudden failure and degradation failure, and fatigue failure;
  * Hardware errors may be caused, for example, by poor workmanship;
  * Components of sub-standard quality,
  * Ageing;
  * Overloading (e.g. clock frequency, voltage or current);
  * High or low temperatures;
  * Frequent temperature changes;
  * Impacts, acceleration or vibration;
  * Electrical, magnetic or electromagnetic fields;
  * Ionising radiation.
### C.2 Software errors or program errors
Software errors or program errors encompass the malfunction of computer
programs. Distinctions are made between the following kinds of software error:
  * Syntax errors: infringements of the grammatical rules of the programming language used. A syntax error prevents compilation of the defective program. Syntax errors are not as a rule of interest in run time, unless a programming language which interprets the program sequentially is used.
  * Run time errors: designates all kinds of errors which occur during running of the program; example: for instance exceeding the value range or incorrect data types for variables on input, when there is no verification or an incorrect version of the operating system is used.
  * Logical errors: occur with an incorrect entry or incorrect algorithm.
  * Design errors: are errors in the basic concept which are caused by the assumption of incorrect requirements or defective software design.
  * Errors as a consequence of physical operating conditions: electromagnetic fields, radiation, mechanical stresses, temperature fluctuations, etc. can lead to errors even in systems that are working within the specification.
Considerations of software and hardware errors can be found, for example, in
IEC 62439 [26], which also deals with concepts of redundancy in industrial
Ethernet networks, or in IEC 62673 [24], in which a general methodology for
dependability assessment and assurance in communication networks throughout
their life cycles is described.
### C.3 Physical link errors
In wireless transmission, the physical link presents a special challenge. The
environment in which it is used has a great influence on the dependability
parameters. A distinction is made between passive influences (where the signal
transmitted is influenced on the way to the target) and active influences
(where additional signals impair recognition of the useful signal at the
target). The passive influences include the distance (distance-related
attenuation of the signal), metallic obstacles (reflection, diffraction and
refraction of the signal), dielectric obstacles (attenuation of the signal)
and heavy rain or fog (absorption of the signal). Active influences result
from the transmission of electromagnetic waves in the vicinity of the
communication devices. The passive and active influences are referred to as
disturbances. These disturbances have an effect on the physical link and can
be the cause of transmission errors.
# Annex D: Change history
This is the last annex for TRs which details the change history using the
following table.
This table can be used for recording progress during the WG drafting process
till TSG approval of this TR.
For TRs under change control, use one line per approved Change Request
Date: use format YYYY-MM
TSG # : use format RAN#55
CR: four digits, leading zeros as necessary
Rev: blank, or number (max two digits)
Cat: use one of the letters A, B, C, D, F
Subject/Comment: for TRs under change control, include full text of the
subject field of the Change Request cover
New vers: use format n[n].n[n].n[n]
* * *
**Change history**  
**Date** **Meeting** **TDoc** **CR** **Rev** **Cat** **Subject/Comment** **New
version** 2017-05 SA1#78 S1-172349 Skeleton for TR 22.804 (\"Study on
Communication for Automation in Vertical domains\") 0.0.0 2017-05 SA1#78
S1-172159 Includes agreements at SA1#78, May 2017, Oporto, Portugal 0.1.0
2017-09 SA1#79 S1-173236 Includes agreements at SA1#79, August 2017, Guilin,
China 0.2.0 2017-12 SA1#80 S1-174273 Includes agreements at SA1#80, November-
December 2017, Reno, Nevada 0.3.0
* * *
[^1]: OPC UA is a platform-independent application layer protocol running over
TCP/IP. It is commonly used for M2M communication in automation systems (e.g.,
MES, SCADA). Apart from providing a client-server based communication
framework, it also defines a sophisticated information model to describe the
semantics of exchanged information.
[^2]: A zone is a well-defined (physical) substructure of the automation
network containing devices needed for a production step and sha­ring