# Foreword
This Technical Specification has been produced by the 3rd Generation
Partnership Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
User experience based network management is important for operators so that
they can provide best quality services. In case of a VR service, which is much
more complex than traditional video streaming in terms of content creation,
network transmission and device requirements, the provision of a satisfying
immersive experience is more challenging. The first step of this effort would
be to find out what factors have an impact on user experience, and define
reference metrics and parameters that would help operators make assessment of
user experience, do trouble shooting and design target solutions.
The analysis of impact on user experience involves the whole E2E VR service
chain:
  * Creation of content. VR content creation would involve multiple steps such as capture, stitching, projection, and encoding, each step would have an impact on the content itself. It is necessary to look into each step and find out what factors are relevant to the VR experience;
  * Network transmission. The amount of video data of VR content entails a high streaming bitrate, and may lead to a risk of network and/or access link congestion and re-buffering, and thus an impediment to limiting latency. Latency is one of the key elements to create the feeling of immersiveness, and thus has considerable impact on user experience.
  * Device requirement. Compared with a traditional device, be it a mobile phone or a tablet, a VR device exhibits many more attributes, designed to help create immersive experience. The degree of freedom it provides to users, the sensitivity of sensors that enable quick catching of head movement, and many other attributes, all need to be studied and evaluated about their relevance to user experience.
This report investigates the QoE metrics relevant with VR experience from the
aforementioned three aspects, and also the way of reporting these QoE metrics
to the network for further analysis.
# 1 Scope
This Technical Report provides a study on the QoE metrics relevant to VR
service. The study focuses on:
  * Defining a device reference model for VR QoE measurement points.
  * Studying key performance indicators that may impact the experience of VR service.
  * Identifying the existing QoE parameters and metrics defined in SA4 standards such as TS 26.247, TS 26.114 which are relevant to Virtual Reality user experience;
  * Identifying and defining new QoE parameters and metrics relevant to Virtual Reality user experience, taking into consideration the use cases listed in TR 26.918, and any sources that show the relevance of new metrics, e.g. scientific literature, specifications/solutions from other standard organizations.
  * Analysing potential improvements to the existing QoE reporting so as to better accommodate VR services.
  * Providing recommendations to future standards work in SA4 on the QoE parameters and metrics and, as necessary, coordinate with other 3GPP groups and external SDOs, e.g. MPEG, ITU-T.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TR 26.918: \"Virtual Reality (VR) media services over 3GPP\".
[3] 3GPP TS 26.247: \"Transparent end-to-end Packet-switched Streaming Service
(PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP (3GP-
DASH)\".
[4] ISO/IEC 23009-1: 2014/Amd. 1:2015/Cor.1:2015\" Information technology --
Dynamic adaptive streaming over HTTP (DASH) -- Part 1: Media presentation
description and segment formats\".
[5] ITU-T P.1203: \"Parametric bitstream-based quality assessment of
progressive download and adaptive audiovisual streaming services over reliable
transport\".
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**example:** text used to clarify abstract rules by applying them literally.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
\ \
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
QoE Quality of Experience
VR Virtual Reality
UL Up-link
# 4 VR QoE overview
## 4.1 Introduction
## 4.2 VR QoE metrics evolution
The goal with this study is to suggest improvements to the existing QoE
reporting, so that suitable QoE metrics are available to better understand the
VR service quality as experienced by the VR users. A complicating factor for
the study is the lack of a thorough scientific understanding on exact how
different VR conditions and impairments relate to the final user quality.
As VR services becomes more mature, and also more standardized, this
understanding will become more clear over time. It is not unlikely that in a
long-term perspective there might be standardized objective quality models,
similar to the ITU-T P.1203 [5], which translate measurable QoE metrics into
the final user experience.
However, there is a significant delay from the time a standard (3GPP, MPEG,
ITU-T etc.) is ready, until the corresponding standard features are actually
implemented large-scale by the device industry. There is also a further delay
until the penetration of such standard-compliant devices gets high enough to
become useful for wider analysis.
Thus, waiting with defining VR-related QoE metrics until a full and complete
understanding has been gained will cause a significant gap in time, where VR
service-quality monitoring cannot be adequately performed, at least not by
standardized means.
The question is how to move forward during this transition period, where a
detailed understanding of the QoE relations is not always available? And how
to do it in a way where stepwise QoE metrics refinement can be done over
several releases?
The key for how to succeed with this can actually be found in the existing QoE
metrics in TS 26.247. When these metrics were standardized in Rel-10, several
of the defined \"metrics\" are actually not really stand-alone QoE metrics.
Rather they are just lists of events initiated by the client or by the user.
A typical example is the PlayList, which contains user and client actions,
together with timestamps and a minimal set of related metadata. As we have
discussed in earlier SA4 meetings, these \"event lists\" are not QoE metrics
by themselves, and there has also been suggestions to enhance 26.247 with
additional \"real QoE metrics\".
However, a big advantage with the event lists is that they contain the basic
information necessary to calculate other derived QoE metrics. For instance,
ITU-T P.1203 needs as one input a metric containing the number of
rebufferings. As P.1203 was not fully standardized until 2017, there was no
knowledge during the Rel-10 QoE work that such a metric would later be needed.
However, due to the event-based PlayList it is now actually possible to derive
this metric, without changing the 26.247 standard. The same is true for
several other metrics needed by P.1203.
A drawback with event lists, especially for the VR case, is that they can
potentially contain a lot of events. For normal 2D streaming the interaction
from the user is much more limited, basically play, stop, jump, etc. For a VR
service all of those are also there, but the main interaction is continuous
head (or even body) movements, which then might also cause the player to
interact towards the network in different ways (fetching different tiles
etc.).
However, there are several ways to handle the report size problem, for
instance using a constant (but configurable) sample time for the \"movement
list\", and/or a (configurable) movement threshold before a movement is
logged. Although it might be difficult to define exactly what sample time or
movement threshold that would be the best compromise between accuracy and
report size, we actually don\'t have to know that right now. It is a decision
which can be made (years) later by the operator or the service provider,
depending on the QoE use case at that point in time.
When VR services become more mature, it is expected that more optimized
versions of the QoE metrics can possibly be standardized, resulting in more
tailored and smaller metrics. But having a basic QoE reporting available as
early as possible is a big advantage, as it is typically during the early
phases of a new service adoption that the need for quality-related feedback
and analysis is most important.
Thus a stepwise evolution of VR QoE metric could be to use event lists
whenever possible for the initially defined metrics, as this seems to be the
most future-proof representation. The lists could allow some basic
configurability to enable a flexibility between accuracy vs. report size.This
would allow later derivation of more specific QoE metrics, as well as other
quality-related aspects, some of which might not even be known today.
## 4.3 Viewport-related QoE aspects
### 4.3.1 Introduction
From the user point of view, one of the main differences between normal 2D
video streaming and VR video streaming is the notion of a viewport. Instead of
always seeing the complete video, the user only sees a cropped part of the
video, the viewport, depending on the direction of the device.
The resulting impairments and quality experience will vary depending on
content authoring strategy, the client rendering strategy, and any network
impact. The following clauses show some (non-exhaustive and simplified)
examples of possible delivery scenarios and resulting impairment types. Note
that while the viewport also affects audio, no audio aspects are included in
the examples.
The examples should not be seen as any endorsement of specific authoring or
delivery technology, they only illustrate some possible impairments as seen
from the user point of view. Also, for the sake of simplicity, all examples
are drawn with square regions, but in practice regions can have different
shapes and also do not need to have clear quality boundaries. Any spatial
distortion due to the mapping into spherical rendering is also not considered.
### 4.3.2 Single stream region-independent coding
In this scenario the content is encoded with the same resolution and quality
settings over the complete 360 content, and delivered as a single stream. The
client decodes the complete stream but shows only the cropped part
corresponding to the viewport (the red rectangle).
{width="3.196527777777778in" height="1.9881944444444444in"}
In the example above the user moves the viewport from left to right, but as
the encoding is the same for any viewport, this scenario is very similar to
the 2D case. The main additional impairments would likely be related to
projection artifacts and any device-internal rendering delay.
### 4.3.3 Single stream region-dependent encoding
The content is encoded with emphasis on one or more regions, where the content
producer believes that most users will direct their viewport. Thus the
resolution and/or coding quality is higher for selected parts of the spatial
area, corresponding to these regions. The video is still delivered as one
single 360 stream, and the client decodes and shows a cropped part
corresponding to the viewport.
{width="3.196527777777778in" height="3.092361111111111in"}
In the example above, two emphasized regions are defined (dark grey), and the
user moves the viewport from left to right. For all three viewports the
average viewport quality is the same (i.e. 50% high quality and 50% low
quality), but it is likely that the quality in the central part of the
viewport has largest importance for the user. Thus you would expect viewport
#1 to be experienced as best, and viewport #3 as worst.
### 4.3.4 Multiple stream region-dependent encoding
Multiple streams can be used, each emphasizing a given region. The receiver
selects to download and render the stream which emphasized region best
corresponds to the actual viewport.
{width="4.086805555555555in" height="3.092361111111111in"}
In the example above, stream #1 and stream #2 have 25% overlapping region-
optimized encoding, and when reaching viewport #3 the receiver decides to
switch to stream #2. Thus while turning the head between viewport #2 and #3,
the user will see an instant change of quality for the left part (going high
to low) and right part (going low to high) of the viewport.
Note that the average viewport quality is the same for viewport #2 and #3
(i.e. \~67% high quality and \~33% low quality) but the dynamic effect of
switching between them is probably clearly visible. If the user moves his head
back and forth between viewport #2 and #3, and the receiver selects to switch
streams, such quality changes can likely be rather annoying.
### 4.3.5 Region-based encoding, simple head movement
With region-based encoding the client typically fetches viewport regions with
high quality, while using low-quality regions for the backround around the
viewport (or even for the complete 360). The figures below (left and right)
illustrate two variants of a simple head movement.
{width="5.670833333333333in" height="4.196527777777778in"}
On the left side, the head movement is aligned with the regions, while on the
right side it is moved a small bit into the next region column, and also a bit
downward. In both cases the main impairment is the visible delay before high-
quality versions of the new viewport regoins have been fetched and rendered.
In the example it likely takes a bit longer to update the right scenario (nine
regions instead of two), but due to the minimal viewport coverage of the seven
outer regions, the user is unlikely to note any quality difference between the
left and right scenario after the update of the first two regions. Thus
although the final update delay probably is different, the experienced quality
might be the same.
Note that the client could in principle instead decide to skip updating the
seven outer regions due to their minimal viewport coverage. Alternatively, the
client could decide to update them, but use an intermediate quality level
instead of the highest quality, resulting in the example below.
{width="3.6708333333333334in" height="0.8847222222222222in"}
### 4.3.6 Region-based encoding, complex head movements
In practice, a user might move his head in more complex patterns, with
continous movements of varying speed. During these movements the region update
times will likely vary depending on network conditions etc. The example below
shows a possible scenario, where the update most of the time lags somewhat
compared to the current viewport at any given time.
{width="6.983333333333333in" height="4.233333333333333in"}
### 4.3.7 Summary
The shown examples form only a small subset of many possible scenarios, but
they still illustrate that the viewport-related interaction between the user,
the client, and the server is complex, and the final impact on the end-user
quality will depend on many factors.
This interaction and it\'s principal effect on the perceived quality needs to
be understood. The interaction also needs to be mapped into a reference
architecture, identifying what information (for instance regarding the
viewport) that might be available at different sub-parts of the system.
Note that QoE metrics as such need not map perfectly to the perceived end-user
quality, as this is a much wider task usually handled by ITU-T with extensive
subjective tests, and advanced quality models. However, QoE metrics should be
designed in such a way that they will be able to characterize typical
impairments in a consistent, discriminative and meaningful way.
## 4.4 Viewport-related information flow
To better understand what metrics are possible to measure, we need to
understand the flow of information inside the VR streaming application. A
reference model for VR QoE measurements is described later in clause 6,
defining a number of observation points. However, in the current clause we
will not specifically focus on observation points, but more on the flow of
information, and we will instead use the following picture (from TS 26.118,
might be revised) to illustrate a possible information flow for a simple use-
case:
**Figure 4.4.1: Client Reference Architecture for VR DASH Streaming
Applications**
The DASH data model in Figure 4.4.2 below is also used as a basis for the use-
case:
{width="6.5375in" height="2.439583333333333in"}
**Figure 4.4.2: DASH Data Model**
As seen in Figure 4.4.2, the adaptation sets are the central part for any
viewport-related handling of the DASH media. Each adaptation set covers a
certain spatial area of the sphere (signalled in CC), and it may also contain
additional information on the relative quality ranking (QR) between the sets.
Thus adaptation sets are selected depending on the user pose, and within each
adaptation set there might be several representations with different encoding
bitrates.
Note that depending on the scenario (e.g. single stream region-independent,
single stream region-dependent, multiple stream region-dependent) multiple
adaptation sets may be used at the same time, for instance a low-quality low-
bitrate full-coverage set used for the full 360 view, and one or more high-
quality sets which mainly covers the intended viewport. In tiled distributions
each adaptation set typically contain only one tile, at a certain resolution.
Figure 4.4.3 below shows a simplified signalling diagram for a use-case with
different changes of user pose:
{width="5.15625in" height="9.525694444444444in"}
**Figure 4.4.3: Simplified signalling use-case**
Although very simplified, the use-case illustrates the possible division of
responsibility between the VR application and the DASH access engine. The VR
application uses the sensor information and the MPD coverage and quality
metadata to continuously decide which adaptation sets that shall be used. The
task of the DASH access engine is to continuously fetch the media segments for
the selected adaptation sets, while possibly adapting between different
representations depending on the bitrate conditions in the network
In this use-case the DASH access engine does not have any knowledge about the
pose or viewport orientation, or other viewport aspects such as field-of-view.
It only tries to fetch suitable segments for the adaptation sets specified by
the VR application, and deliver these for decoding and rendering.
Thus any metrics related to pose or viewport handling must either use specific
non-viewport-related events known at DASH level (such as change of requested
adaptation sets and the later delivery of the related media frames), or use
combined trigger events derived from the VR application and the
Decoder/Renderer (which might be difficult).
# 5 Use case for VR QoE
## 5.1 Introduction
There are 12 use cases defined in TR26.918 [2], classified as UE consumption
of managed/3^rd^ party VR content and VR services including UE-generated
content.
## 5.2 3DoF VR Streaming
Although all the 12 use cases are possible scenarios of VR service, the
current standard work majorly focuses on 3DoF VR streaming, e.g. in MPEG.
According to [2], the use case of VR streaming is defined as:
"_A user watches a VR on-demand video content with a HMD. The content is
streamed over the 3GPP network using unicast. The user can navigate within the
360 degree video by moving his head and watch different fields of view within
the video_ "
For this use case,
1) Content part: study needs to be conducted on which metadata would help
analyze user experience.
2) Delivery part: changing network conditions may lead to problems in user
experience, especially the impact of transmission latency on user experience,
e.g. the initial loading latency, the transmission latency contributing to the
latency between head/eye movement and presentation of high-resolution content
to the user.
3) Device part: device capabilities will also have impact on user experience,
e.g. the decoder capability, the sensor detect latency in case of head
movement.
QoE metrics relevant with the above aspects need to be studied under this
study item, and based on the result of this study, user experience of 3DoF VR
streaming could be evaluated, and relevant provisioning and streaming
techniques toward improving user experience could be designed.
# 6 VR QoE reference model under consideration
## 6.1 General description
A reference model for VR QoE measurement is illustrated in Figure 1.
{width="4.132638888888889in" height="2.901388888888889in"}
Figure Reference model for VR QoE measurement
## 6.2 Observation point 1
The network access element issues network request for VR content depending on
the data received from Sensor and projection/orientation metadata carried in
network manifest file receives network response containing VR content and
decapsulates VR media stream from the network. The interface from the network
access element towards metric collection and computation (MCC) is referred to
as observation point 1 (OP1).
The OP1 is defined to monitor the following information:
  * a sequence of transmitted network requests, each defined by its transmission time, contents, and the TCP connection on which it is sent; and
  * for each network response, th reception time and contents of the response header and the reception time of each byte of the response body.
  * The projection/orientation metadata carried in network manifest file.
## 6.3 Observation point 2
The media processing element carries out demux and audio, image, video. The
interface from the media processing element towards MCC is referred to as
observation point 2 (OP2).
The OP2 consists of encoded media samples, and OP2 is defined to monitor:
  * media type, e.g. media resolution, media codec, media frame rate, media projection, etc
  * media decoding time
## 6.4 Observation point 3
The sensor element acquires user's head or body position, orientation and
motion, the sensor may also acquire environmental data such as light,
temperature, magnetic fields, gravity and biometrics etc. The interface from
the sensor towards MCC is referred to as observation point 3 (OP3).
The OP3 is defined to monitor the information such as:
  * Motion tracking information, e.g. 3DoF (Pitch, Yaw and Roll), 6DoF (X, Y, Z, Pitch, Yaw and Roll)
  * Timestamp when user movement is captured
  * Depth
## 6.5 Observation point 4
The rendering element carries out colour conversion, projection, media
composition and view synthesis for each VR media element.
The media presentation element synchronizes and presents mixed nature and
sythentic VR media elements to provide a full immersive VR experience to the
user. The interface from the media presentation towards MCC is referred to as
observation point 4 (OP4).
The OP4 is defined to monitor the information such as:
  * The media type
  * The media sample presentation timestamp
  * Wall Clock counter
  * Actual presentation viewport
  * Actual presentation time
  * Actual playout frame rate
  * Audio-to-video synchronization
  * Video-to-motion latency
  * Audio-to-motion latency
## 6.6 Observation point 5
The VR client control and management element manages client parameters such as
display resolution, frame rate, field of view (FOV), eye to screen distance,
lens separation distance, etc. The interface from the VR client control &
management towards MCC is referred to as observation point 5 (OP5).
The OP5 is defined to monitor the information such as:
  * Display characteristics, e.g. display resolution, display PPI, etc.
  * OS support, e.g. OS type, OS version
MCC may acquire data from multiple OPs and derive or compute specific VR
metrics such as latency or viewport package loss
# 7 VR content impact on QoE
## 7.1 Introduction
# 8 Transmission impact on VR QoE
## 8.1 Introduction
There are many similarities between VR and traditional streaming in terms of
transmission impact on user experience. And some of the QoE metrics defined in
TS26.247 could be directly applied for VR service. And of course new QoE
metrics are also possible considering specific properties of VR service. All
the transmission impact information could be collected by OP1 and OP2 of the
reference model described in Section 6.1.
## 8.2 QoE metrics relevant with network transmission
### 8.2.1 Average Throughput
Section 10.2.4 in [3] defines the metric for average throughput information.
This information could be observed by OP1 of the reference model.
### 8.2.2 Buffer Level
Annex D.4.5 in ISO/IEC 23009-1 [4] defines the metrics for buffer level status
events. This information could be observed by OP1 of the reference model.
### 8.2.3 Play List
Section 10.2.7 in [3] defines the metrics for event that may happen due to
user action, the end of the content or a permanent failure. This information
could be observed by OP1, OP2, OP3 and OP4 of the reference model.
# 9 VR device impact on QoE
## 9.1 Introduction
Compared with traditional streaming video, the key feature of VR service is to
create immersive experience and enable smooth interactivity between user and
the environment, in which VR device would play an important role. This
contribution proposes device information relevant to user experience of VR
service. All the device property information could be collected by OP5 of the
reference model described in Section 6.1.
## 9.2 QoE metrics relevant with VR device
### 9.2.1 Field of View
One of the factors that contribute to the uniqueness of 360 video experience
is the level of immersion induced by the wider FoV of HMD, which represents
the extent of observable environment at any given time. A wider FoV could help
provide a more authentic feeling of immersion. Thus FoV of the HMD is an
important parameter that helps evaluate to what extent a VR device could help
create immersive experience.
### 9.2.2. Resolution
Resolution here is defined as for per eye. An appropriate screen resolution
would provide the best and comfortable experience.
### 9.2.3. Refresh Rate
Refresh rate is the number of times per second the display grabs a new image
from the graphic processing unit. Lower refresh rate would contribute to
processing latency and lead to VR sickness, i.e. viewing glitches on the
screen. While higher refresh rate adds to the sense of presence in virtual
worlds.
### 9.2.4. Decoder capability
The support of codec profile and level is an important property that decides
the content types it could decode.
### 9.2.5 Detailed QoE metrics
The QoE metrics relevant with VR device as listed in Table 1 is necessary for
assessment of device impact on user experience, and suggested to be collected
by VR client. And since these metrics are generally static for VR device, the
metric is only logged at the start of each QoE reporting period.
Table QoE metrics relevant with VR device
Key Type Description
* * *
DeviceInformationList List A list of device information objects.  
_Entry_ Object A single object containing new device information.  
resolution Object Display resolution for each eye  
videowidth Integer Number of pixels in display width videoheight Integer
Number of pixels in display height refreshRate Integer The number of times in
a second that a display hardware updates its buffer  
decoderCapability Set Codec profile and level the device supports  
fieldofview Object Information of end device FoV capability.  
horizontalFoV Integer Horizontal FoV of the device in degrees. verticalFoV
Integer Vertical FoV of the device in degrees.
# 10 VR QoE reporting
## 10.1 Introduction
# 11 Conclusions
#