# Foreword
This Technical report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
The present document documents investigations on test methodologies for
immersive audio systems. Audio is a key component of an immersive multimedia
experience and 3GPP systems are expected to deliver immersive audio with a
high Quality of Experience. 3GPP needs to characterize the performance of
immersive audio systems, as part of the IVAS_Codec [1], VRStream [2] and
FS_CODVRA [3] work and study items, have prompted the investigations on the
present document.
# 1 Scope
The present document aims to:
1) Identify a minimum set of relevant perceptual audio quality attributes
(e.g. timbre, localization, overall quality, etc.) for the assessment of
immersive audio systems relevant to 3GPP.
2) Identify available test methodologies for the perceptual audio quality
attributes of interest and their applicability in the context of immersive
audio systems relevant to 3GPP.
3) Document subjective and objective test methodologies for assessing the
capturing, coding, transmission and rendering blocks of immersive audio
systems.
4) Document the diverse considerations for the testing of immersive audio
systems such as:
a) Impacts of audio rendering systems
b) Impacts of Head Related Transfer Functions (HRTFs)
c) Impacts of simultaneous video presentation to the perceived audio quality /
psychophysical response
d) Use of naïve and/or experienced listeners
e) Appropriate statistical methods for data analysis and reporting
f) Repeatability and reproducibility
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] SP-170611: \"EVS Codec Extension for Immersive Voice and Audio Services\".
[2] SP-170612: \"Virtual Reality Profiles for Streaming Media\".
[3] SP-170617: \"Study on 3GPP codecs for VR audio\".
[4] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
# 3 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [4] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [4].
# 4 Perceptual Audio Quality Attributes for Immersive Audio
## 4.1 Overview
Immersive audio quality has been generally described in the literature as a
multidimensional problem, with different audio quality attributes been
proposed. This clause reports on contributions identifying and proposing
attributes that could be of relevance to 3GPP immersive audio systems.
## 4.2 Considerations on quality attributes for assessing immersive audio
systems.
### 4.2.1 Input provided in S4-170836 - Test Methodology for the Assessment of
Audio Systems
#### 4.2.1.1 Proposal of relevant perceptual quality attributes
One of the objectives of the work item is the identification of relevant
perceptual audio quality attributes -- some are already mentioned in the WI
description [1]: timbre, localization and overall quality.
The source proposes to modify the spatial attribute title from
\"localization\" to \"immersion\" in order to put the focus on the entire
acoustic scene and not on the position of individual objects. Additionally, a
common issue for audio reproduction systems are nonlinear distortions, e.g.,
due to clipping or dynamic compression. A separate quality attribute
\"distortion\" could be added to include explicitly these effects in the
evaluation.
#### 4.2.1.2 Listening Test Methodology
There are several fundamental test methodologies available for the assessment
of signal processing or transmission systems. At their very core, they can all
be classified according to the number of stimuli that are presented to the
participant of the listening test. Either just a single stimulus is presented
(and the participant has to rate it individually) or multiple (often two)
stimuli are presented and the participant has to rate the difference(s)
between the stimuli.
A listening test was conducted following the first approach (according to [2])
with an Absolute Category Rating (ACR). The results of this test are presented
and the applicability of this methodology is discussed. Subsequently, a case
is made for the application of a comparative evaluation methodology in this
context.
#### 4.2.1.3 Experiments with Absolute Category Rating
Details on the listening test can be found in [3]; only a short overview of
the most important aspects is given here. Music tracks were recorded
binaurally in different cars, resulting in a test corpus of 161 stimuli
(including reference and anchor conditions) which were then evaluated using
equalized headphone playback. The number of participants was 45.
The most important result for the question of applicability of this ACR
methodology in the context of audio systems is the reluctance of the
participants to award excellent grades -- even for the reference signals (the
best condition in the listening test was rated at approx. 3.5 MOS, the best
stimulus at approx. 4.1 MOS). This is something that does not happen when
using this methodology in other contexts. This leads to a significantly
reduced spread of the stimuli (and conditions) on the MOS scale.
#### 4.2.1.4 Possibilities with Comparative Listening Tests
Methodologies for comparative listening tests are also available in [2], e.g.,
Degradation Category Rating (DCR) and Comparison Category Rating (CCR). Both
methodologies should be considered in the context of immersive audio systems.
As the work item description states that, \"the assessment of the capture,
coding, transmission and rendering blocks of the immersive audio system may
require distinct test methodologies.\"
A degradation category rating seems to be a natural match for the assessment
of coding and transmission. There is a clear reference, the input signal, and
a (usually) degraded signal that can be compared with the reference. An
example user interface for a DCR test with four rating scales can be seen in
the following figure.
* * *
{width="4.122222222222222in" height="3.2506944444444446in"}
* * *
Figure 1: Example user interface for a DCR test with four rating scales
Testing different capture or rendering approaches, however, is a different
situation. There is (in general) no perfect system, which can be used as a
reference. Only a comparison between different approaches is possible. Thus, a
CCR test (with multiple rating scales) is the option for this task -- an
example for the user interface of such a test is given in the following
figure.
* * *
{width="4.930555555555555in" height="3.2402777777777776in"}
* * *
Figure 2: Example user interface for a CCR test with four rating scales
#### 4.2.1.5 Conclusions
As an additional perceptual quality attribute, a dimension \"distortion\" is
proposed for the auditory assessment of (spatial) audio quality.
It is proposed to conduct tests with an explicit reference, i.e. not as an ACR
test. Depending on the specific element that will be tested, different
variants of comparative listening tests are useful.
Both proposals should be considered for inclusion into the subjective test
plan for the LiQuImAS work item.
#### 4.2.1.6 References
+---------+-----------------------------------------------------------+ | > [1] | > 3GPP S4-170746, „New WID on Test Methodologies for | | | > Evaluation of perceived Listening Quality in Immersive | | | > Audio Systems,\" Qualcomm Incorporated, HEAD acoustics | | | > GmbH, Sony Mobile Communications, Fraunhofer IIS, | | | > Huawei Technologies Co. Ltd., Ericsson LM, | | | > Sophia-Antipolis, France, 26-30 June 2017. | +---------+-----------------------------------------------------------+ | > [2] | > ITU-T Recommendation P.800, „Methods for subjective | | | > Determination of Transmission Quality,\" 08/1996. | +---------+-----------------------------------------------------------+ | [3] | J. Reimes, T. Deutsch, A. Fiebig und M. Oehler, | | | Comparison of Auditory Testing Environments for Car Audio | | | Systems,\" Fortschritte in der Akustik - DAGA 2017, Kiel, | | | 2017. | +---------+-----------------------------------------------------------+
### 4.2.2 Input provided in S4-170914 -- On Audio Quality Attributes
#### 4.2.2.1 Introduction
The work item LiQuImAS [1] has, as the first of its objectives, to identify
and agree on a minimum set of relevant perceptual audio quality attributes
(e.g. timbre, localization, overall quality, etc.) for the assessment of
immersive audio systems relevant to 3GPP. This document presents a discussion
on the topic of audio quality attributes/descriptors.
#### 4.2.2.2 Audio Quality Attribute Elicitation
The perceptual measurement of the Audio Quality of Immersive Audio Systems is
a multidimensional problem that includes several quality attributes [2]. These
quality attributes can be thought of as being organized in a hierarchical
structure. At the top layer, there is a basic or overall audio quality that
encompasses all aspects of the listening experience. At lower layers of the
hierarchy, attributes can be elicited through a vocabulary development
process. One example of such a process, recently utilized by Francombe et.al
in [3] is shown on Figure 3.
{width="6.897222222222222in" height="0.7270833333333333in"}
Figure 3: Audio Quality Attribute Elicitation Process
It is obvious that the replication of such methods does not produce a
completely identical set of attributes. The attributes derived are highly
dependent on the stimulus chosen during the word elicitation process, and
different panels of assessors may come up with different vocabularies.
Therefore, many different audio quality attributes have been proposed in the
literature. Examples of audio quality attributes found are available e.g. in
Zacharov, Pedersen and Pike (2016) [4]; Choisel and Wickelmaier (2006) [5];
Rumsey and Zelinski (2005) [6], Lindau et. Al (2014) [7] and Guastavino and
Katz (2004) [8]. A list of the attributes found in those references is
provided in Appendix A.
Nonetheless, some commonality can be observed by a careful analysis of the
attributes in Appendix A. With some degree of orthogonality, attributes can be
roughly clustered in the following categories:
\- Signal artefact related attributes
\- Noise artefact related attributes
\- Spatial related attributes
\- Timbre related attributes
\- Loudness related attributes
\- Dynamics related attributes
In addition to the cluster of attributes above, there are also certain
attributes that are highly idiosyncratic and do not fit exactly into any of
the categories above. E.g. clarity, presence, transparency, etc. Nonetheless,
they exhibit a material degree of correlation with the categories above and
may be mostly covered by the other attributes categories.
#### 4.2.2.3 The Sound Quality Wheel
In assisting with the visualization of the hierarchical structure inherent to
spatial audio quality, an \"attribute wheel\" (donut chart presentation) has
been proposed [4]. Figure 2 illustrates the first three layers of the
\"attribute wheel\" in [4].
{width="2.9555555555555557in" height="2.4in"}
Figure 4: Example of \"Attribute Wheel\" donut chart for visualization of
hierarchical nature of multi-dimensional audio quality
\- OVRL = Basic audio Quality
\- TRAN = Transparency
\- LOUD = Loudness
\- DYNA = Dynamics
\- SPAT = Spatial
\- TIMB = Timbre
\- BASS = Bass
\- MIDR = Mid-Range
\- TREB = Treble
\- ARTF = Artefacts
\- SIGN = Signal Related Artefacts
\- NOIS = Noise Related Artefacts
NOTE: Lower layers from [4] are not shown here for brevity.
The source understands that the deeper a listening test goes in the
hierarchical structure of quality descriptors, the narrower the diagnostic of
the source of the basic/overall audio quality degradation. However, this
increased diagnostic capability comes with the following caveats: (1) test
time is significantly increased; and (2) several attributes of lower layers
may never be exercised during the test, simply because the Immersive Audio
System block under test does not trigger degradation for those attributes.
Therefore, the source proposes to limit assessment to the first category
layer, except for \"Artefacts\", which the source sees value in distinguishing
further between signal and noise related artefacts for certain types of tests.
It is further proposed that \"Loudness\" is left out of the assessment as, for
most 3GPP purposes, loudness is normalized prior to sample presentation.
Finally, it is also proposed that the \"Dynamics\" cluster be handled under
the \"Signal\" cluster.
#### 4.2.2.4 Proposals on Quality Attributes for Audio Capture Tests
For the purposes of 3GPP\'s LiQuImAS Audio Capture (including all system
components prior to encoding) listening tests, the source proposes the
following:
\- All tests will include a \"basic audio quality\" (or overall) attribute,
corresponding to the 1^st^ hierarchical audio quality layer.
\- The following attributes should be adopted for the 2^nd^ layer:
\- Noise Related Artifacts;
\- Rationale: Noise in the capture system can be a significant source of
degradation for the basic audio quality.
\- Timbre;
\- Significant distortions to timbre can also be a significant source of
degradation for the basic audio quality with certain sound field capture
techniques.
\- Signal Related Artifacts;
\- Certain front-end processing techniques can have impact to the signal
integrity, other aspects such as saturation are also covered by this
dimension.
\- Spatial Quality;
\- All aspects of spatial audio quality, such as localizability, envelopment,
source width, height, depth and distance, are covered here.
Because a capture system has no ideal reference for comparison, the source
further proposes that systems are evaluated either against the listener\'s
\"internal reference\", as e.g. proposed in the SAQI work [7], or through ABX
or Comparison Category Rating tests.
#### 4.2.2.5 Quality Dimensions for Audio Coding and Transmission Tests
For the purposes of 3GPP\'s LiQuImAS Audio Coding and Transmission listening
tests (including all system components prior to rendering, starting with the
encoding process), the source proposes the following:
\- All tests will include a \"basic audio quality\" (or overall) attribute,
corresponding to the 1^st^ hierarchical audio quality layer.
\- The following attributes should be adopted for the 2^nd^ layer:
\- Signal Related Artifacts;
\- All perceived distortions to the signal imposed by the encoding/decoding
and transmission process.
\- Noise Related Artifacts [OPTIONAL]
\- This dimension should be included only if it is expected that the systems
under test will generate such degradation.
\- Timbre [OPTIONAL]
\- This dimension should be included only if it is expected that the systems
under test will generate such degradation (e.g. mixed bandwidth tests, etc.)
\- Spatial Audio Quality;
\- All distortions to the spatial audio quality, such as localizability,
envelopment, source width, height and distance, are covered here.
\- Because a capture system has an ideal reference for comparison, (i.e. the
signal prior to encoding) the source further proposes that audio coding and
transmission systems are evaluated through a DCR / MUSHRA based evaluation.
#### 4.2.2.6 References
+---------+-----------------------------------------------------------+ | > [1] | > 3GPP S4-170746, „New WID on Test Methodologies for | | | > Evaluation of perceived Listening Quality in Immersive | | | > Audio Systems,\" Qualcomm Incorporated, HEAD acoustics | | | > GmbH, Sony Mobile Communications, Fraunhofer IIS, | | | > Huawei Technologies Co. Ltd., Ericsson LM, | | | > Sophia-Antipolis, France, 26-30 June 2017. | +---------+-----------------------------------------------------------+ | > [2] | > N. Zacharov und S. Bech, Perceptual Audio Evaluation - | | | > Theory Method and Practice, Wiley, 2006. | +---------+-----------------------------------------------------------+ | > [3] | > J. Francombe, T. Brookes, R. Mason und J. Woodcock, | | | > „Evaluation of Spatial Audio Reproduction Methods (Part | | | > 1): Elicitation of Perceptual Differences,\" Journal of | | | > the Audio Engineering Society, Bd. 65, Nr. 3, pp. | | | > 198-211, March 2017. | +---------+-----------------------------------------------------------+ | > [4] | > N. Zacharov, T. Pedersen und C. Pike, „A Common Lexicon | | | > for Spatial Audio Quality Assessment - Latest | | | > Developments,\" Eight Conference on Quality of | | | > Multimedia Experience (QoMEX), 6-8 June 2016. | +---------+-----------------------------------------------------------+ | > [5] | > S. Choisel und F. Wickelmaier, „Extraction of Auditory | | | > Features and Elicitation of Attributes for the | | | > Assessment of Multichannel Reproduced Sound,\" Journal | | | > of the Audio Engineering Society, Bd. 54, Nr. 9, pp. | | | > 815-826, September 2006. | +---------+-----------------------------------------------------------+ | > [6] | > F. Rumsey, S. Zielinski, R. Kassier und S. Bech, „On | | | > the relative importance of spatial and timbral | | | > fidelities in judgments of degraded multichannel audio | | | > quality,\" Journal of the Acoustical Society of | | | > America, Bd. 118, Nr. 2, pp. 968-976, August 2005. | +---------+-----------------------------------------------------------+ | > [7] | > C. Guastavino und B. F. Katz, „Perceptual evaluation of | | | > multi-dimensional spatial audio reproduction,\" Journal | | | > of the Acoustical Society of America, Bd. 116, Nr. 2, | | | > pp. 1105-1115, August 2004. | +---------+-----------------------------------------------------------+ | > [8] | > A. Lindau, „Spatial Audio Quality Inventory (SAQI). | | | > Test Manual v1.2,\" Berlin, 2014 | +---------+-----------------------------------------------------------+
#### 4.2.2.7 Appendix A
{width="6.667361111111111in" height="4.808333333333334in"}
### 4.2.3 Input provided in S4-171225 -- On Spatial Audio Quality Assessment
#### 4.2.3.1 Introduction
This contribution provides a summary of (relatively) recent investigations
conducted in Orange on spatial audio quality assessment; this summary is
focused on results that are in the scope of the LiQuiMAS work item. This
summary is provided to complement the current findings reported in TR 26.861.
A review of spatial audio quality assessment can be found in [6]. The study of
spatial audio quality assessment brings up two open issues:
\- The identification of perceptual dimensions that represent spatial
perception (e.g. timbre, source location, naturalness/realism, etc.)
\- The development/selection of test methodologies to measure perception
according to the different dimensions
Both issues are reviewed separately in the following sections.
#### 4.2.3.2 On audio attributes and categories
##### 4.2.3.2.1 Audio attributes in ITU-R recommendations
Existing test methodologies such as ITU-R BS.1116-3 or BS.1534-3 focus on so-
called basic audio quality (BAQ). It can be noted these specifications list
additional attributes that may also be used such as: _Stereophonic image
quality_ for stereo systems _, Front image quality_ and _Impression of
surround quality_ for multichannel systems, _Timbral quality, Localization
quality (with subcategories), Environment quality (with subcategories)_ for
advanced systems. However, only BAQ is typically used, which is not sufficient
to evaluate spatial audio quality.
Furthermore there is the general recommendation in both ITU-R BS.1116-3 and
BS.1534-3:
_\"Only one attribute should be graded during a trial. When assessors are
asked to assess more than one attribute in each trial they can become
overburdened or confused, or both, by trying to answer multiple questions
about a given stimulus. This might produce unreliable grading for all the
questions. If multiple properties of the audio are to be judged independently,
it is recommended that basic audio quality be evaluated first.\"_
It was found in [8] that these existing test methodologies have some bias in
terms of anchors (low/medium quality references which are not necessarily
relevant or sufficient), presence of an explicit reference (which implies the
focus is on fidelity more than on quality), and grading scale interpretations
(meaning for translated labels depending on languages).
It should also be noted that ITU-R BS.1285, which has also been used in ITU-T
SG12 for some subjective tests, refers to attributes defined in ITU-R BS.1116.
##### 4.2.3.2.2 Audio attributes and categories from a lexical study
Some audio attributes, such as coloration, brightness, distortion,
localization have been listed to complement basic audio quality. However, it
is difficult to include them in a listening test due to issues with their
definitions and understanding by subjects. These issues can be reduced by
using categories.
In [1], a study was conducted to reduce the number of categories for (spatial)
audio quality assessment. The planned application was the integration of
categories into audio quality assessment tests, especially for codecs,
rendering or capture systems, etc. It is important to note that the study was
only lexical, based on semantic tests, i.e. subjects did not listen to sounds,
subjects had to rely on their \"every day\" listening experience.
Two experimental methods, multidimensional scaling (MDS) and free
categorization, were used and a clustering analysis was applied on the
results. For free categorization, subjects had to group 28 attributes into
families. For MDS, subjects had to rank the similarity between couples of
attributes (e.g. stability vs. reverberation).
In both cases the clustering gave three categories: _timbre_ , _space_ and
_artefacts_ (called \"defects\" in [1]). Timbre and space categories have been
identified in previous work (prior to [1]). One rationale for identifying the
artefacts category is that codec artefacts were explicitly included. The
identified categories include the following attributes:
\- Timbre:
\- sound color, e.g. brightness, tone color, coloration, clarity, hardness,
equalization, richness
\- homogeneity, stability, sharpness, realism, fidelity and dynamics
\- Space: depth, width, localization, spatial distribution, reverberation,
spatialization, distance, envelopment, immersion
\- Artefacts: noise, distortion, background noise, hum, hiss, disruption
It should be noted that attributes and categories refer in this context to
different concept in [1]: an audio attribute refers to terms used to qualify
listening experience, while a category is obtained after clustering, which is
by nature more generic, less subject to misinterpretation by subjects. The
meaning of categories may potentially overlap.
Some audio attributes, such as coloration, brightness, distortion,
localization have been listed to complement basic audio quality. However it is
difficult to include them in a listening test due to issues with their
definitions and understanding by subjects. These issues can be reduced by
using categories.
In [1], a study was conducted to reduce the number of categories for (spatial)
audio quality assessment. The planned application was the integration of
categories into audio quality assessment tests, especially for codecs,
rendering or capture systems, etc. It is important to note that the study was
only lexical, based on semantic tests, i.e. subjects did not listen to sounds,
subjects had to rely on their \"every day\" listening experience.
Two experimental methods, multidimensional scaling (MDS) and free
categorization, were used and a clustering analysis was applied on the
results. For free categorisation, subjects had to group 28 attributes into
families. For MDS, subjects had to rank the similarity between couples of
attributes (e.g. stability vs. reverberation).
In both cases the clustering gave three categories: _timbre_ , _space_ and
_artefacts_ (called \"defects\" in [1]). Timbre and space categories have been
identified in previous work (prior to [1]). One rationale for identifying the
artefacts category is that codec artefacts were explicitly included. The
identified categories include the following attributes:
\- Timbre:
\- sound color, e.g. bright- ness, tone color, coloration, clarity, hardness,
equal- ization, richness
\- homogeneity, stability, sharpness, realism, fidelity and dynamics
\- Space: depth, width, localization, spatial distribution, reverberation,
spatialization, distance, envelopment, immersion
\- Artefacts: noise, distortion, background noise, hum, hiss, disruption
It should be noted that attributes and categories refer in this context to
different concept in [1]: an audio attribute refers to terms used to qualify
listening experience, while a category is obtained after clustering, which is
by nature more generic, less subject to misinterpretation by subjects. The
meaning of categories may potentially overlap.
##### 4.2.3.2.3 Attributes related to binaural rendering
For the purpose of developing tools to offer a personalized binaural listening
experience, the identification of audio attributes for binaural listening with
non-individualized HRTFs has been studied in [2,3]. in [2] verbal exploration
(elicitation) was used and it resulted in 12 attributes:
\- Externalization
\- Immersion
\- Spectral modification (bass / treble)
\- Realism
\- Depth of sound field
\- Localization accuracy
\- Lateral location
\- Elevation
\- Front/back location
\- Reverberation
\- Sound level
\- Depth
In [3] MDS was used with a database of 46 HRTFs and 10 expert listeners to
build a \"perceptual space\"; this study identified 4 categories (perceptual
dimensions):
\- spectral modification (amount of high frequencies)
\- spaciousness/volume/width of audio scene
\- spatial centroid
\- depth/distance of objects
The identification of corresponding objective criteria was left for further
study.
A comprehensive review of perceptual dimensions for binaural listening can be
found in [4] where a difference is made between physically-related attributes
(timbre, source location, width, room effects) and psychoacoustic/emotive
attributes (naturalness, ability to discriminate sounds, emotion).
#### 4.2.3.3 On subjective test methodologies
##### 4.2.3.3.1 Choice of listening instrument: loudspeakers vs. headphones
Spatial audio testing can be conducted over loudspeakers or headphones. If
loudspeaker rendering is used, one has to pay attention on the different
layouts and room effects, which may differ from one lab to another. For this
reason, the focus in [1,3,4,5,7,8] has been on listening over headphones. This
focus is consistent with the way most subjective tests have been conducted for
telephony/conversational applications.
Listening to spatial audio over headphones brings up the issue of how binaural
reproduction is realized, and in particular the selection of HRTFs (non-
individual, personalized or individual). We would recommend to study this
issue in LiQuiMAS.
##### 4.2.3.2.2 Methodology used to evaluate the binaural rendering of
different audio capture systems
In [5] a test methodology was developed to rank spatial audio capture systems
(native binaural recording using dummy heads, AB stereo, multichannel arrays,
ambisonic microphones, mixing based on spot microphones). All audio samples
were rendered on headphones and binaural rendering used non-individualized
HRTFs. The objective was to evaluate and compare binaural rendering of audio
formats captured with 13 different methods, and to evaluate the \"robustness\"
of capture methods with respect to binaural rendering. It is important to note
that the influence of personalized or individual HRTFs was not evaluated.
Moreover, not all capture systems were used at the same time during recording
phase, different sessions took place and the variability of recorded was kept
minimal.
To take into account the multidimensional nature of spatial audio quality
assessment, the listening experiments were to split in two tests:
1) A localization test (test 1) where each test condition was associated with
a given audio capture system: the task was to identify sources, indicate their
positions and potential movements and subjects had to draw the answer on a
paper sheet with a specific framework. In addition, there was also a basic
questionnaire asking about realism, spatial accuracy, spaciousness and maximum
perceived distance.
2) A preference test (test 2) with a questionnaire (listing 10 possible
attributes related to binaural listening - subjects had to indicate if the
definition was understandable and if the attribute was used to assess
preference). A binaural dummy head (KU100) was selected as the reference
system and subjects had to evaluate their level of preference between A and
REF on a 7-degree rating scale. Each pair was presented twice in a different
order.
Naive and experienced / expert listeners participated in these 2 tests. The
main criterion used to analyse localization test results was the ratio between
correct and incorrect localization. For the preference test, the consistency
between scores from double presentation was analysed and the average
preference score was computed. Interestingly, the reference system (KU100) was
the preferred capture system for quality (in test 2), however it did not give
the highest localization scores. It was concluded in [5] that the overall
perception of audio scenes (as evaluated in test 2) may not depend on how well
systems reproduce sounds in terms of location of real/recorded scenes (as
evaluated in test 1).
It was also concluded in [5] that the choice of a spatial audio solution will
strongly depend on the target (e.g. reproduce sounds at specific expected/real
locations as in video games, or render sound to favour immersion, timbre,
accuracy, etc.). Furthermore, the use of a preference test had the advantage
that it avoids defining grading scales / attributes that are still an open
issue when trying to characterize correctly the multidimensional perception of
spatial audio scenes.
##### 4.2.3.2.3 Extended MUSHRA methodology with anchors to cover multiple
degradations/categories
In [1] four categories of attributes have been identified: \"overall
quality\", \"timbre\", \"space\", and \"artefacts\". In [7] a listening test
using binaural audio was decomposed in two sessions. The first one was the
evaluation of the overall quality and the second one was the simultaneous
assessment of the 3 attributes (\"timbre\", \"space\" and \"artefacts\"). The
protocol was inspired from ITU-R BS.1534. Still, the test included no explicit
reference, the original version was only an hidden reference.
The development of relevant anchors for multidimensional quality evaluation
was addressed. The test included 3 anchors, each one focused on an given
attribute (timbre, space and artefacts): 3.5 kHz low-pass filtered anchor,
pink noise with clicks, and alternated channel inversion / mono / original. It
was found in investigations that the choice of anchors, especially for the
\"space\" attribute is important to ensure that the associated scores for
anchors is not too high and has limited variance across items - for instance
L/R inversion can have a limited impact on some items, with a score strongly
varying depending on items; some improved anchors for the \"space\" attribute
may include a reduced (squeezed) spatial and rotated image. It was also shown
also be possible to combine different degradations to have a single anchor
(e.g. add noise and clicks, apply fluctuating image, and low-pass filtering)
without affecting the scores in different categories.
Note that subjects were instructed to rank at least one of the test sequences
at the highest quality level and there was no explicit reference, to focus on
quality (and not fidelity).
More details on test results and their analysis (e.g. correlation between the
3 attributes and overall quality) can be found in [7]. Linear regression
analysis on scores gave the equation:
OQ (Overall Quality) = 0,67 artefacts+0,5 timbre+0,29 space−0.45
which tends to show that the \'artefacts\' category has a dominant impact in
overall quality for the tested conditions (incl. HE-AAC at 40 kbit/s, AMR-WB+
at 40 kbit/s, MP4 at 64 kbit/s, AAC at 32 kbit/s).
#### 4.2.3.4 Conclusion
We proposed to capture the previous summary in TR 26.861 and take this
information into account for the LiQuIMAS work.
#### 4.2.3.5 References
[1] S. Le Bagousse, M. Paquier, C. Colomes, Categorization of sound attributes
for audio quality assessment - a lexical study, Journal of the Audio
Engineering Society, Audio Engineering Society, 2014, 62 (11), pp.736-747
[2] L. Simon, N. Zacharov, B. Katz, Perceptual attributes for the comparison
of head-related transfer functions, The Journal of the Acoustical Society of
America 140, 2016
[3] P.Y. Michaud, R. Nicol, Measurement of QoE for spatial audio: example of
binaural sound with the BiLi project (in French), JISFA, 2015
[4] R. Nicol et al., A Roadmap for Assesssing the Quality of Experience of 3D
Audio Binaural Rendering, Proc. of the EAA Joint Symposium on Auralization and
Ambisonics, 2014
[5] R. Nicol et al., Comparative study of rendering for different spatial
audio capture techniques after binauralization (in French), CFA / VISHNO,
April 2016
[6] S. Le Bagousse, M. Paquier, C. Colomes, State of the art on subjective
assessment of spatial sound quality, AES (Audio Engineering Society). 38th
International Conference: Sound Quality Evaluation, Jun 2010
[7] S. Le Bagousse, M. Paquier, C. Colomes, and S. Moulin. Sound Quality
Evaluation based on Attributes - Application to Binaural Contents, Proceedings
of 131st AES Convention, 2011.
[8] S. Zielinski et al., Potential biases in MUSHRA listening tests, Proc.
123th AES Convention, 2007
### 4.2.4 Input provided in S4-180125 -- On Auditory Assessment of Audio
Systems
#### 4.2.4.1 Introduction
The work item LiQuImAS [1] aims at specifying and developing test
methodologies for the assessment of immersive audio systems. Proposals were
made in a previous contribution [2] regarding the relevant perceptual
attributes and the listening test methodology.
This contribution describes a recently conducted comparison category rating
(CCR) listening test which uses four rating scales (timbre, distortion,
immersion and overall quality).
#### 4.2.4.2 Listening Test Material
The audio material for the listening test consists of binaural recordings of
{width="0.5444444444444444in" height="0.19236111111111112in"} different songs
played over {width="0.5680555555555555in" height="0.19236111111111112in"}
different audio systems ranging from cheap, small and portable active speakers
to expensive and large high-end loudspeaker pairs. Each signal had a fairly
short length of approx. four seconds. There are good arguments for long and
short signal lengths: A long signal would offer a more thorough evaluation of
the system performance while a short signal simplifies the comparison between
the test items. Thus, a small preliminary listening test was carried out to
determine the appropriate signal length. Experienced and naïve listeners alike
preferred shorter signals.
An additional aspect that was covered by the preliminary test was the choice
of signals. It is desirable to have realistic, critical signals for conducting
any listening test. A CCR test allows to quantify the usefulness of each
signal based on two goals: The signal should elicit clear judgements and these
judgements should be consistent across the test subjects. These goals can be
expressed by the mean absolute value and the standard deviation of all votes
when using a certain signal. The aforementioned preliminary listening test was
conducted with a larger number of signals, six of which were then used for the
complete test.
In a CCR test where each comparison is only assessed once (i.e. not in both
possible orderings), the number {width="0.14791666666666667in"
height="0.18125in"} of necessary comparisons can be calculated according to
{width="1.8173611111111112in" height="0.3861111111111111in"}
For the numbers in this test, this leads to 90 comparisons.
#### 4.2.4.3 Listening Test Methodology and Environment
Altogether, 38 normal-hearing naïve test subjects participated in the auditory
test. After a short introduction with six comparisons to familiarize the test
subjects with the task, each subject listened to all 90 comparisons. Both the
order of the comparisons as well as the order of the signals in the individual
comparison were individually randomized. The subjects were allowed to listen
to the signals as many times as they wanted (but had to listen to each signal
at least once) and had to give their votes on the four rating scales from
Figure 2 in [2] (-3 to +3 on the scales timbre, distortion, immersion and
overall quality). The overall test duration varied strongly between 45 and 90
minutes as some subjects used the possibility for signal repetition only
sparingly while others repeated most of the signals multiple times.
#### 4.2.4.4 Results
##### 4.2.4.4.0 General
In the context of LiQuImAS, the applicability of the test design is the main
question. Hence, the analysis of the results mostly focuses on the reliability
of the votes of the subjects and on techniques for mapping from a comparative
scale to an absolute scale. Additionally, a minor investigation on the chosen
rating scales is carried out.
##### 4.2.4.4.1 Postprocessing of Votes
Comparative listening tests have the advantageous property that they
inherently provide reliability information on the votes. A test subject that
is giving inconsistent votes produces a large ratio of circular triads.
Circular triads occur when three test items {width="0.10208333333333333in"
height="0.18125in"}, {width="0.10208333333333333in" height="0.18125in"} and
{width="0.10208333333333333in" height="0.18125in"} are compared to each other
and the results are that {width="0.3972222222222222in" height="0.18125in"},
{width="0.3972222222222222in" height="0.18125in"} and
{width="0.3972222222222222in" height="0.18125in"}. Accordingly, a reliable
test subject would vote {width="0.3972222222222222in" height="0.18125in"} in
the last comparison. This is readily defined for paired comparison tests where
no intermediate steps (e.g., \"{width="0.10208333333333333in"
height="0.18125in"} is slightly better than {width="0.10208333333333333in"
height="0.18125in"}\") are available. For the CCR test, the definition is not
as straightforward as each comparison essentially results in a comparison
result {width="0.5791666666666667in" height="0.18125in"} on a scale from
{width="0.19236111111111112in" height="0.18125in"} to
{width="7.916666666666666e-2in" height="0.18125in"} and different
possibilities for treating the results are possible:
The straightforward relation to the definition from the paired comparison test
would be to only look at the signs of the comparison results. A circular triad
with this definition occurs when
{width="2.9541666666666666in" height="0.20416666666666666in"}
This would be very strict regarding small differences (e.g., this set of votes
would be a circular triad with this definition:
\"{width="0.10208333333333333in" height="0.18125in"} is slightly better than
{width="0.10208333333333333in" height="0.18125in"}\",
\"{width="0.10208333333333333in" height="0.18125in"} and
{width="0.10208333333333333in" height="0.18125in"} are about the same\" and
\"{width="0.10208333333333333in" height="0.18125in"} and
{width="0.10208333333333333in" height="0.18125in"} are about the same\") but
might miss other critical cases (e.g., this set of votes would not be a
circular triad with this definition: \"{width="0.10208333333333333in"
height="0.18125in"} is much better than {width="0.10208333333333333in"
height="0.18125in"}\", \"{width="0.10208333333333333in" height="0.18125in"} is
much better than {width="0.10208333333333333in" height="0.18125in"}\" and
\"{width="0.10208333333333333in" height="0.18125in"} is slightly better than
{width="0.10208333333333333in" height="0.18125in"}\").
Thus, the magnitudes of the votes has to be taken into account. The source
proposes to calculate the absolute value {width="0.10208333333333333in"
height="0.18125in"} of the sum over a closed loop of comparisons according to
{width="2.5104166666666665in" height="0.18125in"}
Obviously, a perfect result would be {width="0.375in" height="0.18125in"} for
all possible sets of comparisons in the test. This is unrealistic as small
differences will always occur with human subjects. Thus, a threshold has to be
defined. The source proposes to use a threshold of {width="0.375in"
height="0.18125in"} for the evaluation. This means that missing the _correct_
value in each comparison by one step on the CCR scale is acceptable. Looking
at the two aforementioned examples, the first set of votes would no longer
give a circular triad while the second set would now constitute a circular
triad.
Two example histograms illustrating the value distribution of
{width="0.10208333333333333in" height="0.18125in"} for different test subjects
are depicted in Figure 5. The left subfigure shows the results for a test
subject that voted quite consistently, i.e., only small values for
{width="0.10208333333333333in" height="0.18125in"} occur. The spread of values
in the right subfigure is much wider and even very large values for
{width="0.10208333333333333in" height="0.18125in"} can be seen.
* * *
{width="3.35in" height="2.5104166666666665in"} {width="3.35in"
height="2.5104166666666665in"}
* * *
Figure 5: Example histograms for triad scores
Dividing the number of circular triads by the total number of possible triads
in the listening test (480) gives the circular triad ratio, a single value
that quantifies the reliability of the test subject. An overview of all the
circular triad ratios is depicted in Figure 6. Most test subjects had no or a
very limited number of circular triads but there are four clear outliers with
circular triad ratios above 10%.
{width="3.9409722222222223in" height="2.941666666666667in"}
Figure 6: Histogram of all circular triad ratios
Based on the ratio of circular triads, the results for the least reliable test
subjects were excluded. From the experience gained so far, a threshold for the
ratio of circular triads of 7-10% appears to be a reasonable choice.
##### 4.2.4.4.2 Mapping to absolute scale
In Absolute Category Rating (ACR) or Degradation Category Rating (DCR) tests,
it is straightforward how to determine the overall ranking of the test
conditions or devices under test, respectively. A comparative test requires an
additional analysis step that maps the results onto an absolute scale. An
overview for different approaches for paired comparison data can be found in
[3].
All approaches can also be used for CCR results if they are interpreted in
analogy to paired comparison results. This analogy requires constructing a
matrix of preferences {width="0.30625in" height="0.18125in"} (cf. from [3]:
\"The result of a paired comparison experiment is a count matrix,
{width="0.10208333333333333in" height="0.18125in"}, of the number of times
that each option was preferred over every other option [...]\") from the CCR
results {width="0.6472222222222223in" height="0.18125in"} by first rescaling
them according to
{width="1.75in" height="0.35138888888888886in"}
and summing over the test subjects:
{width="1.7951388888888888in" height="0.3972222222222222in"}
This ensures that the resulting matrix can then be processed with exactly the
same methodologies that are described in [3]. Doing so for the results of this
CCR test leads to different rankings on the different quality scales, e.g.,
the best system regarding timbre only reaches the third place for immersion.
As an example, the votes on the timbre scale for one of the songs are
considered. Table 1 gives the resulting values after rescaling and summation
over all subjects.
Table 1: Example results of a part of the listening test
* * *
             System 1   System 2   System 3   System 4   System 5   System 6
System 1 19.17 9.5 9.17 23.67 11.33 System 2 14.83 12.17 10.17 22.17 12.67
System 3 24.5 21.83 16.67 24 16.17 System 4 24.83 23.83 17.33 27.5 18.5 System
5 10.33 11.83 10 6.5 7 System 6 22.67 21.33 17.83 15.5 27
* * *
The values in the table quantify preferences between the individual systems:
e.g., system 4 was clearly preferred to system 5 (27.5 vs. 6.5).
Using the maximum likelihood estimate from [3] results in values on an
absolute scale as shown in Table 2 (direct output from the estimation
algorithm). Note that it is in general not possible to define an absolute
minimum/maximum quality index on this absolute scale. The intention of this
procedure is to provide a ranking between the different systems. The distances
between the systems also describe the quality difference observed between the
systems in a subjective test.
In addition, rescaled values on an MOS scale arbitrarily assigning a value of
5.0 to the best and 1.0 to the worst system. Other mappings can be considered
as well.
Table 2: Example maximum likelihood estimate
* * *
                  System 1   System 2   System 3   System 4   System 5   System 6
Direct output -0.16 -0.17 0.24 0.36 -0.52 0.25 MOS 2.67 2.63 4.45 5.00 1.00
4.53
* * *
The resulting ranking on an absolute scale is based on all comparisons. This
can be seen quite clearly when looking at the results for systems 1 and 2.
System 1 is clearly favoured in the direct comparison (19.17 vs. 14.83) but
system 2 does not lose as clearly as system 1 against the three better
systems. In the final ranking, the two systems are very close to each other.
##### 4.2.4.4.3 Predictability of overall quality
Another previous contribution [4] presented a linear regression analysis for
the relation between individual quality dimensions and overall quality based
on a MUSHRA test. That contribution used the three quality dimensions timbre,
artefacts and space. The analysis led to this relation between the dimensions:
Overall quality = 0.5*timbre + 0.67*artefacts + 0.29*space -- 0.45
The CCR test here uses the dimensions timbre, distortion and immersion which
are very similar to the dimensions in [4]. A linear regression between the
individual dimensions and the overall quality leads to the result that is
depicted in **Figure 3**. The correlation between the auditory result and the
regression result is very high and no outliers can be observed.
{width="3.7395833333333335in" height="3.7395833333333335in"}
Figure 7: Linear regression between individual dimension and overall quality
In this test, the relation between the dimensions was:
Overall quality = 0.6*timbre + 0.25*distortion + 0.31*immersion + 0.03
While the impact of timbre and space/immersion is quite similar between the
two tests, there is a clear discrepancy in the weight of artefacts/distortion.
The reason for this might be the very different audio material used in the two
tests. The test in [4] included different audio codecs at lower bit rates
which presumably generated very noticeable signal processing artefacts. The
CCR test presented here did not use different codecs but different audio
systems -- some of these also produced nonlinear distortions but the higher
quality systems mostly had no perceivable distortions at all.
#### 4.2.4.5 Conclusions
A CCR test comparing different audio systems was conducted with 38 naïve
listeners. Aspects of test design, signal choice and evaluation of the results
were discussed. An analysis of the results revealed that the test subjects
mostly gave reliable results with only a limited number of circular triads.
This inherent reliability information was used to exclude the results for the
test subjects that rated the test items in an inconsistent manner.
A linear regression analysis of the relation between the overall quality and
the individual dimensions timbre, distortion and immersion revealed that the
overall quality can be closely approximated already from this simple linear
regression. A brief comparison with a linear regression that was carried out
for another listening test with similar quality dimensions supported the
choice of quality dimensions. As there were some differences in the exact
coefficients for the linear regression, these should be determined over a
larger corpus of auditory tests.
#### 4.2.4.6 References
[1] 3GPP S4-170746 New WID on Test Methodologies for Evaluation of perceived
Listening Quality in Immersive Audio Systems,\" Qualcomm Incorporated, HEAD
acoustics GmbH, Sony Mobile Communications, Fraunhofer IIS, Huawei
Technologies Co. Ltd., Ericsson LM, Sophia-Antipolis, France, 26-30 June 2017.
[2] 3GPP S4-170836 Test Methodology for the Assessment of Audio Systems,\"
HEAD acoustics GmbH, Belgrade, Serbia, 09-13 October 2017.
[3] K. Tsukida und M. Gupta, „How to analyze paired comparison data,\"
Department of Electrical Engineering, University of Washington, Seattle, 2011.
[4] 3GPP S4-171225 On spatial audio quality assessment,\" Orange S.A.,
Albuquerque, New Mexico (US), 2017.
### 4.2.5 Input provided in S4-180144 - High Dimensional Assessment of Spatial
Audio Quality
#### 4.2.5.1 Abstract
In this contribution the source reports about a testing methodology for high
dimensional assessment of spatial audio quality that has recently been
published [1]. The methodology is suitable to assess various perceptual
attributes of spatial sound in a consistent and reproduceable manner. We
report on perceptual attributes that can be evaluated, on the testing
methodology and on tests illustrating its resolution, variability and
reproducibility.
#### 4.2.5.2 Perceptual attributes of spatial audio
One of the most fundamental attribute of sound is overall quality. Its direct
assessment may become very difficult if the sound is associated with various
higher-level attributes that test subjects may consider in their assessment
ratings in different, non-consistent manners. There are attempts to solve this
problem with corresponding listener instructions such as to consider only a
certain aspect like overall sound quality while ignoring other aspects.
However, such instructions may be ambiguous and each listener may still apply
an own understanding of the evaluated attribute. Also, too detailed or forcing
instructions may bias the ratings and compromise reliability and
representativeness of the overall test results. It is thus important to apply
a testing methodology that is able to assess different sound attributes in a
neutral way avoiding bias and that allows combining them to a compound metric
of perceived overall quality.
Attributes that are relevant for the perception of spatial audio can be
classified in a hierarchical order. Examples of higher-level spatial
attributes are the following:
\- Location
\- Spatial coordinates
\- Diffuseness/density of sound source
\- Sound source width (within boundaries)
\- Distance from listener and externalization
\- Motion
\- Trajectory of motion
\- Velocity of source
\- Relative position
Other relevant examples of higher-level quality attributes are:
\- Sound fidelity with respect to original artistic or technical intent of the
content creator
\- - Spectral naturalness/Timbre
#### 4.2.5.3 Adaptive audio (ADA) test methodology
The ADA test methodology aims to complement features not easily captured or
assessed with other approaches, including ways to measure:
1) system performance with respect to consistency in the perceptual
reproducibility of the original artistic or technical intent of the content
creator,
2) the perceived robustness of individual systems to deviations from specified
speaker placements or virtual source localization,
3) consistency in the robustness of the experience across individual
listeners, content items and endpoints, and
4) combined metrics of perceived overall quality in addition to individual
dimensions of system performance.
This is accomplished by collecting multiple metrics contributing to the total
perceptual experience to develop a thorough assessment framework for advancing
system development.
The ADA data collection environment consists of two parts:
1) a testing room or location with visually demarcated spatial landmarks, and
2) a corresponding iOS application, the ADA app.
The testing environment places the listener at centre of a controlled
listening space with standardized visual and spatial landmarks enabling
orientation of their perceptual environment with their perceptual experience.
For visual landmarks, 4 concentric rings are centred around the chair spanning
a 6 foot radius at intervals of 18 inches. Rings can be created using tape or
any flexible or curved rigid material. Most recently, we have been working
with brightly colored acrylic rings that are highly visible, fluorescent, and
can be identified in extremely low light conditions. Achieving visibility in
low light can be important for free-field listening conditions where the
desired testing paradigm includes comparison of different rendering techniques
across similar or variable speaker layouts. The ability to obscure speaker
positions to the listener can be an important and relevant feature for both
free-field listening and virtual source location conditions (Figure 8).
Figure 8: Visual markers using tape or acrylic rings. A listener sits on a
chair in the centre of the rings in a controlled listening environment for
free-field or binaural sound playback.
The ADA app presents a visual representation of the test environment: the
listener position, room walls, floor and ceiling, and the concentric rings.
Using a touch screen interface such as an iPad with touch-sensitive and
adjustable digital brush sizes, the collection app allows finger or stylus
drawing of shapes on an orthographic view of the concentric rings to depict
sound with features such as localization, boundaries, elongation, dispersion,
etc., relating to perceived sound emission (Figure 9). Height can also be
captured in a perspective view, allowing for spatial assessment in the
vertical plane and representation of the full 3-dimensional environment.
Following spatial entry, users also report measurements for selected
attributes such as spectral or timbral naturalness, or any other metrics that
provide useful characterization of the test signals.
Figure 9: Orthographic, perspective, and spectral naturalness rating input
modes of the ADA app (top and bottom left). The lower right panel shows a PCA
ellipsoid fit to a drawn object and example measurements of maximum
externalization (M) and dispersion (D).
Collection of user responses are followed by analysis to derive quantifiable
metric in a suitable data management environment. For stationary sounds,
shapes input by listeners will often be a circular or ovoid representation of
the acoustic emission. These are optimally fit using principal component
analysis (PCA) to extract salient dimensions of representation. These
representations are analysed in an automated manner to derive features such
as: elevation and azimuth of centroid location, average, minimum and maximum
externalization, scene boundaries, elongation, and dispersion. Dynamics of
data capture such as time, relative position, and velocity, are also recorded
and analysed enabling trajectory assessment and comparison. Further trajectory
analysis can include optimal curve fitting parameters, assessment of variance
in start and endpoints, variance of velocity and vectors of motion, and
overall travel length of sound trajectories.
#### 4.2.5.4 Suitability analysis of ADA methodology
##### 4.2.5.4.1 ADA in free-field listening environments
To evaluate the resolution, variability and reproducibility of listener
responses using the ADA assessment system, we conducted tests using speakers
placed at known distances and locations, fully obscured by an acoustically
transparent opaque cloth that did not obstruct visibility of the anchoring
rings. Test participants for these and related tests were drawn from an
external (non-employee) test pool of critical listeners who had strong audio
backgrounds and keen listening skills but no familiarity of development
research objectives. These experiments provided data to quantify per- and
across-listener variability, accuracy and consistency of spatial localization
and reporting.
For our first test, we selected 8 target speaker locations placed just beyond
the outermost visual marker ring and visually obscured by the curtain (5 to
the side/front and 3 to the rear) and varied these target directions ±10 _◦_
for a total of 24 directions. These were grouped into 3 blocks for listener
presentation as depicted in Figure 10. Four mono sources were selected for
playback at each location:
\- Male dialog
\- Female dialog
\- Pink noise bursts
\- Clarinet tone
This calibration test was designed to capture the angle of sound presentation
at a fixed distance. Listeners were instructed to record their responses on
the third ring represented in the ADA app to represent angle and dispersion.
Representative user responses are shown in Figure 10. These results enabled us
to visualize listener variability for reports of angle and dispersion.
{width="3.9319444444444445in" height="4.625694444444444in"}
Figure 10: Free-field fixed-distance test of localization: Angular locations
of speakers (top) and four representative listener responses of perceived
angle and dispersion of sound localization corresponding to speaker positions
from set 1
In a second series of tests, we incorporated both location and distance,
varying speaker locations along 12 target distances between the 1st and 4th
reference ring, with more distances represented near the 2nd ring (36\" from
centre) where greater resolution was desired. A \"pie\" shaped curtain
configuration visually obscured speakers while preserving visibility of the
rings. The same 4 mono audio signals used in the first test were measured, and
user reports allowed characterization and accuracy of measurements for angle,
dispersion, and distance or externalization.
{width="3.9451388888888888in" height="5.15625in"}
Figure 11: Free-field variable-distance localization test: Ring quadrants were
visually obscured by a curtain (top, shaded area) and multiple speaker
locations tested. Representative responses for four listeners showing angle,
distance, and dispersion of perceived sound (bottom)
##### 4.2.5.4.2 Analysis of free-field listening environments
Our two free-field tests demonstrated that the ADA methodology could
consistently and robustly capture differences in distance and location in a
free-field listening environment across and within subjects. Subsequent
analysis informed several aspects of individual and average listener
performance metrics. These included target accuracy in angle and distance,
presence or absence of perceived elevation, area and size of dispersion, and
rated spectral naturalness.
Averages and variability of responses demonstrated performance metrics that
could be reliably tracked and measured. We visualized these recordings as
either boxplots (medians and quartiles) or means with 95% confidence intervals
(Figure 12). The difference between target source location and reported
location based on ellipse centroids exhibited minor variation within _±_ 5 _◦_
with one location (rear surround at -110) slightly higher. Measurements of
dispersion demonstrated consistent reporting of less than a foot of source
diameter, and was mostly invariant across angles tested (Figures 12 and 13).
{width="3.9319444444444445in" height="2.9583333333333335in"}
Figure 12: Free-field fixed-distance listening tests. Listener responses
represented as scatter- or boxplots (left) and means/confidence intervals
(right) demonstrate azimuth localization accuracy and variability of
dispersion for multiple speaker angles tested.
{width="3.9444444444444446in" height="3.6041666666666665in"}
Figure 13: Free-field variable-distance listening tests of localization.
Listener responses, represented as means/confidence intervals, show accuracy
in perceived azimuth (top, left) and distance (top, right) along with
relatively preserved dispersion (bottom) across all azimuth positions tested.
#### 4.2.5.5 Applicability of ADA
##### 4.2.5.5.1 ADA assessment for binaural sound systems
While any endpoint can be used with ADA, a representative use of the ADA test
system in research and development is to drive optimal experiential
performance of a virtualized binaural audio system. In order to succeed in
this goal, perceptual cues of height and externalization need to be
realistically introduced, while, for example, unnatural reverb, distortion, or
timbral coloration may need to be minimized. The ADA framework allows
individual metrics to be combined and weighted to reflect the qualities that
make them desired or undesired, providing an objective and trackable
quantification of the system under test.
In a representative experiment we conducted, three binaural virtualizer
systems were compared, labeled Systems A, B and C. System C contained an
adjustable parameter designed to influence perceptual externalization from the
listener at multiple channel positions tested: centre, right, and left rear
surround. These System C tunings, named \"near\", \"mid\" and \"far\",
resulted in mean externalization distances that increased monotonically with
the desired effect of parameter adjustment (Figure 14). An associated decrease
in spectral naturalness was observed that inversely correlated with
externalization. This was unsurprising, since externalization cues can impart
coloration that can affect timbre and listeners\' judgment of naturalness of
sound. Taken together, these two points of evaluation (externalization and
spectral naturalness) provided information useful for balancing algorithm
performance between desired perceptual externalization and undesired
coloration and loss of spectral naturalness.
{width="3.9340277777777777in" height="5.194444444444445in"}
Figure 14: Spectral naturalness ratings and externalization for multiple
binaural virtualizer systems evaluated at centre, left surround and right
virtual 5.1 speaker locations. System C, tuned for externalization parameters
of \"near\", \"mid\" and \"far\" distance, exhibited monotonic increase in
perceived externalization, while showing an inverse correlation for reported
spectral naturalness ratings.
##### 4.2.5.5.2 Trajectory Analysis
In addition to the characterization of single or multiple stationary sound
sources, the ADA assessment framework can be applied to critical evaluation of
system performance for dynamic aspects of sound localization such as
trajectories or control over variable dispersion or diffuseness. Figure 15
illustrates representative user responses to multiple systems for binaural
virtualization of mono sound signals panned in a semicircle behind the
listener\'s head. For each of these responses, start and end point, and
centres of mass for the shapes defined by the curve boundaries, can be
established. In the examples shown, different systems can be evaluated by the
order and extent to which they externalize behind the listener. Assessment
also reveals the occurrence of front-back reversals as in the case of listener
three where panning was perceptually localized in front of the listener for
all virtualizers except for System E, represented by the purple trace, which
strongly tracked the intended trajectory route with good accuracy and
externalization.
{width="3.9291666666666667in" height="4.15in"}
Figure 15: Trajectory analysis using the ADA framework. Responses for three
listeners to a mono source panned along a semicircle behind the head using
multiple binaural virtualizer systems.
#### 4.2.5.6 References
[1] Daniel P Darcy, Kent B Terry, Grant A Davidson, Richard Graff, Alex
Brandmeyer, and Poppy A C Crum: \"Methodologies for High-Dimensional Objective
Assessment of Spatial Audio Quality\"; 140^th^ AES convention; 2016; Paris,
France.
## 4.3 Quality attributes of relevance for 3GPP immersive audio systems
Based on the investigations in clause 4.2, the following attributes were
identified as relevant to 3GPP Immersive audio systems: _Basic Audio Quality,
Timbre, Spatial_ and _Artifacts_. Additional attributes, or a different set of
attributes may be considered in future activities.
# 5 Considerations on Test Methodologies for Immersive Audio Systems
## 5.1 Input provided in S4-180464 - On the validity of the CIBR baseline
testing for VR Stream
### 5.1.1 Summary
The Source conducted two binaural renderer tests using a Comparison Category
Rating (CCR) based testing methodology. The test results suggest that
listeners can reliably discriminate between different rendering solutions
using the CCR testing paradigm. The Source also observed that the Common
Informative Binaural Renderer (CIBR), as proposed in [2], provides a
reasonable quality comparison point for characterizing a proposed VRStream
Audio Profile Reference Renderer.
### 5.1.2 Introduction
Document [1], containing agreed submission information for VRStream Audio
Profiles, specifies that \"all VRStream Audio Profile Proponents shall conduct
a _Reference Binaural Renderer Quality Characterization Test (_ Test 3)\".
Because there is no \"true\" reference for certain binauralized immersive
audio formats, the Source suggested using a _Rendering Comparison_ test based
on CCR, following the proposal in [1a].
The following concerns have been expressed with _Test 3_ in general:
\- No experience with such test
\- Unsure if a reference is needed
\- Unsure if overall preference is a reasonable or if there are more
dimensions necessary.
And the following concern was expressed about the proposed CIBR in particular:
\- The conversion of the virtual loudspeaker feeds into the Ambisonics domain
would materially degrade objects and channel-based signals and not be a
relevant baseline comparison point for the _Reference Renderer._
To investigate the significance of these concerns the source conducted two
binaural renderer tests using a CCR based testing methodology.
### 5.1.3 Comparison Category Rating Tests for Reference Renderer
#### 5.1.3.1 Test design
##### 5.1.3.1.1 Test Material
Since the concerns expressed have been particular to the impact of the
proposed CIBR for channel and object-based test content, the Source selected
eight critical test items with immersive channel-based content (22.2 and
7.1+4). Two of these critical test items also included audio objects. All
eight test items were binauralized in three different conditions: (1)
Condition REF, (2) Condition CIBR and (3) Condition N1.
**1) Condition REF** : A direct binauralization (convolution) of the 22.2 and
7.1+4 loudspeaker feeds with the associated HRTFs corresponding to the
loudspeaker azimuth and elevation. This condition could be considered as the
_Reference Renderer_ per definition in [2].
**2) Condition CIBR** : All 22.2 and 7.1.4 loudspeaker feeds (and objects
where present) are treated as objects and panned according to 16 virtual
loudspeaker positions (Table 3). These 16 positions are also specified as the
sampling points for the Equivalent Spatial Domain for 3^rd^ order Ambisonics
in [3]. These 16 virtual loudspeaker feeds are then binauralized with the CIBR
as proposed in [4].
Table 3: Virtual speaker positions for the CIBR condition
+--------------------+-----------+-----------+ | Virtual Speaker ID | Azimuth | Elevation | | | | | | | (degrees) | (degrees) | +--------------------+-----------+-----------+ | 1 | 0 | 90 | +--------------------+-----------+-----------+ | 2 | 0 | 41 | +--------------------+-----------+-----------+ | 3 | 64 | -26 | +--------------------+-----------+-----------+ | 4 | -14 | -59 | +--------------------+-----------+-----------+ | 5 | 66 | 28 | +--------------------+-----------+-----------+ | 6 | 117 | -10 | +--------------------+-----------+-----------+ | 7 | -79 | -26 | +--------------------+-----------+-----------+ | 8 | 16 | -10 | +--------------------+-----------+-----------+ | 9 | -126 | 0 | +--------------------+-----------+-----------+ | 10 | 132 | 37 | +--------------------+-----------+-----------+ | 11 | -161 | 37 | +--------------------+-----------+-----------+ | 12 | 173 | -11 | +--------------------+-----------+-----------+ | 13 | 123 | -62 | +--------------------+-----------+-----------+ | 14 | -36 | 0 | +--------------------+-----------+-----------+ | 15 | -81 | 35 | +--------------------+-----------+-----------+ | 16 | -141 | -51 | +--------------------+-----------+-----------+
**1) Condition N1** : All 22.2 and 7.1.4 loudspeaker feeds (and objects where
present) are treated as objects and panned according to 4 virtual loudspeaker
positions (Table 4). These 4 positions are also specified as the sampling
points for the Equivalent Spatial Domain for 1^st^ order Ambisonics in [3].
These 4 virtual loudspeaker positions are then converted to 1^st^ order
Ambisonics and binauralized with the 1^st^ Order Ambisonics HRTFs.
Table 4: Virtual speaker positions for condition N1
* * *
Virtual Speaker ID Azimuth Elevation 1 0 90 2 0 -19 3 120 -19 4 -120 -19
* * *
##### 5.1.3.1.2 HRTF Selection
Different HRTFs can have a material impact to externalization, coloration and
localization of virtual sound sources. To eliminate the influence of different
HRTFs from the rendering comparison, the HRTFs used in all three conditions
are based on the same set of measured manikin transfer functions. In Condition
REF, each loudspeaker feed is directly convolved with the associated HRTFs
corresponding to the loudspeaker azimuth and elevation. For Condition CIBR and
Condition N1, the binauralization is carried out in the spherical harmonics
domain.
##### 5.1.3.1.3 Listeners
10 listeners participated in two tests that were designed using the CCR
paradigm. All 10 listeners had previous experience in audio development and
subjective assessment, although this was not a requirement for participation
in the test. A larger listening panel is of course required for reduced
confidence intervals and should be considered when documenting _Test 3_.
##### 5.1.3.1.4 Test Description, Interface and Randomization
Test 1 is a CCR of Condition REF against Condition N1. Test 2 is a CCR of
Condition REF against Condition CIBR. In each trial, subjects listened over
headphones to the two stimuli labeled as A and B. Each stimulus had a duration
of about 11 to 16 seconds and a sampling rate of 48 kHz. Listeners were asked
to rate their preference on a 7-point scale as can be seen in the GUI below
(Figure 16) from the STEP software (https://www.audioresearchlabs.com/step/).
{width="2.83125in" height="3.6215277777777777in"}
Figure 16: GUI of CCR Listening Test
The order of trials and the order of the conditions in each trial was
randomized, resulting in eight double-blind AB test conditions per CCR test.
All listeners listened to all eight test items. The average testing time per
listener was 10min.
#### 5.1.3.2 Results
The average scores across test items are depicted in Figure 17 (Test 1) and
Figure 18 (Test 2).
In Test 1, the overall score across all test items shows that Condition REF is
statistically significantly preferred over Condition N1. Further, for half of
the test items the REF condition is statistically significantly preferred over
Condition N1.
These results suggest that the N1 renderer may not be an adequate candidate
for a common informative binaural renderer because this renderer significantly
degrades the binaural signal for the channels and object based test materials
chosen.
{width="5.519444444444445in" height="3.066666666666667in"}
Figure 17: CCR Test 1 for a Reference Renderer (REF) vs. a 1^st^ order
Ambisonics Renderer (N1)
In Test 2 (Figure 18) the overall score across all test items indicate that
there is no statistically significant difference in the preference between
Condition REF and Condition CIBR. This suggest that the CIBR provides a
quality that is similarly preferred to the REF condition.
Somehow surprising, the listeners statistically significantly preferred the
proposed CIBR for item _Sig5_. A possible explanation is that _Sig5_ contains
objects and rendering of the object elements to the 16 virtual loudspeakers
grid (Table 3) of Condition CIBR is preferable to the more limited 11
loudspeaker configuration (7.1+4) of Condition REF. In contrast, item _Sig1_
tends to be preferred in Condition REF. _Sig1_ is channel-based music content
that would not necessarily benefit from the larger loudspeaker feed count
provided by the CIBR.
{width="5.519444444444445in" height="3.066666666666667in"}
Figure 18: CCR Test 2 for a Reference Renderer (REF) vs. the proposed CIBR
Figure 19 plots the difference scores of the listeners between the two CCR
tests (all listeners participated in both tests). Six of the eight test items
show a statistically significant preference of the CIBR condition over the N1
condition. For the test item _Sig6_ neither condition is preferred. Overall, a
statistically significantly higher preference for Condition CIBR over
Condition N1 can be inferred. This outcome is reassuring because the CIBR was
considered to provide a higher spatial accuracy than the N1 condition.
{width="5.519444444444445in" height="3.066666666666667in"}
Figure 19: Difference score between the 1^st^ order Ambisonics Renderer (N1)
and the proposed CIBR
#### 5.1.3.3 Conclusion and Proposal
This document provides two sets of test results for the CCR based assessment
of a renderer. The results of both tests in Clause 3.2 suggest that listeners
in a CCR test can discriminate between renderer solutions if critical test
materials are used.
The results in Test 2 indicated that renderer preference can be item-
dependent. These dependencies may be better understood by also assessing the
underlying quality dimensions as already proposed in [1].
Based on these test results it is clear that the proposed CIBR renderer **is
capable of** providing a reasonable quality comparison point for
characterizing a Proposed VRStream Audio Profile _Reference Renderer_. Thus,
the Source restates its proposal to adopt the CIBR described in [4] for the
purposes of characterization _Test 3_ of [1]**. The Source further proposes
that the CCR test methodology is adopted for _Test 3._ The Source further
proposes to discuss the potential use of Condition N1 as a low or mid quality
anchor condition for the test.**
**These results and conclusions are for one set of HRTFs and might differ if
individualized HRTFs or other HRTF sets are used.**
#### 5.1.3.4 References
[1] S4‑180318 - _Proposal for Submission Information for VRStream audio_.
Qualcomm Incorporated, Fraunhofer IIS, ORANGE.
[1a] S4-180125, Auditory Assessment of Audio Systems, Source: HEAD Acoustics.
[2] S4-AHVIC119 - _On Definitions for VRStream Audio_. Qualcomm Incorporated.
[3] 3GPP TS 26.260 v0.0.4 - _Objective test methodologies for the evaluation
of immersive audio systems._
[4] S4-180140 - Proposal for a Common Informative Reference Renderer, Qualcomm
Incorporated.
## 5.2 Input provided in S4-180520 -- On the impact of individualized HRTF and
HpTF
### 5.2.1 Summary
The present contribution presents results of a study designed to assess the
impact of Head Related Transfer Functions (HRTF) and Headphone Transfer
Functions (HpTF) to the perceived localization, externalization and coloration
of a sound source. The results indicate, as expected, that the use of
individualized HRTFs in combination with individualized HpTFs offers a
statistically significant improvement in all these aspects. The test
methodology can be used to evaluate the suitability of a HRTF/HpTF set for a
given individual.
### 5.2.2 Introduction
The LiQuImAS work item description [1], lists among its objectives: _\"Develop
new subjective and objective test methodologies (if applicable) considering:
[. . .] (c) Possible impacts of dynamic rendering, headphone equalization and
use of Head Related Transfer Functions (HRTF) to the perceived audio quality /
psychophysical response, either in isolation or in combination with other
processing blocks of the immersive audio system.\"_ To assess the impact of
headphone equalization and use of individualized HRTFs, the source designed a
multidimensional Degradation Category Rating (DCR) listening test and
conducted a preliminary experiment.
### 5.2.3 Test Methodology
#### 5.2.3.1 Goal of the Experiment
The goal of the experiment is to detect to what extent a given set of HRTFs
and an HpTF impact the perceived externalization, coloration and localization
of discrete sound sources.
#### 5.2.3.2 Listening Experiment Paradigm
The listening experiment paradigm is a Degradation Category Rating. The
listeners task is to assess the perceived quality of a binauralized
presentation over headphones in comparison to a direct loudspeaker
presentation. The listeners assess three quality attributes that are
simultaneously presented on a Graphical User Interface (GUI): Externalization,
Localization and Coloration. These three attributes were chosen from a range
of attributes used in previous experiments, see generally [2].
#### 5.2.3.3 Test Environment
The test environment consisted of a critical listening room, equipped with a
28.2 loudspeaker layout configuration according to [3] and reproduced in Table
5 for convenience. The loudspeakers were Genelec 8250A with individual
equalization and level calibration.
Table 5: 28.2 Speaker layout positions
|  |  |  |  |  | Output Formats | Input Formats |  |  |  |  |  |   
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
No. | LS  
Label | Az ° | Az.  
Tol. ° | El. ° | El. Tol. ° | O-5.1 | O-8.1 | O-10.1 | O-22.21 | I-8.1 | I-9.1 | I-11.1 | I-22.2  
1 | M+000 | 0 | ±2 | 0 | ±2 | X |  | X | X |  | X | X | X  
2 | M+030 | 30 | ±2 | 0 | ±2 | X | X | X | X | X | X | X | X  
3 | M-030 | -30 | ±2 | 0 | ±2 | X | X | X | X | X | X | X | X  
4 | M+060 | 60 | ±2 | 0 | ±2 |  |  |  | X |  |  |  | X  
5 | M-060 | -60 | ±2 | 0 | ±2 |  |  |  | X |  |  |  | X  
6 | M+090 | 90 | ±5 | 0 | ±2 |  |  |  | X |  |  |  | X  
7 | M-090 | -90 | ±5 | 0 | ±2 |  |  |  | X |  |  |  | X  
8 | M+110 | 110 | ±5 | 0 | ±2 | X | X | X |  | X | X | X |   
9 | M-110 | -110 | ±5 | 0 | ±2 | X | X | X |  | X | X | X |   
10 | M+135 | 135 | ±5 | 0 | ±2 |  |  |  | X |  |  |  | X  
11 | M-135 | -135 | ±5 | 0 | ±2 |  |  |  | X |  |  |  | X  
12 | M+180 | 180 | ±5 | 0 | ±2 |  |  |  | X |  |  |  | X  
13 | U+000 | 0 | ±2 | 35 | ±10 |  | X |  | X |  |  | X | X  
14 | U+045 | 45 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
15 | U-045 | -45 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
16 | U+030 | 30 | ±5 | 35 | ±10 |  | X | X |  | X | X | X |   
17 | U-030 | -30 | ±5 | 35 | ±10 |  | X | X |  | X | X | X |   
18 | U+090 | 90 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
19 | U-090 | -90 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
20 | U+110 | 110 | ±5 | 35 | ±10 |  |  | X |  | X | X | X |   
21 | U-110 | -110 | ±5 | 35 | ±10 |  |  | X |  | X | X | X |   
22 | U+135 | 135 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
23 | U-135 | -135 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
24 | U+180 | 180 | ±5 | 35 | ±10 |  |  |  | X |  |  |  | X  
25 | T+000 | 0 | ±2 | 90 | ±10 |  |  | X | X |  |  | X | X  
26 | L+000 | 0 | ±2 | -15 | +5-25 |  | X |  | X |  |  |  | X  
27 | L+045 | 45 | ±5 | -15 | +5-25 |  |  |  | X |  |  |  | X  
28 | L-045 | -45 | ±5 | -15 | +5-25 |  |  |  | X |  |  |  | X  
29 | LFE1 |  45 | ±15 | -15 | ±15 | X | X | X | X | X | X | X | X  
30 | LFE2 | -45 | ±15 | -15 | ±15 |  |  |  | X |  |  |  | X  
* * *
  1. 
#### 5.2.3.4 HpIR Measurement & Compensation Filter Generation
At the beginning of the session, all participants had their individualized
Headphone Impulse Responses (HpIR) measured. The headphones used for this test
were Sennheiser HD800 headphones. The headphone equalization by placing a
DPA4060 microphone in a custom-made 3D print holder. This custom-made
apparatus provides repeatability for the headphone impulse response
measurements and is depicted in Figure 20:
{width="1.5097222222222222in"
height="2.4930555555555554in"}{width="1.8756944444444446in"
height="2.4930555555555554in"}
Figure 20: Custom-made DPA4060 holder for BRIR and HpIR measurements
For the individualized HpIR measurements, participants were instructed to
place the Custom-made DPA4060 holder as shown in Figure 20, with the wire
wrapping around the ear. This setup corresponds to a blocked-ear canal
measurement procedure.
The HpIRs were then measured as the mean of 5 repeats per subject, by playing
one log sine sweep per ear per repeat. Between each of the five repeats, the
participants removed and replaced the headphones. The resulting HpIRs were
truncated and windowed to 2048 samples. The measurement repeatability is
illustrated for one representative subject in Figure 21.
{width="6.696527777777778in" height="3.71875in"}
Figure 21: Example of measured Headphone Transfer Function measured for one
subject (5 repeats)
After measurement of the five HpIR repeats, the test administrator generated
an individualized headphone compensation filter for each subject. The
compensation filters were generated using the AKTools
AKregulatedinversionDemo.m from TU Berlin script with the following
configuration parameters:
Dynamic range: 60dB
\- Target function: 2nd order 100 Hz lowpass cascaded with 1st order 20kHz
highpass
\- Inversion method: 4
\- regularized LMS inversion
\- Fine octave smoothing: 1/6th octave
\- Course octave smoothing: 1 octave
\- Regularization beta: 0.2
\- Regularization response: between 1kHz and 20kHz
The entire HpIR Measurement & Compensation Filter Generation took less than 5
min per subject.
#### 5.2.3.5 BRIR Measurement
Following the measurement of the Headphone Impulse Response and immediately
prior to the administration of the listening test, all participants had their
Binaural Room Impulse Responses measured. To limit test time, the test
administrator measured 6 of the 28 speaker locations, indicated in Table 6.
Table 6: Loudspeaker directions used for this experiment
* * *
**Code** **Speaker Number** **Azimuth** **Elevation** F (front) 1 +000 +000 FL
(front-left) 2 +030 +000 L (left) 6 +090 +000 RR (rear-right) 11 -135 +000 FLU
(front-left-up) 14 +045 +035 FRD (front-right-down) 28 -045 -015
* * *
The BRIRs were measured from 6 loudspeaker locations without removing the
microphones positioned for the last HpIR measurement. BRIR from each
loudspeaker location were measured as the average of 4 logarithmic sine sweep
repeats. BRIRs were then truncated and windowed to be 8192 samples. This
length was chosen as sufficient to contain all the audible reverb in the test
environment.
In addition to the individualized BRIRs, a set of BRIRs was obtained for the
GRAS Head and Torso Simulator in the same environment.
The entire BRIR Measurement took approximately 5 min per subject.
#### 5.2.3.6 Test and Reference Conditions generation
##### 5.2.3.6.1 Source Material
Only one source material was used for this test. The source material is a
Maximum Length Sequence (MLS) burst of order 15 at a 48kHz sample rate
(approximately 0.7s). This test material was chosen based on the experience in
previous Binaural Synthesis experiments where the experimenters observed that
the audio quality discrimination with the broadband bursts is superior to
other sounds (e.g. drum snare, speech, etc.).
##### 5.2.3.6.2 Test Conditions
For each subject, the source material was convolved in the time domain with 6
BRIR sets (corresponding to different loudspeaker locations), generating 6
sets of binauralized signals. Each of these 6 binauralized signals were then
convolved with the Individualized Headphone Equalization filters and a
Headphone Equalization Filter obtained using the GRAS head and torso
simulator. The Headphone Equalization Filter obtained using the GRAS head and
torso simulator followed the same equalization process, including using the
DPA4060 custom-mount microphones. In addition, for testing, the non-headphone
equalized sets of binauralized signals were also used.
All test sources were normalized to be full-scale MLS signals (i.e. they were
divided by the maximum of the absolute value of the two channels before
writing to 16 bit wav).
This process resulted in the following test condition matrix, with each matrix
cell having 6 different loudspeaker locations for evaluation:
Table 7: Test Condition Matrix
* * *
**BRIR/HpIR** **none** **individualized** **gras** **individualized** C01 C02
C03 **gras** C04 C05 C06
* * *
**\- C01:** An individualized BRIR with no Headphone Equalization
**\- C02:** An individualized BRIR with individualized Headphone Equalization
**\- C03:** An individualized BRIR with GRAS Headphone Equalization
**\- C04:** A GRAS BRIR with no Headphone Equalization
**\- C05:** A GRAS BRIR with individualized Headphone Equalization
**\- C06:** A GRAS BRIR with GRAS Headphone Equalization
##### 5.2.3.6.3 Reference Conditions
Six reference conditions were also generated for the test as shown:
Table 8: Matrix of Anchor Conditions
* * *
**loc1** **loc3** **ext1** **ext3** **col1** **col3** R01 R02 R03 R04 R05 R06
* * *
**\- R01** : A \"bad\" localization anchor; 90 degree clockwise rotation of
BRIR (A+090 becomes A+000)
**\- R02** : A \"fair\" localization anchor; 30 degree clockwise rotation of
BRIR (A+030 becomes A+000)
**\- R03** : A \"bad\" externalization anchor; BRIR truncated and windowed to
256 samples
**\- R04** : A \"fair\" externalization anchor; BRIR truncated and windowed to
2048 samples
**\- R05** : A \"bad\" coloration anchor; lowpass filtered by 2^nd^ order
Butterworth with 3.5kHz cutoff
**\- R06** : A \"fair\" coloration anchor; lowpass filtered by 2^nd^ order
Butterworth with 7kHz cutoff
##### 5.2.3.6.4 Test and Reference Conditions Naming Convention
The file naming test convention adopted was as follows:
**\
_\_\_\_A\_E\.wav**
**\ ** name of subject
**\ ** [ext1, ext2, loc1, loc3, col1, col3] or empty (only present
for reference condition files)
**\- ext1** : bad externalization anchor; BRIR truncated and windowed to 256
samples
**\- ext3** : fair externalization anchor; BRIR truncated and windowed to 2048
samples
**\- loc1** : bad localization anchor; 90 degree clockwise rotation of BRIR
(A+090 becomes A+000)
**\- loc3** : fair localization anchor; 30 degree clockwise rotation of BRIR
(A+030 becomes A+000)
**\- col1** : bad coloration anchor; lowpass filtered by 2^nd^ order
Butterworth with 3.5kHz cutoff
**\- col3** : fair coloration anchor; lowpass filtered by 2^nd^ order
Butterworth with 7kHz cutoff
**\ ** [ind, gras]
**\- ind** : individually recorded BRIR used for compensation
**\- gras** : GRAS KEMAR recorded BRIR used for compensation
**\ ** [none, ind, gras]
**\- none** : no headphone compensation
**\- ind** : individually calculated headphone compensation filter used
**\- gras** : GRAS KEMAR recorded headphone compensation filter used
**\ ** [+,-]ddd
\- e.g. +000, -135, +090, etc...
#### 5.2.3.7 Training Session
Following conclusion of the equalization and sample generation process, a
short training session was administered to the listeners, to familiarize the
listeners with the task. The listeners were instructed to listen to the
reference sample over loudspeakers without wearing the headphones and then
wear the headphones and listen to the binauralized presentation. The listeners
were further instructed to always look at the centre of the screen (positioned
in front of the listeners) when listening to the samples and making the
assessments.
The short training session consisted of 7 randomized trials, with the same
reference conditions for each subject (true individual and 6 anchor
conditions):
\- \_ind_ind_A+000_E+000.wav (individual 8192 BRIR, individual HpIR)
\- \_ext1_ind_ind_A+000_+E000.wav (individual 256 BRIR, individual
HpIR)
\- \_ext3_ind_ind_A+000_+E000.wav (individual 2048 BRIR, individual
HpIR)
\- \_loc1_ind_ind_A+000_+E000.wav (\_ind_ind_A+090_+E000)
\- \_loc3_ind_ind_A+000_+E000.wav (\_ind_ind_A+030_+E000)
\- \_col1_ind_ind_A+000_+E000.wav (true individual with 2^nd^ order
butterworth lp 7khz)
\- \_col3_ind_ind_A+000_+E000.wav (true individual with 2^nd^ order
butterworth lp 3.5khz)
The training session took \~5-10 minutes and the test administrator was
present in the room during the training session to answer the subject\'s
questions.
#### 5.2.3.8 Listening Session
All listeners assessed 42 randomized trials with the 6 reference conditions
(as described in _Test File Naming Convention_) and 36 test materials (6
conditions with 6 speaker locations each).
The listeners had access to a printed instruction sheet containing the
information in Table 9:
Table 9: Instructions for listeners
* * *
**Comparing the headphone presentation with the loudspeaker presentation, how
do you judge the headphone presentation with regards to the following
qualitative attributes?** Coloration: 5 Excellent (source coloration matches
reference) 4 Good (source coloration nearly matches reference) 3 Fair (source
coloration differs somewhat from reference) 2 Poor (source coloration differs
substantially from reference) 1 Bad (source coloration differs extremely from
reference) Localization: 5 Excellent (source azimuth and elevation match
reference) 4 Good (source azimuth and elevation nearly match reference) 3 Fair
(source azimuth and elevation differ somewhat from reference) 2 Poor (source
azimuth and elevation differ substantially from reference) 1 Bad (source
azimuth and elevation differ extremely from reference) Externalization: 5
Excellent (source externalization and distance matches reference) 4 Good
(source is externalized and distance nearly matches reference) 3 Fair (source
is externalized, but distance differs somewhat from reference) 2 Poor (source
is externalized, but distance differs substantially from reference) 1 Bad
(source has no externalization and/or distance differs extremely from
reference)
* * *
A cross-platform listening test GUI was developed using wxPython and used to
administer the test. Only integer based voting was allowed through the GUI.
{width="3.0284722222222222in" height="3.4034722222222222in"}
Figure 22: GUI used for the listening tests
Each trial took an average of \~45 seconds, for a total of \~35 minutes.
Subjects were given a 5minute break after 21 trials (half-way through the
test)
### 5.2.4 Participants
Twelve participants, including male and female participants with ages ranging
between 25 to 60yrs old participated in this experiment.
### 5.2.5 Results
Results are summarized in Table 10.
Table 10: Summary of results for experiment on assessment of BRIR and HpTF to
binaural perception
{width="6.694444444444445in" height="2.19375in"}
### 5.2.6 Analysis
As expected, C02, the condition of individualized BRIR with individualized
headphone equalization outperformed all other test conditions on the average
of the three ratings. C02 was also the only condition to produce a mean equal
or higher than 4.0 for all three attributes. Compared to the more commonly
used condition in listening tests, C04, i.e. the BRIRs of a HATS with no
headphone equalization, there are statistically significant differences in
favor of C02 for all three attributes, with localization a full 1.0 MOS apart.
However, C06, using the GRAS BRIRs along with the GRAS HpTF compensation, also
has statistically significant advantages over C04 and is a practical
alternative to reduce localization and coloration errors in LiQuImAS
experiments.
Finally, consistent with the observations from experiments in [2], \"mixing\"
HpTFs with BRIRs from different sources leads to some problems. For example,
when using a HATS BRIR is preferable to use the HATS HpTF compensation instead
of an individualized HpTF compensation. This can be observed by comparing the
coloration results of C05 and C06.
### 5.2.7 Conclusion
A method to derive Mean Opinion Scores for externalization, coloration and
localization when using different headphone equalization and different head
related transfer functions is proposed. The methodology addresses the aspects
of headphone equalization and individualized BRIRs and demonstrates the impact
of these components when attempting to accurately binauralize a signal. The
methodology can be performed in less than one hour, including customized
filter derivation procedure and assessment. The Source proposes to adopt this
methodology for the evaluation of BRIRs and Headphone Equalization in case
individualization is desired. The Source further proposes to perform Headphone
Equalization corresponding to the set of BRIRs used for binauralization when
performing listening tests over headphones.
## 5.3 Input provided in S4-180472 -- On ITU-R BS.1534 (MUSHRA)
### 5.3.1 Introduction
The ITU-R BS.1534 (MUSHRA) methodology [1] is a commonly used test methodology
in the field of audio codec evaluation. It is intended for testing, evaluation
and reporting procedures for the subjective assessment of intermediate audio
quality. It is well understood by the respective community and listening labs.
It has been used for the 3GPP audio codec standardization effort, resulting in
the specifications for eAAC+ [2] and AMR-WB+ [3], and is also used in many SA4
input contributions. MUSHRA has also been used for spatial audio evaluation,
e.g. MPEG-H 3D Audio [4] has been selected and verified using MUSHRA [5].
### 5.3.2 Description
#### 5.3.2.1 General
ITU-R BS.1534 [1] specifies e.g. attributes, rating scales, test materials,
presentation, and reference conditions.
### 5.3.3 Conclusion
BS.1534 (MUSHRA) is a well-established standard in the field of audio codec
evaluation, including 3D audio coding, and should be a recommended test
methodology used for the characterization of VRStream Audio profiles.
### 5.3.4 References
[1] Recommendation ITU-R BS.1534-3 (10/2015): \"Method for the subjective
assessment of intermediate quality level of audio systems\".
[2] 3GPP TS 26.401 \"General audio codec audio processing functions; Enhanced
aacPlus general audio codec;\ General description\"
[3] 3GPP TS 26.290 \"Audio codec processing functions; Extended Adaptive
Multi-Rate - Wideband (AMR-WB+) codec;Transcoding functions\"
[4] Annex 3 of \"MPEG-H Call for Proposals for 3D Audio\" -- Available at:
https://mpeg.chiariglione.org/standards/mpeg-h/3d-audio/call-
proposals-3d-audio
[5] MPEG-H 3D Audio Verification Test Report, Geneva, 2017.
http://mpeg.chiariglione.org/standards/mpeg-h/3d-audio/mpeg-h-3d-audio-
verification-test-report.
## 5.4 Input provided in S4-180805 -- Verification of CIBR configuration for
FOA
### 5.4.1 Summary
This contribution presents the verification of the CIBR configuration for FOA
conversion. In particular, we compare the performance of the conversion to FOA
with ESD4 as specified by 3gpp (one virtual speaker in the front and two in
the back), ESD4-F (two virtual speakers in the front and one in the back) and
ESD16. The test results suggest that rendering objects to ESD16, converting to
HOA and then truncating to FOA could lead to a higher quality rendering
process in the CIBR configuration.
### 5.4.2 FOA rendering in CIBR
#### 5.4.2.1 Test Design
The rendering to the ESD domain for the CIBR configuration [1] has been
specified as
{width="3.4583333333333335in" height="2.43125in"}
Figure 23: Rendering to the ESD domain for the CIBR configuration
with rendering to ESD16 for HOA and rendering to ESD4 for FOA. The locations
of the virtual speakers for ESD16 and ESD4 have been provided in [3, Clause
5.1], and the conversion matrices specified in [4].
When examining this configuration, it seemed unclear to us why we would render
objects to ESD4, as opposed to always render to ESD16 and then truncate to FOA
in the ambisonics domain. We decided to verify our belief that for objects,
rendering to ESD16 and then converting to FOA in the HOA domain would result
in better performance. The reference for such \"better performance\" is,
naturally, playback of objects rendered directly to the playout system: 7.4.1
loudspeakers in this test.
In the performed test, we have also added an updated ESD4 configuration,
ESD4-F, which rotates the original ESD4 virtual speaker locations (from [3])
by 180 degrees: bottom speakers updated to +/-60 and 180 degrees (instead of 0
and +/- 120 degrees). This rotation results in two virtual speakers in the
front and one in the back (instead of one in the front and two in the back).
This rotated configuration, ESD4-F, should preserve the spatial left-right
frontal image, unlike the original ESD4 setup which collapses it.
There is no audio compression in this test; the test only assesses the
renderer processing configuration.
#### 5.4.2.2 Test Material
Test materials included in the test were all the objects-based content from
the 3GPP VRStream characterization tests.
Table 11: Test Items
* * *
**Track** **Content Type** 8Obj_Music+Bird Object-based (8 Objects) Fork
Object-based (10 objects) 8Obj_reservoir Object-based (8 Objects)
CICP19_Festival Mixed (Channels-7.1.4 + 2 Objects) JammJam Object-based (12
Objects) Spoon Object-based (12 Objects)
* * *
#### 5.4.2.3 Listening Environment
The tests were performed in a BS.1116-3 compliant listening lab in XPERI\'s
Calabasas office over a 7.1.4 speaker layout.
#### 5.4.2.4 Listening Panel
The listening panel for this test consisted of seven XPERI/DTS employees (2
Female, 5 Male), all experienced in critical listening tests. All listeners
passed post screening as specified in ITU-R BS.1534-3 section 4.1.2.
#### 5.4.2.5 Results
All presented results include average scores and 95% confidence intervals
(t-distribution).
{width="6.222222222222222in" height="3.529166666666667in"}
Figure 24: Results per System
{width="6.229861111111111in" height="3.5388888888888888in"}
Figure 25: Results per System and Test Item
### 5.4.3 Conclusions
The performed tests indicate the following:
\- Rendering objects to ESD16 and then truncating to FOA in the ambisonics
domain performs better than rendering to ESD4; the former performed about 10
MUSHRA points higher and was statistically better than the latter
\- There is not as much difference between ESD4 and ESD4-F as we thought there
might be (at least for this test material)
The presented results suggest that rendering to ESD16, converting to HOA, and
then truncating to FOA could lead to a higher quality rendering process in the
CIBR configuration.
### 5.4.4 References
[1] 3GPP TS 26.259 \"Subjective test methodologies for the evaluation of
immersive audio systems\"
[2] ITU-R BS.1534-3 \"**Method for the subjective assessment of intermediate
quality levels of coding systems\"**
[3] TR 26.861 \"On the validity of CIBR baseline testing for VR Stream\"
[4] AHVIC-157 \"ESD Conversion Matrices for VRStream Audio Profile
Characterization\"
#