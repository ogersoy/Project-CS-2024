# Foreword
This Technical Report has been produced by the 3^rd^ Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
Virtual Reality is the ability to be virtually present in a space created by
the rendering of natural and/or synthetic image and sound correlated by the
movements of the immersed user allowing interacting with that world.
The immersive multimedia experience has been an exploration topic for several
years. With the recent progress made in rendering devices, such as Head
mounted displays (HMD), a significant quality of experience can be offered.
Before any possible standardization, it is necessary to study the field
  * to understand how the equipment used for creating such an immersive experience works, e.g. by collecting information on the optical systems and audio rendering processes;
  * to evaluate the relevance to Virtual Reality for 3GPP;
  * to identify the possible points of interoperability, and hence potential standardization.
Use cases for Virtual Reality need to be listed and mapped to the already
existing 3GPP services if applicable.
Media formats required for providing the immersive experience need to be
identified and potentially evaluated subjectively so as to extract
requirements on minimum device and network capabilities.
# 1 Scope
The present document ...
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] _Savino, Peter J.; Danesh-Meyer, Helen V. (1 May 2012)._ Color Atlas and
Synopsis of Clinical Ophthalmology -- Wills Eye Institute -- Neuro-
Ophthalmology. _Lippincott Williams & Wilkins. p. 12._
_[3] Dagnelie, Gislin (21 February 2011)._ Visual Prosthetics: Physiology,
Bioengineering, Rehabilitation. _Springer Science & Business Media. p. 398._
_[4]_ T.E. Boult and G. Wolberg, \"Correcting chromatic aberrations using
image warping\", _Computer Vision and Pattern Recognition 1992. Proceedings
CVPR \'92. 1992 IEEE Computer Society Conference on_ , pp. 684-687, 1992, ISSN
1063-6919.
[R0] Levoy, Marc, and Pat Hanrahan. \"Light field rendering.\"  _Proceedings
of the 23rd annual conference on Computer graphics and interactive
techniques_. ACM, 1996.
[R1] J. P. Snyder, "Flattening the Earth: Two Thousand Years of Map
Projections," University of Chicago Press, 1993.
[R2] http://wiki.panotools.org/Equirectangular_Projection
[R3] http://wiki.panotools.org/Cubic_Projection
[R4] Skupin, R., Sanchez, Y., Hellge, C., & Schierl, T. Tile Based HEVC Video
for Head Mounted Displays.
[R5] http://newatlas.com/best-vr-headsets-comparison-2016/45984/, accessed on
January 9, 2017.
[4bis] Michael A. Gerzon,  _Periphony: With-Height Sound Reproduction_.
Journal of the Audio Engineering Society, 1973, 21(1):2--10.
[5] C. Schissler, A. Nicholls & R. Mehra, \"Efficient HRTF-based Spatial Audio
for Area and Volumetric Sources\", IEEE Trans on Visualization and Computer
Graphics, 2016, Vol 22, No 4, pp 1356 -- 1366.
[6] Durlach N.I, Mavor A.S, "Virtual reality Scientific and Technological
Challenges" National Academy Press, 1995.
[7] Draper M.H, "The adaptive effects of virtual interfaces: Vestibulo-ocular
reflex and simulator sickness", PhD Thesis, University Washington, Sponsored
by US Airforce Department, 1995.
[8] Di Girolamo S, Picciotti P, Sergi B, Di Nardo W, Paludetti G, Ottaviani F
"Vestibulo-ocular reflex modification after virtual environment exposure.",
Acta Oto-Laryngologica. 2001 Jan; Vol 121 Issue 2 pp 211-215.
[9] Jombı´k P, Bahy´l V, "Short latency disconjugate vestibulo-ocular
responses to transient stimuli in the audio frequency range." Journal of
Neurology, Neurosurgery, and Psychiatry; Vol 76 No 10, Oct 2005 pp 1398-1402.
[10] Amin M.S, "Vestibuloocular Reflex Testing" Medscape Article Number
1836134, Feb 10 2016.
[11] Jerald J, "Scene-Motion- and Latency-Perception Thresholds for Head-
Mounted Displays" University of North Carolina PhD Dissertation, 2010.
[11a] Brungart et al., "Effects of Headtracker Latency in Virtual Audio
Displays", Proceedings of International Conference on Auditory Display;
ICAD-05, July 2005.
[11b] Lindau, Alexander: "The Perception of System Latency in Dynamic Binaural
Synthesis, 2009".
[12] Wang D, Brown G.J, " Computational auditory scene analysis: principles,
algorithms and applications". Wiley Interscience. 2006, ISBN 9780471741091.
[13] Recommendation ITU-R BT.1359-1. "Relative Timing of Sound and Vision for
Broadcasting." 1998.
[14] EBU Technical Recommendation R37 "The relative timing of the sound and
vision components of a television signal" 2007.
[15] 3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia telephony;
Media handling and interaction\".
[16] 3GPP TR 26.223: \"Telepresence using the IP Multimedia Subsystem (IMS);
Media handling and interaction\".
[17] Brungart et al., "Effects of Headtracker Latency in Virtual Audio
Displays", J. Audio Eng. Soc., Vol. 54; February 2006
[18] Lindau, Alexander: "The Perception of System Latency in Dynamic Binaural
Synthesis , 2009", https://www2.ak.tu-
berlin.de/\~akgroup/ak_pub/2009/Lindau_2009_The_Perception_of_System_Latency_in_Dynamic_Binaural_Synthesis.pdf#
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**Diegetic:** part of the VR scene and rendered according to HMD head-tracking
information.
**Non-diegetic:** independent of the VR scene and rendered independently of
HMD head-tracking information.
## 3.2 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply.\ An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
AOP Acoustic Overload Point
BRIR Binaural Room Impulse Response
CBA Channel-Based Audio
DAW Digital Audio Workstation
EPG Electronic Program Guide
FOA First Order Ambisonics
FOV Field of view
HMD Head Mounted Display
HOA High Order Ambisonics
OBA Object-Based Audio
SBA Scene-Based Audio
SNR Signal to Noise Ratio
TV TeleVision
VR Virtual Reality
# 4 Introduction to Virtual Reality
## 4.1 Definition
### 4.1.1 Virtual reality
Virtual reality is a rendered version of a delivered visual and audio scene.
The rendering is designed to mimic the visual and audio sensory stimuli of the
real world as naturally as possible to an observer or user as they move within
the limits defined by the application.
Virtual reality usually, but not necessarily, requires a user to wear a head
mounted display, to completely replace the user's field of view with a
simulated visual component, and to wear headphones, to provide the user with
the accompanying audio. Some form of head and motion tracking of the user in
VR is usually also necessary to allow the simulated visual and audio
components to be updated in order to ensure that, from the user's perspective,
items and sound sources remain consistent with the user's movements.
Additional means to interact with the virtual reality simulation may be
provided but is not strictly necessary.
### 4.1.2 Augmented reality
Augmented reality is when a user is provided with additional information or
artificially generated items or content overlaid upon their current
environment. Such additional information or content will usually be visual
and/or audible and their observation of their current environment may be
direct, with no intermediate sensing, processing and rendering, or indirect,
where their perception of their environment is relayed via sensors and may be
enhanced or processed.
Augmented reality and virtual reality are related and may be very similar,
especially if the augmented reality is presented indirectly to the user.
However, Augmented reality is out of scope of this study.
### 4.1.3 Mixed reality
Mixed reality can be considered as an advanced form of augmented reality where
some virtual elements are inserted into the physical scene with the intent to
provide the illusion that these elements are part of the real scene. Mixed
reality is also out of scope of this document.
## 4.2 Video systems
### 4.2.1 Introduction
Virtual Reality has the promise to place users into immersive worlds that
interact with their head movements. At the video level, this is achieved by
providing a video experience that covers as much of the field of view (FOV) as
possible together with the synchronization of the viewing angle of the
rendered video with the head movements. Although many different types of
devices may be able to provide such an experience, head mounted displays (HMD)
are the most popular. They rely either on dedicated screens integrated into
the system and running with external computers (_Tethered_) or on a smartphone
inserted into the HMD (_Untethered_). The first approach has the advantages of
only requiring lightweight screens and benefiting from a high computing
capacity compared to smartphone-based systems, which offer a higher mobility
and are less expensive to produce. In both cases, the video experience is
generated the same way thanks to lenses-based systems as described in the
following clauses as well as some basic principles on the Human Visual System.
### 4.2.2 Field of view
#### 4.2.2.1 Definition
The Human field of view (FOV) is defined as the area of vision at a given
moment (with a fixed head). It is the angle of visible field expressed in
degrees measured from the focal point. The monocular FOV is the angle of the
visible field of one of the eyes whereas the binocular FOV is the combination
(not addition) of the two eyes fields.
#### 4.2.2.2 Horizontal FOV
The horizontal monocular FOV is the addition of the _nasal FOV_ (from pupil to
nose, 60°) and the _temporal FOV_ (from pupil to the side of the head,
100-110°) [2]. The monocular horizontal FOV is around 170° in average. The
binocular horizontal FOV is around 200-220° degrees [3].
{width="3.8305555555555557in" height="2.3041666666666667in"}
Figure 4.1: Horizontal human field of view
The central vision is also called the comfort zone where sensibility to
details is the most important even if the maximum acuity is only a few degrees
(3-5°) around the focal point (called the fovea zone). Although less sensible
to definition, the peripheral vision is more receptive to movements. The
common area covered by both monocular FOV is the area where depth perception
is possible (binocular vision: 120°). Figure 4.1 summarizes the different
viewing angles composing the horizontal FOV.
#### 4.2.2.3 Vertical FOV
The vertical FOV is composed of the central vision area (60°) and the upper
and lower peripheral visions (30° and 45° respectively) as illustrated in the
figure 4.2 below.
{width="2.973611111111111in" height="2.5395833333333333in"}
Figure 4.2: Vertical human field of view
The vertical FOV is typically around 135°[2]. For both eyes the combined
visual field is 130-135° vertical and 200° horizontal [3].
### 4.2.3 Lenses for a wider FOV
In order to ensure an immersive experience, a large enough FOV is required.
Due to the limited size of the screens, lenses are used in between the eyes
and the screens in order to fill up the human field of view as much as
possible. The Figure 4.3 below describes the principle.
{width="2.9631944444444445in" height="2.8055555555555554in"}
Figure 4.3: Use of lenses for VR
Due to the properties of the lenses, the perception of looking at a much
larger scene is achieved. Each light ray is expanded through the lens.
### 4.2.4 Optical aberrations
#### 4.2.4.1 Introduction
Although lenses are used so as to increase the field of view, the downside is
that aberrations are introduced that need to be corrected or compensated so as
to offer a good quality of experience. There are mainly two types of
aberrations created by the lenses: The lens distortion and the chromatic
aberration.
#### 4.2.4.2 Lens distortion
When light crosses the lens, it is deviated from its original direction
(refraction) proportionally to its distance from the axis of the lens (rays
crossing the lens at its axis are not deviated) as illustrated in Figure 4.3.
In such a case, images are spherically distorted; their corners stretch
outwards and the lines start to curve. In the Figure 4.3, pixel P0 is further
from the axis than pixel P1. The projected pixel P0 on the virtual screen is
then perceived larger than the projected pixel P1. This distortion is called
the _Pincushion distortion_.
Pincushion distortion is a lens effect that causes images to become pinched to
the centre. The Pincushion distortion effect increases with the object
distance from the optical axis of the lens as shown in Figure 4.4 below.
{width="3.9340277777777777in" height="1.2826388888888889in"}
Figure 4.4: Illustration of the optical Pincushion effect
In order to compensate such an effect and remove the apparent distortion, the
opposite distortion is applied on the display. This is called the _Barrel
distortion_. The Barrel distortion is the diminution of the image
magnification with the radial distance of every point from the optical axis
(the further is a point from the centre, the higher its distance from it is
reduced). Figure 4.5 depicts the resulted image when the Barrel distortion is
applied on a screen and its result after a lens Pincushion on it.
The direct correlation of the field of view with the amount of distortion of
the image is that the wider the field of view is, the more distorted the image
is.
{width="3.9409722222222223in" height="1.320138888888889in"}
Figure 4.5: Barrel distortion for correcting the pincushion effect
Even if with such a process the apparent distortion is removed, there are
mainly to issues introduced:
First, applying the Barrel/Pincushion combination implies a reduction of pixel
density. Even if it barely remains the same at the centre, the reduction is
particularly important at the edges of the picture thus loosing fidelity in
those regions. However, this process is considered acceptable because the
assumption is made that, most of the time, the user looks straight ahead and
rather turn his head instead of his eyes. Moreover, as explained in Clause
4.4.2.2, the peripheral vision is much less sensible to resolution than the
centred vision.
The second issue of applying a Barrel/Pincushion combination is that the image
size is reduced. Edge areas of the original picture are lost, thus reducing
the FOV. Such a limited FOV with visible black areas all around the picture is
called a tunnel effect, which make the immersion feeling to be lost as shown
on the Figure 4.6 below:
{width="6.4944444444444445in" height="1.5013888888888889in"}
Figure 4.6: Consequence of the Barrel/Pincushion effect on image size
One solution for solving this size reduction is to use a higher resolution
image at the source so as to get the desired resolution after correction. This
would require video decoders and graphic-buffers capabilities to be higher
than what the display can render.
#### 4.2.4.3 Chromatic aberration
A common problem with the use of lenses occurs when not all the wavelengths of
colours originated from the same location converge to the same focal plane as
shown in Figure 4.7. This is called the chromatic aberration. It is a colour
separation effect where the light of different wavelength refracts differently
on the glass of a lens (also called colour fringing). This chromatic
aberration can be corrected by separately resampling the colour channels of an
image [4].
{width="1.6583333333333334in" height="1.5173611111111112in"}
Figure 4.7: Illustration of the chromatic aberration
### 4.2.5 VR Video Systems
[Ed. Note] Figure X0 and Sections 4.2.5.7 and 4.2.5.8 do not cover
conversational services. Conversational services will be treated separately.
#### 4.2.5.1 Introduction
[editor's note: Need some text for introducing the figure]
[editor's note: This figure is subject to be refined so as to provide
consistency with other functional workflow descriptions (audio systems and VR
architectures)]
{width="6.995833333333334in" height="3.490972222222222in"}
Figure X0: Overview of the VR video processing chain
#### 4.2.5.2 Capture
VR content may be represented in different formats, e.g. panoramas or spheres
depending on the capabilities of the capture systems. Many systems capture
spherical videos covering the full 360°x180°. Capturing of such content is
typically done by multiple cameras. Various camera configurations can be used
for recording 2D and 3D content.
For 2D content, cameras can be mounted on a ring to capture horizontal
panoramas or mounted on a sphere to capture spherical (360°) video. Multi-
camera arrangements for capturing panoramic videos require focal points of all
camera views to coincide at a common point so that stitching can be performed
with minimum parallax effect.
3D content can be captured by stereo camera pairs with a relatively small
overlap arranged in a star configuration ("segmented stereo"). However, such
camera systems may suffer from parallax errors. On the other hand, mirror-
based systems can capture 3D images using camera pairs reducing parallax
errors. Another option is "stereo by extreme overlap" in which stereoscopic
content is created from overlapping images captured by either fish-eye or
wide-angle lenses, or by clusters of cameras. During processing each image
sensor is split into a left and right section and corresponding left and right
panoramas are stitched from these sections.
{width="4.009722222222222in" height="1.8152777777777778in"}
Figure X1. (Left) Segmented stereo (right) Stereo by extreme overlap
Light field rendering is another promising approach for creating 3D content
[R0]. However, light-field rendering requires dense camera grids. Hence,
existing approaches use depth-based rendering to generate the intermediate
camera views and reduce the required number of cameras (Figure X2). The
disadvantage of this approach is that very accurate depth maps and
sophisticated depth-based processing are needed which increases the required
computational power and makes the approach error-prone.
{width="3.714583333333333in" height="2.517361111111111in"}
Figure X2. (Left) Depth-based light field processing (Right) Generation of
intermediate views
#### 4.2.5.3 Sphere stitching
Captured views from each camera are stitched together to combine the
individual views of the omnidirectional camera systems to a single panorama or
sphere. The stitching process should avoid parallax errors and visible
transitions between the single views. Parallax errors occur because cameras do
not have a common optical center when arranged in a star configuration. This
results in blind regions (gaps) between the camera views. Also, in the overlap
areas, objects recorded from different viewpoints appear on different
positions in the single views. Blind regions and artefacts in overlap areas
impede the stitching process (Figure X3). Stitching captured camera views
parallax-free is less complex for camera arrangements with a common focal-
point, e.g., mirror-based systems. Such systems can reduce parallax down to
objects within 1m.
{width="3.821527777777778in" height="2.6909722222222223in"}
Figure X3. Illustration showing how parallax errors occur
Stitching can be done offline during post-production stage or in real-time.
For live transmission, a real time stitching process is required which should
be able to process a large amount of data from multiple cameras and provide a
high quality, error free panorama or sphere. Real time stitching is highly
dependent on the omnidirectional camera system and is still a big challenge
especially in terms of avoiding the parallax errors.
Taking into account the camera arrangements and stitching techniques, existing
camera systems can roughly be classified into: mirror-based systems (direct
stitching), systems with depth-aware stitching (segmented stereo and stereo by
extreme overlap), and systems with depth-enabled light field rendering. Table
T1 gives a summary of the advantages and disadvantages of such camera systems,
respectively:
+----------------------+----------------------+----------------------+ | **System** | **Pros** | **Cons** | +----------------------+----------------------+----------------------+ | Direct stitching by | - Parallax-free | - Bulky system | | mirror-based systems | setup | | | | | - Calibration | | | - Easy stitching | needed | | | | | | | - Almost no overlap | - Sensitive to | | | | damages | | | - High resolution | | | | (>10k, 60fps) | | | | | | | | - Full lens control | | | | | | | | - Real-time 2D&3D | | | | processing | | | | | | | | - Capable of live | | | | transmission | | +----------------------+----------------------+----------------------+ | Depth-aware | - Small form | - Usually closed | | stitching | factor, light weight | system, no lens | | | | control | | (Segmented stereo, | - Robust and | | | stereo by extreme | compact systems | - Restricted | | overlap) | | real-time | | | - Easy handling | processing, limited | | | | use for live events | | | - No calibration at | | | | setup | - Reduced | | | | resolutions (due to | | | - Existing | overlap) | | | stitching software | | | | tools | - Anisotropic | | | | resolution of | | | - Established | wide-angle and | | | post-production | fisheye lenses | | | | causing sometimes | | | | extreme distortions | | | | in the resulting | | | | panoramas (for | | | | stereo by extreme | | | | overlap) | +----------------------+----------------------+----------------------+ | Depth-enabled light | - Parallax-free | - Complex computing | | field rendering | rendering | | | | | - Supervised | | | - Enables producing | post-production | | | novel views of the | | | | scene thus moving | - Still an | | | freely inside the | error-prone process | | | scene | | | | | - No real-time | | | | capabilities yet | +----------------------+----------------------+----------------------+
Table T1: Comparison of existing VR capture/stitching systems
#### 4.2.5.4 Projection
Modern video coding standards are not designed to handle spherical content.
Therefore, in VR systems, projection is used for conversion of a spherical (or
360°) video into a two-dimensional rectangular video before the encoding
stage. A sphere can be projected onto a plane in various ways [R1]. However,
no projection method can be distortion free. The distortion caused by the
conversion from spherical to planar domain is referred to as "sampling
distortion". Final reconstruction quality of a spherical video is a function
of both sampling and coding distortions.
The most commonly used projection method is **equirectangular** projection, in
which the horizontal and vertical coordinates simply correspond to longitude
and latitude, respectively, with no transformation or scaling applied [R2].
However, equirectangular projected images have large redundancy near the poles
because they are stretched in latitude direction. This causes a redundant
number of bits to be spent to encode the poles of image (relative to the
actual information content).
A projection that reduces the sampling distortion (compared to
equirectangular) is **cubic** projection in which a portion or whole of the
sphere is projected to planar images. The images are arranged as the faces of
a cube each of which has a 90°x90° FoV. Cube mapping is a sub-case of
rectilinear (gnomonic) projections in which straight lines in real 3D space
are mapped to straight lines in the projected space. Hence, each cube face
retains straight lines [R3].
{width="3.3180555555555555in" height="1.492361111111111in"}
{width="2.4131944444444446in" height="1.4840277777777777in"}
Figure X4: Equirectangular projection (left) vs Cubic mapping (right) of the
same spherical content. Cube maps provide an approximately equal-area
projection whereas equirectangular projection contains redundant samples
towards poles of the image.
Some other projection types (classified according to the type of geometry used
for rendering) are listed below:
  * Sphere
  * Squished Sphere
  * Cylinder
  * Platonic Solid
    * Cube (6 surfaces)
    * Octahedron (8 surfaces)
    * Icosahedrons (20 surfaces)
  * Truncated Pyramid
  * Segmented Sphere
  * Direct Fisheye
#### 4.2.5.5 Region-wise mapping (packing)
After projection, the obtained two-dimensional rectangular image can be
partitioned into regions that can be rearranged to generate "packed" frames.
The operations to produce packed frames from projected frames (denoted as
"packing" or "region-wise mapping") might include translation, scaling,
rotation, padding, affine transform, etc. Reasons to perform region-wise
mapping include increasing coding efficiency or viewport dependent stream
arrangement (see multi-stream approach in Section 4.2.5.6.2).
{width="1.6083333333333334in" height="1.5791666666666666in"}
Figure X5: Example of a multi-resolution cube map. Some of the rectangular
areas in the projected frame can be downsampled to construct the packed frame.
If region-wise mapping is not used, the packed VR frame is identical to the
projected frame.
#### 4.2.5.6 Encoding & Decoding
Current 360 video services offer a limited user experience since the
resolution in the user viewport and hence the visual quality are not on par
with traditional video services. Multiple times UHD resolution is needed to
cover the full-360 surroundings in a visually sufficient resolution. This
poses a major challenge to the established video processing chain and to the
available end devices.
There are mainly three approaches that can be considered for 360 video
delivery. All solutions can be grouped into:
  * Single stream approach
  * Multi-stream approach
  * Tiled stream approach
##### 4.2.5.6.1 Single stream approach
For HMDs, one straightforward approach would be to encode an exact or over-
provisioned viewport for each user, i.e. crop the interesting part (e.g.
viewport) for the user at the server side and encode it. However, although
this approach minimizes the number of non-viewport samples to be decoded, it
comes at the cost of an encoding overhead when considered for large-scale
deployments. Another option that is considered as a single stream approach is
to encode the full 360 video, transmit it to the receiver and decode the full
360 video while showing only the viewport.
Therefore, solutions that lie within this group have the drawback that either
they may not be scalable or they may impose a big challenge in terms of
required network resources (high bitrate of high resolution video) and
required processing at the client side (decode a very high resolution video).
Mobile devices typically contain hardware video decoders tailored to
resolutions used in traditional video services (HD or UHD). Therefore, it is
important to limit the overall resolution to be transmitted and decoded in the
mobile devices.
Using single stream approach, the receiver decodes the entire video
(corresponding to either the viewport (exact or over-provisioned) or the full
360 video).
##### 4.2.5.6.2 Multi-stream approach
The multi-stream approach consists of encoding several streams, each of them
emphasizing a given viewport and making them available for the receiver, so
that the receiver decides which stream is delivered at each time instance. The
number of the streams made available can vary and be optimized; with a larger
number of streams, a better match to the users' viewport can be obtained.
However, this requires more storage capacity at the server side. Even though
multiple streams are encoded and made available, only a single stream needs to
be decoded depending on the users' viewport.
There are two ways of generating viewport-dependent video bitstreams for the
multi-stream approach:
  * [Projection/Mapping based]{.underline}: A viewport dependent projection (e.g. Truncated Pyramid) or a projection (e.g. cubic) plus a viewport dependent mapping/packing (e.g. multi-resolution cubemap (Fig. X5)) is used so that the number of samples is higher at the viewport and lower at surrounding areas. The encoding is done as usual, i.e. viewport unaware.
  * [Encoding based:]{.underline} The encoder is configured so that the samples of the viewport are encoded at a higher quality, e.g. with a lower quantization parameter (QP).
Using multi-stream approach, the receiver decodes the corresponding entire
video which results in different resolution or different quality areas. Mobile
devices typically contain hardware video decoders tailored to resolutions used
in traditional video services (HD or UHD). Therefore, it is important to limit
the overall resolution to be transmitted and decoded in the mobile devices.
##### 4.2.5.6.3 Tiled stream approach
Another approach is to use HEVC tiles or separate video streams for 360 video
delivery. It allows emphasizing the current user viewport through transmitting
non-viewport samples with decreased resolution, i.e. selecting the tiles from
the viewport at a high-resolution version and the tiles that do not belong to
the viewport at a lower resolution [R4]. Hence, the full 360° surroundings are
always available on the end device but the number of samples that lie outside
the user FoV is reduced. For this purpose, at the encoder side the full 360
video is projected into a frame, mapped (e.g. cube map) and encoded into
several tiles at different resolutions.
Fallback schemes comprising low resolution tiles and other hybrid tiling
approaches combining low and high resolution tiles may be considered.
When tiled stream approach is followed each of the tiles can be encoded as
motion-constrained HEVC tiles or as separate video streams.
Using motion-constrained tile HEVC streams, the varying spatial resolution in
video picture can be achieved by merging motion-constrained HEVC tiles of
different resolutions into a single common bitstream and therefore this
approach allows to use a single decoder.
Using separately encoded video streams, several decoders are required at the
receiver side, as many as the number of the video streams the receiver chooses
to decode.
Using tiled-streaming approach, the receiver can choose to decode only a
subset of the received video stream depending on the current viewport position
and/or device capabilities (e.g. video decoder capabilities).
#### 4.2.5.7 File/DASH encapsulation/decapsulation
360 video might require additional signaling in DASH. For instance, projection
and mapping formats might be required to be signaled at the MPD so that client
can request the appropriate Representations and/or Adaptation Sets.
File/DASH encapsulation is performed differently depending on the type of the
considered solution (single-stream, multi-stream, tiled stream).
The receiver can choose to decapsulate only a subset of the received video
stream depending on the current viewport position and/or device capabilities
(e.g. video decoder capabilities).
#### 4.2.5.8 Delivery
A panoramic or 360 video can be delivered in unicast, multicast or broadcast
mode. In all of these modes, the delivery can be realized in the form of
download or streaming and in real-time or non-real time.
For unicast delivery, DASH can be used. For multicast or broadcast delivery,
DASH over MBMS can be used.
For unicast delivery, all three approaches mentioned in Section 2.5 (single
stream, multi-stream, tiled stream) could be used.
For unicast and MBMS delivery, the DASH client requests appropriate segments
depending on the viewport position, available network throughput, device
capabilities and service requirements. E.g. for multi-stream approach, the
DASH client requests the stream (representation) that matches best to the
expected viewport position (subject to network latency and user movement).
#### 4.2.5.9 Rendering
Considering the limited FoVs of the existing VR displays, only a part of the
video frame needs to be displayed to the user. For instance, FoVs of the most
currently available HMDs range between 100°-110° [R5].
Since the actual rendered video will cover only a portion of the scene, the
renderer should receive metadata related to the current user viewport. The
renderer also uses the video characteristics such as projection and mapping
formats.
## 4.3 Audio systems
### 4.3.1 Introduction
This clause describes Audio Systems for use with Virtual Reality. Clause 4.3.1
provides a high level overview of requirements for audio VR, Clause 4.3.2
contains an overview of audio capture systems for VR, Clause 4.3.3 describes
Content Production Workflows for VR and Clause 4.3.4 describes audio rendering
systems for VR.
In general, Virtual Reality requires Audio Systems that deliver to the
consumer a perception of immersion in a virtual environment. However,
immersion itself is not a sufficient condition for successful commercial
deployment of virtual reality multimedia services. To be successful
commercially, audio systems for VR must deliver the perception of immersion
through content creation tools, workflows, distribution and rendering systems
that are practical to use and economically viable to the content creator and
consumer.
Whether a VR system is practical to use and economically viable for successful
commercial deployment, depends on the use case and level of sophistication
expected for both the production and consumption of the use case. For example,
uploading a short spherical video of a vacation trip to a social media website
may not afford the same level of production complexity found in a blockbuster
cinematic production. Similarly, when compared to the casual user, "hardcore"
gamers may have different tolerance levels to both the quality of the
immersion being delivered as well as the equipment necessary to achieve so.
In addition to considerations of immersion, production, distribution and
rendering complexity, VR audio use cases that support two-way communication
have further constraints in the form of low mouth to ear latency requirements
for conversational quality. As illustrated on Figure 4.8, careful
consideration of the following aspects is therefore required:
(1) the Audio Capture System
(2) the Content Production Workflow
(3) the Audio Production Format (see clause 6.2)
(4) the Audio Storage Format (see clause 6.2)
(5) the Audio Distribution Format (see clause 6.2)
(6) the Audio Rendering System
Figure 4.8: Audio System Components for VR
### 4.3.2 Audio capture system
#### 4.3.2.1 Introduction
The audio capture system design for VR is dependent on the choice of the audio
content production format, which itself hinges on the considerations described
in clause 4.3.1. For example, applications requiring use of scene-based audio
may support a single spherical microphone array to directly capture the
auditory scene. Applications that make use of channel or object-based formats
may choose to use one or more microphones optimized and arranged for the
specific sound source being recorded.
#### 4.3.2.2 Audio capture system for scene-based audio representation
Scene-based Audio is one of the three immersive audio representation systems
defined, e.g. in ITU-R BS.2266-2. Scene-based Audio is a scalable and
loudspeaker-independent soundfield representation grounded on orthogonal basis
functions such as spherical harmonics. Examples of Scene-Based audio formats
commercially deployed for VR include B-Format First Order Ambisonics and the
more accurate Higher-Order Ambisonics
Ambisonics is a periphonic audio system [4bis], i.e., in addition to the
horizontal plane, it covers sound sources above and below the listener. An
auditory scene for Ambisonics can be captured through the use of a first or
higher-order Ambisonics microphone. Alternatively, separate monophonic sources
can be captured and panned to desired positions.
**B-format Microphones**
The "B-Format", or First Order Ambisonics (FOA) format uses the first four
low-order spherical harmonics to represent a three-dimensional sound field
using four signals:
**_W_** : the omnidirectional sound pressure
**_X_** : the front/back sound pressure gradient at the capture position
**_Y_** : the left/right sound pressure gradient at the capture position
**_Z_** : the up/down sound pressure gradient at the capture position
The four signals can be generated by processing the raw microphone signals of
the so-called "Tetrahedral" microphone, which consists of four capsules, in a
Left-Front-Up (LFU), Right Front Down (RFD), Left-Back-Down (LBD) and Right-
Back-Up (RBU) configuration, as illustrated on figure 4.9.
{width="1.9375in" height="1.9777777777777779in"}
Figure 4.9: The "tetrahedral" microphone
**Horizontal only B-format microphones**
Other microphone array configurations can be deployed for portable spherical
audio and video capture devices, with real time processing of the raw
microphone signal components to derive the _W, X, Y_ and _Z_ components. Some
configurations may support a _Horizontal-only B-format_ in which only the W,
X, and Y components are captured. In contrast with the 3D audio capability of
FOA and HOA, a horizontal-only B-format foregoes the additional sense of
immersion provided by the height information.
**Higher-Order Ambisonics microphones**
The spatial resolution and listening area of First Order Ambisonics can be
greatly augmented by enhancing the number of directional audio components.
These are _second_ , _third_ , _fourth_ and Higher Order Ambisonics systems
(collectively termed HOA). The number of signal components needed for a three-
dimensional Ambisonics system of order _N_ is given by _(N+1)^2^_ and
illustrated on figure 4.10.
{width="3.432638888888889in" height="1.7041666666666666in"}
Figure 4.10: Spherical harmonics from order N=0 (top row) to order N=3 (bottom
row)
{width="1.4479166666666667in" height="1.625in"}
Figure 1.11: A Higher Order Ambisonics capable microphone
Several formats exist for Higher Order Ambisonics data exchange. The component
ordering, normalization and polarity must be properly defined and further
details are provided in Clause 6.2.
**General considerations for scene-based audio capture systems**
Some general considerations of the audio capture system that affect the
perception of immersion include:
  * Signal to Noise ratio (SNR): Noise sources that are not part of the audio scene detract from the feeling of realism and immersion. Therefore, the audio capture system must have a low enough noise floor such that it is properly masked by the recorded content and not perceptible during reproduction.
  * Acoustic Overload Point (AOP): Non-linear behaviour of the audio capture system may detract from the feeling of realism. The microphones must have a sufficiently high acoustic overload point to avoid saturation for the types of audio scenes and use cases of interest.
  * Microphone frequency response: Microphones must have a frequency response that is generally flat along the audio frequency range.
  * Wind Noise Protection: Wind noise may cause non-linear audio behaviour that detracts from the sense of realism.
  * Microphone element spacing, crosstalk, gain- and directivity matching: These aspects ultimately enhance or detract of the spatial accuracy of the scene-based audio reproduction.
  * Latency: If two-way communication is required, the mouth to ear latency must be low enough to allow a natural conversational experience.
[Editor's note]: Specific requirements for each of these considerations can be
considered within the study.
#### 4.3.2.3 Audio capture system for channel-based audio representation
Audio capturing using microphones and post-processing techniques for channel-
based representation are well known in the industry, as they have been the
standard for decades.
Multiple microphones are used to capture sounds from different directions;
either coincident or spaced microphone arrays are used. Depending on the
number and arrangement of microphones different channel-based formats are
created, as e.g. stereo from XY mic pairs, or 5.1 by main microphone
techniques or 8.0 by using microphone arrays. Alternatively, microphones built
into VR cameras can be used to create channel-based audio representations.
Microphone post-processing allows different formats; products implementing
post-processing of raw microphone recordings exist, e.g. a professional camera
for VR with built-in microphones delivers 4.0, 5.0, 7.0 or 8.0 audio output
channels. Consumer cameras provide e.g. 5.1 channel-based audio output.
For cinematic VR the microphone signals are modified and mixed further in
post-production as described in the following sections.
#### 4.3.2.4 Audio capture system for object-based audio representation
Object-based representations represent a complex auditory scene as a
collection of singular elements comprising an audio waveform and associated
parameters or metadata. The metadata embody the artistic intent by specifying
the translation from the audio elements to the final reproduction system.
Sound objects generally use monophonic audio tracks that have been recorded or
synthesized through a process of sound design. These sound elements can be
further manipulated, e.g. in a digital audio workstation, so as to be
positioned in a horizontal plane around the listener, or in full three-
dimensional space using positional metadata. An audio object can therefore be
thought of as a "track" in a digital audio workstation.
The spatial accuracy of object-based audio elements is dependent on the
metadata and rendering system. It is not directly tied to the number of
delivered channels.
Experiences provided by object-based audio typically go beyond a single point
audio capture collocated with the camera in order to meet the artistic intent.
### 4.3.3 Content production workflow
#### 4.3.3.1 Introduction
Figure 4.12 depicts a basic workflow for content creation and delivery for VR
today.
{width="6.697916666666667in" height="2.9479166666666665in"}
Figure 4.12: Event Multicast to VR enable user equipment
The workflow depicted in Figure 4.12 shows a complete VR audio system
involving scene-based, object-based and channel-based audio representations.
Not all of the components depicted must be used for a particular use case.
#### 4.3.3.2 Content production workflow for Scene-Based Audio
In applications involving real-time streaming of user generated content, the
use of a live mixing engineer or post-production may not always be
possible/desired. In those applications, the content production workflow can
be simplified to a direct transmission of a scene-based audio stream. The
sound field is captured with an appropriate microphone array configuration and
transmitted to the far-end in a format such as Ambisonics or Higher-Order
Ambisonics.
Even though no user manipulation of the content is performed, it is still
possible that the content transmitted is modified in the transmission chain.
In particular, Ambisonics and Higher-Order Ambisonics are built on a layered
approach that makes the technology suitable for scalability. Scalability may
be desirable depending on e.g.: the bandwidth available, the computational
resources on the renderer side, etc.
For example, the content could be scaled according to link budget conditions.
An example of such approach is depicted in Figure 4.13, where a Higher-Order
Ambisonics content being transmitted adapts based on link conditions. When
link conditions are adequate, higher spatial audio resolution could be
transmitted, enhancing the sense of immersion and spatial localization.
Conversely, if link conditions degrade, the network may opt to lower the
spatial audio resolution, saving resources.
**Figure 4.13: Example of a pure Scene-Based Audio workflow showing link
adaptation**
#### 4.3.3.3 Channel-based content production workflow
In most cases the mic signals, effects, foley and music are mixed in post-
production, either in a live mixing console or a DAW as shown in Figure 4.12.
Tools and workflows for channel-based audio production are well established in
the industry. Most DAWs support immersive audio formats or can host plugins to
support mixing for immersive output formats like 9.1, 11.1, or 13.1.
In the context of VR, 5.1 surround represents a popular audio format enabling
sounds from all directions on the horizontal plane, as e.g. used by existing
VR platforms for mobile consumption. In order to provide immersive sound, a 3D
format is required. There is a trade-off between the spatial resolution and
the number of audio channels can be made.
#### 4.3.3.4 Production and post production for Object-Based audio
representation
The process of production and post-production for linear VR experiences is
similar to traditional cinematic content. A set of audio elements obtained via
spatial/soundfield recordings as well as spot microphones reach an audio
mixing console or digital audio workstation where an audio engineer crafts an
audio mix suitable for binaural reproduction over headphones. This creative
process is paramount to deliver a high-quality, hyper-real experience. Hyper-
realism is used to describe forms of compositional aesthetic where certain
sounds that are present in the real environment are handled in a way so that
they are either removed or somehow exaggerated.
An essential component of VR mixing for cinematic content as well as for live
content is the positioning of the different audio elements of the mix in space
(i.e. panning) so that they match the video reference. For the mentioned
linear content use-cases, this may be achieved through a user interface where
the sound objects are positioned into a room-model in reference to a viewer.
For VR, the mixing interfaces must be adapted to account for the fact that the
video/visuals can encompass the entire sphere or even an entire 3D region
around the nominal listening position.
For live produced content generally the same principle applies. Different
sound sources obtained via spatial/soundfield recordings, typically captured
at different camera locations, as well as spot microphones reach a live audio
mixing console. There an audio engineer assisted by automated components
developed for live VR production crafts an audio mix suitable for binaural
reproduction over headphones. The most obvious difference to the process for
cinematic content is that the mix needs to be created in real-time.
[Ed'Note: Still needs to decide the most appropriate location for this figure
{width="6.495833333333334in" height="1.9972222222222222in"}
Figure 2: Example of a content creation process for live content
]
A key component in live workflows is the renderer/presentation manager that
generates multiple mixes from a set of input audio elements, e.g. for the
different camera viewpoints. This ability to output customized mixes for the
different camera viewpoints is more critical for VR than for traditional
broadcast use cases as a tight audio-video consistency is a requirement for
immersion.
Object-based audio is well suited to create the hyper-realistic mixes required
by professional live VR applications, where creating a soundscape that matches
the camera viewpoint must be balanced with crafting an interesting mix. This
may require enhancing far away elements that cannot be captured solely from
the camera location but are nonetheless important to follow the action. A
solution is to author audio objects in 'world-space', i.e. the stadium or
venue and let the presentation manager transform their position to match
multiple viewpoints (e.g. for different cameras). In addition, an increasing
number of systems are appearing which can be used to tie audio to real-time
tracking systems in order to define dynamic audio objects.
#### 4.3.3.5 Object metadata and controls for VR
Similar to traditional cinematic use cases, cinematic VR mixing often requires
some audio elements to have specific playback-time behaviour. For instance,
some non-diegetic background elements or music should preferably be kept
'head-referenced' i.e. non-head-tracked, while the diegetic sound effects or
dialog should be 'scene-referenced' (i.e. head-tracked).
Similarly, it may be desirable for some elements to be rendered with higher
timbral fidelity by bypassing binaural processing at playback time.
Another category of controls for VR applications determines the environmental
model. For spherical videos / 3DOF content, the environmental model is often
pre-baked into the audio elements themselves. This means that the
reverberation, distance, source directivity and other room characteristics can
be static, as with 3DOF interaction the viewer can watch, but cannot freely
move within a given space.
Finally, a last category of VR specific controls relates to gaze-based
interaction where the end-user can emphasize or even mute/unmute some of the
elements in the mix by looking at specific points or directions.
These specific behaviours or properties can be easily authored and attached to
the audio elements as object metadata.
### 4.3.4 VR audio production formats
#### 4.3.4.1 Introduction
ITU-R BS.2266-2 presents a framework of future audio representation systems
and the need for a production exchange file format. The framework recognizes
channel, object and scene-based audio representations. Table 4.1 describes
these different audio representations.
+------------------------------------+--------------------------------+ | **Signal type** | **Examples** | +------------------------------------+--------------------------------+ | **Channel-based audio** | e.g. Full Mix, Music | | | | | - Mixes or mic array recordings | | | for a specific loudspeaker | | | layout | | | | | | - e.g. Stereo, 5.1, 7.1+4 | | +------------------------------------+--------------------------------+ | **Object-based audio** | e.g. Dialogues, Helicopter | | | | | - Audio elements with positional | | | metadata | | | | | | - Rendered to target speaker | | | layout or headphones | | +------------------------------------+--------------------------------+ | **Scene-based audio** | e.g. Crowd in Sports, Ambience | | | | | - B-Format (First-order | | | Ambisonics) | | | | | | - Higher-Order Ambisonics (HOA) | | +------------------------------------+--------------------------------+
Table 4.1: Audio production formats
Note, that all signal types from Table 4.1 can describe 3-dimensional audio as
necessary for an immersive VR experience. All signal types require audio
metadata for control of the rendering e.g.:
  * Channel configuration,
  * Type of Scene-Based normalization scheme and Ambisonics coefficient ordering,
  * Object configuration and properties, e.g. position in space
  * Diegesis, i.e. change upon head-tracking or steady with respect to the head -- examples below:
    * Non-diegetic: A narrator who isn't visible in the scene can be rendered independently from the head-tracking.
    * Diegetic: An actor's dialogue is rendered according to his position in the scene, taking the head-tracking information into account.
A range of file formats and metadata supporting VR video streaming and
playback is in active use today. However, no commonly agreed mechanism to
exchange content between these multiple VR file formats exists. This can make
the user's production work frustrating as one may need to re-render the
material for each platform where the video will be consumed. VR audio content,
including object, channel and scene-based audio, is accompanied by metadata
that allows the receiving party to properly interpret the audio data being
transmitted. To facilitate production and distribution of content, it is
desirable that the accompanying metadata can be interpreted by the different
file formats used. This clause surveys the existing VR file formats and how
essential metadata is handled.
#### 4.3.4.2 Survey of existing spatial audio formats
**VR streaming portal - Vendor 1**
Vendor 1 describes an open metadata scheme to allow .mp4 containers to
accommodate spatial (scene-based) and non-diegetic audio. The Spatial Audio
Box (SA3D) contains information such as Ambisonics type, order, channel order
and normalization. The Non-Diegetic Audio Box (SAND) is used to indicate audio
that should remain unchanged by listener head rotation (e.g. commentary,
stereo music, etc.).
At the time of this writing, the channel ordering was based on Ambisonic
Channel Number (ACN), and the normalization was Schmidt semi-normalization
(SN3D).
**VR streaming portal - Vendor 2**
At the time of this writing, vendor 2 service accepts videos in .mp4 or .mov
containers. Audio is in AAC format with AAC-LC profile. The following formats
are supported:
  * 1 Channel, Mono
  * 2 Channel, Stereo (Left, Right)
  * 4 Channel, Ambisonic (1st Order, ACN channel ordering, SN3D normalization)
  * 6 Channel, 5.1 Surround (Left, Right, Center, LFE, Left Surround, Right Surround)
Additionally, vendor 2 supports \"Binaural audio\" and \"Quadraphonic audio\"
through the use of four mono or stereo audio tracks. These audio tracks
correspond to the four 90 degree cardinal directions a user looks in the video
(0deg, 90deg, 180deg, and 270deg). An open source audio and video conversion
tool (FFmpeg) is suggested to build mp4 files with the "binaural" or
"quadraphonic" audio formats.
**VR streaming portal - Vendor 3**
Vendor 3 also supports spatial audio with support for a proprietary 8 channel
audio output format, first order Ambisonics with ACN ordering and SN3D
normalization (AmbiX) or Furse-Malham ordering. In addition, non-diegetic
audio support can be enabled through content production tools provided by the
vendor. Files are saved under an .mp4 container. Vendor 3 also supports
playback of audio and video generated using Vendor 2 metadata scheme.
**ITU-R BS.2076-0 -- Audio Definition Model (ADM)**
The Audio Definition Model (ADM) is an open standard that seeks compatibility
across object, channel and scene-based audio systems using XML representation.
It aims to provide for a way to describe audio metadata such that each
individual track within a file or stream is correctly rendered, processed or
distributed.
The model is divided into a content part and a format part. The content part
describes what is contained in the audio such as dialogue language and
loudness. The format part contains technical information necessary for the
audio to be decoded or rendered correctly, such as the position coordinate for
a sound object and the order of an HOA component.
The recommendation provides for a series of ADM elements such as
audioTrackFormat (describing what format the data is in), audioTrackUID
(uniquely identifying a track or asset with a recording of an audio scene),
audioPackFormat (grouping audio channels), etc. Guidelines for the use of IDs,
coordinate systems and object-based parameter descriptions are also provided.
Finally, a series of examples of ADM usage for channels, object and scene-
based audio, including XML sample code and UML diagrams, concludes the
recommendation.
Metadata that is specific to virtual reality such as indication of whether a
content is diegetic or non-diegetic was not part of the recomendation at the
time of this writing.
Currently, ADM is more of a production format rather than a format that is
conducive to streaming applications, but this may change in the future. For
streaming, a container format that allows both audio and video packets
(compressed and uncompressed) along with ADM metadata would be desireable.
**ETSI TS 103 223 -- Multi-Dimensional Audio (MDA)**
The Multi-Dimensional Audio (MDA) is a metadata model and bitstream
representation of an object-based soundfield for linear content. The target
for this specification is cinema and broadcast applications.
The metadata can indicate when an object occurs on the program timeline or
where it is positioned within the soundfield. The reference renderer is based
on Vector Base Amplitude Panning [Clause 4.1] and renders an object to its
desired sound location. A bitstream representation is defined and mappings
between common URI values and shorter Label values are provided to minimize
overhead.
A broadcast extension (clause 7) is defined in the specification, introducing
support for Higher Order Ambisonics within the MDA stream. The channel
numbering follows the ACN convention. The default normalization is N3D but
other HOA normalization types are also supported (FuMa, SN3D, N2D, SN2D).
Additional broadcast extensions are provided for Loudness and dynamic range
compression management.
Metadata that is specific to virtual reality such as indication of whether a
content is diegetic or non-diegetic was not part of the specification at the
time of this writing.
**AmbiX**
The AmbiX [_Christian Nachbar; Franz Zotter; Etienne Deleflie; Alois Sontacchi
(June 2--3, 2011)._ _]_ [ ]{.underline} supports HOA scene-based audio
content. AmbiX files contain linear PCM data with word lengths of 16, 24, or
32 bit fixed point, or 32 bit float, at any sample rate valid for .caf
(Apple\'s Core Audio Format).
AmbiX adopts ACN ordering and SN3D normalisation and can provide for full-
sphere and mixed-order Ambisonics support. The ACN+SN3D convention for full-
sphere Ambisonics is adopted by Vendors 1, 2 and 3 listed above. This
convention is gaining rapid traction as a popular format for the exchange of
Ambisonics content.
Additional metadata to indicate non-diegetic content must be added to this
scheme.
**ETSI TS 103 190**
[Editor's note: TBD]
**ISO/IEC 23008-3**
[Editor's note: TBD]
#### 4.3.4.3 Observations
For scene-based audio, existing commercial solutions seem to be converging
towards the use of ACN ordering and SN3D normalization (with a trivial
conversion between N3D and SN3D). However, such metadata is not currently
agreed as a standard but exists as extension boxes to the .mp4 container.
For the simultaneous handling of objects, channels and scene-based audio, e.g.
ITU-R's ADM provides a platform to exchange broadcast content that can be
possibly expanded to handle aspects specific to VR (e.g. support for mixed
diegetic/non-diegetic content, possibly some reference renderer, streaming,
etc.).
To reduce fragmentation in the industry, a common production, metadata
stamping and rendering workflow could be of interest for VR audio (similar to
broadcast efforts undertaken in ITU-R and elsewhere [Editor's note: to be
completed]). Figure X displays an example processing chain where universal VR
audio metadata can be added to the content during production and retrieved at
the rendering side, regardless of the presence of possible spatial audio
compression:
{width="5.283333333333333in" height="2.9694444444444446in"}
Figure X: Example processing chain with universal VR audio metadata
[Ed'Note: This figure needs further updates]
### 4.3.5 Audio rendering system
[Editor's note]: To be completed by Object and Channel based audio
#### 4.3.5.1 Audio rendering system for Scene-Based Audio
In Scene-Based Audio (SBA), the rendering system is independent of the sound
scene capture or creation. The sound scene rendering is typically performed on
the receiving device and can produce real or virtual loudspeaker signals. The
vector of loudspeaker signals S=[S~1~...S~n~]^T^ can be created by:
S=D.B
Where B is the vector of SBA signals B=[B~(0,0)~...B~(n,m)~]^T^ and D is the
rendering matrix for the target loudspeaker system.
More typically, for VR applications, the presentation of the audio scene is
done binaurally over headphones. The binaural signals can be obtained through
a convolution of the virtual loudspeaker signals S and a matrix of binaural
impulse responses of the loudspeaker positions IR~BIN~.
S~BIN~=(D.B)*IR~BIN~
In VR applications, it is desired to rotate the sound-field relative to head-
movement. Such a rotation can be applied through a multiplication of a matrix
F to the SBA signals.
B' = F.B
F = D.L
Where the matrix F is produced by multiplying the rendering matrix D with a
matrix L. The matrix L consists of spherical harmonics of the loudspeaker
points rotated by the amount of the head movement.
For efficiency, the binauralized rendering matrix D~IR~ =(D.F)*IR~BIN~ can be
computed ahead of real time processing.
### 4.3.6 Audio rendering system
Independent of the format, audio signals need to be rendered to form the
appropriate audio output signal for headphone playback. This rendering needs
to take into account the sensor data reflecting the user orientation. The
rendering process typically involves binaural rendering technology to render
virtual speakers, scene-based audio or sound objects in the auditory space
around the listener.
For mobile devices computational complexity is an important factor in the
characterization of a rendering system. Another highly-relevant performance
metric is motion-to-sound event latency, i.e. the time lag between the motion
of the users head and the update of the sound position arriving at the user's
ears. The threshold of noticeability for the head-tracking latency reported in
literature is **60-85ms** [17]. Other sources claim a mean latency detection
threshold at \~**108ms with a standard deviation of 30ms** [18]. Those results
are limited to the conditions of the study and may not be directly related to
all aspects of the user experience (e.g. motion sickness). The applicability
of these results needs to be verified in the context of VR.
## 4.4 Example Service Architectures
### 4.4.1 Introduction
This clause introduces different service architectures that permit the
distribution of VR services. Different architectures are introduced in order
to identify commonalities and differences of different service scenarios. The
architectures the interfaces presented do not imply that they create any
normative functions or interfaces for 3GPP standardization, but are expected
to be used in the implementation and gap analysis discussion for the use
cases.
### 4.4.2 Streaming Architecture
The architecture introduced in this clause addresses service scenarios for the
distribution of VR content in file or segment based download and streaming
services, including PSS HTTP-based unicast (as for example defined in
TS26.247) and MBMS Download Delivery including DASH-based services.
Figure XXX considers a functional architecture for such scenarios. VR Content
is acquired and the content is uploaded for preparation using interfaces B,
split in audio B~a~ and video/image in B~v~. Along with the content
preparation, metadata is available that may be used in the encoding and in the
file format encapsulation. After media encoding, the content is made available
to file format encapsulation engine as elementary streams E and the file
format may generate a complete file for delivery or segmented content in
individual tracks for DASH delivery over interface F. Content may be made
available in different viewpoints, so the same content may be encoded in
multiple versions.
At the receiving end, there is an expectation for the availability of a VR
application that communicates with the different functional blocks in the
receivers VR service platform, namely, the delivery client, the file format
encapsulation, the media decoding, the rendering environment and the viewport
sensors. The reverse operations are performed. The communication is expected
to be dynamic, especially taking into account the dynamics of sensor metadata
in the different stages of the receiver. The delivery client communicates with
the file format engine, and different media receivers decode the information
and provide also information to the rendering.
{width="6.294444444444444in" height="3.785416666666667in"}
Figure XXX Example Architecture for VR Streaming Services
# 5 Use cases for Virtual Reality
## 5.1 General overview
### 5.1.1 Introduction
Considering the VR application space it can be divided by the freedom or
opportunity with which users can interact with the "_non-physical world"._
  1. A single observation point (or a series of single points under producer/user control)
  2. Continuously varying observation point under user control
### 5.1.2 Single observation point
As a minimum, VR users should be able to look around from a single observation
point in 3D space defined by either
a) a producer (in the case of movie/entertainment content) or
b) the position of a capturing device(s) (in the case of live content).
This ability to look around and listen from a point in 3D space is usually
described in terms of the 3 degrees of freedom; pitch, roll and yaw but it's
worth noting that this point in space isn't necessarily static - it may be
moving. Users or producers may also select from a few different observational
points but each observation point in 3D space only permits the user the 3
degrees of freedom.
360 degree video content captured in this way is widely available and when
combined with simultaneously captured audio, binaurally rendered with an
appropriate Binaural Room Impulse Response (BRIR), many and varied interesting
VR applications can be enabled.
Most currently available movie content also features 360 degree view video
with the audio component making use of multi-channel audio combined with
binaural rendering.
### 5.1.3 Continuously variable observation point
This represents the ultimate goal for VR Systems where users are able to look
around from observation points in 3D space and to move within that space under
their own control with _apparently_ full freedom. To achieve this apparent
full freedom of movement it's necessary to add translational movement at least
in a horizontal plane to the 3 degrees of freedom; pitch, roll and yaw above.
Such translational movement through the virtual space is very likely to be,
but doesn't necessarily need to be, constrained to be consistent with
conventional movement (walking in any direction in a horizontal plane, bending
down etc.) - although for it to appear convincing to the user such consistency
may be desirable. As a consequence, it is likely that not all of any 3D space
is going to be reachable as an observation point by the user, as would be the
case in the real world i.e. between \~0.15m and \~1.8m from the floor on which
the user is standing.
Technology exists that can provide a basis for the video component of a truly
variable observation point VR but, although audio component solutions exist, a
harmonized combination of the two which is capable of realistically matching
the audio to that image is still a fruitful research topic [5]. For example,
the complexities of matching the Doppler of the sources with any movement of
the observer and adaptively varying the BRIR acoustics to remain consistent
with the environment around the user and those of the path between the user
and the source, for each and every source, including shadowing when mobile
objects in the scene come between source and listener seem far off. Based upon
the above, it therefore seems likely that in the near-term such full movement
will be constrained and under the control of a content producer who is able to
take care of these acoustic effects.
### 5.1.4 Background to the use cases
In surveying the available technologies for VR in compiling this TR, it seems
most appropriate to concentrate on those applications and use cases which
allowing VR users to be able to look around [and listen] from a single
observation point, or set of observation points, in 3D space.
### 5.1.5 Simultaneous diegetic and non-diegetic rendering
In extension of the below use cases, some media elements of a scene (certain
sounds or parts of the video) are rendered as non-diegetic i.e. independently
from head-tracking, and other elements of the scene are rendered as diegetic
i.e. based on head-tracking
## 5.2 Event broadcast/multicast use cases
### 5.2.1 Introduction
Combining delivery of spherical video, head tracking, and 3D audio capability,
VR delivers immersive video consumption experiences for a variety of events.
Events may take the form of sport events, concerts, game shows, TV shows, etc.
Broadcast / Multicast of events is a relevant use case for VR, enabling new
content delivery business models for the industry.
### 5.2.2 "Infinite seating" content delivery via multicast
At the event capture side, omnidirectional camera and microphones rigs can be
placed at certain seating/viewing locations. Each audio and video capture rig
delivers a distinct experience, corresponding to unique seating locations at
the event side.
In "pay-per-view" applications, the content provider may market these
locations at different price tiers or the viewer may be able to switch between
them.
The "infinite seating" audio experience can be further augmented with
additional audio sources (e.g. commentator microphones, ball-kick microphones,
additional sources from the crowd and field). The "infinite seating" video
experience can be further augmented with techniques such as action replay,
cuts from different vantage points, etc.
The experience/sound of the scene adapts continuously as the viewer changes
position, providing an optimized experience of the audio/video scene,
according to the individual viewer's actual position in the environment.
The figure 5.1 below illustrates such a use case.
Figure 5.1: \"Infinite Seating\" multicast to VR enabled user equipment
### 5.2.3 Event multicast to VR receivers
The content provider may deliver a version of the event as an omnidirectional
VR program to its viewers for consumption via VR equipment. The video and
audio of the VR program is delivered via multicast to many VR viewers
simultaneously as depicted on figure 5.2.
The experience/sound of the scene adapts continuously as the viewer moves,
providing an optimized experience of the audio/video scene, according to the
individual viewer's actual position in the environment.
Figure 5.2: Event Multicast to VR enable user equipment
## 5.3 VR streaming
### 5.3.1 Use case
A user watches a VR on-demand video content with a HMD. The content is
streamed over the 3GPP network using unicast. The user can navigate within the
360 degree video by moving his head and watch different fields of view within
the video.
### 5.3.2 Areas of investigation
Some areas are to be studied as part of VR video streaming, such as:
  1. Use of adaptive streaming formats for 360 video delivery, with content adaptation and/or selection affected by, among others, the following factors:
a. Changing network conditions
b. Device capabilities
c. Field of view in response to user interaction, e.g., head movement
  2. 360 video content provisioning and streaming techniques toward improving the network bandwidth efficiency without compromising user experience and interactivity
## 5.4 Distributing 360 A/V content library in 3GPP
### 5.4.1 Introduction: content library and target devices
A Service provider has access to a library of 360 A/V content. The library is
a mixture of content formats from user generated content, documentaries,
promotional videos, as well as highlights of sports events. The latter content
is replaced and updated daily. The content enables to change the field-of-view
based on user interaction. The Service provider wants to create a portal to
distribute the content to mobile devices. The service provider wants to target
two types of applications:
  * View on a screen with the field-of-view for the content adjusted by manual interaction (e.g. mouse input or finger swipe)
  * View in a HMD with head motion tracking. The Service provider expects different types of consumption and rendering devices with different capabilities in terms of decoding and rendering capabilities.
The Service provider has access to the original footage of the content and may
encode and transcode it to appropriate formats. The footage includes different
types of VR content, including
  * For video:
    * Monoscopic video, i.e. a spherical video without real depth perception.
    * Stereoscopic video, i.e., a spherical video using a separate input for each eye.
    * Different spatial and temporal resolutions, e.g. up to 16K and 240fps with 10 bit and 12bit as well as different colour spaces and transfer characteristics.
  * For audio:
    * Channel-based audio
    * Object-based audio
    * Scene-based audio
    * Mixtures of the above
The service provider wants to reach as many devices as possible and wants to
minimize the amount of different formats that need to be produced while
ensuring that the content is presented in highest quality on the different
devices.
In particular, the service provider wants to reach devices that are already in
the market or emerging mobile devices,
### 5.4.2 Downloading content
The service provider wants to enable that a certain amount of the content can
be downloaded to devices through HTTP and is played back on the device after
downloading. The service provider wants to ensure that a device downloads only
content that it can decode and render while providing the best user experience
for the device capabilities.
### 5.4.3 Non real-time MBMS distribution of VR content
Many of the devices support MBMS file delivery services. The service provider
agrees with an MNO that certain content is pre-cached on mobile devices for
offline consumption using MBMS file delivery services. The service provider
wants to ensure that MBMS delivered content can be consumed by as many devices
as possible.
### 5.4.4 Streaming content
For certain contents, the service provider wants to ensure that content is
rendered instantaneously after selection, so a DASH-based streaming is
considered. The service provider wants to ensure that a device accesses only
content that it can decode and render while providing the best user experience
for the device capabilities. The service provider also wants to ensure that
the available bandwidth for the user is used such that the rendered content
for the user is shown in the highest quality possible.
### 5.4.5 Gap Analysis
The use case may be implemented using a service architecture as introduced in
clause 4.4.2. In this case the delivery may use progressive download, DASH-
based streaming or DASH-over-MBMS for encapsulation and delivery.
Based on the architecture in Figure XXX, a system as above requires to define
the following components:
  * Original content formats on interface B
    * For audio that can be used by a 3D audio encoding engine
    * For video that can be used by preprocessing and image/video encoding
  * Mapping formats from a 3-dimensional representation to a 2D representation in order to use regular video encoding engines
  * Encapsulation of the media format tracks to ISO file format together, adding sufficient information on to decode and render the VR content. the information may be on codec level, file format level, or both.
  * Delivery of the formats through regular download, DASH delivery and DASH over MBMS delivery
  * Static and dynamic capabilities and environment data that is collected from VR application and the VR platform. This includes decoding and rendering capabilities, as well as sensoring data.
  * Media decoders that support the decoding of the formats delivered to the receiver.
  * In case decoding and rendering are not performed in an integrated block (decoder/renderer), information for audio and video rendering present the information on the VR display and rendering environments.
Based on the considerations above, to support the use case, the following
functions are missing in the current 3GPP specification:
  * Consistent content contribution formats for audio and video for 360/3D AV applications including their metadata. This aspect may be informative and may be considered outside the scope of 3GPP, but there should at the minimum an assumption on these formats.
  * Efficient encoding of 360 video content. In the initial versions, this encoding is split in two steps, namely a projection mapping from 360 video to 2D (projection mapping) and a regular video encoding. In order to address the latter, high-end video decoding platforms should be targeted. The TV video profile codecs in TS26.116 [add reference] may fulfill the requirements. In an extension to basic encoding, viewport specific encoding may be considered. This may for example be supported by the use of specific projection maps or tile-based encoding. This aspect is considered as an optimization rather than essential feature. In addition, the appropriate encoding of metadata to use used by the display/rendering, is necessary. Typically SEI messages are defined to support the rendering.
  * Efficient encoding of 3D audio.
  * Encapsulation of VR media into a file format for download delivery. This requires extensions to the 3GP file format
  * Providing the relevant enablers for DASH delivery of VR experiences based on the encoding and encapsulation.
  * Providing the necessary capabilities for static and dynamic consumption of the encoded and delivered experiences in the Internet media type and the DASH MPD.
  * A reference client architecture that provides the signalling and processing steps for download delivery as well as DASH delivery as well as the interfaces between the VR service platform, the VR application (e.g. sensors), and the VR rendering system (displays, GPU, loudspeakers)
  * Decoding requirements for the defined 360 video formats
  * Decoding requirements for the defined 3D audio formats
  * Possibly rendering requirements or recommendations for the above formats, for both separate and integrated decoding/rendering
## 5.5 Live services consumed on HMD
### 5.5.1 Introduction
Roger has subscribed to a regular live TV service from a service provider that
is distributed over 3GPP networks. The service provider shows a tennis match
of a Grand Slam tennis tournament and Roger is watching the TV program on his
mobile device using a HMD. The TV program is rendered on a virtual screen in
front of him. During the regular break while regular subscribers get ad
breaks, the premium subscriber is offered the highlights of the recent points
in an immersive environment providing a 360-degree view from different seats
in the stadium. Roger can now consume during the ad break the full immersive
experience for the highlights from a stadium viewpoint.
### 5.5.2 Streaming content
The service provider offers the first rounds of the tournament as a unicast
DASH-based live streaming service. The service provider wants to ensure that a
device accesses only content that it can decode and render while providing the
best user experience for the device capabilities. The service provider also
wants to ensure that the available bandwidth for the user is used such that
the rendered content for the user is shown in the highest possible quality and
with an end-to-end latency that matches TV services.
### 5.5.3 MBMS delivery of content
With the success of service and the tournament heading towards the finals, the
service provider decides to offer the live TV service over MBMS to address
scalability issues. Both the regular TV content as well as the HMD-dedicated
content are delivered over MBMS. The service provider wants to ensure that
MBMS delivered content can be consumed by as many devices as possible.
### 5.5.4 Mixed delivery of content
With even more success of the service at the next tournament, the service
provider decides to offer the live TV service over MBMS. The regular TV
including regular ads are delivered over MBMS, but different 360 experiences
are created and provided over unicast streaming and the user can select
different views.
## 5.6 Social TV and VR
### 5.6.1 Base use case
Alex and his friends have signed up to a service provider for a new live
football experience. Alex watches a live football match on his HMD. His
friends do the same, each at different places. All of them wear a headset with
microphone to communicate. They connect socially to at least communicate
verbally with each other. Everyone sees the football match in such a way that
they can comment and discuss the actions of the match live as if they would
sit in front of the same TV. They experience at least the sound communication
as if the friends are sitting next to each other at different fixed positioned
seats.
### 5.6.2 360-degree experience
In an extended version each of them are placed at a virtual stadium seat and
watch the match while being able to follow the action by head movement.
## 5.7 Cinematic VR use cases
### 5.7.1 Introduction
Both interactive and linear cinematic experiences are possible with VR. In
interactive VR, the viewer can experience/interact with the story as an actual
observer in the middle of the action. Although this has been realized with
traditional video techniques, the unique immersive experience of VR offers new
appeal for this use case.
At content creation additional post-production and related technical
considerations are necessary, if e.g. revoicing the cinematic VR experience to
different languages need to happen. These considerations are applied before
encoding for delivery.
### 5.7.2 Linear cinematic VR
A viewer enjoys a VR movie from a fixed point in the scene. The viewer of the
movie can freely turn and move his head to observe other details in the scene
and may follow the actors by listening and watching. The viewer can change his
view to explore the details in the scene, looking around freely. At the same
time the user experiences an accurate binaural representation of the scene, so
that he can follow the story without the necessity to (visually) search the
virtual scene to find the main action. The accurate audio rendering supports
the natural behavior of the viewer to look around, and then is able to quickly
re-focus on a sudden event by 'hearing' accurately where it happens.
As an alternative to binaural rendering, the audio can be rendered through
loudspeakers in a listening environment using a multichannel loudspeaker
setup.
### 5.7.3 Interactive cinematic VR
Interactive Cinematic VR poses some paradigm shifts for movie production. The
viewer can look anywhere within the scene and may make decisions as to how the
story would branch, removing some of the control that directors have over
framing, audio effects, etc. In the case of interactive stories, a number of
video clips corresponding to the user choices in the conduction of the story
are required.
## 5.8 Learning application use cases
### 5.8.1 Introduction
VR can enable a number of educational applications with varying degrees of
interactivity. At a basic level, the ability to watch a pre-recorded class
remotely with spherical video and audio enables a higher level of immersion
and assists learning. The student is able to view other students that are
physically present in the class, detect which student is making a question,
etc.
If immersive and interactive environments are offered to a student,
psychological processes similar to when people construct knowledge through
interaction with objects take place. Virtual environments created for learning
tasks in this context are therefore expected to be highly interactive and can
benefit from both object and scene-based 3D audio capture and rendering.
Examples of interactive learning applications include surgeon's training,
interactive learning games for K-12 students, etc.
### 5.8.2 Remote class participation
In a Remote Class Participation situation, the remote student can be assigned
to a "virtual seat" in the class. The "virtual seat" corresponds to the
location where a 3D camera and microphone are positioned in the classroom,
allowing the student to participate in the class remotely.
Remote Class Participation can be realized through streaming of the 3D
audio/video content. In a streaming situation the student would be able to
follow the class remotely and view/listen other students physically present in
class, leveraging the spatial audio cues delivered by the Virtual Reality
system to know which student is talking. This application is analogous to the
Event Broadcasting application described in clause 5.2.
In a more interactive scenario, the student participates in real-time with the
class, making questions to the professor or interacting with colleagues. This
would have strict latency constraints so that the class flow is not disrupted.
## 5.9 VR calls use cases
### 5.9.1 Spherical video based calls
In a Spherical Video based call, two parties communicate with each other. The
parties experience the feeling of being immersed in the far-end party's
environment. For example, one call participant may be calling home from a
beach. The call participant at home is able to have a spherical video and
audio experience of the beach location.
On the capture side, the Spherical Video Call can be accomplished through the
use of an omnidirectional camera and 3D audio capture microphone arrays,
possibly augmented through the use of certain speech processing techniques.
On the rendering side, the Spherical Video Call can be accomplished through
the use of a Head Mounted Display (HMD) and spatial audio rendering
techniques, including binauralization of audio presented over headphones.
Few opportunities exist for audio manipulation outside of real time speech
processing due to the latency constraints of two-way full duplex
communication. Techniques that can capture the 3D sound field/audio objects
and render it in real time with minimal latency would be useful to enable such
an experience.
### 5.9.2 Videoconferencing with 360 video
Anne is holding a video conference call with her team from her home office.
The team is located in a meeting room around a 360 camera A microphone array
is located on top of the camera to capture spatial sound. The conferencing
application on her tablet shows a section of the 360°-view of the conferencing
room. On voice activity, the camera shows the person currently talking. Anne
can swipe the image to look into any direction in the conference room.
Anne is wearing a stereo headset. She can clearly localize the voices of all
participants. The sound scene rotates as she swipes the screen keeping the
sounds and images spatially aligned.
When Anne talks to her team, the speech signal is played back with low latency
from an integrated speaker located below the camera.
### 5.9.3 Gap analysis for VR calls use cases
The following gaps are observed within the related 3GPP services, namely MTSI
in TS 26.114 [15] and IMS-based Telepresence in TS 26.223 [16] from a media
handling perspective:
To support VR calls use cases with spatial audio and 360 degree video, it may
be necessary to define SDP-based mechanisms for the negotiation of VR
capabilities across MTSI / TP senders and receivers during both call setup and
mid-call. VR capabilities here may include the related codecs, formats and
media handling mechanisms for delivery of spatial audio and 360 degree videos.
[Editor's note]: Gap analysis subject to be completed after the feasibility
study.
## 5.10 User generated VR use cases
### 5.10.1 User generated 360 video and audio recording
Anne is attending a rehearsal of her friend Fred's band and he asks her to
share her experience with their friends. Anne uses a handheld 360° camera that
captures 360° video and spatial audio. Anne uploads the content to a media
store and shares a link with her friends.
Her friend Ben downloads and plays the recording on his VR glasses, connected
to the smartphone using headphones. He can turn his head to see all musicians
in the rehearsal room. He hears the sound from the band and the crowd from the
corresponding locations and also Anne's voice commenting the show.
### 5.10.2 User generated live streaming - "See what I see"
Anne is attending a street parade and likes to share her experience with her
friend Ben. Anne uses a smartphone with an integrated 360° camera. The phone
uses multiple microphones with post-processing for immersive audio.
Ben accepts the incoming stream and consumes the parade on his smartphone
using headphones. He can turn the phone to see the crowd or marching band. He
hears the sound from the band and the crowd from the corresponding locations
and also Anne's voice commenting the show.
## 5.11 HMD-based legacy content consumption
### 5.11.1 Introduction
Unlike use cases based on 360 degrees video and audio formats, the immersive
experience is not created by the delivered media itself but rather by the
application-generated scene inside which the media is presented.
### 5.11.2 Private TV
Bob is at home and wants to watch a nature documentary from the 3GPP TV
bouquet but his family has decided to rather watch a song contest, "_The best
3GPP Voice"_ , on TV. Bob then decides to use his HMD for watching his
documentary while his family watches their favorite program on TV.
Bob receives the TV programs usually distributed over 3GPP services to his
smartphone. They are mapped by the device into a 360 environment as shown in
the Figure 5.3. Bob has multiple choices for selecting the virtual environment
such as a movie theater or a living room...
The TV channel is rendered in front of him by default in a virtual screen but
may be fixed, meaning that head movement tracking is not active.
{width="4.722222222222222in" height="2.808333333333333in"}
Figure 5.3: TV channel mapping into a 360 environment
### 5.11.3 Floating mosaic
Bob browses TV contents on his HMD. He receives multiple streams in thumbnail
version that are displayed around him on small virtual screens, as illustrated
in Figure 5.4. He can quickly monitor a channel by turning his head in the
direction of the desired channel. When pointing to a mosaic channel he can
hear the associated audio.
{width="4.722222222222222in" height="2.8833333333333333in"}
Figure 5.4: Mosaic representation in a 360 environment
When interested by a specific channel, Bob selects it and the mosaic channel
becomes the main channel with a higher resolution and is centered in front of
the user.
### 5.11.4 Enriched TV
Bob is watching a TV program on his HMD. The TV program is rendered on a
virtual screen in front of him. A 360 scene covers the background of the
spherical environment.
Bob can activate the contextual menu so as to display additional information
related to the current program such as the EPG (electronic program guide), a
selection of different viewing angles... The Figure 5.5 illustrates such a use
case.
{width="4.722222222222222in" height="3.175in"}
Figure 5.5: enriched TV representation in a 360 environment
# 6 Media formats for VR
## 6.1 Video formats
[Ed Note: Placeholder for documenting existing formats and also capturing
potential video formats performance requirements]
## 6.2 Audio formats
[Ed Note: Placeholder for documenting existing formats and also capturing
potential audio formats performance requirements]
### 6.2.1 Introduction
As discussed in clause 4.3 Audio Systems, VR audio systems require the
delivery of a feeling of immersion in the virtual environment. Audio formats
supporting VR need to render a spatial audio impression that corresponds to
the virtual environment where the listener is immersed. Possible audio formats
to deliver spatial audio include channel-based audio (CBA), object-based audio
(OBA) and scene-based audio formats (SBA). A combination of those formats
(e.g. scene-based audio augmented with objects) may also be of interest for
certain use cases.
### 6.2.2 Data exchange for Scene-Based Audio formats
For a successful transmission and reception of Scene-Based audio content, the
sending and receiving parties need to be aware of the format being used for
the data exchange. In case of Ambisonics and Higher-Order Ambisonics,
different possibilities exist for choices of Spherical Harmonics Component
Ordering and Normalization.
**Spherical Component Ordering Conventions**
[Editor's note]: populate this section with a description for each format
  * Furse-Malham higher-order format (FuMa)
    * [Editor's note]: see http://www.york.ac.uk/inst/mustech/3d_audio/higher_order_ambisonics.pdf
  * Single Index Designation (SID)
    * [Editor's note]: Jerome Daniel, _Spatial Sound Encoding Including Near Field Effect: Introducing Distance Coding Filters and a Viable, New Ambisonic Format_ , 23rd AES Conference, Copenhagen 2003, p. 13
  * Ambisonic Channel Number (ACN) (used in AmbiX)
    * [Editor's note]: Michael Chapman et al., _A standard for interchange of Ambisonic signal sets, Ambisonics Symposium_ , Graz 2009
**Spherical Component Normalization Conventions**
  * maxN
    * [Editor's note]: see http://www.york.ac.uk/inst/mustech/3d_audio/higher_order_ambisonics.pdf
  * SN3D (used in AmbiX)
    * [Editor's note]: see e.g. Christian Nachbar, Franz Zotter, Etienne Deleflie, and Alois Sontacchi: AmbiX - _A Suggested Ambisonics Format Ambisonics Symposium 2011_ , Lexington (KY) 2011
  * N3D
# 7 Interfaces and systems aspects
[Ed Note: Place holder for capturing potential systems requirements]
# 8 Latency and synchronization and aspects
## 8.1 Interaction latency
### 8.1.1 Introduction
Interaction latency is the time delay between the user interacting with a VR
system and the system responding to that user interaction.
Although other interactions in VR systems may be envisaged, the main feature
of virtual reality, as stated in clause 4, is that the user is able to move
and for the output sensory stimuli of the simulation to change in a manner
which is consistent with those movements. It is well known that conflicts
between the movement of the user and their senses, usually the visual and the
vestibular senses (sensory conflict theory) may lead to nausea or motion
sickness which in this case is known as virtual reality sickness. Therefore,
from a performance point of view, the accuracy with which the movements of the
user are reflected in the visual and audio cues and the latency before these
cues respond to the user's movements are key parameters for any VR system.
### 8.1.2 Video interaction (Motion-to-photon) latency
The main driver on performance of the video interaction latency, often
referred to as the motion-to-photon latency, comes from the angular or
rotational vestibulo-ocular reflex (where the gaze is shifted in direct
response to head orientation changes detected by the vestibular system by an
equal and opposite reaction). Although research has shown that adaptation to
sensory conflicts and other shortcomings in VR systems is possible [6,7,8] at
real world (1.0x) magnifications, it has also been shown that sensitivity to
virtual reality sickness is sometimes worse depending upon gender, general
health and other factors and hence it seems reasonable that VR systems should
strive to mimic the real world experience as closely as possible.
The latency of action of the angular or rotational vestibulo-ocular reflex is
known to be of the order of 10ms [9] or in a range from 7-15 milliseconds [10]
and it seems reasonable that this should represent a performance goal for VR
systems. The frame rate from the renderer to the viewer for VR video is
usually at least 60 frames per second but more recently systems have been
reporting frame rates up to 90 frames per second (\~11 ms) or higher, which
are more consistent with the latency requirements of the angular or rotational
vestibulo-ocular reflex, albeit without any allowance for the detection of
user movement and image processing times. When such detection times and image
processing delays are taken into account it would seem appropriate to set a
requirement of 20ms, although it's clear that some acute users will be able to
discern much lower interaction latency times [11]. It would therefore seem
useful to set an objective for the interaction latency time around 10ms.
### 8.1.3 Audio interaction (Motion-to-sound) latency
The response time of the human auditory system in the presence of head
movement is less well characterized than the visual system but it is known to
be dependent upon the nature of the sounds being heard and their direction in
relation to the user.
From studies of human perception of the effect of motion-to-sound latency,
there is evidence that the requirements for VR and AR are quite different
[11a]. In the case of VR, the user is immersed in a wholly artificial
situation where there is no real world zero latency "reference" to highlight
the non-zero latency of the audio rendering system.
The conclusions of [11a] state that the most sensitive listeners in the test
were able to detect latencies of 60 ms (with 70% probability) for isolated
auditory stimuli (VR) and of 38 ms (with 70% probability) when a low-latency
reference tone was also present in the stimulus (AR). AR is out of the scope
of this technical report.
These studies [11a,11b] do not include a simultaneously rendered video
component which may influence the perception of these latencies and the user
experience. It is unclear whether such simultaneous rendering of video and
audio will result in a relaxation or tightening of the motion-to-sound latency
requirements. More extensive studies are highly desirable however; considering
greater numbers of subjects, personalized Head Related Transfer Functions
(HRTFs), and using VR/AR equipment more closely representative of the current
state-of-the-art and assessing the impacts on the user experience.
The audio component interaction latency requirements of a VR system are for
further study.
8.2 Audio/Video synchronization
Due to the relatively slower speed of sound compared to light it is natural
that users are more accustomed to, and therefore tolerant of, sound being
relatively delayed with respect to the video component than sound being
relatively in advance of the video component. This effect is seen in Figure 2
of [13] depicting the detectability thresholds obtained through subjective
viewing experiments. These results show that in a range from 125ms (audio
delayed) to 45ms (audio advanced) it is difficult for viewers to detect the
lack of synchronization. In recent years though the results of [13] have
received significant scrutiny mainly because they were obtained with
interlaced video of 25 or 30 Hz.
More recent studies have led to tighter recommendations e.g. [14] recommending
an accuracy of between 15ms (audio delayed) and 5ms (audio advanced) for the
synchronization, with recommended absolute limits of 60ms (audio delayed) and
40ms (audio advanced) for broadcast video. These figures therefore lead to an
indication of an appropriate range.
In applying absolute limits for the audio/video synchronization thresholds to
the VR application there needs to be appropriate account taken of the apparent
distance in virtual space from the user to the source of the sound under
evaluation. The limits they should be computed relative to the delays expected
due to the speed of sound over the free-space path length in the virtual
environment.
# 9 Conclusion
# Annex A: Change history
* * *
**Change history**  
**Date** **TSG #** **TSG Doc.** **CR** **Rev** **Subject/Comment** **Old**
**New** 2016-04 SA WG4 S4-160394 Initial TR skeleton from the Rapporteur 0.0.1
2016-04 SA WG4 S4-160512 First agreed version of the structure of the TR 0.0.1
0.1.0 2016-07 SA WG4 S4-160754 Inclusion of agreed use cases and system
descriptions 0.1.0 0.2.0