# Foreword
This Technical Report has been produced by the 3^rd^ Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
\
# 1 Scope
The present document ...
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] ETSI TS 102 796, "Hybrid Broadcast Broadband TV", V1.2.1, November 2012.
> [3] ETSI TS 102 809, "Digital Video Broadcasting (DVB); Signalling and
> carriage of interactive applications and services in hybrid
> broadcast/broadband environments", V1.2.1, July 2013.
>
> [4] HbbTV 2.0 Specification, available at:
> http://www.hbbtv.org/pages/about_hbbtv/HbbTV_specification_2_0.pdf
>
> [5] DVB Blue Book A168; DVB-DASH, available at:
> https://www.dvb.org/standards/dvb-iptv
>
> [6] ATSC Candidate Standard, "Interactivity Service Standard" (A/105:2014).
[7] CEA-2014-A, "Web-based Protocol and Fraework for Remote User Interface on
UpnP Networks and the Internet (Web4CE)", July 2007.
> [8] ISO/IEC 23009-1:2014: Information technology -- Dynamic adaptive
> streaming over HTTP (DASH) -- Part 1: Media presentation description and
> segment formats.
>
> [9] 3GPP TS 26.247, "Transparent end-to-end Packet-switched Streaming
> Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP
> (3GP-DASH).
>
> [10] W3C Recommendation 28 October 2014, HTML5: \"A vocabulary and
> associated APIs for HTML and XHTML\", http://www.w3.org/TR/html5/.
>
> [11] Piesing, Jon, "Liaison Letter on Mapping MPEG DASH Events to HTML5 Text
> Tracks and Cues", http://lists.w3.org/Archives/Public/public-
> html/2013Dec/0015.html.
>
> [12] IETF RFC 5261, "An Extensible Markup Language (XML) Patch Operations
> Framework Utilizing XML Path Language (XPath) Selectors", September 2008,
> https://tools.ietf.org/html/rfc5261.
>
> [13] DASH Industry Forum, "Guidelines for Implementation: DASH-IF
> Interoperability Points", Version 3.0, April 7, 2015, http://dashif.org/wp-
> content/uploads/2015/04/DASH-IF-IOP-v3.0.pdf.
>
> [14] 3GPP TR 26.848, "Multimedia Broadcast/Multicast Service (MBMS);
> Enhanced MBMS Operation".
>
> [15] W3C Recommendation 27 June 2001, XML Linking Language (XLink) Version
> 1.0.
>
> [16] ISO/IEC 23008-11, MPEG Composition Information.
...
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
TR 21.905 [x] and the following apply. A term defined in the present document
takes precedence over the definition of the same term, if any, in TR 21.905
[1].
**Interactive service:** An MBMS or PSS service characterized by the ability
of users to interact with the content/program in one or both of the following
ways: 1) by changing the presented content (e.g. via access to auxiliary
information, change of camera angle, supplementary media content overlaid on
main program, concurrent display of text with the main video, etc.); 2) by
returning end-user-supplied information or -initiated action to the service
provider or content provider through the unicast channel (for example to vote
for a particular choice, order a product, or participate in an on-screen
quiz).
**Interactivity experience:** The end-user experience that result from the
occurrence of one or more interactivity events during the presentation of an
interactive service.
**Interactive media:** Media content, as part of an interactive service,
presented to an end-user to prompt explicit action by the user, and/or in
response to user input.
**Trigger:** A notification mechanism delivered via unicast or broadcast to
establish the timing of interactivity events of an interactive service.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
Symbol format (EW)
\ \
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in TR 21.905
[1] and the following apply. An abbreviation defined in the present document
takes precedence over the definition of the same abbreviation, if any, in TR
21.905 [1].
ACR Automatic Content Recognition
AIT Application Information Table
API Application Programming Interface
ATSC Advanced Television Systems Committee
AVC Advanced Video Coding
CENC Common Encryption
CSS Cascading Style Sheet
DAE Declarative Application Environment
DO Declarative Object
DOM Document Object Mode
DSM-CC Digital Storage Media -- Command and Control
DTV Digital TeleVision
DTVCC DTV Closed Caption
DVB Digital Video Broadcasting
EBU-TT European Broadcasting Union Timed Text
FP FingerPrinting
HbbTV Hybrid Broadcast Broadband TV
HTML HyperText Markup Language
HTTP HyperText Transfer Protocol
MPEG Motion Picture Experts Group
IDTV Integrated digital television (receiver)
ISOBMFF International Organization for Standards, Base Media File Format
NRT Non Real Time
OIPF Open IPTV Forum
PVR Persona Video Recorder
SMT Service Map Table
STB Set-Top Box
TDO Triggered Declarative Object
TPT TDO Parameters Table
UDO Unbound Declarative Object
VoD Video on Demand
WM WaterMark or WaterMarking
XHTML Extensible HyperText Markup Language
> XML EXtensible Markup Language
# 4 Interactivity Support for 3GPP-Based Streaming and Download Services (IS3)
## 4.1 Introduction
_\ _
## 4.2 Use Cases, Working Assumptions, Recommended Requirements and Gap
Analysis
## 4.2.1 Use Cases
## 4.2.1.1 Use Case #1: Mobile TV with Auxiliary Data and User Interactivity
Frank is watching a TV talent show program "America's Top Singers" on his UE.
Delivered along with the broadcast content is auxiliary data content including
web links to access additional information on the performer's background and
competition status, on-screen display for real-time user feedback
opportunities and results. The auxiliary contents are synchronized with the
A/V stream and rendered as a side-bar along with the main program by the
client application on the UE. The auxiliary data is updated at different times
during the main program for user engagement, such as display of buttons and
links that can be selected by the user to obtain additional multimedia
information on the chosen singer. After the performances for the program have
completed, audience participation via "vote buttons" alongside the performer's
names are displayed to enable viewer selection of their favorite performer
among the competitors. The user's choice is sent to the program's vote
compilation server, and during the voting period, tallied results are
displayed in real-time. After the allowed time period for user interaction has
elapsed, and upon the show host announcement of the evening's results, the
final vote results are displayed to indicate the 1^st^, 2^nd^ and 3^rd^ place
performers of the evening. The winner, Beyondme, walks up to the podium in
tears of joy as the audience wildly applauses, and the host makes the
obligatory congratulatory remarks and reiterates that "America's Top Singers"
is the top-rated TV talent show in the country.
## 4.2.1.2 Use Case #2: Click for Info
The viewer of the "My European Vacation with Tom" video service is able to
interact with the content by clicking on a combination of pop-up buttons and
web links. Some of these buttons and links are statically displayed alongside
the main display throughput the program, while others appear and later
disappear dynamically, at specific times during the travelogue program to
obtain more information related to the specific cities and tourist sites
featured during the program segment. The returned information may contain
advertisements on cruises and vacation packages.
## 4.2.1.3 Use Case #3: Dynamic Interactive Ads During Live Sports Event
Live streaming of the 2020 Superior Bowl football game is offered by mobile
operator Horizon. The game features the Patriots with 42 year-old quarterback
T. Bradley and the Broncos with 43-year old quarterback P. Manny. Early in the
1^st^ quarter, a vicious sack of Manny leaves him unconscious on the field. As
a time-out is called, an interactive wine commercial is displayed to viewers
with an on-screen link that enables user access to more information on the
wines produced by the sponsor, along with a chance to enter a drawing to win a
winery tour. Later in the game, with 20 sec remaining in the 4^th^ quarter,
with the score tied at 20-20 tie, the Patriots have the ball and Bradley goes
for an unexpected quarterback sneak and scores the winning touchdown. The fans
go crazy, and as the field is swamped with players and fans, the service
provider decides to interrupt the broadcast with another interactive ad.
## 4.2.1.4 Use Case #4: Dynamic and Personalized Interactive Ads During Live
Sports Event
Jack and Jill are watching the same football game as described in the previous
use case. During the aforementioned injury timeout in the 1^st^ quarter, a
personalized interactive ad is displayed on Jack's screen inviting him to view
a sport car commercial at he end of which he is asked to answer three
questions and is notified that he will be entered in a drawing to win that
car. At the same time, a different customized ad is presented on Jill's screen
on women's couture, for which she is invited to pick her favorite dresses
among those displayed and submit her vote online. During the second
aforementioned game interruption, another set of personalized and interactive
ads are presented on Jack's and Jill's UEs.
## 4.2.2 Working Assumptions
> NOTE: The tentatively agreed working assumptions as shown below are for
> further study, with the intent to represent the operational environment of
> service interactivity as described by the use cases in clause 2, as opposed
> to solution framework. Additional working assumptions are expected to be
> added to this clause.
The following working assumptions are applicable to the use cases in this
clause:
  * Auxiliary data components associated with the interactive main service/program are carried over one or more delivery sessions (e.g. MBMS download sessions, PSS sessions).
  * Information about the user, such as a profile, can be used to enable a personalized interactivity experience, for example personalized offers or ads displayed during the main program.
## 4.2.3 Use Case Analysis
### 4.2.3.1 Scene Update Processing
In the following discussion on the use of scene updates to support
interactivity use cases, MPEG Composition Information (CI) [16] is cited as an
example format for describing scene updates. The MPEG CI document will require
processing by a Javascript or a native CI engine. In the latter case, the CI
engine isthen a separate processor from the web runtime engine. In the former
case, the Javascript for processing the CI documents is delivered as part of
the presentation (just as is the case for the DASH MPD processor). The use of
other formats for describing scene updates, including proprietary formats such
as a Javascript Framework, may also be used for this purpose.
### 4.2.3.2 Use Case #1: Mobile TV with Auxiliary Data and User Interactivity
In this use case, a composite scene with a main video and side content is
used. The side content is updated throughout the lifetime of the program.
Interactivity in form of voting is offered at specific points of the
presentation.
The following HTML5 document provides an example of such presentation when
authored in HTML5.
+----------------------------------------+ | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ \** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | +----------------------------------------+
Interactivity with the server is performed using HTML input elements and
events and is transmitted to through HTTP using XmlHttpRequest API.
Scene updates are delivered separately. The format of scene update
information/document may either be proprietary or an existing standard such as
MPEG CI [2] may be used for the purpose.
If MPEG CI is used as scene update format, the scene update to address the
current use case might look as follow:
+----------------------------------------------------------------------+ | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | +----------------------------------------------------------------------+
The example shows how the side content is first shown and then hidden after 30
minutes. That side content element may contain all the interactivity
information, e.g. to perform voting and to display side information about the
main show.
### 4.2.3.3 Use Case #2: Click for Info
Similar to the above example, this use cases can be addressed through timing
the appearance and hiding of content in " "div" elements of the HTML5. The
scene update information will provide CSS attribute modifications to be
applied to the referenced "div"
### 4.2.3.4 Use Case #3: Dynamic Interactive Ads During Live Sports Event
This use cases is about unpredicted events that trigger customized ad
insertion. This use case can be addressed by issuing a scene update and
delivering the scene update document to the receivers over MBMS, with clear
identification to accelerate retrieval and processing at the UE.
Upon reception of the scene update, the document will contain information to
change the video source to an ad by pointing to the MPD of the ad. The
resolution of the MPD URL may be used to serve custom ads.
If MPEG CI is used as a scene update format, the scene update document might
look as follows:
+----------------------------------------------------------------------+ | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ http://www.example.com/ad.mpd\** | | | | **\ ** | | | | **\ ** | | | | **\ ** | | | | **\ ** | +----------------------------------------------------------------------+
### 4.2.3.5 Use Case #4: Dynamic and Personalized Interactive Ads During Live
Sports Event
This use case is about customized ads based on user profiles. HTML5 can build
this logic into scripting or address resolution.
5 Architecture Models for 3GPP Service Interactivity
_\ _
# 5.1 DASH Service Delivery
5.1.1 General
The reference architectures for service interactivity are specific to unicast
and broadcast delivery of DASH-formatted streaming services. Emphasis of these
architecture is on the transport and signaling functionality at the service
layer in support of interactivity. In particular, DASH Events [2] could serve
as an appropriate trigger or dynamic notification mechanism to initiate the
execution of service application logic pertaining to service interactivity,
and is assumed as the interactivity trigger in the architecture and
interaction diagrams in clauses 5.2.2 and 5.2.3. Examples of service
interactivity are captured in the use case descriptions in clause 4.2.
The architecture models below are exemplary in depicting interactivity in the
context of DASH service delivery and assumes the use of DASH Events as the
interactivity trigger. Other reference architecture models and trigger
mechanisms are not precluded.
> Editors Note: It may be possible to simplify the architectures and call
> flows below by a unified representation for unicast and broadcast DASH
> service delivery.
## 5.1.2 Unicast DASH
Figure 5.1 depicts the proposed system architecture and high level sequence
flow for unicast/HTTP delivery of DASH streaming services with interactivity
support. Optional message steps are shown by dashed lines, and optional
functionality in support of interactivity are shown inside dashed boxes. The
Multimedia Framework is a software framework in the UE that handles media
delivered through a network. It may provide built-in software-based codecs for
popular media formats, and may also support integration with hardware codecs.
The Multimedia Framework may support session management, time-synchronized
rendering, transport control, and DRM. An example is the multimedia framework
provided by the Android operating system. The Interactivity System
collectively represents the network-side functionality that enables the
desired interactive service experience by communicating with, and delivering
interactivity application data and interactivity media to, the UE.
Figure 5.1: Service Interactivity Architecture for Unicast DASH
A high-level message sequence for interactivity event occurrence during a
unicast streaming program, assuming the use of DASH Events as the
interactivity trigger mechanism, is as follows:
1) Detection of an occurrence of a program-specific event, such as an injury
time-out during a live football game, provides indication to the Interactivity
System in the network that an interactivity notification and related event
data are to be sent to the interactivity agent function of the service
application in the UE.
2) The Interactivity System will produce related interactivity event data and
pass that information to the DASH Encoder/Segmenter.
3) and 4) The DASH Segmenter/Encoder will create one or more DASH Event >
messages as defined in [2], and send those either as MPD Events, > or inband
event messages together with the Segments (in the Event > Message box 'emsg')
to the DASH client, via the HTTP Server.
> 5a) or 5b) The DASH client delivers the Event message to the Interactivity
> Agent function residing either in the Multimedia Framework or in the Service
> Application.
>
> 6a) or 6b) (Optional) The Interactivity Agent (in the Multimedia Framework
> or in the Service Application) may fetch additional interactivity event data
> from the Interactivity System to execute the interactivity application
> logic, in turn creating the interactivity experience provided to the end
> user.
The required synchronization for the display of interactivity-related media
information to the user, relative to the main program, is handled by the
Interactivity Agent in the device in conjunction with the Interactivity
System. The start of the interactivity event and the sequence of scenes
displayed in the interactivity experience are supported by the dynamic, real-
time delivery of the associated DASH Event messages to the Interactivity
Agent. The DASH client is not involved in the processing of the Event messages
pertaining to interactivity, and merely transfers that data as an opaque
object to the Agent. The timing information and message data carried in DASH
Event messages enable the Interactivity Agent to execute the interactivity
logic and to display interactivity media at precisely the right times.
## 5.1.3 Broadcast DASH
Figure 5.2 depicts the proposed system architecture and high level sequence
flow for broadcast delivery of DASH streaming services with interactivity
support. Optional message steps are shown by dashed lines, and optional
functionality in support of interactivity are shown inside dashed boxes.
Figure 5.2: Service Interactivity Architecture for Broadcast DASH
A high level message sequence for the interactivity event occurrence during a
broadcast streaming program, assuming the use of DASH Events as the
interactivity trigger mechanism, is as follows:
1) and 2) Same as steps 1 and 2 in previous call flow.
```{=html}
``` 3) and 4) The DASH Segmenter/Encoder will create one or more DASH Event >
messages as defined in [2], and send those either as MPD Events, > or inband
event messages together with the Segments (in the Event > Message box 'emsg')
to the DASH client, via the BM-SC.
```{=html}
``` 5) The DASH client retrieves MPD and Segments from the local HTTP
proxy/cache in the UE.
> 6a) or 6b) The DASH client delivers the Event message to the Interactivity
> Agent function residing in either the MBMS Application or in the Multimedia
> Framework, whereupon the interactivity application logic may be executed by
> the Interactivity Agent in producing the interactivity experience for the
> end user.
>
> 7a) or 7b) (Optional) The Interactivity Agent in the MBMS Application or
> Multimedia Framework may fetch additional interactivity event data from the
> Middleware to execute the interactivity application logic, in turn producing
> the interactivity experience for the end user.
# 6 Interactivity Mechanisms in Broadcast and Broadband TV
This clause contains descriptions of interactive service framework and
mechanisms as defined in DVB/HbbTV and ATSC specifications
# 6.1 The HbbTV / DVB Interactive Environment
6.1.1 Introduction
The HbbTV specification [2] provides a platform for signalling, transport, and
presentation of enhanced and interactive applications designed to run on
hybrid terminals that include both a DVB compliant broadcast connection and a
broadband connection. The HbbTV platform is open and is not based on a single
controlling authority or aggregator; so services and content from many
different and independent providers are accessible by the same terminal.
Figure 1 in [2] provides the system overview of HbbTV.
Standard functions of the terminal are available to all applications;
sensitive functions of the terminal are available only to trusted
applications. HbbTV is applicable to various types of terminals, including
IDTVs, STBs and PVRs. Services and content may be protected. Both broadcast-
related and broadcast-independent applications are supported. Broadcast
applications can be presented on terminals which are not connected to
broadband.
In the context of HbbTV, the main uses of the broadcast connection are the
following:
  * Transmission of broadcast TV, radio and data services;
  * Signalling of broadcast-related applications;
  * Transport of broadcast-related applications and associated data;
  * Synchronization of applications and TV/radio/data services.
The main uses of the broadband connection are the following:
  * Carriage of both on-demand and live content;
  * Transport of broadcast-related and broadcast-independent > applications and associated data;
  * Exchange of information between applications and application > servers.
## 6.1.2 DVB Signalling and Carriage of Interactive Applications
The DVB specification for application signaling and carriage [3] provides a
framework for the signalling and carriage of interactive applications or
services in both broadcast and broadband networks, covering the following
aspects:
  * Signalling of interactive applications or services
> This includes how the receiver identifies the applications associated with a
> service and finds the locations from which to retrieve them. Signalling is
> included that enables the broadcast service provider to manage the
> lifecycles of applications, and that enables the receiver to identify the
> sources of broadcast data required by the applications of a service. All
> application signalling is carried in the Application Information Table
> (AIT), which is carried in the PMT of the broadcast stream, in an elementary
> stream of private sections. All application signalling is carried in the
> Application Information Table (AIT), which is carried in the PMT of the
> broadcast stream, in an elementary stream of private sections. The XML form
> of the AIT is used for application signalling on broadband networks.
  * Distributing the file resources of interactive applications or > services
> Carriage of file resources is specified for two cases: MPEG-2 DSM-CC Object
> Carousel for broadcast carriage, and HTTP 1.1 for carriage on broadband
> networks.
  * Synchronising interactive applications or services to video or audio > content
> Synchronisation is carried out by the use of DSMCC stream events, which can
> comprise either "do-it-now" events for immediate triggers, or stream events
> according to DVB timeline, for better timing accuracy. An XML equivalent of
> DSMCC stream event is defined, for usage on broadband networks.
  * Referencing video, audio or subtitle content from interactive > applications or services
> The URL form "dvb:" has been defined for referencing DVB services. The DVB
> application signalling specification is independent of any particular
> technology for interactive applications or services. It enables a wide range
> of different application models depending on which of the optional features
> are selected for the respective application environment.
## 6.1.3 HbbTV Platform Characteristics
Figure 2 of [2] provides an overview of the functional components of the HbbTV
terminal. HbbTV applications are presented by an HTML/JavaScript browser. The
terminal browser environment is based on:
  * OIPF Release 2:
    * Volume 5 - Declarative Application Environment
    * Volume 7 -- Authentication, service protection and content > protection
```{=html}
``` \- TV functionality JavaScript API (OIPF Volume 5)
  * CE-HTML (CEA-2014) (via OIPF Volume 5)
  * W3C DOM2, CSS2, XHTML (via OIPF Volume 5 and CEA-2014)
The supported media formats are summarised by:
  * OIPF Release 2, Volume 2 -- Media formats, which mandates support for H.264/AVC video and HE-AAC audio, with many more optional formats defined;
  * MPEG-DASH and --CENC;
  * ISOBMFF live profile (applied to both on-demand and live content).
6.1.4 HbbTV Specification Evolution
The first version (V1.1.1) of the HbbTV specification [2] was published in
June 2010. The specification was revised in November 2012 and published by
ETSI as [2] V1.2.1. This version is also commonly referred to as "HbbTV 1.5",
and it is the basis for all current HbbTV deployments. A further revision was
published recently by HbbTV in February 2015, and is commonly known as "HbbTV
2.0" [4]. The corresponding ETSI specification revision is under way.
The major new features of HbbTV V2.0 are:
  * Support for companion screens (tablets or phones) and their synchronisation to broadcast delivered content;
  * Privacy, based on W3C "do not track";
  * Subtitles for broadband delivered content, based on EBU-TT-D;
  * Interoperation with CI Plus V1.4;
  * Push VoD;
  * Technology updates, in particular:
```{=html}
``` \- HTML5;
  * Addition of HEVC video content;
  * MPEG-DASH delivered content according to DVB-DASH [5] (ETSI > equivalent expected to be published soon).
HbbTV 2.0 compliant receivers are expected to start appearing in the market in
2016.
# 6.2 ATSC Service Interactivity
6.2.1 Introduction to ATSC
ATSC, or Advanced Television Systems Committee, Inc., is an international,
non-profit organization which develops standards for digital television
transmission over terrestrial, cable, and satellite networks. It was formed in
1982 by the member organizations of the Joint Committee on InterSociety
Coordination (JCIC): the Electronic Industries Association (EIA), the
Institute of Electrical and Electronic Engineers (IEEE), the National
Association of Broadcasters (NAB), the National Cable Telecommunications
Association (NCTA), and the Society of Motion Picture and Television Engineers
(SMPTE). Its current member organizations represent the the broadcast,
broadcast equipment, motion picture, consumer electronics, computer, cable,
satellite, and semiconductor industries.
## 6.2.2 ATSC Service Interactivity Specification
ATSC published its "Interactive Services Standard" (ISS), A/105:2014 [6] in
2014, as part of a set of so-called ATSC 2.0 standards. It defined a set of
services for operating over the ATSC 1.0 Digital TV (DTV) transmission system,
one of which is user interactivity with the main program. The specification is
based on interactive applications specified in HbbTV [2] and which are in turn
derived from the Open IPTV Forum specifications. In particular, the ISS [6]
adopts the so-called Declarative Application Environment (DAE), which is a
declarative language, i.e. Web browser application environment based on
CEA-2014-A [7] for presentation of user interfaces, including scripting
support for interaction. The base set of technologies employed for the DAE in
ATSC's ISS includes HTML, CSS, DOM and ECMAScript objects, with APIs for TV
environments.
Three types of interactive applications, or Declarative Objects (DOs), are
defined in the ISS [6]:
1) Triggered DOs (TDOs), DOs which provide synchronized enhancements for
linear TV services. TDOs can be **be launched, suspended, terminated, or
caused to take special actions by "Triggers" delivered to receivers in real
time, via broadcast or unicast delivery, and tightly synchronized with the
main audio/video programming.** A Trigger is a data structure that provides
dynamic notification of the time incidence of an interactivity event, and is
bound to a particular program segment.
2) NRT DOs (NOs), providig interactivity for Non-Real-Time (NRT) services, and
whose operation is independent of Triggers.
3) Unbound DOs (UDOs), which are launched or terminated under user control:
    -   Clickable "Link" (bookmark) installed on the TV receiver, and
        > points to a remote UDO, or
    -   Clickable "Packaged Application", or widget installed on the
        > receiver, also referred to as a local UDO.
In ATSC 2.0, the data components that enable interactivity during linear TV
services are the following:
  * _TDO Parameters Table (TPT)_. Data structure containing information about the TDOs of a program and associated interactivity-related events initiated by Triggers
  * _TDOs (Triggered Downloadable Object)._ Downloadable interactivity application which may be associated with declarative content items (e.g. text, graphics, scripts and audio) to be displayed during the interactivity event. The functionality and behavior of a TDO is specific to the main program that it accompanies.
  * _Interactivity Content Items_. Data files required by the interactivity apps, for example media assets to be displayed during the interactivity event controlled by an application instance.
  * _Triggers_. Data objects, bound to a particular program segment, that perform location and timing-related signaling in support of interactivity.
## 6.2.2.1 Interactivity Data Delivery
For dynamic interactivity associated with linear services, TDOs, which define
the interactivity logic, are executed by the Triggers at precisely designated
times. Figure 6.1 illustrates the delivery alternatives of interactivity data
components in ATSC along with the main A/V content.
**Figure 6.1 -- Delivery Alternatives of Interactivity-related Data
Components**
As shown in Fig. 6.1, TDOs, TPT and Triggers can be delivered in different
ways, depending on the system architecture, broadcaster's preference, and
device capabilities, for example,
  * Within the broadcast stream:
    * In an NRT service as file objects delivered over FLUTE sessions, or
    * Within the DTV Closed Captioning DTVCC) channel, via extension to CEA-708.
  * Over the Internet:
    * Delivery of Triggers using ACR (Automatic Content Recognition) process, via watermarking (WM) or fingerprinting (FP), or
    * Delivery of TDO, TPT and Triggers by broadcaster's designated interactive TV server.
The reason for for the DTVCC and ACR approaches is that for redistribution via
cable or satellite TV system and set-top-box (STB) delivery to reach the TV
set, auxiliary content (such as NRT service delivered on dedicated virtual
channel) may be stripped out by the redistribution system technology such that
only signals encapsulated with the main audio or video components, such via
closed captioning, are not affected. Furthermore, in some scenarios, even the
DTVCC channel imay become unavailable, for example when HDMI interface is used
to connect the STB to the TV.
An example scenario whereby all interactivity data components are delivered
via broadcast is shown in Fig. 6.2 below:
**Figure 6.2 -- Main program and Interactivity Data Components are Delivered
via Broadcast**
In the above diagram, the SMT (Service Map Table), a type of service signaling
table, provides bootstrapping information to discover and access the FLUTE
session(s) in which the TPT and TDO, as NRT files, are delivered. The Triggers
are carried in the DTVCC caption stream and in the above example, comprises
two types: a Media Time Trigger and the Activation Trigger. The Media Time
Trigger provides the time base for synchronizing the interactivity events to
the playout time of the program segment. The Activation Trigger references the
TDO and associated Event ID that should be executed in providing the
interactivity features and display to the user, such as creation of displays
in specific locations on the screen, conducting polls, launching other
specialized DOs, etc., all synchronized with the audio-video program.
## 6.2.2.2 TDO Parameters Table
The TDO Parameters Table (TPT) contains the overall metadata about the
interactive events pertaining to a given segment of the main program. The
following information is described by the TPT:
  * one or more TDOs representing the interactivity application(s),
  * zero or more content items consisting of data files required by the TDO, e.g. media components, to be rendered during the interactivity event associated with the execution of that application;
  * one or more interactivity events targeted to a given TDO, each uniquely identified by an event-ID and defined action for the TDO when the event is activated (e.g. "execute" or "suspend").
**The data structure of the TPT is shown in Figure 6.3 below.**
**Figure 6.3 -- TDO Parameters Table (TPT) as Defined in A/105:2014 [6]**
### 6.2.2.3 Trigger Timing Example
A Trigger timing example for live content is shown in Figure 6.4:
**Figure 6.4 -- Example of Trigger Timing for a Live Program**
For live content, although the TPT contains data for different interactive
events, actual playout times of those events cannot be known until the action
in the program unfolds. Two types of Activation Triggers are shown in Fig.
6.4: the Retiming Event Trigger used to dynamically adjust interactivity event
time, and the Immediate Event Trigger which initiates immediate execution of
the interactivity event. In particular, nine triggers are shown in association
with the program segment number 2. The sequence of Trigger initiated events
are as follows:
1) A Pre-Load Trigger is sent to enable the receiver to acquire the TPT and
interactive content via FLUTE sessions for the program segment of concern, if
those contents have not been already downloaded;
2) A Media Time Trigger establishes the playout timing for segment #2;
3) A Retiming Trigger ndicates Event_1 in the TPT should be retimed to occur
at Media Time 250;
4) Another Media Time Trigger is sent;
5) A Retiming Trigger indicating that Event_4 in the TPT should be retimed to
occur at Media Time 455;
6) and 7): Additional Media Time Triggers;
```{=html}
``` 8) An Immediate Trigger indicates that Event_11 in the TPT should be
executed immediately;
9) A Retiming Trigger indicates Event_23 in the TPT should be retimed to occir
at Media Time 979.
# 7 Interactivity Support for Streaming and Download Services
_\ _
# 7.1 Component Model for Interactivity
Figure 7.1 depicts the proposed component model for service interactivity in
the context of live streaming service delivery. It is not a network
architecture, but represents how interactivity signaling and data components
as shown enable the launch and execution of the interactivity service logic in
providing the interactivity experience to end users. The model is applicable
to either unicast or broadcast/MBMS transport mode for the service.
Figure 7.1: Component Model for 3GPP Service Interactivity -- Live Streaming
Service Delivery
In this component model, triggers provides dynamic notification of the
occurrence of interactivity events. Triggers may contain or reference
application data, as well as references interactivity media that are used and
displayed by the interactivity application which is executed by the
interactivity agent. The execution of the interactivity application results in
the scene display of scenes associated with the interactivity sequence.
These building blocks of the component model are further described as follows.
  * _Triggers._ Triggers provides the dynamic notification of interactivity events and their timing during a content segment (main content or advertisement) to an interactivity-aware service application. The presentation of interactivity events to the end user involves the display of interactivity-specific media content, and may include explicit user engagement with that content. The trigger notification may be tied to a non-deterministic and real-time event such as a time-out called during a live football game. A potential trigger mechanism for interactivity event notification is DASH Events as defined in MPEG DASH standard [8].
  * _Interactivity Media._ These correspond to interactivity media content, such as video clips, images or text files, to be played out during the interactivity event launched by triggers.
```{=html}
``` \- _Application Data._ Application data pertains to the description of
scene information for an interactive sequence. It may include display icons,
layout information, or the text of display buttons or announcements to be
overlaid on, or presented inline with, the interactivity media. Application
data is referenced by, or could be directly carried in, the triggers (e.g. in
the usage of DASH Events, corresponding to the **Event** \@messageData
attribute, or the message_data field of 'emsg' box).
  * _Interactivity Sequence._ This comprises a set of one or more interactivity scenes to be rendered during an interactivity event. For example, an ad which includes sidebar display of a hyperlink for additional information on a sports car, and which when clicked by the user, may lead to an offer for entering sweepstakes drawing to win that car. The contents making up those scenes may consist of a combination of interactivity media s as described above, and application data.
```{=html}
``` \- _Interactivity Application._ An interactivity application is a
component of the service application that contains the logic associated with a
specific interactivity use case (i.e. separate logic pertaining to "Click for
Info" vs. "Voting" use cases). It may be delivered as a file to the device to
be stored in advance of the interactivity event occurrence.
  * _Interactivity Agent._ The interactivity agent is another component of the service application. It executes the interactive application logic and provides rendering capabilities of scenes to be displayed during an interactivity sequence. It may also implement a pre-determined overall layout for a given interactivity use case, for example, indicating where on the screen the interactivity content should be displayed relative to the main program. In one possible implementation, the Interactivity Application takes the form of a Web application (HTML/Javascript) and the Interactivity Agent is the web runtime engine required for interpreting and executing the app.
# 7.2 Interactivity Trigger Functions
The main functionality of the Interactivity Trigger are described in clause
7.1, and in the case of DASH service delivery, in the interactive service
architecture and related message flows indicated in clause as well in clause
5.1 of the TR. In addition, interactivity notification functionality is
described in clauses 6.1 and 6.2 for DVB and ATSC 2.0 interactive
applications/services, respectively.
Interactivity Triggers provide dynamic notification of the occurrence of
interactivity events, and related timing information. The trigger notification
may be tied to a non-deterministic and real-time event such as a time-out
called during a live football game. Triggers may contain or reference
application data, as well as references interactivity media that are used and
displayed by the interactivity application. Triggers and their associated
message contents are delivered to the UE, and are typically forwarded to and
processed by the responsible application entity, for example the Interactivity
Agent as described in clause 7.1 and as shown in Figure 7.1. In the case of
DASH services delivered over the unicast or MBMS bearers, and assuming the use
of DASH Events to convey the interactivity triggers notifications, DASH
Events, in the form of an Event Stream, are produced by the DASH Segmenter/MD
Generator. The DASH Event Stream is then sent to the DASH client by the HTTP
Server or BM-SC, via unicast or broadcast service delivery, respectively, to
be in turn forwarded to the Interactivity Agent as shown in Figures 5.1 and
5.2.
### 7.2.1 DASH Events
A potential trigger mechanism for interactivity event notification is DASH
Events as defined in MPEG DASH standard [8], and referenced in the Rel-13 3GP-
DASH specification, TS 26.247 [9]. DASH Events are logically generic
notification messages that can be contained either in the MPD or inband to the
Representation, to signal aperiodic information to the DASH client or to an
application. More details on the use of DASH Events as interactivity trigger
mechanism are provided in clause 9.1.1.
### 7.2.2 HTML5 Text Track
This interactivity trigger mechanism is specific to the use of Web
applications as the service application and/or interactivity agent which
executes the interactive application logic pertaining to the scenes to be
displayed during an interactivity event. It might not be applicable to a
service application/interactivity agent implemented as a native application,
i.e., written for a certain mobile device or platform. As defined in the W3C
HTML5 standard [10], a media element (e.g. audio, video) can have a group of
associated _text tracks_ , known as the media element's list of text tracks.
The text tracks corresponding to the \ element children of the media
element. The \ element and its associated text tracks provide a
standardized mechanism to add subtitles, captions, screen reader descriptions,
chapters and metadata to video and audio, via the attribute kind, which can be
subtitles, captions, descriptions, chapters or metadata. Each text track has a
corresponding TextTrack object. The track element's _src_ attribute points to
a text file that holds data for timed track _cues_ , which can potentially be
in any format a browser can parse.
The ability to carry structured data in cues enables flexible use of the track
element. For example, for interactivity support, the type (i.e., kind) of cue
data may be set to metadata, for use by Javascript. Such cue data could convey
interactivity notifications to which an interactivity-enabled Web application
can listen for as cue events, extract the text of each cue as it fires, parse
the data, and then use the results to make DOM changes for presentation of the
interactivity display, synchronized with media playback. One the other hand,
when cue data is not of a form such as captioning or subtitles, which can be
handled directly by the media player, the application is then responsible for
handling it. There can be issues, however, in the timing of how the
application receives this event data, which has been identified by the HbbTV
Association [11]. In particular, interactivity events of duration less than
250 msec may be missed by the application.
# 8 Summary of Application, Service and Transport Level Requirements in
Support of Service Interactivity
_\ _
# 9 Utilization of Existing Tools in MBMS and DASH
_\ _
# Interactivity Trigger Mechanism
### 9.1.1 DASH Events
As discussed previously in clause 7.2.1, DASH Events as defined in the MPEG
DASH standard [8], and referenced by TS 26.247 [9]. The characteristics of
DASH Events are as follows:
  * Events are timed -- the validity of an Event is defined by a specific media presentation time, and each Event typically has a duration.
  * An Event pertains to one of two types of notifications: 1) DASH-specific, or 2) application-specific. In the latter category, an appropriate scheme identifier is used to reference the application to which the DASH client will forward the Event.
  * Events of the same type are clustered in Event streams, i.e., a sequence of Event messages of the same type. A DASH client may subscribe to an Event Stream of interest and ignore all other, non-relevant Event Streams.
  * Each Event message with the Event stream may contain a message body, whose syntax and semantics is defined by the owner of the scheme identified by the scheme identifier.
  * Three types of DASH-specific Events are defined in the MPEG DASH standard [8]:
    * An Event message conveying the impending expiration of the > current MPD;
    * An Event message conveying the impending expiration of the > current MPD, and in addition, the Event message includes an > MPD _Patch_ , which complies to the XML Patch Operations > framework as defined in IETF RFC 5261 [12];
    * An Event message conveying the impending expiration of the > current MPD, and in addition, the Events message encapsulates > a complete (and valid) instance of an MPD that updates the > to-be-expired MPD
For application-specific Events (i.e., non DASH-specific Events as described
immediately above, the MPEG DASH standard [3] does not define the usage of
Events. Instead the related semantics and syntax are left to the owner of the
Event scheme and associated application, in the form of a \@schemeIdUri
attribute that provides a URI to identify the Event scheme and an optional
attribute \@value defining the value space of that scheme. Such usage of
\@schemeIdUri and \@value is in accordance to the use of MPD descriptors.
The use of MPD Events (Events signaled in the MPD) is indicated by the
presence of one or more **Period.EventStream** elements, each instance
denoting Events of a common type. The use of inband Events is signaled in the
MPD by one or more **InbandEventStream** elements in the **AdaptationSet** or
**Representation** element of the MPD. The functionality of an application-
specific Event message is the same regardless of whether it's carried in the
MPD or inband to a Representation.
The semantics of the **Period.EventStream** element is shown in Figure 9.1
below.
Figure 9.1: Semantics of the **EventStream** element in the MPD
### 9.1.1.1 DASH Events for Interactivity Triggers
DASH Events, either delivered in the MPD or inband to Segment, fulfil the
majority of the necessary functions for triggering the occurrences of service
interactivity. A key salient feature of DASH Events, given its definition at
the DASH/ISOBMFF level, is that it can be used as the interactivity event
notification mechanism for both interactivity-enabled Web applications and
native applications. Each Event conveys the start time of the associated
interactivity event (as presentation time of the event relative to the start
of the Period) and may optionally indicate the validity interval of the
interactivity event. In addition, the payload of the Event message, via either
the Period.EventStream.Event\@messageData attribute of MPD Events, or the
message_data [ ] field of the Event Message box 'emsg' for inband Events, can
convey the necessary data related to the interactivity event and application.
For example, the message may include an Event identifier, application data
pertaining to the layout of the interactive display, location where the
interactivity media assets to be rendered during the interactivity event can
be obtained, information on retiming of the occurrence of the interactivity
event, etc.
Another potentially salient feature of the DASH Events mechanism for use as
interactivity triggers is its basic built-in support for personalization of
the interactivity experience for the end user. Similar to the \@xlink:href
attribute in the **Period** element, the **EventStream** element in the MPD
may optionally include \@xlink:href. As described in the DASH-IF
Interoperability Points guidelines [13], as well as in TR 26.848 [14] on
targeted advertising functionality, there can be various ways to provide the
DASH client a customized remote element entity via XLink [15] resolution. In
this case, similar to returning a customized remote **Period** element
pertaining to a personalized Ad Period, a customized external **EventStream**
element may be returned by the XLink resolver, pertaining to personalized
interactivity event notification messages. Different collections of
interactivity event notifications whose components are timed to fire at
different times, and which may reference different interactivity application
or media data, can result in customized interactivity experiences depending on
the targeted end users. More recently, the MPEG DASH is undergoing amendments
to add support for advanced and generalized HTTP feedback information". Here,
the changes to the DASH spec pertain to client-side insertion of custom
parameters into HTTP GET requests, to enable customized content to be returned
to the HTTP response. In particular, the use of query template in the XLink
URL is specified.
The suitability of the DASH Events mechanism as interactivity triggers may
depend on how close in time the interactivity event fired by the trigger must
match the corresponding program incident that initiated the delivery of the
Event. The interactivity event signaled by an instance of the DASH Event
message is defined to start at a specific media presentation time (relative to
the start of the containing Period). For example, should **_the media content
be played out from the time-shift buffer, then the execution of the
interactivity event will be delayed by the amount of time shift in the actual
play-back of content. Therefore, DASH Events may be unsuitable as
interactivity triggers for launching interactivity features associated with
emergency alerts, for which the related interactivity experience must be
rendered very close in real time to the occurrence of the alert. On the other
hand, for live programs such as a football game or a car race, the program_**
content is typically consumed at the live edge, allowing the time-shift buffer
depth to be set to a small enough value such that **_media time and the real-
time are sufficiently close to each another to meet the requirement of the
service/content provider delivering the interactivity experience._**
# 10 Summary of Functional Gaps in MBMS and PSS Service Layer Specifications
on Interactivity Support
_\ _
# 11 Application/Presentation vs. Transport/Service Layer Functions on
Interactivity
_\ _
# 12 Measurement and Reporting of Interactivity Consumption
_\ _
# 13 Implementation Guidelines
_\ _
# 14 Summary and Recommendations
_\ _
#