# Foreword
This Technical Specification has been produced by the 3rd Generation
Partnership Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# 1 Scope
This document describes the service and performance requirements for the
operation of professional video, audio and imaging via a 5G system, including
a UE, NG-RAN and 5G Core network.
The aspects addressed in this document include:
\- Network service requirements specific for the operation of professional
video, imaging and audio for PLMN and non-public networks (NPN)
\- New key performance indicators (KPIs) for PLMN and NPN
\- KPIs for Multicast and Broadcast Services
\- Network Exposure Requirements
\- Clock synchronisation
\- Application Specific Requirements for video, imaging and audio
\- Mobile and airborne base stations for NPNs
\- Service continuity
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] Internet Engineering Task Force (IETF), IETF RFC 4175
[3] 3GPP TS 22.104: \"Service requirements for cyber-physical control
applications in vertical domains\"
[4] 3GPP TS 22.261: \"Service requirements for the 5G system\".
[5] ST 2059-2:2015 - SMPTE Standard -- \"SMPTE Profile for Use of IEEE-1588
Precision Time Protocol in Professional Broadcast Applications\"
[6] IEEE 1588-2008 - \"IEEE Standard for a Precision Clock Synchronization
Protocol for Networked Measurement and Control Systems\"
# 3 Definitions of terms, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**AV Production:** the process by which audio and video content are combined
in order to produce media content. This could be for live events, media
production, conferences or other professional applications.
**Compressed Video:** a means of making video file or stream sizes smaller to
meet various applications. Different applications have different compressions
methodologies applied.
**\- Mezzanine compression** : low latency and non-complex compression applied
to a video signal in order to maintain the maximum amount of information
whilst reducing the stream size to allow for the available bandwidth.
**\- Visually lossless compression** : the maximum amount of compression that
can be applied to a video signal before visible compression artefacts appear.
**\- Highly compressed** : use of compression to distribute content over very
low bandwidth connections where the content is more important than the quality
of the image.
**Communication service availability** : as defined in TS 22.261 [4].
**Communication service reliability:** as defined in TS 22.104 [3].
**End-to-end Latency:** as defined in TS 22.261 [4].
**Isochronous** : The time characteristic of an event or signal that is
recurring at known, periodic time intervals.
NOTE 1: Isochronous data transmission is a form of synchronous data
transmission where similar (logically or in size) data frames are sent linked
to a periodic clock pulse.
NOTE 2: Isochronous data transmission ensures that data between the source and
the sink of the AV application flows continuously and at a steady rate.
**Imaging System Latency:** The time that takes to generate an image from a
source, to apply a certain amount of processing, to transfer it to a
destination and then to render the resulting image on a suitable display
device, as measured from the moment a specific event happens to the moment
that very same event is displayed on a screen.
**In-Ear-Monitoring (IEM):** A specialist type of earphone usually worn by a
performer in which an audio signal is fed to a wireless receiver and attached
earphone.
**Media Clock:** Media clocks are used to control the flow (timing and period)
of audio / video data acquisition, processing and playback. Typically, media
clocks are generated locally in every mobile or stationary device with a
master clock generated by an externally sourced grand master clock.
NOTE 3: Currently GPS but transitioning to 5G in future.
**Mouth-to-ear Latency:** End-to-end maximum latency between the analogue
input at the audio source (e.g. wireless microphone) and the analogue output
at the audio sink (e.g. IEM). It includes audio application, application
interfacing and the time delay introduced by the wireless transmission path.
**Non-public network:** as defined in TS 22.261 [4].
**Survival time:** as defined in TS 22.261 [4].
**Uncompressed Video:** Uncompressed video is digital video that either has
never been compressed or was generated by decompressing previously compressed
digital video.
NOTE 4: RTP payload is described in [2].
**Video, imaging and audio:** The means of digital capture, transmission and
storage of still and moving pictures and sound for professional use.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
> T~frame~ Time interval between consecutive audio frames at application
> layer. Also used to denote the transfer interval in this document.
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
> AV Audio-Visual
>
> IEM In-Ear-Monitoring
# 4 Overview on video, imaging and audio professional applications
## 4.1 Introduction
The present document introduces requirements related to professional video,
imaging and audio services. Unlike other consumer multimedia applications
envisioned for 3GPP systems, the applications in which this document focuses
have more demanding performance targets and includes user devices that are
managed in different workflows when compared to typical UEs.
This document focuses on services for the production of audio-visual data for
any area that requires high quality images or sound. This may include AV
production, medical or gaming applications.
To enable devices such as professional cameras, medical imaging equipment and
microphones to use the 5G network either directly or via the addition of a
dedicated intermediate technology certain key parameters are required.
## 4.2 Key parameters
### 4.2.1 System latency
The overall system latency has an important impact on the applications that
this specification targets. In video production, overall system latency is
referred to as imaging system latency and has an impact on the timing of
synchronized cameras. For audio applications, overall system latency is
referred to as mouth to ear latency and it is critical to maintain lip sync
and avoid a performer to be put off by hearing their own echo. Finally, in
medical applications the system latency impairs the achievable precision at a
given gesture speed as it translates the time needed to traverse the whole
imaging system into a geometrical error of the instruments position.
Figure 4.2.1.-1 depicts the general functional blocks of an AV production or
medical system.
{width="6.75625in" height="2.08125in"}
Figure 4.2.1-1: Overall system Latency for video, imaging and audio
applications.
The overall system latency comprises different latency elements as illustrated
in Figure 4.2.1-1, where:
T1 = Time for image or audio frame generation
T2 = T4 = Time Delay through 5G Network, defined as the end-to-end latency in
TS 22.261 [4]
T3 = Application processing time
T5 = Time for image display or audio playback
So that the overall system latency results from the sum of the of T = T1 \+ T2
+ T3 + T4 + T5
### 4.2.2 Bandwidth
Video and imaging applications have extremely high bandwidth requirements and
while compression may be used to mitigate this in certain user cases it often
degrades the picture to the extent onward processing required by some
applications is compromised. For Video Production certain standards have been
determined which indicate the maximum allowable compression for a given type
of production. In medical imaging, compression may introduce artefacts which
can impact on diagnosis of critical illness and may also introduce additional
delays which, in image assisted surgery, translate into misalignment between
perceived instruments position on screen and their real position into
patients' body.
### 4.2.3 Reliability
Reliability is another key parameter for VIAPA. Late or lost packets can
result in dropped audio/video frames or inconsistency of motion which can
degrade a video or audio signal to below acceptable levels.
## 4.3 AV production
AV production includes television and radio studios, live news-gathering,
sports events, music festivals, among others. Typically, numerous wireless
devices such as microphones, in-ear monitoring systems or cameras are used in
these scenarios. In the future, the wireless communication service for such
devices could potentially be provided by a 5G system. AV production
applications require a high degree of confidence, since they are related to
the capturing and transmission of data at the beginning of a production chain.
This differs drastically when compared to other multimedia services because
the communication errors will be propagated to the entire audience that is
consuming the content on both live and on recorded outputs. Furthermore, the
transmitted data is often post-processed with filters which could actually
amplify defects that would be otherwise not noticed by humans. Therefore,
these applications call for uncompressed or slightly compressed data, and very
low probability of errors. These devices will also be used alongside existing
technologies which have a high level of performance and so any new
technologies will need to match or improve upon the existing workflows to
drive adoption of the technology.
The performance aspects that are covered in this document also target the
latency that these services experience. Since these applications involve
physical feedback on performances that are happening live, the latency
requirements are very strict. One example is the transmission on professional
microphones and in-ear monitors. These systems provide feedback for what the
musicians are playing, and even small delays may affect their sensation of
timbre, and ability to keep to the tempo of the music.
This document also refers to how the network structure of the 5G system is
configured in order to accommodate these applications. Many of these are
nomadic scenarios that require simplified deployment often in different
countries. For this reason, this the 5G system should enable non-public
networks that can be deployed in an agile ad-hoc way.
AV production also relies on a number of other technologies that will be
deployed by a 5G system such as the use of UAV's to capture video and high
bandwidth connectivity for file transfer. Some aspects of specific 5G
specifications such as direct communications between devices or
multicast/broadcast could also be used to enable future user cases such as the
connection of microphones to cameras and cameras to video monitors. Where this
is the case then these requirements will be in line with the specifications in
those specific areas.
AVProd workflows also require accurate timing protocols for 2 reasons
  1. To enable multiple cameras and microphones to be synchronized thus avoiding the capture of mis-matched audio and video.
  2. To provide IEEE-1588-2008 PTP [6] with an SMPTE 2059-2 [5] profile which is used for the accurate time stamping of IP packets
It is anticipated that the 5G system will act as a master clock and media
clocks will be generated by UE applications. Requirements for this are in line
with those in 22.104. If suitable sources are available, then each device my
operate from its own master clock
## 4.4 Medical applications
\"Medical applications\" is a generic concept covering medical devices and
applications involved in the delivery of care to patients.
Medical applications deployed into operating rooms consume communication
services delivered by a 5G system over an NPN. In this document, we'll deal
with hybrid operating rooms which are rooms typically equipped with advanced
imaging systems such as e.g. fixed C-arms (x-ray generator and intensifiers),
CT scanners (Computer Tomography) and MR scanners (Magnetic Resonance). The
whole idea is that advanced imaging enables minimally-invasive surgery that is
intended to be less traumatic for the patient as it minimizes incisions and
allows to perform surgery procedure through one or several small cuts. This is
as an example useful for cardio-vascular surgery or neurosurgery to place deep
brain stimulation electrodes.
In hybrid rooms, the different type of medical images that can be transmitted
by 5G systems and processed by medical applications are e.g.:
\- Ultra-high-resolution video generated by endoscopes where it is expected
that some scopes will produce up to 8K uncompressed (or compressed without
quality loss) video, with the perspective to support also HDR (High Dynamic
Range) for larger colour gamut management (up to 10 bits per channel) as well
as HFR (High Frame Rate), i.e.; up to 120 fps. This will allow surgeons to
distinguish small details like thin vessels and avoid any artefact that could
potentially conduct surgeons to take wrong decisions.
\- 2D Ultrasound images: A 2D ultrasound typically produces a data stream of
uncompressed images of 512x512 pixels with 32 bits per pixel at 20 fps (up to
60 fps in the fastest cases), resulting in a data rate of 160 Mbit/s up to 500
Mbit/s.
\- 3D Ultrasound volumes: Dedicated 3D probes tend to work at higher data
rates, i.e. above 1 Gbit/s of raw data, and are expected to reach multi
gigabit data rates in future (e.g. producing 3D Cartesian volumes of 256 x 256
x 256 voxels each encoded with 24 bits at 10 volumes per second or better).
\- CT/MR scans: Images can range from a resolution of 1024x2024 to 3000x3000
pixels where higher resolutions are used for diagnosis purpose and lower ones
are more suitable to fluoroscopy. In general, the frame rate is variable (5 to
30fps typically) where higher values are used to monitor moving organs in real
time. Finally, colour depths of 16bits is considered in general.
In another deployment option, when specialists and patients are located at
different places, medical applications can consume communication services
delivered by PLMNs. In this case, the 5G system helps decoupling location from
quality of care, and save countless hours for doctors and surgeons, who will
be able to "beam" themselves to operating rooms, incident sites and medical
houses, rather than having to be physically present.
The same type of images as in hybrid rooms is assumed when considering a
communication over a PLMN although with different tradeoffs on image
resolution, end to end latency and compression algorithms. The key here is to
allocate the necessary high priority resources fulfilling SLAs suitable to the
transport of medical data (with special care taken on medical data integrity
and confidentiality) over a geographical area covering the place where the
care is delivered.
Finally, in all types of deployments, it shall also be noted that each
equipment involved in image generation, processing and display shall be
synchronized thanks to a common clock either external or provided by the 5G
system. The synchronization is often achieved through dedicated protocols such
as e.g. PTP version 2 and allows to e.g. guarantee correct recombination of
two data streams in a single and accurate A/R image by the A/R application, or
enable offline replay of the whole procedure.
# 5 Service requirements
## 5.1 Non-public network requirements
The 5G system enables an NPN for video, imaging and audio for professional
applications. The related requirements are described in 3GPP TS 22.261 [4]:
\- Generic NPN requirements can be found in clause 6.25.
\- Requirements on the subscription aspects can be found in clause 6.14.
\- Authentication requirements can be found in clause 8.3.
## 5.2 Application specific requirements
The 5G system shall support media flows from open standard based broadcast
workflows and be agnostic to the data carried.
## 5.3 Clock synchronization
The 5G network shall be able to provide a time reference information to a
3^rd^ party application acting as a master clock with an accuracy of 1
microsecond.
## 5.4 Network exposure requirements
The 5G system shall support mechanisms to allow 3^rd^ party application to
update information associated to UE configuration (e.g. media compression,
resolution, frame rate) for a UE or group of UEs using the application.
## 5.5 Service continuity
The 5G system shall be able securely reconnect within a short period of time
(\99.99999 | >1 year | 99.9999 | >1 year | 99.9999 | >1 year | 99.99 | >1 month | 99.999 | >>1 month (99.999 | >>1 month (99.99999 | >1 day | 99.9999 | >1 day | < 250 ms | < [2 Gbit/s] | DL | ~1500 - ~9000 (note 1) | ~16 ms | stationary | <10 | 400 km  
NOTE 1: MTU size of 1500 bytes is not generally suitable to gigabits connections as it induces many interruptions and loads on CPUs. On the other hand, Ethernet jumbo frames of up to 9000 bytes require all equipment on the forwarding path to support that size in order to avoid fragmentation. |  |  |  |  |  |  |  |  |  |   
#