# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
# 1 Scope
**This Technical Report describes relevant use-cases and proposes respective
potential service requirements for 5G systems to support production of audio-
visual (AV) content and services.**
**Previous work assessed certain aspects for local applications (e.g. ultra-
reliable low-latency and time synchronization demands.) This study addresses
implications for 3GPP from wide-area media production and additional local
applications. Topics to be studied include demanding locally-distributed
production scenarios or ad-hoc deployments of high-bandwidth networks
providing increased mobility and coverage and lowest streaming latencies.**
Investigation is considered for example:
\- provision of pre-defined bandwidth capacity, end-to-end latency and other
QoS requirements for e.g. larger live music events or high-quality cinematic
video production;
\- time synchronisation among all devices (cameras, microphones, in-ear
monitors etc.), optionally using a production-based master clock or time code
generator, which is broadcasted;
\- coverage-related issues dealing with nomadic and ad-hoc production
deployments, future ways for electronic news gathering, usage of airborne
equipment and support for higher ground speeds of up to 400 km/h;
\- interoperability issues related to existing audio-visual production
standards and protocols;
\- dependability assurance and related topics (network isolation, QoS
monitoring etc.).
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] J. Pilz, B. Holfeld, A. Schmidt and K. Septinus, \"Professional Live Audio
Production: A Highly Synchronized Use Case for 5G URLLC Systems,\" in  _IEEE
Network_ , vol. 32, no. 2, pp. 85-91, March-April 2018.
[3] European Broadcasting Union (EBU), EBU Tech 3371, tech.ebu.ch, 2018
[4] Internet Engineering Task Force (IETF), IETF RFC 4175
[5] 3GPP TR 22.261: \"Service requirements for the 5G system\"
[6] 3GPP TS 22.179: \"Mission Critical Push to Talk (MCPTT) over LTE\"
[7] 3GPP TS 22.468: \"Group Communication System Enablers for LTE (GCSE_LTE)\"
[8] ST 2110-10:2017 - SMPTE Standard - \"Professional Media Over Managed IP
Networks: System Timing and Definitions\"
[9] IEEE 1588-2008 - IEEE Standard for a Precision Clock Synchronization
Protocol for Networked Measurement and Control System
[10] SMPTE ST 2059-2:2015x SMPTE Profile for use of IEEE-1588 Precision Time
Protocol in Professional Broadcast Applications
[11] SMPTE 2042-1:2017 -- SMPTE Standard -- VC-2 Video Compression
[12] 3GPP TS 23.501: \"System architecture for the 5G System (5GS)\"
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**AV Contribution:** Audio or video content sent from a location to a
broadcast centre to become programme content.
**AV Production:** The process by which audio and video content are combined
in order to produce media content. This could be for live events, media
production, conferences or other professional applications.
**Audio Clean Feed (mix minus feed):** Programme output (minus audio
contribution) sent from broadcast centre to reporter so the reporter can hear
studio output also known as a mix-minus feed.
**Broadcast Centre:** A location from where production and distribution are
co-ordinated. This may include studio facilities and/or technical areas where
content is received, routed, created and managed. Typical broadcast centres
act as hubs for live content as well as playout centres for pre-recorded
material.
**Broadcast over IP:** Carriage of broadcast signals over an IP network.
**Clock Synchronisation Service:** The service to align otherwise independent
UE clocks.
**Clock Synchronicity:** The maximum allowed time offset within the fully
synchronised system between UE clocks.
NOTE: This definition was taken from clause 3.1 in 3GPP TS 22.104 \" Service
requirements for cyber-physical control applications in vertical domains\"
**Control Room:** The place in a broadcast centre where outside sources are
received, monitored and routed to the production gallery.
**Compressed Video:** A means of making video file or stream sizes smaller to
meet various applications. Different applications have different compressions
methodologies applied.
  * **Mezzanine compression** : low latency and non-complex compression applied to a video signal in order to maintain the maximum amount of information whilst reducing the stream size to allow for the available bandwidth.
  * **Visually lossless compression** : the maximum amount of compression that can be applied to a video signal before visible compression artefact appear.
  * **Highly compressed** : use of compression to distribute content over very low bandwidth connections where the content is more important than the quality of the image.
**Cue / Talkback:** Audio messages sent from broadcast centre to location
usually to instruct a presenter when to speak. This is not audible in the
broadcast audio.
**End-to-end Latency:** the time that takes to transfer a given piece of
information from a source to a destination, measured at the communication
interface, from the moment it is transmitted by the source to the moment it is
successfully received at the destination.
NOTE: This definition was taken from clause 3.1 in 3GPP TS 22.261 \"Service
requirements for the 5G system\"
**Isochronous** : The time characteristic of an event or signal that is
recurring at known, periodic time intervals.
NOTE 1: Isochronous data transmission is a form of synchronous data
transmission where similar (logically or in size) data frames are sent linked
to a periodic clock pulse.
NOTE 2: Isochronous data transmission ensures that data between the source and
the sink of the AV application flows continuously and at a steady rate.
**In-Ear-Monitoring (IEM):** A specialist type of earphone usually worn by a
performer in which an audio signal is fed to a wireless receive device and
attached earphone.
**Media Clock:** Media clocks are used to control the flow (timing and period)
of audio / video data acquisition, processing and playback. Typically, media
clocks are generated locally in every mobile or stationary device with a
master clock generated by an externally sourced grand master clock (currently
GPS but transitioning to 5G in future).
**Mouth-to-ear Latency:** End-to-end maximum latency between the analogue
input at the audio source (e.g. wireless microphone) and the analog output at
the audio sink (e.g. IEM). It includes audio application, application
interfacing and the time delay introduced by the wireless transmission path.
**Multi-Cam:** The use of two or more cameras in an outside broadcast which
can be cut between; important considerations are colour imagery, timing,
framing and picture size and frequency.
**Network Media Open Specifications:** A set of open specifications that
describe how media devices are managed on a network.
**Network Operator** : The entity which offers 3GPP communication services.
**Outside Broadcast:** A production where content is being acquired away from
the broadcast centre and controlled from the location. Generates output for
broadcast which may be sent back to the broadcast centre for inclusion into a
programme or for onward distribution.
**Public Address System:** An electronic system increasing the apparent volume
(loudness) of acoustic sound sources. They are used in any public venue that
requires amplification of the sound sources to make then sufficiently audible
over the whole event area.
**Programme Making and Special Events:** This is a term used, typically in
Europe, to denote equipment that is used to support broadcasting, news
gathering, theatrical productions and special events, such as cultural events,
concerts, sport events, conferences and trade fairs. In North America, the use
of spectrum to provide these services is usually called broadcast auxiliary
service.
**Production (TV) Gallery:** An area in a TV studio where producers, directors
and technical staff work together to produce content. Functions include
control of cameras, lighting, sound and video feeds bringing together feeds
from both local and outside sources.
**Production (TV) Studio:** An area used to create media content usually
consisting of a studio floor with cameras, presenters and microphones which
are controlled from the production gallery.
**Quasi Error Free** : This refers to the reception of less than one
uncorrected event per hour at the input of any given receiver.
**Radio Microphone:** A microphone that uses a wireless connection to transmit
either an analogue or digital audio channel on a dedicated radio frequency or
multiplex to one or more dedicated receivers which then output an audio signal
suitable for onward processing.
**Remote Production:** Content being acquired is remote to the broadcast
centre but configured and controlled from the broadcast centre. this may
include video or audio content but also command and control functions to
operate the technical facilities located at the outside broadcast site.
**SMPTE 2110:** The SMPTE ST 2110 standards suite specifies the carriage,
synchronization, and description of separate elementary AV essence streams
over IP for real-time production, playout, and other professional media
applications [8].
**Uncompressed Video:** Uncompressed video is digital video that either has
never been compressed or was generated by decompressing previously compressed
digital video. RTP payload is described in [4].
**VC-2** : Low-latency, low-complexity video compression algorithm as
described in SMPTE 2042 [11].
**Video Clean Feed:** Studio output with no graphics or text overlays.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
T~frame~ Time interval between consecutive frames at application layer. Also
used to denote the transfer interval in this document.
T~processing~ Latency introduced by the processing of audio data
T~radio\ misalign~ Latency introduced by awaiting radio opportunity to
transmit data
T~delay~ Latency introduced by protocol layers required for data transmission
T~end-to-end~ End-to-end latency including T~radio\ misalign~ and T~delay~
T~audio\ async~ Latency introduced by re-alignment of audio frame boundaries
on different devices with respect to one another
T~latency~ The overall latency experienced by the user calculated from the
above values
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
AV Audio-Visual (can include both audio and video combined, or either
separately)
IEM In-Ear-Monitoring
LMPF Live Media Production Function
MTU Maximum transmission unit
NMOS Network Media Open Specifications
OB Outside Broadcast
QEF Quasi Error Free
PA Public Address
PMSE Programme Making and Special Events
RTP Real-Time Transport Protocol
SMPTE Society of Motion Picture and Television Engineers
# 4 Overview
The 3GPP system already plays an important role in the distribution of audio-
visual (AV) media content and services. Release 14 contains substantial
enhancements to deliver TV services of various kinds, from linear TV
programmes for mass audiences to custom-tailored on-demand services for mobile
consumption. However, it is expected that also in the domain of AV content and
service production, 3GPP systems will become an important tool for a market
sector with steadily growing global revenues. There are several areas in which
3GPP networks may help to produce audio-visual content and services in a cost
efficient and flexible manner.
AV content and service production can be broadly categorized. The most obvious
distinction is production within a fixed production environment versus
production at a location outside the premises of a production company.
Furthermore, live or non-live productions may come with very different
requirements. Mobile 3G and 4G networks are utilized quite frequently
nowadays. Several mobile devices are employed simultaneously in order to
achieve required data rates and guarantee stable communication. In the
broadcasting world this is called bonded cellular contribution.
Newsgathering is an AV production category which is vital for broadcasting
companies around the world. Their job is to offer news covering any kind of
event or incident which may be of interest to the public. This refers to
events which cannot be planned as they just happen. Incidents in politics and
economy or natural catastrophes often occur without notice and production
companies need to react swiftly. The time to set up equipment, for example a
local communication network, is a crucial factor. As soon as an important
incident becomes known a newsgathering team is sent to some location to cover
what is happening. Reporters may capture audio and video which need to be sent
to the home base production facilities. This requires fast and efficient
communication links. In newsgathering high levels of data compression may be
acceptable if no communication is otherwise possible. For HD video feeds 5-10
Mbit/s are needed as minimum.
In a typical setting of newsgathering more than one camera is used. Depending
on the circumstances a single camera may be fed back to a central production
facility or, sometimes several distributed single cameras are fed
simultaneously. However, quite often, multiple cameras are fed into a local
vision mixer/switcher before being sent as a single stream back to the
production facility. The latter is called a multi-camera feed. In this case,
operator communication on the location of the event or incident needs to be
established as well. Furthermore, all devices such as cameras and mixers are
operated by the production crew at the location of the incident. News-
gathering may take place outdoors or indoors.
One use case often occurring in production is the ability to transfer file-
based AV content or other assets to and from the broadcasting facility. For
example, programmes that are pre-produced at the event location and need to be
made available in the broadcaster playout system to illustrate a live
contribution. Another example is if the mixing of the live signal is performed
at the event location and archive clips or video overlays need to be available
for insertion into the programme. The difference compared with the live feed
transmission is that this material is sent between the two locations but not
necessarily in real time, usually as a file. This means that two-way file
transfer capabilities need to be available to upload or down load files on
location. These files are usually extremely large (> 1GByte per file) and
transfer speeds need to be capable of delivering this within a reasonable
timescale, although not necessarily as fast as real time. Support for growing
files is also useful so that an editor on location can start work on a clip
without waiting for the whole file to be delivered.
Another important category of AV production is called "Outside Broadcast"
(OB). In contrast to newsgathering the date of an event is sometimes known a
long time before it actually takes place. Examples are elections or sport
events such as football championships or the Olympic Games. Notice period
aside, OB productions are quite similar to news-gathering in terms of setting,
however, the scale of the event is usually larger. More equipment and more
people are required and very likely for a longer period of time. Usually, a
large number of wireless audio links (e.g. 100+) and several wireless video
cameras (e.g. 20+) are employed in one regular single event. They have to be
carefully synchronized in time, at the moment of recording and capture, in
particular in live production as well as transmitted with the associated
timestamp or delta to a master clock. Large scale events could also utilize
several hundred remote microphones and cameras not involved in the main
broadcast which could be mobile or stationary and all competing for bandwidth.
The equipment, devices and communication infrastructure used today is carried
to the location of the event using large vans. These OB vans act as a
communication hub for the event. They are potentially capable of supporting
many cameras, microphones, mixers, etc. On-location, reliable and scalable
wireless communication links between directors, technicians and other staff
are needed, in particular audio links.
Satellite or IP connections are typically established for OB productions to
send audio and video content back to the base production facilities. More
recently there is a trend becoming more and more important to remote control
production equipment, for example cameras from the central home base
production facilities rather than on location. Remotely operated equipment
requires reliable telemetry and control communications. The quality of audio
and video in OB productions are high, calling for potent communication links
in terms of data rates and data capacity.
Most OB productions take place in a defined location. However, there are also
events which are not stationary. Coverage of cycling events is a typical
example. This requires the production team to follow the event including
carrying production equipment along the way. Communication hubs in OB vans are
often replaced by helicopters and planes. These kinds of events also come with
the requirement to cope with very high velocities. In Formula 1 races the
cameras mounted on the vehicles need to be operated at speeds up to 400 km/h.
Even though today audio and video material is sent back to the home base
production facility for post-processing in order to prepare the final TV or
radio services, there is a growing trend to carry post-processing remotely.
This requires the ability to access resources from the base production
facility as well as utilizing cloud services be it computational power or
storage.
In addition to production outside the premises of production companies,
studio-based production is of paramount importance. Most studios currently use
mainly wired and purpose-built communication infrastructure, which can be is
costly and inflexible. Many production studios still utilize fixed line
connections between cameras, mixers and galleries. However, in order to become
more flexible and agile, fully wireless workflows would be preferable. Studio
productions are typically where the highest quality and communication
requirements are encountered. While under mobile or nomadic conditions
concessions can be made regarding the maximum available data rate for data
transfer this is not the case for studio productions and uncompressed or at
least loss-less data transmissions should be utilized. Uncompressed TV signals
can require a network bandwidth of over 12 Gbit/s for a high-resolution high
frame-rate video.
Covering an event which takes place on a stage in a theatre or a concert hall
lies somewhat between an OB and a studio production. Quite often there is
infrastructure available at the location of the event which can be used by
production companies. The question of seamless cooperation between different
infrastructures arises under this condition such that the production
requirements can still be met.
Capturing a stage event involves many wireless microphones, in-ear monitors,
and a variety of other service links. In a typical professional live-
performance scenario, performers on stage use wireless microphones while
hearing themselves via the wireless in-ear monitor system. The audio signals
coming from the microphones are streamed to a mixing console, where different
incoming audio streams are mixed into several outgoing streams, for example
the Public Address (PA), the in-ear monitoring mixes or recording mixes. These
applications come with stringent requirements in terms of end-to-end latency,
jitter, synchronicity, communication service availability, communication
service reliability and number of wireless links per site. For complex stage
productions the number of simultaneous links might be very high, i.e. more
than 100 in the same location.
Conventional broadcast signals have been carried over dedicated
infrastructure. In recent years broadcast centres have been moving to
commodity IP-based workflows. This has several benefits but has meant
significant work on the definition of IP streams that carry audio, video and
data. The standards bodies who have defined these systems are actively looking
at how these protocols may be carried by a wireless network. It is desirable
that 3GPP contribution should be compatible with these best practice
architectures to make interfacing and adoption as simple as possible.
Current best practice for IP production infrastructure is set out in EBU Tech
3371 [3].
3GPP offers production teams and broadcasters the opportunity to explore new,
more flexible, reliable and mobile ways of creating content. In order to
achieve this ambition, it is desirable that the 3GPP system is capable of
meeting requirements latency, reliability, synchronization and bandwidth.
Applications may be deployed on both PLMN and NPN.
# 5 Use cases
## 5.1 On-site Live Audio Presentation
### 5.1.1 Description
In a typical on-site live audio presentation situation, one or several persons
(presenters) are holding a talk in front of an interested audience. Usually
the audience interacts with the presenter/s, for instance by posing questions.
Other scenarios include the moderation of corporate events, panel discussions
or conferences.
On-site live audio presentation scenarios are typically confined to a local
area, e.g. conference rooms, lecture halls, press centres and trade fairs.
They can be located indoors or outdoors. Typical operation has a defined
duration known in advance. Characteristic for this use case is that all
production equipment is available at the location, the wireless communication
service is limited to the local area and all audio processing such as audio
mixing is done in real time.
Wireless microphones are used for capturing audio from presenters within the
local service area. A number between 5 and 300 simultaneously active wireless
microphones can be expected. These wireless microphones can be scattered into
different rooms, stages or spaces within the same complex.
The captured audio signals are transmitted to a central audio mixing console.
Each active wireless microphone produces an audio, which has a data rate
dependent on the audio codec, and may vary between 50 kbit/s and 250 kbit/s,
possibly higher, if uncompressed audio is required. In addition, control
signals with a maximum of 50 kbit/s are transferred bidirectionally for
management and remote-control purposes. The audio mixing console creates the
new desired audio streams. These streams delivered to downstream equipment and
applications, such as amplifiers and loudspeakers of a public-address system,
streaming services for hearing impaired participants, translation services,
recordings, etc.
The typical mouth-to-ear latency between the wireless microphone (analogue
audio source) and the audio sink (e.g. loudspeaker, assistive listening
device) is below 20 ms. For small scenarios where the distance between
presenters and listeners is very short, the mouth-to-ear latency is below 5ms.
In a live scenario, where a PA is used, the mouth-to-ear latency related to
the speaker must not exceed 20 ms, otherwise performance may be impaired. If
part of the audience is very close to the speaker the direct audio signal will
arrive earlier at the listener's ear than the PA audio signal, which leads to
an uncomfortable experience. Therefore, the system latency shall be below 10
ms for most of the cases, and below 5 ms for acoustically challenging
surroundings.
The packet error ratio of the wireless transmission needs to be kept lower
than 10^-5^ to assure that no audio dropouts or audible interference occur. A
packet error ratio above 10^-5^ would likely lead to disturbance of the
audience and degradation of the audio content at the very beginning of the
production value chain.
Table 5.1.1-1 lists the system parameter and its value range for the on-site
live audio presentation use case.
Table 5.1.1-1: System parameters for on-site live audio presentation use case
+----------------------+----------------------+----------------------+ | | Characteristic | Comment | | | system parameter | | +======================+======================+======================+ | **Mouth-to-ear | 5 ms - 20 ms | End-to-end maximum | | latency** | | latency between the | | | | input at the audio | | | | source (wireless | | | | microphone) and the | | | | output at the audio | | | | sink (PA). It | | | | includes | | | | application, | | | | application | | | | interfacing and the | | | | time delay | | | | introduced by the | | | | wireless | | | | transmission path. | +----------------------+----------------------+----------------------+ | **Audio data rate** | 50 kbit/s -- 1.5 | Different user data | | | Mbit/s | rates per audio | | | | stream need to be | | | | supported for | | | | different audio | | | | demands (e.g. | | | | compressed vs. | | | | uncompressed audio). | +----------------------+----------------------+----------------------+ | **Control data | 1 kbit/s - 10 kbit/s | Data rate per | | rate** | | control stream | | | | (UL/DL) | +----------------------+----------------------+----------------------+ | **Packet error | \  NOTE 2: This is the maximum end-to-end latency allowed for the 5G system to
> deliver the service in the case the end-to-end latency is completely
> allocated to the 5G system from the UE to the Interface to Data Network.
NOTE 3: Latency is less important than QEF, longer latency can be accepted to avoid errors (e.g. existing satellite delays can be up to 5 seconds). |  |  |  |  |  |  |  |  |  |   
[PR 5.5.6-003]: The 5G system shall be able to distribute high-precision
timing information as per the Precision Time Protocol (IEEE 1588-2008) [9] in
SMPTE ST2059 profile [10]. To enable the synchronous timing of broadcast
devices over the network and to provide data to enable the use of a UE media
clock
[PR 5.5.6-004]: The 5G system shall have the ability to support open standard
based broadcast workflows.
## 5.6 Single- source compressed Outside Broadcast Contribution
### 5.6.1 Description
When working remotely from a broadcasting centre there is a requirement for
producers, journalists and cameras crews to contribute audio, video and other
data back to a broadcast centre for inclusion in a TV programme.
This is of a high quality using professional video cameras and other
equipment. In some instances, this may use mobile phone technologies.
It may be a single live stream of content that is intended to go straight to
air in a news programme. Or a longer OB such as a live sporting event
In this use case compressed video may be used as it will go straight into a
programme with little or no further signal processing. Speed is more important
that quality in this use case.
There is also a use case for contribution and delivery of close to air or live
content where uncompressed links are unavailable.
Ability to use 5G connectivity as a contribution feed of video, audio and
ancillary data to a TV studio (e.g. news). We should be able to select content
at standard definition (SD) (270 Mbit/s), High definition (HD 1920x1080 pixels
(3 Gbit/s)) or UHD up to 24 Gbit/s. This should be possible up to a maximum
frame rate of 120fps (25fps, 50 fps or 100fps for Europe). Various forms of
compression may be used in order to provide a balance between quality, speed
and available bandwidth. It should be noted that compression is usually on the
video side and audio and data usually remain uncompressed.
To identify different bandwidth requirements, it may be helpful to look at
different tiers of production and their compression and latency requirements.
Table 5.6.1-1: Compression tiers
**Tier** **Type of use** **Bandwidth for UHD \@50 FPS** **Bandwidth for HD
(1080i50)** **Link latency** **description**
* * *
Mezzanine compression Studio and high end production 3 Gbit/s 1 Gbit/s \ 20 Mbit/s 10 Mbit/s 20
ms Single camera contribution from a camera or smart phone Reverse video/proxy
Monitoring and non-broadcast 10 Mbit/s \ NOTE 1: The 5G system shall be able to deliver Quasi Error Free (QEF)
> services, meaning less than one uncorrected error event per hour at the
> input of the video decoder. One or more retransmissions of network layer
> packets may take place in order to satisfy the reliability requirement. NOTE
> 2: This is the maximum end-to-end latency allowed for the 5G system to
> deliver the service in the case the end-to-end latency is completely
> allocated to the 5G system from the UE to the Interface to Data Network.
> NOTE 3: Latency is less important than QEF, longer latency can be accepted
> to avoid errors (e.g. existing satellite delays can be up to 5 seconds).
|  |  |  |  |  |  |  |  |  |   
[PR 5.6.6-009]: The 5G system shall be able to distribute high-precision
timing information as per the Precision Time Protocol (IEEE 1588-2008) [9] in
SMPTE ST2059 profile [10]. To enable the synchronous timing of broadcast
devices over the network and to provide data to enable the use of a UE media
clock.
[PR 5.6.6-010]: The 5G system shall have the ability to support open standard
based broadcast workflows.
[PR 5.6.6-011]: The 5G system shall support seamless uplink and downlink
service continuity as described in Table 5.6.6-1 while switching between co-
located PLMN and NPN (e.g., due to mobility).
[PR 5.6.6-012]: The 5G system shall support seamless service continuity as
described in Table 5.6.6-1: for an uplink stream while performing traffic
steering, switching, and splitting among co-located PLMN(s) and NPN(s); for
downlink while switching between co-located PLMN and NPN.
## 5.7 Professional TV Production Contributions from an Off-Site, Remotely-
Produced, Multi-Camera Outside Broadcast
### 5.7.1 Description
The director of a live sports programme needs to broadcast the events
unfolding at a sports arena in a remote location. He does not have the budget
to send many people or lots of equipment. He can only send three cameras with
operators and two presenters. The director remains in a different location and
will produce the programme live from his Production Gallery. The Production
Gallery has connectivity to a 5G network, either via radio or a direct
connection to a 5G network.
The director needs to receive ultra-reliable, low latency, uncompressed,
synchronous and coherent audio and video programme feeds from the remote
arena.
The director needs to have ultra-reliable connectivity as his company will
potentially have millions of viewers, both the broadcaster and the consumers
could have paid a premium for the rights to transmit and receive the content.
Low latency is required to facilitate intelligible and rapid communications
and immediate responses in terms of verbal replies as well as reacting to
instructions.
The director needs to make remote camera adjustments in as close to real time
as possible.
Any latency needs to be constant and predictable and should remain so over the
period of production which could be hours or even days.
The director needs to be able to cut between two or more different cameras
covering the same action without an obvious difference in timing.
He needs to be able to mix between audio sources without differences in timing
causing audio disturbance.
The director also needs to be able to establish voice communication with each
of the camera operators and presenters individually. He may decide to
communicate with the group using Production Gallery controls.
The camera operators and presenters also need to be able to communicate with
one another locally either individually or in small groups.
The three camera operators each need to be able to see and hear a preview
programme feed to line up shots with graphics overlays.
The presenters also need to be able to see and hear a return programme feed
from the Production Gallery.
The director also needs the ability to control the on-site equipment from the
Production Gallery remotely and receive confirmation messages back, all with
low latency.
This use case is likely to use compressed video in as high a quality and at as
low latency as possible. Current thinking envisages the availability of two
streams of video per camera, one a lower quality proxy and one in as higher
quality as possible.
The low-quality feed is sent back to the broadcast centre to allow for
monitoring and mixing with other sources while the higher quality is held at
site. When a decision to cut or mix between cameras is made at the broadcast
centre the high-quality feed of the selected camera is sent form the location
to be broadcast.
{width="6.752777777777778in" height="3.5145833333333334in"}
Figure 5.7.1-1: An example of a multi-camera outside broadcast where
contributions are independently fed back to a remote production gallery
complete with return communications.
### 5.7.2 Pre-conditions
  * The Production Gallery is fully set up and capable of producing a programme of this type.
  * There is connectivity to an existing public 5G network at the sports arena.
  * There is either 5G or other internet access at the Production Gallery.
  * The cameras and other production equipment are connected to a 5G network.
  * The public 5G network will likely be concurrently used by multiple media production users.
  * The public 5G network can be expected to be under heavy utilisation at times when media coverage is required.
### 5.7.3 Service Flows
\- The director deploys content-gathering crews to the area of interest with
5G coverage and appropriate equipment.
\- The camera operators each configure their production equipment with the
connection details (destination addresses) of the Production Gallery.
\- The director in the Production Gallery receives all content streams and
makes any necessary fixed timing adjustments.
\- The director in the Production Gallery influences the received content
through voice and control communications and is in constant interaction with
the camera operators and the presenter/journalists.
\- The director will also adjust on-site equipment via remote control
interfaces.
\- In the Production Gallery, the director creates the programme for
transmission in real time.
### 5.7.4 Post-conditions
Final production programme output to go to air either via 5G network or other
directly connected hand-off.
### 5.7.5 Existing features partly or fully covering the use case
functionality
The 5G system shall support the provision of elemental flows of synchronous
video, audio and data feeds in a transparent way.
The 5G system shall support application layer protocols for discovery,
registration, connection management, network control and security compatible
with current broadcast best practice.
The 5G system shall allow for handover between multiple cells with minimal
disruption on the same network.
### 5.7.6 Potential new requirements needed to support the use case
Video link bandwidth shall support compressed video up to UHD 4K at 50 fps as
per table below.
Table 5.7.6-1: Potential new requirements.
**Use Case** | **mobility** | **Uni or bi directional** | **Downlink data rate per UE** | **Uplink data rate per UE** | **Link latency** **(note 2,3)** | **Max packet size** | **Total delay (inc. application)** | **Reliability** **(Note 1)** | **# of active UE** | **Service area**  
---|---|---|---|---|---|---|---|---|---|---  
[PR 5.7.6-001] Remote OB | stationary or slow  NOTE 1: The 5G system shall be able to deliver Quasi Error Free (QEF)
> services, meaning less than one uncorrected error event per hour at the
> input of the video decoder. One or more retransmissions of network layer
> packets may take place in order to satisfy the reliability requirement. NOTE
> 2: This is the maximum end-to-end latency allowed for the 5G system to
> deliver the service in the case the end-to-end latency is completely
> allocated to the 5G system from the UE to the Interface to Data Network.
> NOTE 3: Latency is less important than QEF, longer latency can be accepted
> to avoid errors.
|  |  |  |  |  |  |  |  |  |   
[PR 5.7.6-002]: All professional TV production connectivity will require
latency to be synchronized to within 5 to 20 ms to allow cuts within a frame
boundary. This shall be fixed, constant and predictable to allow for a fixed
correction buffer.
[PR 5.7.6-003]: The 3GPP system shall allow service continuity between
multiple cells with minimal disruption on different networks that the user has
access to.
[PR 5.7.6-004]: The 5G system shall be able to distribute high-precision
timing information as per the Precision Time Protocol (IEEE 1588-2008) [9] in
SMPTE ST2059 profile [10] . This is to enable the synchronous timing of
broadcast devices over the network and to provide data to enable the use of a
UE media clock.
[PR 5.7.6-005]: The 5G system shall have the ability to support open standard
based broadcast workflows.
## 5.8 Simple Live Sports Commentary
### 5.8.1 Description
Location based reporter makes live audio contribution to programming by
connecting to broadcast centre facilities. Typically, this would be for a
sports event commentary or a news report.
Potentially, contributions will have a long, uninterrupted duration (e.g. pre-
match scene setting, 1^st^ half, half time, 2^nd^ half, post-match analysis
during a football match or full period of play during a cricket match of up to
three hours).
There may be an element of interaction with broadcast staff located at the
broadcast centre facility.
The (OB) venue is known, and will have dedicated, fixed points for
connectivity.
There will be redundancy (such as multi-path resilience) in the provision of
static connectivity.
The Reporter will have transported and set up all user equipment needed for
their task.
There will likely be no additional technical assistance on-site.
On-site (e.g. sports ground) actors and system components are depicted in Fig.
5.8.1-1 and Fig. 5.8.1-2.
{width="6.752777777777778in" height="4.148611111111111in"}
5.8.1-1: On-site actors and equipment
{width="6.732638888888889in" height="3.5145833333333334in"}
5.8.1-2: On-site and Base Studio components
The use case features the following actors:
  * Reporter: on-site providing audio contribution into live programmes
  * Broadcast Staff: based at broadcast centre, communicating with on-site (OB) reporters
  * Technical Assistance: broadcast staff able to assist reporters in the configuration of user equipment and static connectivity in the event of issues
Special challenges to the 5G system associated with this use case include the
following aspects:
  * Very stringent requirements on connectivity service availability, packet error ratio and end-to-end latency throughout the whole operation time.
  * Ease of use of equipment and services is vital -- the (reporter) staff operating in the field are specialists in a particular field of knowledge (e.g. Football) and not of technology
  * Portability of equipment needed is paramount. Much of this work is carried out by single reporters, each responsible for their own workflow.
  * The ability to guarantee 5G access in a heavily contended area (such as a football stadium) for the duration of the call is key to the successful adoption of the technology
It should be noted the current method of audio contribution (ISDN) is both
cheap and reliable; it also has been available in a standard form pretty much
globally, using standard codecs and signalling. It is familiar to the users
and large amounts of capital have been invested in the necessary hardware.
Table 5.8.1-1 -- Use case performance tolerances
* * *
                                            **Tolerance**                                                 **Comments**
**End-to-end latency** \6 ms uplink channels to upload
live video streams from multiple devices, e.g. cameras.
[PR 5.10.6-6] The 5G system shall be able to support ultra-reliable [10E-x]
and low latency [xx] ms communication downlink channels to allow LMPF to
remotely control multiple cameras in a real-time manner.
Editor\'s note: values above to be specified.
## 5.11 Video Streaming in Professional Coverage of Live Performances
### 5.11.1 Description
Producing and capturing a live event, i.e. for further exploitation of the
cultural and creative content, involve many wireless links. For instance,
artists on stage use wireless microphones to capture their voices or
instruments' sound while hearing themselves via a wireless in-ear monitoring
system. Cameramen operate their wireless cameras capturing the performance.
The technical crew, the production team and the security staff are usually
connected to each other via an intercom system. Lighting, video and sound
effects are remotely controlled over stage control systems. The term PMSE
equipment is used to sum up all wireless audio and video equipment involved in
professional AV productions.
Live events take place typically in theatres, concert halls, stadiums,
studios, and the like. The stage can be located indoors and/or outdoors.
Typical operation has a defined and in-advanced known duration. All PMSE
equipment required for the production and capturing of an event is always
available at the location of the event. Either it belongs to the
infrastructure of the stage or a rental company has been engaged to deploy it
for the event.
Characteristic for a live event is that the PMSE equipment is available on the
stage, the wireless communication service is limited to the event area and all
audio and video processing such as audio and video mixing is done in real time
during operation.
Wireless cameras are used for capturing the picture, voice and music signals
of the artists on- and around the stage. PMSE content capture sits at the very
beginning of the supply- and value chain for a wide range of products, such as
recordings of live performances or the archiving of culturally significant
material. Consequently, content capture is expected to provide the highest
quality possible, with producers and program makers taking steps to ensure the
quality and robustness of content capture and delivery. For these reasons,
quality and reliability of the radio link are fundamental to PMSE users. For
live PMSE productions especially, the commercial pressures on users are
significant as there is no opportunity for recovery (See Figure 5.11.1-1, no
possibility to ask the singer: "Please, repeat!"), and so the tolerance for
disturbance to the QoS is extremely low.
In a typical live event, a number between 1 and 5 simultaneously active
wireless cameras can be expected. Each wireless camera signal is streamed to a
central video mixing console. Each active wireless camera produces a video
stream of up to 100 Mbit/s (4K\@100fps) and receives a control and video
return signal of maximum 12 Mbit/s (1080i\@50fps).
The video mixing console does the mixing and combining of the different video
streams. Most cameramen rely on receiving a personalized video mix of the
event streamed back to his camera viewfinder. In this context, personalized
means that each cameraman can receive a different video mix (i.e. point to
point downlink transmission) fully adapted to his/her needs and preferences.
Sometimes a group of cameramen in the production may want to receive the same
video-mix. For this latter case, a point to multipoint downlink transmission
could be chosen. A stage video monitoring device receives a video stream of up
to 12 Mbit/s. The maximum non-public network latency tolerated by a
professional cameraman between his wireless camera and the return video in his
viewfinder coming back from the mixing console is limited to 40 ms. The
maximum round trip time for the network is set to 6 ms which leaves 34 ms for
the codec and the mixing console. The video mixing console produces further
outgoing streams for the stage video monitoring device, playout and recording.
{width="6.663194444444445in" height="3.6930555555555555in"}
Figure 5.11.1-1: Live content production network
However, it is hoped that with the latest generation of 3GPP technologies that
other wireless systems in common use by production may be incorporated into
this workflow. In an ideal world uncompressed video would be the preference
but for practical reasons a number of compressed workflows are in use.
The compressed wireless systems in common use are:
  * Steadicam
  * Handheld camera
  * Drone
  * Cable-cam
  * Rail-cam
  * Onboard camera
  * Helicopter with camera operator
  * Motorbike with camera operator
  * Microwave relay helicopter or fixed wing aircraft
**Steady-cam**
A Steadicam is a gyro stabilized mount/harness worn by a specially trained
camera operator. The camera is carefully balanced and takes care of all the
vibrations caused by the movement of the operator. These wireless cameras are
used most of the time for studio productions like "the Voice" or the
Eurovision song contest. Sometimes Steadicams are used outside for covering
concerts or sporting events and could also be operated from a Segway, snow
scooter, etc.
For high end content production (compressed) we would need:
\- HQ video from camera
\- LQ video Mbps to camera
\- 6 ms latency (RTT)
**Handheld camera**
For studio productions and outside broadcasts sometimes, handheld cameras are
used in situations where the director needs to have multiple angle shots from
competitors or artists. So, in situations where maximum flexibility is
demanded and for obvious reasons no cables are allowed, handheld cameras are
equipped with transmitters and receivers. These cameras can operate line of
sight and non-line of sight.
For high end content production (compressed) we would need:
  * HQ video from camera
  * Control and LQ video to camera
  * 6 ms latency (RTT)
**Drone**
Since regulations are getting in place for Drone usage the demand for getting
live shots from these highly sophisticated aerial platforms is getting bigger.
The onboard camera is already producing 4K footage but for locale storage
only.
For high end content production (compressed) we would need:
  * HQ Video from camera
  * control Video to camera
  * 6 ms latency (RTT)
**Cable-cam**
During large football events, ski jumping or open-air concerts sometimes a
cable-cam is used. The camera is mounted on a small platform which is attached
to steel cables. This platform can virtually go to any position on the
football field. The system is fully remote operated.
For high end content production (compressed) we would need:
  * HQ Video Mbps from camera
  * control to camera
  * 6 ms latency (RTT)
**Rail-cam**
For Formula One, cross-country skiing, and speedskating a rail-cam is used.
This system is using a specific track for instance during Formula 1 it
operates in the pitlane. And during speed skating the camera track is just
above the boarding. The system is fully remote operated.
For high end content production (compressed) we would need:
  * HQ Video Mbps from camera
  * control Mbps to camera
  * 6 ms latency (RTT)
**Onboard cameras**
Formula 1 and MotoGP provide the director with a lot of onboard camera shots
including important sensor data for the graphics department. These systems
require terrestrial coverage.
For high end content production (compressed) we would need:
  * HQ Video Mbps from camera (per camera)
  * control Mbps to camera (combined)
  * 6 ms latency (RTT)
**Helicopter with camera operator (downlink mode)**
Making aerial shots during sports or event productions most of the time a
twin-engine helicopter is used which is equipped on the outside with a so-
called gyro stabilized camera for instance a Cineflex or Shotover. The camera
operator is operating the system from within the helicopter and follows the
cues from the director. The operating distance can vary from 150 m to 150 km.
For high end content production (compressed) we would need:
  * HQ Video Mbps from camera
  * Control, comms and control Mbps to camera
  * 6 ms latency (RTT)
**Helicopter with camera operator (up-link mode)**
Making aerial shots during sport or event productions most of the time a twin-
engine helicopter is used which is equipped with a so-called gyro stabilized
camera for instance a Cineflex or Shotover. The camera operator is operating
the system from within the helicopter and follows the cues from the director.
The operating distance can vary from 150 m to 150 km. Depending on the
altitude of the camera helicopter the signals can go direct to the receive
site or need to be transmitted towards a fixed wing relay plane operating at a
higher altitude and from there the signal are combined in a mux and
transmitted towards the receive site.
For high end content production (compressed) we would need:
  * HQ Video Mbps from camera
  * Control LQ video and comms Mbps to camera
  * 20 ms latency (RTT) via aerial relay
**Motorbike with camera operator**
For covering Cycling races and Marathons several motorbikes with camera
operator are involved. Especially cycling races that can last for up to 8
hours the technology has to function over large distances. For that reason,
most of the time a microwave midpoint is used for receiving and re-
transmitting the footage. This midpoint can either be a helicopter or a fixed
wing airplane. For Marathon coverage a terrestrial solution would be
preferred.
For high end content production (compressed) we would need:
  * HQ video from camera {per camera)
  * Comms and control Mbps to camera (combined)
  * 6 ms latency (RTT) terrestrial
  * 20 ms latency (RTT) via aerial relay
**Microwave relay helicopter or fixed wing aircraft**
Long distance live event coverage like triathlons, cycling races and WRC rally
racing is done by means of a microwave aerial relay. This relay can either be
a helicopter or a fixed wing aircraft handle up to 8 uplinks of HQ video. and
will send at least a mux down to the receive site.
For high end content production (compressed) we would need:
  * HQ Video from camera {per camera) to airplane
  * Control and comms to camera (combined) from airplane
  * HQ MUX video main downlink from airplane to receive site
  * HQ MUX back-up downlink from airplane to receive site
  * Video, comms and control main uplink from receive site
  * Video comms and control back-up uplink from receive site
  * 20 ms latency (RTT) via aerial relay
**All services carried by these Bi-directional IP-link**
From camera:
  * 4K video with 8 times embedded audio
  * 4 separate audio channels
  * Intercom
  * AR sensor data
  * Synchronization data
  * Camera ID data
  * Link status
  * RS232/485/422 data
To camera:
  * return HD video programme
  * return HD video autocue
  * focus pulling data
  * camera control
  * lock data
  * intercom
  * link status
  * PTZ commands
\- RS232/485/422 data
### 5.11.2 Pre-conditions
\- All PMSE equipment required to produce the event is available at the
desired location (event)
\- Sufficient bandwidth to satisfy the requirements of the wireless production
links is available at the event location during the whole operation time.
\- All wireless cameras on stage are switched on and connected to a central
video mixing console through a NG-RAN.
\- The central video mixing console may act as an edge computing application
server.
\- All active wireless cameras are synchronized at the application level
within 1Âµs accuracy.
\- The central video mixing console is connected to the playout system and, if
required, to a recording server.
### 5.11.3 Service Flows
\- Setting up a temporary Broadcast IP infrastructure using a 5G local non-
public network
\- Multiple cameras connect via this local non-public 5G network to the studio
or outside broadcast van.
\- Additional data is sent via this local non-public 5G network between the
cameras and the studio or outside broadcast van for communication, camera
control, GPS data, AR sensor data, and return video.
### 5.11.4 Post-conditions
A temporary 5G non-public local network that provides;
  * Intracell and intercell service continuity for speeds up to 200 km/h.
  * Terrestrial coverage in an 10000 m^2^ area.
  * Low latency with a round trip time of 6 ms.
Quasi Error Free operation when handling multiple 4K (compressed video)
cameras including communication, camera control, GPS data, AR sensor data, and
return video
### 5.11.5 Existing features partly or fully covering the use case
functionality
The 5G system shall support the deployment of a live content production
network as a non-public network (NPN), both physically or virtually, providing
coverage within the specific event area (up to 10000 m^2^ indoor and outdoor)
The 5G system shall support the operation of a live content production network
deployed as a standalone non-public network (NPN).
The 5G system shall support identifiers to uniquely identify a non-public
network.
The 5G system shall support data integrity protection and confidentiality
methods that serve URLLC and energy constrained devices.
The 5G system shall support a suitable framework (e.g., EAP) allowing
alternative (e.g., to AKA) authentication methods with non-3GPP identities and
credentials to be used for UE network access authentication in non-public
networks.
The 5G system shall support suitable APIs to allow MNOs to offer automatic
configuration services (for instance, interference management) to non-public
networks deployed by 3rd parties and connected to the MNO's Operations System
through standardized interfaces. (TS22.261 clause 6.10).
The 5G system shall be able to transport IPv4 and IPv6 multicast traffic.
The 5G system shall be able to support seamless intra-cell and inter-cell
service continuity in combination with all the above-mentioned latencies,
speeds and bitrates.
### 5.11.6 Potential New Requirements needed to support the use case
**Table 5.11.6-1: Potential new requirements**
Use Case | **mobility** | **Uni or bi directional** | **Downlink data rate per UE** | **Uplink data rate per UE** | **Max Packet size** | **Link latency** **(Note 2,3)** | **Total delay (inc. application)** | **Reliability** **(Note 1)** | **# of active UE** | **Service area**  
---|---|---|---|---|---|---|---|---|---|---  
[PR 5.11.6-001] NPN radio Camera UHD |  NOTE 1: The 5G system shall be able to deliver Quasi Error Free (QEF)
> services, meaning less than one uncorrected error event per hour at the
> input of the video decoder. One or more retransmissions of network layer
> packets may take place in order to satisfy the reliability requirement. NOTE
> 2: This is the maximum end-to-end latency allowed for the 5G system to
> deliver the service in the case the end-to-end latency is completely
> allocated to the 5G system from the UE to the Interface to Data Network.
> NOTE 3: Latency is less important than QEF, longer latency can be accepted
> to avoid errors.
|  |  |  |  |  |  |  |  |  |   
Editor\'s Note: The desire is to extend the existing functionality of modern
broadcast networks, as described in [3], out to remote devices connected via a
5G network. Therefore, the following requirements are critical to the adoption
of 5G technologies (also applicable to uncompressed use case).
[PR 5.11.6-003]: The 5G system shall be able to distribute high-precision
timing information as per the Precision Time Protocol (IEEE 1588-2008) [9] in
SMPTE ST2059 profile [10] . To enable the synchronous timing of broadcast
devices over the network and provide timing information to an attached UE
media clock.
[PR 5.11.6-004]: The 5G system shall have the ability to support open standard
based broadcast workflows.
NOTE: VSF and SMPTE and exploring the requirements for SMPTE compatible video
over wireless links.
[PR 5.11.6-005]: The 5G system shall support operation of ultra-low latency
point to point and point to multipoint communication for periodic
deterministic traffic.
NOTE: The end-to-end latency (40 ms) is required to include the uplink (lens
to central mixing console) and downlink (central mixing console to camera
viewfinder and studio floor video monitoring device).
[PR 5.11.6-006]: The 5G system shall support UE speeds of up to 200 km/h over
short distances (up to 500 m) for URLLC communication services.
[PR 5.11.6-007]: The 5G system shall be able to do an ultra-quick reconnect
and authentication (20 ms) after a UE network connection loss due to RF
interference, in/out/in of coverage or battery change.
## 5.12 Authentication of devices on a shared non-public network
### 5.12.1 Description
The television production environment at a music festival or a live sporting
event may be based on a non-public network operated by the host facility. This
network may be offered to all broadcasters present at the event. This helps to
manage the proliferation of networks at the event, and the overload of any
macro network resources.
Figure 5.12.1-1: Devices on a shared non-public network
This scenario raises the question of how provisioning of the broadcaster's
equipment is done, and how agile the process can be in a rapidly changing
production environment.
It can be expected that the equipment brought onsite by the broadcasters do
not have the credentials of the Non-Public Network (NPN). It can be assumed,
however, that this heterogeneous equipment has been provisioned correctly to
work with the Broadcaster.
In this scenario, we require the NPN to be able to advertise the broadcaster
which have been enabled on the NPN.
Alternatively, the equipment brought onsite by the broadcasters already have
installed the subscriptions and credentials of a PLMN which has agreement with
the Non-Public Network (NPN). The agreement allows the equipment that have
subscriptions with the PLMN to access the services provided by the NPN as well
as PLMN via NPN (in MVNO- or roaming-like scenario). As such, the equipment
can get access to NPN/PLMN services via a non-public network access by using a
single subscription (i.e. the PLMN subscription) and a single set of
credentials (i.e. the PLMN credentials), as shown in Figure 5.12.1-2.
Figure 5.12.1-2: Access to PLMN services via non-public network (as from a
roaming UE)
### 5.12.2 Pre-conditions
Audio/visual equipment has been provisioned with credentials that can be
authenticated to gain access to a Broadcaster 1 non-public network, or
Broadcaster 1 as a third-party service provider offered by another non-public
network.
### 5.12.3 Service Flows
Use Case 1:
1\. NPN Operation, Administration, & Maintenance (OA&M) adds the broadcaster
as a third-party service provider supported on the NPN
2\. NPN advertises that it supports service for devices configured for this
third-party service provider
3\. Audio/visual production equipment detects NPN advertising a known third-
party service provider
4\. Audio/visual production equipment attempts to authenticate to NPN using
credentials associated with third-party service provider (e.g. Broadcaster 1)
5\. NPN requests authentication by third party service provider (e.g.
Broadcaster 1)
6\. Authentication succeeds and is accepted by the NPN
7\. Audio/visual production equipment can access services provided by the
specific third-party service provider on NPN
Use Case 2:
1-3: same as Case 1.
4: Audio/visual production equipment accesses to NPN and requests for
registration.
5: NPN requests for authentication from Home PLMN.
6\. Authentication succeeds and is accepted by the Home PLMN.
7\. Audio/visual production equipment can access services provided by the NPN.
8\. If accessing to PLMN service is needed, the NPN selects a session anchor
in the PLMN to access the PLMN service.
### 5.12.4 Post-conditions
Audio/visual equipment from all broadcasters provisioned to access service via
the SXSW NPN can get connectivity.
### 5.12.5 Existing features partly or fully covering the use case
functionality
NPNs as defined in TS 23.501 can support necessary functionality such as
authentication based on non-3GPP credentials, differentiated QoS, etc. [12]
An NPN shall be able to restrict services, available quality of service, and
capacity to subscribers of a specific third-party service provider.
### 5.12.6 Potential Requirements
[PR-5.12.6-001] The 5G system shall enable an NPN to be able to request a
third-party service provider to perform alternative (e.g., to AKA)
authentication with non-3GPP identities and credentials to be used for UE
network access authentication in non-public networks based on non-3GPP
identities and credentials supplied by the third-party service provider.
[PR-5.12.6-002] The 5G system shall enable an NPN to support multiple third-
party service providers.
[PR-5.12.6-003] The 5G system shall enable an NPN to be able to request a PLMN
to perform NPN access network authentication of a UE based on 3GPP identities
and credentials supplied by the PLMN.
## 5.13 Onboarding of audio-visual IoT devices onto a non-public network
### 5.13.1 Description
This scenario describes an onboarding procedure for granting connectivity via
an NPN to audio visual production equipment. In this scenario, a studio with a
deployed NPN brings a new device into service for the first time (onboarding)
by retrieving the credentials required to access the NPN; whereby the
retrieval of credentials potentially uses credentials provided by the device
manufacturer.
As an initial consideration, it cannot be expected that IoT devices, including
audio visual production devices such as microphones, pickups, etc, are
provisioned with the credentials for an NPN at the time of manufacture. IoT
devices, even higher-value audio-visual IoT devices, are expected to be
produced agnostic to the eventual purchaser and operator.
In this scenario, a studio owner purchases some new audio production devices
for use in a recording studio facility. The devices are procured from
different vendors, for example, a set of audio pickups for stringed
instruments, and a set of in-ear monitors for studio engineers. None of the
devices have any user-interface (i.e. screen, keyboard etc.)
The studio owner wishes to connect these audio-visual production devices to
his existing non-public network (NPN) deployed in his studio. Further, it is
desirable to perform this connection without physical interfacing with the
audio-visual device, for example by avoiding pushing credentials into the
device with a debug cable interface, or a smartcard interface. This translates
to a "zero-touch" onboarding requirement.
In this scenario the audio-visual devices can connect to the NPN for
provisioning purposes only and, without NPN credentials, gain access to a
provisioning service which allows it to be authorised to securely retrieve NPN
credentials.
### 5.13.2 Pre-conditions
1\. An NPN has been deployed in a recording studio.
2\. The studio owner procures an audio-visual production device from an audio-
visual device vendor (e.g. an in-ear monitor)
3\. The audio-visual device does not have credentials to make an operational
connection to the NPN within the studio
### 5.13.3 Service Flows
1\. Device is powered on within coverage of the NPN
2\. Device connects to the NPN for the purpose of obtaining credentials to
access the NPN
3\. Device is provisioned with credentials to be operational in the NPN
NOTE: This step may involve interaction between a provisioning system
potentially with a system maintained by the IoT device manufacturer.
### 5.13.4 Post-conditions
1\. Device begins operating within the NPN
NOTE: Device can operate within the NPN without any further interaction with
any system maintained by the IoT device manufacturer.
### 5.13.5 Existing features partly or fully covering the use case
functionality
TS 22.261, clause 6.14.2 [5]:
Based on operator policy, the 5G system shall support a mechanism to provision
on-demand connectivity (e.g. IP connectivity for remote provisioning). This
on-demand mechanism should enable means for a user to request on-the-spot
network connectivity while providing operators with identification and
security tools for the provided connectivity.
The 5G system shall support a secure mechanism for a home operator to remotely
provision the 3GPP credentials of a uniquely identifiable and verifiably
secure IoT device.
### 5.13.6 Potential Requirements
[PR-5.13.6-001] Based on operator policy, the 5G system shall support a
mechanism to provision on-demand connectivity (e.g. IP connectivity for remote
provisioning).
[PR-5.13.6-002] The 5G system shall support an on-demand mechanism for a user
(human or software) to request on-the-spot network connectivity while
providing the network operator with identification and security tools for the
provided connectivity.
[PR-5.13.6-003] The 5G system shall support a secure mechanism for a network
operator of an NPN to remotely provision the non-3GPP identities and
credentials of a uniquely identifiable and verifiably secure IoT device.
# 6 Security Aspects
Editor\'s note: to be provided.
# 7 Additional considerations
## 7.1 Reliability
Packet error ratio might not be sufficient to describe the reliability of a
communication system for audio use cases. The distribution of packet errors is
essential to guarantee the functionality of an audio application. A packet
error ratio, in terms of not transmitted / received packets per time, is only
valid as a measurement in such use cases if packet errors are equally
distributed over time. In any other case, additional measurements, e.g. those
which can reflect the distribution of packet errors, are also required to
evaluate the reliability of a communication system in audio contexts.
For video use cases the loss of a packet is critical as it will result in
picture freeze or breakup on the output. While latency can be increased in
order to mitigate packet loss there are limits on the amount of latency that
can be tolerated dependent on the use case.
## 7.2 Interface to reduce time misalignment
The transition from application to communication system can generate
relatively large amounts of delay in audio use cases with ultra-low latency
data transfers. Such delays can occur when audio capturing and processing is
not time-aligned to a utilized communication system e.g. with periodic
transmission slots. The packaging of audio transfers is generally flexible
with a granularity of a single audio sample (e.g. 20.83Âµs at 48 kHz sample
rate) and could be aligned to a communication system if appropriate interfaces
are available. The delay from misalignment could be minimized in this way, and
the end-to-end latency requirements for some use cases could be relaxed if
such an interface exists.
## 7.3 Spectral Efficiency
Spectral efficiency of digital communication systems is a measure of how
efficiently a limited range of spectrum is utilized by protocols of the two
lowest layers, the physical layer and the data link control layer, and refers
to the net bitrate which is transmitted over a given bandwidth of the
communication channel. It is measured in bit/s/Hz.
Spectral efficiency does not include a latency requirement, which is one of
the most challenging KPIs considering audio streaming. Therefore, it is not
possible to derive the possible number of audio streams just bye knowing the
system's spectral efficiency only. Latency requirements must be considered. In
addition, it makes it very difficult to compare the performance of a system
without latency requirements with a system having strong latency requirements.
That aside, a decrease of latency goes hand in hand with a decrease of
spectral efficiency. To be able to compare different communication systems, it
could be useful to define a new performance indicator which combines the
latency requirement, the number of audio streams, and the occupied bandwidth.
## 7.4 Hand-over between nodes
The 5G system shall be able to support seamless intra-cell and inter-cell
service continuity whilst in use, there should be no visible freezing,
glitching or artefacts as transmission devices switches between receive nodes
in NPN-NPN or PLMN-PLMN scenarios and where possible NPN-PLMN and PLMN to NPN
should also be seamless. Latency should be maintained across the node
boundaries.
## 7.5 Audio and video synchronisation
For video use cases it should be assumed that audio is carried alongside or
embedded in the video stream. These should remain synchronous to within frame
boundaries of the video signal.
## 7.6 Predictable and constant latency
When multiple devices are deployed across the 5G network a fixed and constant
latency should be applied. This is to enable two or more AV signals to be
synchronised without any drift between the two signals over time.
## 7.7 PTP IEEE 1588 with SMPTE ST-2059-2 profile
IEEE 1588 with an SMPTE ST-2059-2 profile [9,10], or wireless equivalent
should be able to be generated by a UE device. When a UE master clock
generates a compliant clock, this should be available for distribution to
other locally connected devices running slave or boundary clocks.
# 8 Consolidated potential requirements
## 8.1 Performance requirements
Every line in each table should be considered as an independent requirement.
Table 8.1-1: Performance requirements of low latency periodic deterministic
communication service
+-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Pr | # of | UE | Se | E2E | Tra | P | Data | Data | | ofile | a | Speed | rvice | la | nsfer | acket | rate | rate | | | ctive | | Area | tency | int | error | UL | DL | | | UEs | | | (Note | erval | rate | | | | | | | | 1) | (Note | (Note | | | | | | | | | 1) | 2) | | | +=======+=======+=======+=======+=======+=======+=======+=======+=======+ | Ad | 20 | 5 | 300 m | 3 ms | 3 ms | 1 | 200 | - | | hoc | | km/h | x 300 | | | 0^-4^ | k | | | | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 8 | stati | 300 m | 3 ms | 3 ms | 1 | - | 200 | | | | onary | x 300 | | | 0^-4^ | | k | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | C | 1000 | 5 | 2 km | 5 ms | 5 ms | 1 | 200 | - | | ampus | | km/h | x 2 | | | 0^-4^ | k | | | | | | km | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Confe | 10 | 5 | 100 m | 3 ms | 3 ms | 1 | 1.5 | - | | rence | | km/h | x 100 | | | 0^-4^ | M | | | | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 4 | stati | 100 m | 3 ms | 3 ms | 1 | - | 1.5 | | | | onary | x 100 | | | 0^-4^ | | M | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Le | 4 | 5 | 10 m | 3 ms | 3 ms | 1 | 50 | - | | cture | | km/h | x 10 | | | 0^-4^ | k | | | room | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 2 | stati | 10 m | 3 ms | 3 ms | 1 | - | 50 | | | | onary | x 10 | | | 0^-4^ | | k | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Fes | 200 | 10 | 500 m | 600 | 250 | 1 | 500 | - | | tival | | km/h | x 500 | Âµs | Âµs | 0^-4^ | k | | | | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 100 | 10 | 500 m | 600 | 250 | 1 | - | 1 | | | | km/h | x 500 | Âµs | Âµs | 0^-4^ | | M | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Mu | 30 | 50 | 50 m | 600 | 250 | 1 | 500 | - | | sical | | km/h | x 50 | Âµs | Âµs | 0^-4^ | k | | | | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 20 | 50 | 50 m | 600 | 250 | 1 | - | 1 | | | | km/h | x 50 | Âµs | Âµs | 0^-4^ | | M | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 10 | - | 50 m | 600 | 250 | 1 | - | 500 | | | | | x 50 | Âµs | Âµs | 0^-4^ | | k | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Se | 10 | 5 | 5 m x | 600 | 250 | 1 | 100 | - | | mi-pr | | km/h | 5 m | Âµs | Âµs | 0^-4^ | k | | | ofess | | | | | | | bit/s | | | ional | | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 10 | 5 | 5 m x | 600 | 250 | 1 | - | 200 | | | | km/h | 5 m | Âµs | Âµs | 0^-4^ | | k | | | | | | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 2 | - | 5 m x | 600 | 250 | 1 | - | 100 | | | | | 5 m | Âµs | Âµs | 0^-4^ | | k | | | | | | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | AV | 20 | 5 | 30 m | 600 | 250 | 1 | 1.5 | - | | produ | | km/h | x 30 | Âµs | Âµs | 0^-4^ | M | | | ction | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 10 | 5 | 30 m | 600 | 250 | 1 | - | 3 | | | | km/h | x 30 | Âµs | Âµs | 0^-4^ | | M | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | Audio | 30 | - | 10 m | 600 | 250 | 1 | 5 | - | | S | | | x 10 | Âµs | Âµs | 0^-4^ | M | | | tudio | | | m | | | | bit/s | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | | 10 | 5 | 10 m | 600 | 250 | 1 | - | 1 | | | | km/h | x 10 | Âµs | Âµs | 0^-4^ | | M | | | | | m | | | | | bit/s | +-------+-------+-------+-------+-------+-------+-------+-------+-------+ | NOTE | | | | | | | | | | 1: | | | | | | | | | | Tra | | | | | | | | | | nsfer | | | | | | | | | | int | | | | | | | | | | erval | | | | | | | | | | r | | | | | | | | | | efers | | | | | | | | | | to | | | | | | | | | | p | | | | | | | | | | eriod | | | | | | | | | | icity | | | | | | | | | | of | | | | | | | | | | the | | | | | | | | | | p | | | | | | | | | | acket | | | | | | | | | | trans | | | | | | | | | | fers. | | | | | | | | | | It | | | | | | | | | | has | | | | | | | | | | to be | | | | | | | | | | con | | | | | | | | | | stant | | | | | | | | | | d | | | | | | | | | | uring | | | | | | | | | | the | | | | | | | | | | whole | | | | | | | | | | oper | | | | | | | | | | ation | | | | | | | | | | and | | | | | | | | | | can | | | | | | | | | | be | | | | | | | | | | de | | | | | | | | | | fined | | | | | | | | | | as | | | | | | | | | | being | | | | | | | | | | the | | | | | | | | | | same | | | | | | | | | | as | | | | | | | | | | the | | | | | | | | | | T~f | | | | | | | | | | rame~ | | | | | | | | | | of | | | | | | | | | | Annex | | | | | | | | | | A. | | | | | | | | | | The | | | | | | | | | | value | | | | | | | | | | given | | | | | | | | | | in | | | | | | | | | | the | | | | | | | | | | table | | | | | | | | | | is a | | | | | | | | | | ty | | | | | | | | | | pical | | | | | | | | | | one, | | | | | | | | | | ho | | | | | | | | | | wever | | | | | | | | | | other | | | | | | | | | | tra | | | | | | | | | | nsfer | | | | | | | | | | inte | | | | | | | | | | rvals | | | | | | | | | | are | | | | | | | | | | pos | | | | | | | | | | sible | | | | | | | | | | as | | | | | | | | | | long | | | | | | | | | | as | | | | | | | | | | the | | | | | | | | | | end-t | | | | | | | | | | o-end | | | | | | | | | | la | | | | | | | | | | tency | | | | | | | | | | is | | | | | | | | | | sm | | | | | | | | | | aller | | | | | | | | | | than | | | | | | | | | | â‰¤ (15 | | | | | | | | | | -- 2 | | | | | | | | | | Ã— | | | | | | | | | | T~fr | | | | | | | | | | ame~) | | | | | | | | | | for | | | | | | | | | | 1-way | | | | | | | | | | comm | | | | | | | | | | unica | | | | | | | | | | tion, | | | | | | | | | | â‰¤ (15 | | | | | | | | | | -- 3 | | | | | | | | | | Ã— | | | | | | | | | | T~ | | | | | | | | | | frame | | | | | | | | | | ~))/2 | | | | | | | | | | for | | | | | | | | | | 2-way | | | | | | | | | | com | | | | | | | | | | munic | | | | | | | | | | ation | | | | | | | | | | se | | | | | | | | | | mi-pr | | | | | | | | | | ofess | | | | | | | | | | ional | | | | | | | | | | prof | | | | | | | | | | iles, | | | | | | | | | | and â‰¤ | | | | | | | | | | (2 -- | | | | | | | | | | 3 Ã— | | | | | | | | | | Tfra | | | | | | | | | | me)/2 | | | | | | | | | | for | | | | | | | | | | the | | | | | | | | | | pr | | | | | | | | | | ofess | | | | | | | | | | ional | | | | | | | | | | prof | | | | | | | | | | iles. | | | | | | | | | | If | | | | | | | | | | there | | | | | | | | | | is an | | | | | | | | | | inte | | | | | | | | | | rface | | | | | | | | | | for | | | | | | | | | | syn | | | | | | | | | | chron | | | | | | | | | | izing | | | | | | | | | | audio | | | | | | | | | | de | | | | | | | | | | vices | | | | | | | | | | and | | | | | | | | | | radio | | | | | | | | | | inte | | | | | | | | | | rface | | | | | | | | | | as | | | | | | | | | | desc | | | | | | | | | | ribed | | | | | | | | | | in | | | | | | | | | | Se | | | | | | | | | | ction | | | | | | | | | | 7, | | | | | | | | | | the | | | | | | | | | | la | | | | | | | | | | tency | | | | | | | | | | r | | | | | | | | | | equir | | | | | | | | | | ement | | | | | | | | | | can | | | | | | | | | | be | | | | | | | | | | re | | | | | | | | | | laxed | | | | | | | | | | as | | | | | | | | | | desc | | | | | | | | | | ribed | | | | | | | | | | in | | | | | | | | | | the | | | | | | | | | | Annex | | | | | | | | | | A.1. | | | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | | 2: | | | | | | | | | | P | | | | | | | | | | acket | | | | | | | | | | error | | | | | | | | | | rate | | | | | | | | | | is | | | | | | | | | | re | | | | | | | | | | lated | | | | | | | | | | to a | | | | | | | | | | p | | | | | | | | | | acket | | | | | | | | | | size | | | | | | | | | | of | | | | | | | | | | tra | | | | | | | | | | nsfer | | | | | | | | | | int | | | | | | | | | | erval | | | | | | | | | | Ã— | | | | | | | | | | data | | | | | | | | | | rate. | | | | | | | | | | Pa | | | | | | | | | | ckets | | | | | | | | | | that | | | | | | | | | | do | | | | | | | | | | not | | | | | | | | | | co | | | | | | | | | | nform | | | | | | | | | | with | | | | | | | | | | the | | | | | | | | | | end-t | | | | | | | | | | o-end | | | | | | | | | | la | | | | | | | | | | tency | | | | | | | | | | are | | | | | | | | | | also | | | | | | | | | | acco | | | | | | | | | | unted | | | | | | | | | | as | | | | | | | | | | e | | | | | | | | | | rror. | | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+-------+
Table 8.1-2: Performance requirements for low latency deterministic periodic
traffic with multicast service.
Profile | # of active UEs | # of UL streams | # of DL streams | UE Speed | Service Area | E2E latency (Note 1) | Transfer interval (Note 1) | Packet error rate (Note 2) | Data rate UL | Data rate DL  
---|---|---|---|---|---|---|---|---|---|---  
Integrated audience services | 50000 | - | 30 multicast streams | 5 km/h | 1.5 km x 1.5 km | 5 ms DL | 5 ms | 10-3 | - | 200 kbit/s  
Intercom system | 1000 | 240 (Note 3) | 30 multicast streams | 5 km/h | 1.5 km x 1.5 km | 3 ms DL 3 ms UL | 3 ms | 10-3 | 100 kbit/s | 100 kbit/s  
NOTE 1: Transfer interval refers to periodicity of the packet transfers. It has to be constant during the whole operation and can be defined as being the same as the Tframe of Annex A. The value given in the table is a typical one, however other transfer intervals are possible as long as the end-to-end latency is smaller than â‰¤ (15 â€“ 3 Ã— Tframe)/2 for the 2-way communication profiles and â‰¤ (15 â€“ 2 Ã— Tframe) for the 1-way communication profiles, which yields a mouth-to-ear latency of 20 ms. If there is an interface for synchronizing audio devices and radio interface as described in Section 7, the latency requirement can be relaxed as described in the Annex A.1. NOTE 2: Packet error rate is related to a packet size of transfer interval Ã— data rate. Packets that do not conform with the end-to-end latency are also accounted as error. NOTE 3: The UL stream originating from a UE may be the source of a DL multicast stream. |  |  |  |  |  |  |  |  |  |   
**\ **
Table 8.1-3: Performance requirements for low latency video.
+-------+-------+-------+-------+-------+-------+-------+-------+ | Pr | # of | UE | Se | E2E | P | Data | Data | | ofile | a | Speed | rvice | la | acket | rate | rate | | | ctive | | Area | tency | error | UL | DL | | | UEs | | | | rate | | | | | | | | | (Note | | | | | | | | | 1) | | | +=======+=======+=======+=======+=======+=======+=======+=======+ | Un | 1 | 0 | 1 | 400 | 10 | 12 | 20 | | compr | | km/h | km^2^ | ms | -^10^ | G | M | | essed | | | | | UL | bit/s | bit/s | | UHD | | | | | | | | | video | | | | | 1 | | | | | | | | | 0-^7^ | | | | | | | | | DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Un | 1 | 0 | 1 | 400 | 1 | 3 .2 | 20 | | compr | | km/h | km^2^ | ms | 0-^9^ | G | M | | essed | | | | | UL | bit/s | bit/s | | HD | | | | | | | | | video | | | | | 1 | | | | | | | | | 0-^7^ | | | | | | | | | DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Mezz | 5 | 0 | 1000 | 1 s | 1 | 3 | 20 | | anine | | km/h | m^2^ | | 0^-9^ | G | M | | c | | | | | UL | bit/s | bit/s | | ompre | | | | | | | | | ssion | | | | | 1 | | | | UHD | | | | | 0^-7^ | | | | video | | | | | DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Mezz | 5 | 0 | 1000 | 1 s | 1 | 1 | 20 | | anine | | km/h | m^2^ | | 0^-9^ | G | M | | c | | | | | UL | bit/s | bit/s | | ompre | | | | | | | | | ssion | | | | | 1 | | | | HD | | | | | 0^-7^ | | | | video | | | | | DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 0 | 1000 | 1 s | 1 | 500 | 20 | | one | | km/h | m^2^ | | 0^-9^ | M | M | | e | | | | | UL | bit/s | bit/s | | vents | | | | | | | | | UHD | | | | | 10^ | | | | | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 0 | 1000 | 1 s | 10^ | 200 | 20 | | one | | km/h | m^2^ | | -8^UL | M | M | | e | | | | | | bit/s | bit/s | | vents | | | | | 1 | | | | UHD | | | | | 0^-7^ | | | | | | | | | DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 7 | 1000 | 1 s | 10^ | 100 | 20 | | two | | km/h | m^2^ | | -8^UL | M | M | | e | | | | | | bit/s | bit/s | | vents | | | | | 10^ | | | | UHD | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 7 | 1000 | 1 s | 10^ | 80 | 20 | | two | | km/h | m^2^ | | -8^UL | M | M | | e | | | | | | bit/s | bit/s | | vents | | | | | 10^ | | | | HD | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 200 | 1000 | 1 s | 10^ | 20 | 10 | | three | | km/h | m^2^ | | -7^UL | M | M | | e | | | | | | bit/s | bit/s | | vents | | | | | 10^ | | | | UHD | | | | | -7^DL | | | | (Note | | | | | | | | | 2) | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Tier | 5 | 200 | 1000 | 1 s | 10^ | 10 | 10 | | three | | km/h | m^2^ | | -7^UL | M | M | | e | | | | | | bit/s | bit/s | | vents | | | | | 10^ | | | | HD | | | | | -7^DL | | | | (Note | | | | | | | | | 2) | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | R | 5 | 7 | 1000 | 6 ms | 10^ | 200 | 20 | | emote | | km/h | m^2^ | | -8^UL | M | M | | OB | | | | | | bit/s | bit/s | | | | | | | 10^ | | | | | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 10 | 500 | 700 | 40 ms | 10^ | 100 | 20 | | g | | km/h | km2 x | | -8^UL | M | M | | round | | | 6000 | | | bit/s | bit/s | | to | | | m | | 10^ | | | | air | | | (Note | | -7^DL | | | | UHD | | | 3) | | | | | | up | | | | | | | | | Link | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 10 | 500 | 700 | 40 ms | 10^ | 80 | 20 | | g | | km/h | km2 x | | -8^UL | M | M | | round | | | 6000 | | | bit/s | bit/s | | to | | | m | | 10^ | | | | air | | | (Note | | -7^DL | | | | HD up | | | 3) | | | | | | link | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 2 | 500 | 700 | 40 ms | 10^ | 20 | 100 | | air | | km/h | km2 x | | -7^UL | M | M | | to | | | 6000 | | | bit/s | bit/s | | g | | | m | | 10^ | | | | round | | | (Note | | -8^DL | | | | | | | 3) | | | | | | UHD | | | | | | | | | down | | | | | | | | | Link | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 2 | 500 | 700 | 40 ms | 10^ | 20 | 80 | | air | | km/h | km2 x | | -7^UL | M | M | | to | | | 6000 | | | bit/s | bit/s | | g | | | m | | 10^ | | | | round | | | (Note | | -8^DL | | | | HD | | | 3) | | | | | | down | | | | | | | | | link | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 10 | 200 | 1 | 3 ms | 10^ | 100 | 20 | | radio | | km/h | km^2^ | | -8^UL | M | M | | C | | | | | | bit/s | bit/s | | amera | | | | | 10^ | | | | UHD | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NPN | 10 | 200 | 1 | 3 ms | 10^ | 80 | 20 | | radio | | km/h | km^2^ | | -8^UL | M | M | | c | | | | | | bit/s | bit/s | | amera | | | | | 10^ | | | | HD | | | | | -7^DL | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NOTE | | | | | | | | | 1: | | | | | | | | | P | | | | | | | | | acket | | | | | | | | | error | | | | | | | | | rate | | | | | | | | | calcu | | | | | | | | | lated | | | | | | | | | c | | | | | | | | | onsid | | | | | | | | | ering | | | | | | | | | 1500 | | | | | | | | | B | | | | | | | | | pac | | | | | | | | | kets, | | | | | | | | | and 1 | | | | | | | | | error | | | | | | | | | per | | | | | | | | | hour | | | | | | | | | as | | | | | | | | | PE | | | | | | | | | R$= \ | | | | | | | | | frac{ | | | | | | | | | 10^{- | | | | | | | | | 5}}{ | | | | | | | | | 3x}$, | | | | | | | | | where | | | | | | | | | $x$ | | | | | | | | | is | | | | | | | | | the | | | | | | | | | data | | | | | | | | | rate | | | | | | | | | in | | | | | | | | | Mb | | | | | | | | | it/s. | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | 2: | | | | | | | | | Could | | | | | | | | | use | | | | | | | | | e | | | | | | | | | ither | | | | | | | | | pr | | | | | | | | | ofess | | | | | | | | | ional | | | | | | | | | equi | | | | | | | | | pment | | | | | | | | | or | | | | | | | | | m | | | | | | | | | obile | | | | | | | | | phone | | | | | | | | | equ | | | | | | | | | ipped | | | | | | | | | with | | | | | | | | | dedi | | | | | | | | | cated | | | | | | | | | new | | | | | | | | | sgath | | | | | | | | | ering | | | | | | | | | app | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | 3: | | | | | | | | | 6000 | | | | | | | | | m = | | | | | | | | | h | | | | | | | | | eight | | | | | | | | | but | | | | | | | | | in a | | | | | | | | | cone | | | | | | | | | form | | | | | | | | | ation | | | | | | | | | (i.e. | | | | | | | | | g | | | | | | | | | round | | | | | | | | | cov | | | | | | | | | erage | | | | | | | | | with | | | | | | | | | a | | | | | | | | | c | | | | | | | | | ircle | | | | | | | | | of | | | | | | | | | dia | | | | | | | | | meter | | | | | | | | | 30 | | | | | | | | | km) | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+
## 8.2 Non-Public network
Based on MNO and NPN policy, the 5G system shall support a mechanism to enable
MNO to update the subscription of an authorized UE in order to allow the UE to
connect to a desired NPN. This on-demand mechanism should enable means for a
user to request on-the-spot network connectivity which is authorized by its
MNO.
The 5G system shall enable an NPN to be able to request a third-party service
provider to perform NPN access network authentication of a UE based on
non-3GPP identities and credentials supplied by the third party service
provider.
The 5G system shall enable an NPN to support multiple third-party service
providers.
The 5G system shall enable an NPN to be able to request a PLMN to perform NPN
access network authentication of a UE based on 3GPP identities and credentials
supplied by the PLMN.
Based on operator policy, the 5G system shall support a mechanism to provision
on-demand connectivity (e.g. IP connectivity for remote provisioning). The 5G
system shall support an on-demand mechanism for a user (human or software) to
request on-the-spot network connectivity while providing operators with
identification and security tools for the provided connectivity.
The 5G system shall support a secure mechanism for a network operator of an
NPN to remotely provision the non-3GPP identities and credentials of a
uniquely identifiable and verifiably secure IoT device.
## 8.3 Multicast and broadcast service
Editor\'s note: text to be provided or section to be voided
## 8.4 Clock synchronization
The 5G network shall be able to provide time reference information to a 3rd
party application acting as a master clock with an accuracy of one
microsecond.
## 8.5 AVPROD application specific requirements
The 5G system shall support media flows from open standard based broadcast
workflows and be agnostic to the data carried
## 8.6 Service Continuity
The 5G system shall be able to securely reconnect within 20 ms from UE
starting first network connection attempt after a UE network connection loss.
The 5G system shall support uplink and downlink service continuity,
maintaining acceptable performance requirements, while switching between co-
located PLMN and NPN (e.g., due to mobility).
The 5G system shall support service continuity by maintaining acceptable
performance requirements: for an uplink stream while performing traffic
steering, switching, and splitting among co-located PLMN(s) and NPN(s) and for
downlink while switching between co-located PLMN and NPN.
## 8.7 Network Exposure Requirements
The 5G system shall support mechanisms to allow 3^rd^ party applications to
update the UE configuration for a UE or group of UEs using the application.
## 8.8 Multi-network connectivity and service delivery
The 3GPP system shall be able to enable a UE to receive low-latency downlink
multicast traffic from one network (e.g. NPN), and paging as well as data
services from another network (e.g. PLMN) simultaneously.
NOTE: Depending on the capabilities and configurations of the UE, limitations
of data-rate and latency may be acceptable.
###### ### Annex A: Real-time audio-streaming latency budget
Many factors influence the total latency that is experienced by a user. In
most of the use cases presented in this study item, the users of the system
will by often in situations where they can easily perceive latency increase,
which poses requirements that are much tighter in comparison to other
voice/audio applications.
Figure A-1 General representation of a wireless production audio system
Figure A-1 shows the elements involved in the communication of professional
audio production. On one hand there is a wireless audio input, which is
represented here as a microphone. The audio is captured at the audio source
and sent over a wireless connection to an audio processing device, which could
be performing mixing of several audio inputs, transcoding, equalization, or
other processing tasks. The result of the processing step is sent over the
wireless output device, which could be either a loudspeaker, an IEM, a
general-purpose device equipped with headphones, or other.
Figure A-2 Time diagram representing the frame capturing, processing, and
playback of a wireless production audio system
The latency elements related to the system in Figure A-1 are shown in Figure
A-2, which includes the time diagram of the audio input, processing and
mixing, and output devices, as well as the communication steps between these
elements. This diagram can be used to clarify what is the total mouth-to-ear
latency, which would be the total latency perceived by a user of this system.
The first latency element is determined by the frame size T~frame~. Digital
audio transmissions are performed mostly by splitting the audio signal into
frames, usually of fixed size in samples, meaning that each audio frame is
collected and processed at regular intervals. This process introduces a
latency of a frame size T~frame~, when no other step is considered. T~frame~
equals the transfer interval in which audio data transfers are initiated. The
size of T~frame~ is not necessarily derived by the audio application and may
be adjusted for optimized data transmission, e.g. for matching the wireless
system transfer interval.
Next, there are steps of audio encoding, processing, and decoding, summed up
with T~processing~, which depends on specific implementations, and will be
performed in each of the elements on the audio stream path. For the sake of
clarity, this element will be considered as being equal in the audio input,
processing device and output.
On the air interface, there is the delay for transmitting the audio frames. As
with any wireless transmission mechanism, this delay is variable, depending on
factors such as network overload, path loss, interference conditions, and the
tradeoff between bandwidth, transmission time and reliability. On one hand,
the transition from audio to radio subsystem often introduces a misalignment
delay. This is caused by the fact that wireless transmissions often can only
start to certain periodic points in time. If the audio application is not
aligned to these, a delay T~radio\ misalign~ is introduced when waiting for
the next possibility to begin a transmission. In worst case this delay is of
the size of one radio transfer period e.g. the slot time. If the radio
subsystem provides an interface to enable the alignment of T~frame~ to the
radio transfer period, this delay could be reduced to T~radio\ misalign~ â‰ˆ 0.
The other delay term T~delay~ refers to the delay caused by protocol messages
exchange, scheduling, encoding/decoding, as well as the wireless transmission
itself. Both of these terms are used to represent the end-to-end delay for the
wireless transmission T~end-to-end~ = T~radio\ misalign~ + T~delay~.
Due to wireless transmission jitter, each audio receiving node has to buffer
enough data in order to deal with the receiving time uncertainty.
Additionally, when mixing audio data from multiple inputs or playing audio on
multiple output devices that are perceived simultaneously by a single
listener, time synchronization of audio samples on each audio device is
required. In order to do so, the system also requires buffering of data to
deal with different delays on different links and furthermore the availability
of a clock synchronization service in the application as described above. The
delay introduced by such buffers will be referred here as T~audio\ async~.
Depending on restrictions of operating systems on some audio playback devices,
the size of such audio buffers is sometimes only available in multiples of
T~frame~.
With all these elements in mind, the latency budget for a complete system with
2 wireless links and a audio mixing and processing, such as the one shown in
Figure A-2 is
* * *
T~latency~ = T~frame~ + 3Ã—T~processing~ + 2Ã—T~End-to-end~ + 2 T~audio_async~.
Equation 1
* * *
Considering a worst-case scenario where T~audio_async~ â‰ˆ T~frame~, we get the
latency estimation as
* * *
T~latency,\ worst\ case~ â‰ˆ 3Ã—T~frame~ + 3Ã—T~processing~ + 2Ã—T~End-to-end~.
Equation 2
* * *
If an interface allows the alignment of the audio devices to the radio
transfer period, the radio misalignment can be reduced to T~radio\ misalign~ â‰ˆ
0. In an optimal case where the jitter of wireless transmission is smaller
than the size of a single audio sample, T~audio\ async~ can also be assumed to
be â‰ˆ 0, resulting in an optimal mouth-to-ear latency of
* * *
T~latency,\ optimal~ â‰ˆ T~frame~ + 3Ã—T~processing~ + 2Ã—T~delay~. Equation 3
* * *
If only one-way transmission is considered, e.g. UL only transmission from a
wireless microphone to a wired loudspeaker, the latency estimations of
Equation 1,Equation 2, and Equation 3 are reduced to:
T~latency,\ 1\ way~ = T~frame~ + 2Ã—T~processing~ + T~ent-to-end~ + T~audio\
async~. Equation 4
* * *
T~latency,\ 1\ way,\ worst\ case~ â‰ˆ 2 Ã—T~frame~ + 2Ã—T~processing~ + T~end-to-
end~. Equation 5  
T~latency,\ 1\ way,\ optimal,~ â‰ˆ T~frame~ + 2Ã—T~processing~ + T~delay~.
Equation 6
###### ### Annex B: Overview of AV system structure using point to multipoint
The objective of this section is to clarify the structure and terms used in in
use cases of this study including point to multipoint communications. This
overview is presented here in order to make clear the performance KPIs that
are presented in tables for each use case.
Figure B-1 shows an overview of an AV system with point to multipoint
communications. In such systems there may be several wireless devices used as
inputs, such as wireless microphones, musical instruments, communication
devices, and cameras, which could be actively sending an UL stream. In this
diagram, several devices could be connected to the AV system, and a number of
these devices could be sending data simultaneously to the central
mixing/processing. The number of simultaneous streams is represented in this
figure and in the tables of the use cases of this study as "'UL streams". The
content of each of UL stream may be combined, mixed and processed by a central
unit that converts all of these inputs into a number of logical streams,
represented in the figure as "# DL streams". As one example, one DL stream may
be a combination of several musical instruments of a band, or the combination
of several cameras. Each of these DL streams may be received by several UEs on
DL, and the combination of all the UEs receiving one of the DL streams is
represented in the figure as "# of active UEs". As one example, many displays
may be playing the tame video content, or several IEM may be playing the same
audio content for a big band.
{width="6.2868055555555555in" height="3.020138888888889in"}
Figure B-1: General overview of an AV system comprising audio/video sources,
and audio/video receivers
#