# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
In the present document, modal verbs have the following meanings:
**shall** indicates a mandatory requirement to do something
**shall not** indicates an interdiction (prohibition) to do something
The constructions \"shall\" and \"shall not\" are confined to the context of
normative provisions, and do not appear in Technical Reports.
The constructions \"must\" and \"must not\" are not used as substitutes for
\"shall\" and \"shall not\". Their use is avoided insofar as possible, and
they are not used in a normative context except in a direct citation from an
external, referenced, non-3GPP document, or so as to maintain continuity of
style when extending or modifying the provisions of such a referenced
document.
**should** indicates a recommendation to do something
**should not** indicates a recommendation not to do something
**may** indicates permission to do something
**need not** indicates permission not to do something
The construction \"may not\" is ambiguous and is not used in normative
elements. The unambiguous constructions \"might not\" or \"shall not\" are
used instead, depending upon the meaning intended.
**can** indicates that something is possible
**cannot** indicates that something is impossible
The constructions \"can\" and \"cannot\" are not substitutes for \"may\" and
\"need not\".
**will** indicates that something is certain or expected to happen as a result
of action taken by an agency the behaviour of which is outside the scope of
the present document
**will not** indicates that something is certain or expected not to happen as
a result of action taken by an agency the behaviour of which is outside the
scope of the present document
**might** indicates a likelihood that something will happen as a result of
action taken by some agency the behaviour of which is outside the scope of the
present document
**might not** indicates a likelihood that something will not happen as a
result of action taken by some agency the behaviour of which is outside the
scope of the present document
In addition:
**is** (or any other verb in the indicative mood) indicates a statement of
fact
**is not** (or any other negative verb in the indicative mood) indicates a
statement of fact
The constructions \"is\" and \"is not\" do not indicate requirements.
# 1 Scope
The present document studies the enhancements of MDA (Management Data
Analytics).
In this TR, the MDA use cases and the corresponding potential requirements are
identified and documented, and their possible solutions with analytics input
and output (report) are developed and evaluated.
The study also captures the MDA functionality and service framework, MDA
process, MDA role in management loop and management aspects of MDA.
Moreover, the TR provides recommendations for the normative specifications
work in full alignment with SA5 5G Services Based Management Architecture
(SBMA).
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TS 28.550: \"Management and orchestration; Performance assurance\".
[3] 3GPP TS 28.533: \"Management and orchestration; Architecture framework\".
[4] 3GPP TS 28.530: \"Management and orchestration; Concepts, use cases and
requirements\".
[5] 3GPP TR 28.861: \"Study on the Self-Organizing Networks (SON) for 5G
networks\".
[6] 3GPP TR 28.805: \"Study on management aspects of communication services\".
[7] 3GPP TS 28.554: \"5G end to end Key Performance Indicators (KPI)\".
[8] 3GPP TS 28.552: \"Management and orchestration; 5G performance
measurements\".
[9] 3GPP TS 22.101: \"service aspects; service principles\".
[10] 3GPP TS 32.500: \"Telecommunication management; Self-Organizing Networks
(SON); Concepts and requirements\".
[11] 3GPP TS 37.816: \"Study on RAN-centric data collection and utilization
for LTE and NR\".
[12] 3GPP TS 37.320: \"Radio measurement collection for Minimization of Drive
Tests (MDT); Overall description\".
[13] 3GPP TS 23.501: \"System Architecture for the 5G System (5GS); Stage 2\".
[14] 3GPP TS 28.310: \"Energy efficiency of 5G\".
[15] 3GPP TR 21.866: \"Study on Energy Efficiency Aspects of 3GPP Standards\".
[16] 3GPP TS 26.247: \"Transparent end-to-end Packet-switched Streaming
Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP
(3GP-DASH)\".
[17] 3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia Telephony;
Media handling and interaction\".
[18] 3GPP TS 23.288: \"Architecture enhancements for 5G System (5GS) to
support network data analytics services\".
[19] 3GPP TS 28.313: \"Self-Organizing Networks (SON) for 5G networks\".
[20] 3GPP TS 28.541: \"Management and orchestration; 5G Network Resource Model
(NRM); Stage 2 and stage 3\".
[21] 3GPP TS 38.304 NR: \"User Equipment (UE) procedures in idle mode and in
RRC Inactive state\".
[22] 3GPP TS 28.545: \"Management and orchestration; Fault Supervision (FS)\".
[23] 3GPP TS 28.813: \"Study on new aspects of Energy Efficiency (EE) for
5G\".
[24] 3GPP TS 28.406: \"Telecommunication management; Quality of Experience
(QoE) measurement collection; Information definition and transport\".
[25] 3GPP TS 32.422: \"Telecommunication management; Subscriber and equipment
trace; Trace control and configuration management\".
[26] 3GPP TS 32.425: \"Telecommunication management; Performance Management
(PM); Performance measurements Evolved Universal Terrestrial Radio Access
Network (E-UTRAN)\".
[27] 3GPP TS 23.273: \"5G System (5GS) Location Services (LCS); Stage 2\".
[28] 3GPP TS 28.532: \"Management and orchestration; Generic management
services\".
[29] 3GPP TS 26.247: \"Transparent end-to-end Packet-switched Streaming
Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP
(3GP-DASH)\".
[30] 3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia Telephony;
Media handling and interaction\".
[31] 3GPP TS 28.405: \"Telecommunication management, Quality of Experience
(QoE) measurement collection; Control and configuration\".
[32] GSMA NG.116: \"Generic Network Slice Template\".
[33] 3GPP TS 28.531: \"Management and orchestration; Provisioning\".
[34] 3GPP TS 28.628: \"Telecommunication management; Self-Organizing Networks
(SON) Policy Network Resource Model (NRM) Integration Reference Point (IRP);
Information Service (IS)\".
# 3 Definitions of terms, symbols and abbreviations
## 3.1 Terms
For the purposes of the present document, the terms given in 3GPP TR 21.905
[1] and the following apply. A term defined in the present document takes
precedence over the definition of the same term, if any, in 3GPP TR 21.905
[1].
## 3.2 Symbols
Void.
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
AI Artificial Intelligence
MDA Management Data Analytics
MDAS Management Data Analytics Service
ML Machine Learning
# 4 Concepts and overview
## 4.1 Overview
The Management Data Analytics is defined in TS 28.550 [2] and has also been
mentioned by various other technical specifications and reports including for
example TS 28.533 [3], TS 28.530 [4], TR 28.861 [5] and TR 28.805 [6].
The MDA provides a capability of processing and analysing the raw data related
to network and service events and status (e.g., performance measurements,
Trace/MDT/RLF/RCEF reports, QoE reports, alarms, configuration data, network
analytics data, and service experience data from AFs, etc.) to provide
analytics report (including recommended actions) to enable the necessary
actions for network and service operations.
The MDA, in conjunction with Artificial Intelligence (AI) and Machine Learning
(ML) techniques, brings intelligence and automation to the network service
management & orchestration.
MDA can help to perform management tasks in preparation, commissioning,
operation as well as in the termination phases. For example, MDA can support
service provisioning by preparing service catalogues, evaluating network
requirements for a new service and carrying out feasibility check. During
operation phase, the MDA can identify ongoing issues impacting the performance
of the network and service, and discover in advance potential issues that
would cause potential failure and/or performance degradation. The MDA can also
assist to predict the network and service demand to enable the timely resource
provisioning and deployments which would allow fast time-to-market network and
service deployment.
The MDAS can be consumed by various consumers, for instance the MFs (i.e., MnS
service producers/consumers for network and service management), NFs (e.g.,
NWDAF), SON functions, network and service optimization tools/functions, SLS
assurance functions, human operators, and AFs, etc.
The MDA is an enabler for the automation and cognition of the network and
service management & orchestration.
## 4.2 MDA functionality and service framework
Figure 4.2-1 illustrates the MDA functionality and service framework.
Depending on the scenario, MDA producer may collect data for analysis by
acting as an MnS Consumer, and/or as an NWDAF subscriber, and/or as a consumer
of another MDAS producer. After analysis, the MDAS Producer exposes the
analysis results to MDAS Consumer(s).
Figure 4.2-1: Functional overview and service framework of MDA
# 5 MDA process and role
## 5.1 MDA role in management loop
The MDA forms a part of the management loop (which can be open loop or closed
loop, see TS 32.500 [10]), and it brings intelligence and generates value by
_processing and analysis of management and network data, where the AI and ML
techniques may be utilized._
The MDA plays the role of Analytics in the management loop illustrated in
figure 5.1-1 below.
Figure 5.1-1: Analytics in management loop
**Observation:** The observation of the managed networks and services. The
observation involves monitoring and collection of events, status and
performance of the managed networks and services, and providing the
observed/collected data (e.g., performance measurements, Trace/MDT/RLF/RCEF
reports, network analytics reports, QoE reports, alarms, etc).
**Analytics:** The data analytics for the managed networks and services. The
MDA described in this TR plays the role of Analytics in the management loop.
The MDA prepares, processes and analyses the data related to the managed
networks and services, and provides the analytics reports for root cause
analysis of ongoing issues, prevention of potential issues and prediction of
network or service demands. The analytics report contains the description of
the issues or predictions with optionally a degree of confidence indicator,
the possible causes for the issue and the recommended actions. Techniques such
as AI and ML (e.g., ML model) may be utilized by MDA with the input data
including not only the observed data of the managed networks and services, but
also the execution reports of actions (taken by the execution step). The MDA
classifies and correlates the input data (current and historical data), learns
and recognizes the data patterns, and makes analysis to derive inference,
insight and predictions.
**Decision:** The decision making for the management actions for the managed
networks and services. The management actions are decided based on the
analytics reports (provided by MDA) and other management data (e.g.,
historical decisions made previously) if necessary. The decision may be made
by the consumer of MDAS (in the closed management loop), or a human operator
(in the open management loop). The decision includes what actions to take, and
when to take the actions.
**Execution:** The execution of the management actions according to the
decisions. During the execution step, the actions are carried out to the
managed networks and services, and the reports (e.g., notifications, logs) of
the executed actions are provided.
## 5.2 Management interaction with NWDAF and gNB
There are two types of data analytics services, one is the network data
analytics service provided by NWDAF, another is the MDAS provided by 3GPP
management system. The MDAS producer provides the analytics data for
management purposes based on the data related to different types of NFs or
entities in the network, e.g., data reported from gNB and other core network
functions. Depending on the scenario and when needed, the MDAS producer may
use the analytics results of NWDAF as input.
MDAS Producer may be deployed as 3GPP domain-specific (e.g., RAN or CN) or as
3GPP cross-domain. Figure 5.2-1 shows an example of the coordination between
NWDAF, gNB and MDAS producer(s) for data analytics purpose.
1\. The NWDAF may consume the MDAS for identified scenarios and provide
analytics service for 5GC NF for control purpose.
2\. The CN Domain MDAS producer may consume the service provided by NWDAF and
other 5GC NFs and provide analytics data for management purpose.
3\. The gNB many consume the MDAS for identified scenarios for RAN control
purpose.
4\. The RAN Domain MDAS producer may consume the service provided by gNB and
provide analytics data for management purpose.
5\. The 3GPP cross domain MDAS Producer may consume (acting as Domain MDAS
consumer) MDAS provided by domain-specific (RAN and/or CN) MDAS producer(s),
and produce MDAS that may be consumed by 3GPP cross-domain MDAS consumer(s).
{width="4.798611111111111in" height="4.298611111111111in"}
Figure 5.2-1: Example of coordination between NWDAF, gNB and MDAS producer for
data analytics
Figure 5.2-2 shows another example of the coordination between NWDAF and MDAS
producer for data analytics purpose.
1\. The NWDAF may consume the MDAS for identified scenarios and provide
analytics service for 5GC NF for control purpose.
2\. The gNB may consume the MDAS for identified scenarios for RAN control
purpose.
3\. The Domain MDAS producer may consume the service provided by NWDAF, other
5GC NFs and gNB, provide analytic data for management purpose.
{width="3.9027777777777777in" height="3.201388888888889in"}
Figure 5.2-2: Example of coordination between NWDAF, gNB and MDAS producer for
data analytics
## 5.3 MDA process
This clause illustrates an example of MDA process scenario where the ML model
and the management data analysis module are residing in the MDAS producer,
other scenarios have not been addressed by the present document.
The MDA may rely on ML technologies, which may need the consumer to be
involved to optimize the accuracy of the MDA results.
The MDA process in terms of the interaction with the consumer, when utilizing
ML technologies, is described in the figure below.
Figure 5.3-1: Example of MDA process
There are two kinds of processes for MDA, the process for ML model training
and the process for management data analysis. In the process for ML model
training, the MDA producer, trains the ML model and provides the ML training
report. The process for ML model training may also get the consumer involved,
i.e., allowing the consumer to provide input for ML model training. The ML
model training may be performed on an un-trained ML model or a trained ML
model. In the process for management data analysis, the MDA producer analyses
the data by the trained ML model, and provides the analytics report to the
consumer. The MDAS consumer may validate the training report and analytics
report and provide a report validation feedback to the MDAS producer.
For each received report the MDAS consumer may provide a feedback towards the
MDAS producer, which may be used to optimize ML model.
**Data classification** : The data input to the MDA producer could be used for
ML model training or for the actual management data analysis. The MDA producer
classifies the input data and passes the classified data along to
corresponding step for further processing.
**ML model training** : The MDAS producer trains the ML model, i.e., to train
the algorithm of the ML model to be able to provide the expected training
output by analysis of the training input. The data for ML model training may
be the training data (including the training input and the expected output)
and/or the report validation feedback provided by the consumer. After the ML
model training, the MDAS producer provides an ML model training report.
**Management data analysis** : The trained ML model analyses the classified
data and generates the management data analytics report(s). Analytics reports
were presented in clause 5.1.
**Report validation** : The consumer may validate the report provided by the
MDAS producer. The report to be validated may be the analytics report and/or
the ML model training report as described above. The consumer may provide a
feedback to the MDAS producer.
As a result of validation, the consumer: (i) may also provide training data
and request to train the ML model and/or (ii) provide feedback indicating the
scope of inaccuracy, e.g. time, geographical area, etc.
# 6 Use cases, potential requirements and possible solutions
## 6.1 Coverage related issues
### 6.1.1 Coverage issue analysis
#### 6.1.1.1 Use case
The coverage issue may cause various UE and network failures and degrade the
network performance offered the UEs.
The coverage issue could be a weak coverage, a coverage hole, a pilot
pollution, an overshoot coverage, or a DL and UL channel coverage mismatch as
described in clause 5.1.1, 3GPP TS 37.816 [11]. The weak coverage may result
in low success rate of random access, paging, RRC connection establishment and
handover, low data throughput, more abnormal releases of RRC connection, DRB
retainability, QoS flow and/or PDU session resources, and dissatisfied QoE.
The coverage hole is a more severe problem and would further lead to the UE
out of service in the area.
The 5G related coverage issue may exist only in 5G (i.e., 5G issue only with
good coverage provided by other RATs) or exist in all RATs (i.e., no RAT
provides good coverage in the area).
Coverage performance should be assured to guarantee user service experience.
It is desirable that the coverage issue can be detected by MDA from the
various symptoms, together with the geographical and terrain data and the
configuration parameters of the RAN.
Once the coverage issue is detected, the MDAS producer provides the analytics
report that precisely describes the coverage issue, and the analytics report
needs to contain sufficient information to enable the MDAS consumer (e.g., SON
CCO function) to take the remedy actions. The MDAS producer may also provide
the recommended actions to solve the identified coverage issue in the
analytics report, so that the MDAS consumer can execute the actions
accordingly or by taking the recommended actions into account.
The MDAS producer is informed when the actions are taken by the MDAS consumer
to solve the coverage issue described in the analytics report, so that the
MDAS producer can start evaluating the result of the executed actions.
The MDAS producer gets the execution reports describing the actions taken by
the MDAS consumer, and takes the execution reports into account to fine-tune
the accuracy of the future (new or updated) analytics report.
The MDAS producer also provide update(s) of the analytics report to indicate
the status change (e.g., solved, mitigated or deteriorated) of the coverage
issue.
#### 6.1.1.2 Potential requirements
**REQ-COV_ANA-CON-1** The MDAS producer should have a capability to provide
the analytics report describing the coverage issue.
**REQ-COV_ANA-CON-2** The analytics report describing the coverage issue
should contain the following information:
\- The identifier of the coverage issue described in the analytics report;
\- Indication of whether the coverage issue is weak coverage or coverage hole,
a pilot pollution, an overshoot coverage, or a DL and UL channel coverage
mismatch;
\- The start time and end time of the coverage issue;
\- The geographical area and location where the coverage issue exists;
\- Root cause of the coverage issue;
\- Whether the coverage issue exists in 5G only or in all RATs;
\- The cells affected by the coverage issue;
\- The severity level (e.g., critical, medium, or cleared) of the coverage
issue;
\- The recommended actions to solve the coverage issue.
#### 6.1.1.3 Possible solutions
##### 6.1.1.3.1 Solution description
The MDAS producer correlates, processes and analyses the data described in the
following subclause within a time period on a regular basis or trigged by
events (e.g., the RLF reports) to identify the coverage issue, and provide the
analytics reports to describe the identified coverage issues (which could be
new issues or the updates of existing issues).
##### 6.1.1.3.2 Data required for coverage issue analysis
The following table describes the data required for coverage issue analysis:
+----------------------------------+----------------------------------+ | Data category | Required data | +==================================+==================================+ | Performance measurements | - Average/distribution of UE | | | reported RSRPs/RSRQs/SINRs of | | | the serving cell when the TA | | | (Timing Advance) or UE rx-tx | | | applied to the UEs is in a | | | specific range; | | | | | | - Average/distribution of UE | | | reported RSRPs/RSRQs/SINRs of | | | each neighbour cell when the UE | | | reported RSRPs/RSROs of the | | | serving cell is in a specific | | | range, measured per NCR | | | (neighbour cell relation), per | | | SSB index and per CSI-RS index | | | of each NCR; | | | | | | - Number of abnormal releases | | | of DRBs, QoS flows, PDU | | | sessions, and UE contexts in the | | | serving cell measured per SSB | | | index and per CSI-RS index | +----------------------------------+----------------------------------+ | MDT reports | MDT reports containing RSRPs and | | | RSRQs of the serving cell and | | | neighbour cells reported by each | | | UE with anonymous id (e.g., | | | C-RNTI) and location | | | information. | +----------------------------------+----------------------------------+ | RLF reports | RLF reports containing RSRP(s) | | | and RSRQ(s) of the serving cell | | | and neighbour cells reported by | | | each UE with anonymous id (e.g., | | | C-RNTI) and location | | | information. | +----------------------------------+----------------------------------+ | RECF reports | RCEF reports containing RSRP(s) | | | and RSRQ(s) of the serving cell | | | and neighbour cells reported by | | | each UE with anonymous id (e.g., | | | C-RNTI) and location | | | information. | +----------------------------------+----------------------------------+ | UE location reports | UE location information provided | | | by the LCS which can be used to | | | correlate with the MDT/RLF/RCEF | | | reports. | +----------------------------------+----------------------------------+ | Geographical data and terrain | - The geographical information | | data of the RAN | (longitude, latitude, altitude) | | | of the deployed RAN (gNBs and | | | eNodeBs, antennas, sector | | | carrier equipment, etc.). | | | | | | - The terrain data for the area | | | of the deployed RAN. | +----------------------------------+----------------------------------+ | Configuration data | - The current NRMs containing | | | the attributes affecting the RAN | | | coverage, such as maximum | | | transmission power of the cell, | | | directions and tilts of the | | | antennas or beams, etc. | | | | | | - The NRM update reports | | | (notifications or logs) | | | containing the creations or | | | changes of the MOIs (Managed | | | Object Instance) affecting the | | | RAN coverage. | +----------------------------------+----------------------------------+
##### 6.1.1.3.3 Analytics report for coverage issue
The analytics report describing the coverage issue contains the following
information:
\- Coverage issue identifier: The identifier of the coverage issue;
\- Coverage issue type indication: Indication that the coverage issue is weak
coverage or coverage hole, pilot pollution, overshoot coverage, or DL and UL
channel coverage mismatch;
\- Start time: The start time of the coverage issue;
\- Stop time: The stop time of the coverage issue;
\- Location: The geographical area and location where the coverage issue
exists;
\- Root cause: Root cause of the coverage issue, e.g., weak transmission
power, blocked by constructions, restricted by terrain, etc;
\- RAT indication: Indication that the coverage issue exists in 5G only or in
all RATs;
\- Affected objects: The MOIs of the cells affected by the coverage issue;
\- Severity level: The severity level (e.g., critical, medium, cleared) of the
coverage issue;
\- Recommended actions: The recommended actions to solve the coverage issue.
The recommended action could be re-configurations of coverage related
attributes, creation of new cells or beams, or manual operations to add or
change the physical units.
#### 6.1.1.4 Evaluation
The solution described in clause 6.1.1.3 requires the analytics inputs as
described in in clause 6.1.1.3.2, wherein
\- the performance measurements for E-UTRAN are available in TS 32.425 [26],
the similar performance measurements can be defined for NG-RAN in TS 28.552
[8].
\- the MDT reports, RLF reports, RECF reports for E-UTRAN and NG-RAN are
available in TS 32.422 [25].- the UE location reports can be accessed by
consuming the LCS via service-based interfaces as defined in TS 23.273 [27].
\- the geographical data and terrain data of the RAN can be modelled in the
NRMs.
\- the configuration data for NG-RAN have been defined in TS 28.541 [20].
The coverage issue analysis may require full set or subset of the above-
mentioned input based different deployment scenarios (e.g., NSA, SA, and
Multi-RAT). With these analytics inputs which either are already defined or
can be defined in the normative work, the analytics output as described in
6.1.1.3.3 can be derived.
Therefore, this solution is a feasible candidate for coverage issue analysis.
### 6.1.2 Slice coverage optimization
#### 6.1.2.1 Use case
In providing a network slice, a 3^rd^ party (i.e. slice tenant) issues a slice
request indicating the desired SLA, which includes among other parameters as
indicated in GSMA NG.116 [32], the slice coverage (also referred to as
coverage area of the network slice or area of service as per GSMA NG.116
[32]). The main challenge of providing the slice coverage is to map the
desired geographical coverage area with the available radio coverage, which
depends on the base station planning and deployment. In 5G the notion of
coverage is represented by a set of one or more Tracking Areas (TAs), which
are contained in a Registration Area (RA), which is assigned to a UE once it
registers to the network.
In mapping the desired slice coverage into a geographical coverage area there
are two main challenges:
\- The coverage area in service profile need to be mapped to TA list assigned
to cells which are selected to support the slice coverage.
\- It is not guaranteed that the desired slice coverage can be mapped
perfectly to the "coverage footprint" of a cell or set of cells (that belong
to a TA) or that the desired slice coverage is even available since radio
coverage may not be available in certain areas.
Currently to resolve these problems more TAs and consequently cells may be
allocated to a slice to enhance the coverage, if such an option is available,
or more capacity resources can be provisioned in the allocated cells. Each
cell can only be associated to one TA at a time and an S-NSSAIList configured
in all cells that form a TA should be the same. This approach may prove to be
inefficient since it wastes network resources.
The distribution of users, the expected radio resource availability and
mobility patterns may govern the configuration parameters of each TA and cell
(e.g. antenna downtilt, SSB beamforming patterns determining the coverage of a
cell, handover parameters, etc.), which can be allocated per slice. Cell
configuration parameters may help to adjust the coverage.
To translate the business slice coverage to the actual radio deployment,
without overprovisioning while leveraging the benefits of flexible gNB radio
features adjustment, MDA can be used. MDA can enable an MDAS consumer to
optimize the slice coverage and load distribution on the slice instantiation
and runtime considering (i) slice-aware statistics, e.g., slice-UE
distributions and mobility patterns, (ii) slice SLA and (iii) access node
capabilities.
Depending on the MDAS producer output, TA and RA planning, i.e. grouping cells
to form a TA and then TAs to an RA, can be optimized and the RAN parameters
can be adjusted to shape the cell edges and load distribution. The main
objective is to fulfill a given slice SLA involving as few cells as possible
by leveraging the benefits of adjusting cell configurations for satisfying the
desired coverage.
#### 6.1.2.2 Potential requirements
**REQ-NS_COV_THR_OPT-1** The MDAS producer should have a capability to provide
the analytics report describing the slice coverage, slice availability and
slice prediction information to authorized consumers, e.g., gNB.
**REQ-NS_COV_THR_OPT-2** The analytics report should contain the following
information:
> \- the identifier that indicates the gNB configuration issue with respect to
> a slice request;
>
> \- the identifier that indicates the TA and RA optimization issue with
> respect to a slice request;
>
> \- the geographical location and the affected TA and/or cell configuration
> attributes;
>
> \- the start and stop time in where the report is valid;
>
> \- the originator issue, that may be a coverage problem due to inefficient
> TA and/or RAN configuration;
>
> \- the recommended actions that involve options to reconfigure TA and/or RAN
> attributes including HO parameters, cell reselection parameters, beam
> configuration, computing resource and slice support in a cell.
#### 6.1.2.3 Possible solutions
##### 6.1.2.3.1 Solution description
The solution considers slice coverage assessment with respect to RAN
deployment. The MDAS producer analyses performance measurements, slice
availability and mobility patterns to identify the RAN attribute configuration
options for adjusting selected cells in order to fulfill the SLA of a slice
request in terms of the coverage.
##### 6.1.2.3.2 Data required
The following data is required to do the required analysis.
+---------------------------------+-----------------------------------+ | **Data category** | **Required data** | +=================================+===================================+ | **Service Data** | S-NSSAI as defined in clause | | | 5.15.2, TS 23.501 [13]. MDAS | | | may derive network topology | | | information. | +---------------------------------+-----------------------------------+ | **Performance Measurements** | Radio resource utilization -- | | | usage of physical radio resource | | | utilization of the network as per | | | clause 5.1.1.3 of TS 28.552 | | | [8]. | | | | | | Virtual resource usage of NF: The | | | resource usage of virtual network | | | functions, see clause 5.7.1 of TS | | | 28.552 [8]. | +---------------------------------+-----------------------------------+ | **Beam Report per Slice** | Beam statistics per slice usage | | | in the serving cell: | | | | | | > \- Beam IDs (SSB IDs or CSI-RS | | | > IDs) which have been serving | | | > users of a specific network | | | > slice | | | > | | | > \- Temporal Information of | | | > these Beams serving a specific | | | > Slice, (i.e., when, e.g., | | | > morning or evening, and how | | | > long) | | | > | | | > \- Number of slice users served | | | > by a specific Beam ID | | | | | | Beam statistics of a neighboring | | | cell measured by specific slice | | | users: | | | | | | > \- Relevant neighboring Beam | | | > IDs (SSB IDs or CSI-RS IDs) | | | > which have been detected by | | | > users of a specific network | | | > slice | +---------------------------------+-----------------------------------+ | **Capacity Planning Data** | Capacity management as per clause | | | 5.4.15 of TS 28.530 [4] and | | | clause 7.15 of TS 28.531 [33]. | +---------------------------------+-----------------------------------+ | **TA Measurements** | - [TA load]{.underline}: Load | | | utilization of an indicated | | | TA | | | | | | - [TA usage]{.underline}: | | | Number of active/passive UEs | | | in TA per cell -- Indicates | | | the potential of | | | re-configuring a TA | | | | | | - [Intra/inter TA | | | handover]{.underline}: Number | | | of handovers among cells with | | | a TA and among cells between | | | neighboring TAs | +---------------------------------+-----------------------------------+ | **Slice Unavailability Report** | Slice unavailability in serving | | | cell: | | | | | | > \- Statistics about the | | | > rejected or remapped PDU | | | > sessions belonging to a | | | > specific slice | | | > | | | > \- How long the UEs stayed in a | | | > cell not supporting specific | | | > slices, e.g., short stay with | | | > Slice Unavailability | | | > | | | > \- Temporal Information of the | | | > UEs stayed in a cell not | | | > supporting specific slices, | | | > e.g., when these events | | | > occurred | | | > | | | > \- Number of UEs stayed in a | | | > cell not supporting specific | | | > slices | | | > | | | > \- Cell IDs to which the UEs | | | > reconnect after staying in a | | | > cell not supporting specific | | | > slices, e.g., after short stay | | | > with Slice Unavailability. | +---------------------------------+-----------------------------------+ | **MDT Reports** | UE measurements related to RSRPs, | | | RSRQs, RLF, RECF of the cells in | | | slice and UE location information | +---------------------------------+-----------------------------------+ | **NWDAF Mobility Analytics** | Group UE mobility analytics as | | | per TS 23.288 [18] | | | | | | All UE mobility analytics within | | | Area of Interest defined via | | | slice coverage as per TS 23.288 | | | [18] | +---------------------------------+-----------------------------------+ | **Configuration Data** | NRM attributes affecting the | | | radio configuration and virtual | | | NF resource allocation and | | | configuration | +---------------------------------+-----------------------------------+ | **Network Topology** | Topology of the network | +---------------------------------+-----------------------------------+
##### 6.1.2.3.3 Analytics report
##### 6.1.2.3.3.1 TA optimization
The cross domain MDAS producer output report contains the following
information:
+----------------------------+----------------------------------------+ | **Information** | **Description** | +============================+========================================+ | **TA Incident Identifier** | Identifier that indicates the TA | | | configuration case for slice coverage | | | enhancement (or slice unavailability) | +----------------------------+----------------------------------------+ | **Type of Analytics** | Statistics or Prediction | +----------------------------+----------------------------------------+ | **Recommended Actions** | List of mapping: | | | | | | - \ | | | ] > ] | | | | | | [ ]: represent a list | | | | | | TAI: tracking area identifier which is | | | comprise of PLMNId + TAC | | | | | | subArea: sub geography area | | | represented by geographic zones, or | | | scope of geo coordinates (latitude, | | | longitude and elevation), etc. | | | | | | \ : map between | | | sub geo area and tracking area, one | | | subArea can be translated to a TAI | | | list | | | | | | \ ] | | | >: map between on NSSP and sub geo | | | area. One NSSP can support a list of | | | sub geo areas, each sub geo area can | | | be translated to a list of TAIs. | +----------------------------+----------------------------------------+
The RAN domain MDAS producer output report contains the following information:
+----------------------------+----------------------------------------+ | **Information** | **Description** | +============================+========================================+ | **TA Incident Identifier** | Identifier that indicates the TA | | | configuration case for slice coverage | | | enhancement (or slice unavailability) | +----------------------------+----------------------------------------+ | **Type of Analytics** | Statistics or Prediction | +----------------------------+----------------------------------------+ | **Recommended Actions** | List of Mappings: | | | | | | [ \ ] | | | | | | [ ]: represent a list | | | | | | TAI: tracking area identifier which is | | | comprise of PLMNId + TAC | | | | | | subArea: sub geography area | | | represented by geographic zones, or | | | scope of geo coordinates (latitude, | | | longitude and elevation), etc. | | | | | | \ : map between | | | sub geo area and tracking area, one | | | subArea | | | | | | List of Cells: | | | | | | [ cell, TAC, [\] | | | ] | | | | | | [ ]: represent a list | | | | | | [\]: list of | | | attributes and corresponding values | | | for the attributes which were proposed | | | to be reconfigured. | | | | | | [ cell, TAC, [\] | | | ]: list of cells with assigned TAC | | | and (re)configured attributes for the | | | cells which were proposed to be | | | created or modified. | +----------------------------+----------------------------------------+
##### 6.1.2.3.3.2 gNB optimization
The network congestion analytics report contains the following information.
+--------------------------------+------------------------------------+ | **Information** | **Description** | +================================+====================================+ | **gNB Incident Identifier** | Identifier that indicates the gNB | | | configuration case for slice | | | coverage enhancement (or slice | | | unavailability) | +--------------------------------+------------------------------------+ | **Type of Analytics** | Statistics or Prediction | +--------------------------------+------------------------------------+ | **Location** | Geographical location affected by | | | the gNB incident | +--------------------------------+------------------------------------+ | **Affected Object Attributes** | Cell Configurations: Antenna Tilt, | | | HO parameters, cell reselection | | | parameters, beam configuration, | | | compute resources, etc. | +--------------------------------+------------------------------------+ | **Start/Stop Time** | Starts/stop time of the incident | +--------------------------------+------------------------------------+ | **Root Cause** | The originator of the issue, e.g. | | | user mobility, load peak, user | | | distribution, beam configuration, | | | etc. | +--------------------------------+------------------------------------+ | **Severity Level** | The severity level (e.g. critical, | | | medium, not important) | +--------------------------------+------------------------------------+ | **Recommended Actions** | Recommendation actions to resolve | | | the issue: | | | | | | > \- Antenna Tilt configuration | | | > options | | | > | | | > \- HO parameters configuration | | | > options | | | > | | | > \- Cell reselection | | | > configuration options | | | > | | | > \- Beam configuration options | | | > | | | > \- Compute resource | | | > configuration options | | | > | | | > \- Enable slice support in | | | > determined Cell(s) | +--------------------------------+------------------------------------+
#### 6.1.2.4 Evaluation
The solution described in clause 6.1.2.3.1 requires the analytics inputs as
described in clause 6.1.2.3.2, wherein:
\- Input PM on RAN resource utilization, Virtual resource usage of NF are
specified in TS 28.552 [8].
\- Input PM that need to be specified include:
> \- Beam statistics per slice usage.
>
> \- Slice unavailability related measurements.
\- MDT data is specified in TS 37.320 [12].
\- NWDAF mobility analytics are specified in TS 23.288 [18].
\- Configuration Data and network topology data, by the NRMs defined in TS
28.541 [20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.1.2.3.3 can be derived.
Therefore, this solution is a feasible candidate for slice coverage
optimization.
### 6.1.3 Paging optimization
#### 6.1.3.1 Use case
As per the current procedures, if the UE goes out-of-coverage (OOC) the paging
which was initiated by the network Access and Mobility Management Function
(AMF) fails. The re-attempts continue to fail until UE comes in the coverage
and reacts to the paging attempts. This repetitive paging attempts result in
the wastage of network resources. As an example, the use case includes a user
or a group of users getting into an area, with no cellular coverage on a
regular basis for a considerably long duration, for e.g., the user gets into a
shielded room for some testing purpose every day for a defined period. The
Network initiated paging for such users will fail until they are back in the
area with cellular coverage. This would result in in-efficient network
resource usage.
It is desirable to use MDAS (Management data analytic service) to optimize the
current paging procedures in 5G networks. MDAS producer provides an analytics
report containing the user(s) paging analytics indicating the time window at
which the user is OOC on a regular basis at the particular location and hence
will not be able to respond on a network-initiated paging. Based on the report
MDAS consumer (e.g., AMF, gNB) decides on whether, when and where to initiate
or not to initiate the paging procedures, thereby ensuring the efficient
paging procedures and optimal network resource utilization, as paging can be
initiated only when there are more chances for it to be successful.
#### 6.1.3.2 Potential Requirements
**REQ-PA_OPT_CON-1:** The MDAS producer should have a capability allowing the
authorized consumer to get the paging analytics report describing paging
results for a particular user or a group of users.
**REQ-PA_OPT_CON-2:** The MDAS producer should have a capability to provide
the paging analytics report describing the paging results based on successful
and un-successful paging attempts at a particular time and duration.
**REQ-PA_OPT_CON-3:** The paging analytics report describing the paging
results should contain the following information:
\- User Identification: Identification of the user or a group of users.
\- Daily-OOC-Duration: Identifying the time window during which UE is out-of-
coverage every day.
\- Daily-OOC-Location: Identifying the last known location before UE going
out-of-coverage every day.
\- Recommended Action: The recommendation may suggest stopping paging the UE
for Daily-OOC-Duration at Daily-OOC-Location.
#### 6.1.3.3 Possible Solutions
##### 6.1.3.3.1 Solution description
The solution requires MDAS producer to collect various data and provide the
paging analytics report. Daily-OOC-Duration and Daily-OOC-Location will be
included in the Paging Analytics Report mentioning the time window during
which UE is out-of-coverage every day at a particular location. The paging is
not initiated for the UE during the period provided as Daily-OOC-Duration if
the last know UE location is the location identified by Daily-OOC-Location.
##### 6.1.3.3.2 Data required
The consumer (e.g. AMF) can subscribe to obtain the paging analytics report
for a user or a group of the user from MDAS producer. The subscription request
may include identification for the target user or a group of the user,
reporting interval etc. The MDAS producer collects the following per UE per
day data from various sources.
Data category Required data
* * *
UE Paging Measurements Number of successful paging attempt. Successful
Timestamp: The timestamp for each successful paging attempt. Successful
Location: Last known location of UE. Number of un-successful paging attempt:
Total number of un-successful paging attempt. Un-Successful Timestamp: The
timestamp for each un-successful paging attempt. Un-Successful Location: Last
known location of UE.
##### 6.1.3.3.3 Analytics report
The paging analytics report contains the following information
Paging Analytics Report Attribute Name Description
* * *
                            User Identification   Identification of the user or a group of users.
                            Daily-OOC-Duration    Identifying the time window during which UE is out-of-coverage every day. This will be provided per UE.
                            Daily-OOC-Location    Identifying the last known location before UE going out-of-coverage every day. This will be provided per UE.
                            Recommended Action    The recommendation may suggest stopping paging the UE for Daily-OOC-Duration at Daily-OOC-Location. This will be provided per UE.
Based on the report and the recommendation, the consumer decides whether to
change the paging strategy for a particular UE or a group of UE. If the paging
policy needs to be changed, the AMF may decide whether, when and where to page
the UE. The AMF may not page the UE during the period provided as Daily-OOC-
Duration if the last know UE location is the location identified by Daily-OOC-
Location.
> NOTE: This use case may require MDAS to know UE Identifier. How that will be
> done is not addressed in the present document.
#### 6.1.3.4 Evaluation
The solution described in clause 6.1.3.3 requires the analytics inputs as
described in in clause 6.1.3.3.2, wherein
\- The UE location reports can be accessed by consuming the LCS via service-
based interfaces as defined in TS 23.273 [27].
\- The required UE paging data can be/should be defined as part of MDT data.
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.1.3.3 can be
derived.
Therefore, this solution is a feasible candidate for paging optimization.
## 6.2 Resource related issues
### 6.2.1 RAN user plane congestion analysis
#### 6.2.1.1 Use case
With the development of diverse communication services and the increasing
number of connections, user data volume demanded by end users grows rapidly
which may not be satisfied by the current deployed 5G network.
In clause 3.1 of TS 22.101 [9], the _RAN user plane congestion is defined as
the situation where the demand for RAN resources to transfer user data exceeds
the available RAN capacity to deliver the user data for a significant period
of time in the order of few seconds or longer. The case where_ a short-
duration burst of user plane traffic is not identified as RAN congestion.
Due to the complexity of 5G network and wireless environment, multiple types
of performance deteriorate are related with RAN user plane congestion, e.g.,
high drop rate of PDCP PDU, the full PRB utilisation, inappropriate mobility
parameters configuration or inefficient usage of radio resources. The root
causes should be analysed and identified to help to resolve the RAN user plane
congestion and improve the end users\' experience, e.g., the issue of lack of
physical or virtual resources or unsuitable resources allocation, unsuitable
mobility parameters. The recommended actions may also be provided, e.g.,
recommended policies of physical and virtual resources allocation, possible
means to improve the radio condition, load balancing mechanisms etc.
The producer of MDAS is able to, from the perspective of the management
aspects, provide the user plane data congestion analytics report related to a
specific cell, specific network slicing instance or subnetwork. This analytics
report can be considered as an input to support SLS assurance to perform
further evaluation.
#### 6.2.1.2 Potential requirements
**REQ-CONG_ANA-CON-1** : The MDAS producer should be able to provide the
analytics report describing the RAN user plane congestion problem.
**REQ-CONG_ANA-CON-2** : The analytics report describing the RAN user plane
congestion problem should contain the following information:
\- The identifier of the RAN user plane congestion;
\- Indication of the RAN user plane congestion type;
\- The start time and end time of the RAN user plane congestion;
\- The geographical area and location where the RAN user plane congestion
affects;
\- Root cause of the RAN user plane congestion;
\- The objects affected by the RAN user plane congestion;
\- The severity level of the RAN user plane congestion;
\- The recommended actions to solve the RAN user plane congestion problem.
#### 6.2.1.3 Possible Solutions
##### 6.2.1.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to identify the RAN user plane congestion problems. As the
table in clause 6.2.1.3.3 shows, the analytics report is able to be provided
by the MDAS producer to describe the root causes and recommendations of
identified RAN user plane problem. This procedure may be triggered by the
request or periodically.
##### 6.2.1.3.2 Data required for RAN user plane congestion problem
The following table shows the potential data required to analyse the RAN user
plane congestion problem.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | UE throughput: The IP throughput of end | | | users, see clause 5.1.1.3 of TS 28.552 | | | [8]; | | | | | | Radio resource utilization: The usage of | | | physical radio resource utilization of | | | the network, see clause 5.1.1.2 of TS | | | 28.552 [8]; | | | | | | PDCP Data Volume: The transmitted PDCP | | | data volume, see clause 5.1.2.1 and | | | 5.1.3.6 of TS 28.552 [8]; | | | | | | TB related measurements: The TB | | | transmitted in a cell, see clause | | | 5.1.1.7 of TS 28.552 [8]; | | | | | | CQI related measurements: the | | | distribution of Wideband CQI (Channel | | | Quality Indicator) reported by UEs in | | | the cell, see clause 5.1.1.11 of TS | | | 28.552 [8]; | | | | | | MCS related Measurements: the | | | distribution of the MCS scheduled for | | | PDSCH RB by NG-RAN, the distribution of | | | the MCS scheduled for PUSCH RB by | | | NG-RAN, see clause 5.1.1.12 in TS 28.552 | | | [8]; | | | | | | RAN UE throughput: A KPI that shows how | | | NG-RAN impacts the service quality | | | provided to an end-user, see clause | | | 6.3.6 of TS 28.554 [7]; | | | | | | Throughput for network slice instance: | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.2 and clause 6.3.3 of TS | | | 28.554 [7]; | | | | | | Throughput at N3 interface: | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR and UE location information. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs related | | | with RAN user plane congestion. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.2.1.3.3 Analytics report for RAN user data congestion
The following table shows the potential information carried in the analytics
report of RAN user plane congestion.
Analytics Report of RAN user plane congestion Attribute Name Description
* * *
                                                  Resource issue identifier                   The identifier of the RAN user plane congestion
                                                  RAN user plane congestion type indication   Indicator of the root cause of the RAN user plane congestion, e.g., PRB resources shortage, unsuitable resource allocation, inappropriate mobility parameters configuration, inefficient usage of radio resources
                                                  Start time                                  The start time of the RAN user plane congestion problem
                                                  Stop time                                   The end time of the RAN user plane congestion problem
                                                  Location                                    The geographical area or the cells where the RAN user plane congestion exists
                                                  Root cause                                  The root cause of the UP congestion issue, e.g. poor radio condition, inappropriate radio resource allocation, bad handover parameters, etc.
                                                  Affected objects                            The MOIs of the cells or subnetworks or network slices affected by the RAN user plane congestion problem
                                                  Severity level                              The severity level (e.g., critical, medium, not important) of the RAN user plane congestion
                                                  Recommended actions                         The recommend actions to solve the RAN user plan congestion problem. The recommended actions could be to update the policies of physical and virtual resources allocation, improve the radio condition quality, or optimize load balancing mechanisms.
#### 6.2.1.4 Evaluation
The solution described in clause 6.2.1.3 requires the analytics inputs as
described in in clause 6.2.1.3.2, wherein
\- the performance data (measurements and KPIs) for are available in TS 28.552
[8] and TS 28.554 [7].
\- the MDT data are available in TS 32.422 [25].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
However, these data cannot provide a clear indication of RAN user plane
congestion. Therefore, the data that can clearly indicate the RAN user plane
congestion are needed so that this solution can be feasible for RAN user plane
congestion analysis.
### 6.2.2 Resource utilization analytics
#### 6.2.2.1 Use case
The network is a resource limited system, it is therefore quite imperative to
ensure an optimum resource utilization for the network so that the required
resources can be efficiently allocated while ensuring no wastage or under
allocation of resources to cause additional CapEx and OpEX.
The resources usage for a network, a portion of network or a network slice
could be higher or lower during different time periods depending on the
traffic patterns. The traffic patterns could also vary in different
geographical locations (e.g., business area, entertainment area and
residential area) of the network, and could vary for different network slices.
It could happen that at some point in some areas or network slices the
resources are in shortage status while in some other areas or network slices
there are abundant resources. This may result in that the users cannot be
satisfactorily served in some areas or network slices due to lack of resources
even though the overall maximum capacity is sufficient. Resource shortage may
affect the QoS and potentially impact user quality of service experience,
e.g., lowering down the user data throughput, prolong the user data delay,
raise the rejections and failures for establishment of new connections (e.g.,
RRC connection), sessions (e.g., PDU session) and resources (e.g., QoS flows,
DRBs, etc.) and increase the drops of the existing connections, sessions and
resources and deteriorates user quality of service experience. Especially in
shared scenarios, it is usually that network slices may share the same RAN
resources, TN resources and/or some CN network resources. When resources
become limited, the suddenly high increase in resource usage by one network
slice may potentially affect the performance of other network slices. If
resources are not limited but one network slice is in relatively higher
resource usage and the others are in lower resource usage, the proportion of
resource distribution may need to be reallocated without provisioning
additional spare resources in order to achieve efficient and optimum network
resource usage, which can effectively reduce the CapEx and OpEx.
Therefore, it would be desirable that the spare resource of the under-utilized
usage areas or network slices can be re-allocated to areas or network slices
that requires more resources within the same period of time to prevent the
resource shortage from happening.
The MDAS producer can analyse the current and historical performance data
related to resource usage, network traffic and user quality of service
experience for the network or network slices, and identify the ongoing issues
on resource utilization and predict potential issues.
The MDAS producer may provide analytics report on capacity planning, resource
requirements, resource utilization, resource availability and resource
reservation proposals to assist the feasibility check by network and network
slice management system before the provisioning of the communication service.
The MDAS producer provides analytics report describing the ongoing and/or
potential resource utilization issues to the authorized consumers. The issues
need to be described precisely, including (but not limited to) the information
about which part of the network or network slice has encountered or is going
to encounter the resource utilization issue, it is resource shortage or
resource excess, over which time period, etc. The MDAS producer may also
provide the recommendations to solve the resource utilization issues
identified by the analytics report. The recommended actions may be for example
to schedule the \"scale-in\" and \"scale out\" of VNFs to dynamically (re-)
allocate the virtualized resources to where they are needed, or to
create/update the resource allocation policy for different network slices to
allow the network slices getting different percentage of resources in
different time periods according the traffic patterns and user quality of
service experience. In case of sharing scenario, the MDAS producer should also
weighs the resource usages of the shared network resources which belong to
different network slices. If reallocating the already allocated network
resource can solve the resource utilization issues, no other newly allocated
network resource is needed.
The MDAS producer gets the execution reports of the actions taken by the MDAS
consumer to solve the resource utilization issue, and takes the execution
reports into account in the following analysis.
The MDAS producer continue analysing the reported issue and provides update(s)
if there is any status change (e.g., solved, mitigated or deteriorated) till
it is solved.
#### 6.2.2.2 Potential requirements
**REQ-RES-ANA-1** The MDAS producer should have a capability to provide the
analytics report describing the resource utilization issue.
**REQ-RES-ANA-2** The analytics report describing the resource utilization
issue should contain the following information:
\- The identifier of the resource utilization issue;
\- Indication that it is an ongoing issue or potential issue;
\- The time period(s) during which the resource utilization issue has happened
or is potentially going to happen;
\- Indication that resource issue is shortage or excess over each time period;
\- Percentage of resource shortage or excess in each time period;
\- The network entities involved in the resource utilization issue;
\- The network slices involved in the resource utilization issue;
\- The recommended actions to solve the resource utilization issue.
**REQ-RES-ANA-3** The MDAS producer should have a capability to weigh the
allocation of shared network resource when providing the analytics report
describing the resource utilization issue.
#### 6.2.2.3 Possible solutions
##### 6.2.2.3.1 Solution description
_The MDAS producer correlates and analyses_ the ongoing and/or potential
resource utilization issues based on the current and historical performance
data related to resource usage and network traffic for the network or network
slices. The required data can be from RAN domain or CN domain or both. Based
on the analysis above, the MDAS producer is able to provide the domain
specific or cross domain analytics report as defined in clause 6.2.2.3.4
related with resource utilization analytics triggered by event or
periodically.
_To assist the feasibility check, the MDAS producer_ may consider the
following information: e.g., capacity planning of the network slice instance
and/or network slice subnet instance, existing active or non-active network
slice instance and/or network slice subnet instance resource information,
slice provisioning requirements, etc.
##### 6.2.2.3.2 Required data for resource utilization analysis for RAN domain
The management data required to analyse the RAN related resource utilization
are defined as the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Identifier | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [2]. MDAS uses | | | | this information to | | | | identify target | | | | network slices for | | | | resource utilization | | | | analytics and may | | | | derive network | | | | topology information | | | | according to | | | | S-NSSAI. | +----------------------+----------------------+----------------------+ | Performance | Measurement data | UE throughput: The | | measurement | | IP throughput of end | | | | users, see clause | | | | 5.1.1.3 of TS 28.552 | | | | [8]; | | | | | | | | RAN UE throughput: A | | | | KPI that shows how | | | | NG-RAN impacts the | | | | service quality | | | | provided to an | | | | end-user, see clause | | | | 6.3.6 of TS 28.554 | | | | [7]; | | | | | | | | Throughput for | | | | network slice | | | | instance: | | | | Upstream/Downstream | | | | throughput for | | | | network and Network | | | | Slice Instance, see | | | | clause 6.3.2 and | | | | clause 6.3.3 of TS | | | | 28.554 [7]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput | | | | at N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | | | | | | | | Radio resource | | | | utilization: The | | | | usage of physical | | | | radio resource | | | | utilization of the | | | | network, see clause | | | | 5.1.1.2 of TS 28.552 | | | | [8]; | +----------------------+----------------------+----------------------+ | MDT Data | Measurement data | UE measurements | | | | related to RSRP, | | | | RSRQ, SINR and UE | | | | location | | | | information, see TS | | | | 37.320 [12] and TS | | | | 32.422 [25]. | +----------------------+----------------------+----------------------+ | Capacity planning | Use case and | Capacity management | | data | procedures | use case and | | | | procedure, see | | | | clause 5.4.15 of TS | | | | 28.530 [5] and | | | | clause 7.15 of TS | | | | 28.531 [33]. | +----------------------+----------------------+----------------------+ | Network topology | Network topology | The topology of the | | | data | network for resource | | | | utilization | | | | analytics. The | | | | topology can be | | | | reflected by the | | | | NG-RAN NRMs as | | | | defined in TS 28.541 | | | | [20]. | +----------------------+----------------------+----------------------+ | User service | Analysis data | User service | | experience | | experience relevant | | | | attributes and/or | | | | analytics analysis | | | | obtained from NWDAF | | | | or AF, see clause | | | | 6.4 of TS 23.288 | | | | [18]. | +----------------------+----------------------+----------------------+
##### 6.2.2.3.3 Required data for resource utilization analysis for CN domain
The management data required to analyse the CN related resource utilization
are defined as the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Identifier | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [2]. MDAS uses | | | | this information to | | | | identify target | | | | network slices for | | | | resource utilization | | | | analytics and may | | | | derive network | | | | topology information | | | | according to | | | | S-NSSAI. | +----------------------+----------------------+----------------------+ | Performance | Measurement data | User subscription | | measurement | | data by performance | | | | measurement for AMF | | | | as defined in clause | | | | 5.2 of TS 28.552 | | | | [8]; | | | | | | | | Performance | | | | measurement data for | | | | SMF as defined in | | | | clause 5.3 of TS | | | | 28.552 [8]; | | | | | | | | Performance | | | | measurement data for | | | | UPF as defined in | | | | clause 5.4 of TS | | | | 28.552 [8]; | | | | | | | | Performance | | | | measurement data for | | | | NF as defined in | | | | clause 5.7 of TS | | | | 28.552 [8]; | | | | | | | | Throughput for | | | | network slice | | | | instance: | | | | Upstream/Downstream | | | | throughput for | | | | network and Network | | | | Slice Instance, see | | | | clause 6.3.2 and | | | | clause 6.3.3 of TS | | | | 28.554 [7]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput | | | | at N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | | | | | | | | PDU sessions for | | | | network slice | | | | instance: number of | | | | PDU sessions and PDU | | | | session | | | | establishment time | | | | of network slice | | | | instance, see clause | | | | 6.4.1 and clause | | | | 6.4.3 of TS 28.554 | | | | [7]; | | | | | | | | Virtualised resource | | | | utilization: | | | | Virtualised resource | | | | utilization of | | | | Network Slice | | | | Instance, see clause | | | | 6.4.2 of TS 28.554 | | | | [7]; | +----------------------+----------------------+----------------------+ | Capacity planning | Use case and | Capacity management | | data | procedures | use case and | | | | procedure, see | | | | clause 5.4.15 of TS | | | | 28.530 [5] and | | | | clause 7.15 of TS | | | | 28.531 [33]. | +----------------------+----------------------+----------------------+ | Network topology | Network topology | The topology of the | | | data | network for resource | | | | utilization | | | | analytics. The | | | | topology can be | | | | reflected by the 5GC | | | | NRMs as defined in | | | | TS 28.541 [20]. | +----------------------+----------------------+----------------------+
##### 6.2.2.3.4 Analytics report for resource utilization analysis
The following table provides the potential contents of the domain specific or
cross domain analytics report of resource utilization analysis based on the
required data received as described in 6.2.2.3.2 and 6.2.2.3.3.
Analytics Report of resource utilization analysis Attribute Name Description
* * *
                                                      Resource utilization issue Identifier           The identifier indicates the resource utilization issue
                                                      Indication of resource utilization issue type   Indicates the type of the resource utilization issue, e.g., ongoing or potential resource utilization issue
                                                      Time period                                     Describes the time period(s) during which the resource utilization issue has happened or is going to happen
                                                      Indication of resource usage demand             Indicates that resource issue is shortage or excess in each time period
                                                      Percentage of resource usage demand             Describes percentage of resource shortage or excess in each time period
                                                      List of network entities                        Lists the network entities involved in the resource utilization issue
                                                      A List of network slices                        List of the network slices involved in the resource utilization issue
                                                      Recommended actions                             Describes the recommended actions to solve the resource utilization issue
#### 6.2.2.4 Evaluation
The solution described in clause 6.2.2.3 requires the analytics inputs as
described in in clause 6.2.2.3.2 and 6.2.2.3.3, wherein
\- the S-NSSAI can be provided along with the performance measurements, NRM
and planning data.
\- the performance measurements are available in TS 28.552 [8].
\- the capacity planning data can be accessed according to TS 28.531 [33].
\- the network topology can be reflected by the NRMs defined in TS 28.541
[20].
\- the user service experience data can be accessed from NWDAF or AF according
to TS 23.288 [18].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.2.2.3.4 can be derived.
Therefore, this solution is a feasible candidate for resource utilization
analytics for RAN domain, CN domain and network slices.
### 6.2.3 Cross-slice resource optimization
#### 6.2.3.1 use case
In TR 28.861 [5], SON use case of cross-slice network resource optimizations
described. The resource allocated for each network slice instance may vary
considering the network resource utilization, traffic patterns, network
bandwidth and the demand of resources for each NSI, the priorities of NSI,
etc.
The MDAS producer is expected to have the capability to provide analytics
report of resource optimization for multiple network slices. Resource
optimizations for multiple network slices may be dependent on each other.
Resource scale out / in of one slice may impact resource allocation strategy
of the other network slices considering the overall resource availability
constraints. Aspects to be considered by MDAS include network slice type,
priority, resource availability, traffic load, QoS requirements and QoS flow
related measurements etc. The recommendations of the most optimal resource
allocation for each network slice and the predictions of the resource demand
for each NSI may also be provided. This analytics report can be consumed by
SON function to help the optimization of cross-slice resource optimization.
#### 6.2.3.2 Potential requirements
**REQ-Multi-SLICE-Resource_MDA-1** : MDAS producer should have the capability
to provide analytics report describing resource optimization across multiple
network slices.
**REQ-Multi-SLICE-Resource_MDA-2** : The analytics report describing the
resource optimization across multiple network slices should include the
following information:
\- Resource optimization threshold for each network slice;
\- Recommendations of resource allocation for each NSI;
\- The predictions of resource demand for each NSI
#### 6.2.3.3 Possible Solution
##### 6.2.3.3.1 Solution description
_The MDAS producer correlates and analyses_ the resource optimization
strategies across multiple network slices based on the current and historical
resource utilization measurements and network traffic for each network slice
instances. Based on the analysis above, the MDAS producer is able to provide
the analytics report as defined in 6.2.3.3.3 related with cross slice resource
optimization analytics triggered by event or periodically.
##### 6.2.3.3.2 Required data for cross-slice resource optimization analysis
The input data in clause 6.2.2.3.2 and 6.2.2.3.3 can be reused as the required
management data to analyse the resource optimization across multiple network
slices.
##### 6.2.3.3.3 Analytics report for cross-slice resource optimization
The following table provides the potential contents of the analytics report of
cross-slice resource optimization.
Analytics Report of cross-slice resource optimization analysis Attribute Name
Description
* * *
                                                                   Cross-slice resource optimization Identifier   The identifier indicates the Cross-slice resource optimization
                                                                   Resource optimization threshold                Indicates the thresholds to trigger network resource optimizations for each network slice;
                                                                   Resource allocation recommendation             Describes the recommendations of resource allocation (e.g. increase or decrease NSI capacities like storage, computing, network bandwidth and radio resources) for each NSI;
                                                                   Resource demand prediction                     Describes the predictions of resource demand for each NSI
#### 6.2.3.4 Evaluation
The solution described in clause 6.2.3.3 requires the analytics inputs as
described in in clause 6.2.3.3.2, wherein
\- the S-NSSAI can be provided along with the performance measurements, NRM
and planning data.
\- the performance measurements are available in TS 28.552 [8].
\- the capacity planning data can be accessed according to TS 28.531 [33].
\- the network topology can be reflected by the NRMs defined in TS 28.541
[20].
\- the user service experience data can be accessed from NWDAF or AF according
to TS 23.288 [18].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.2.3.3.4 can be derived.
Therefore, this solution is a feasible candidate for cross-slice resource
optimization.
### 6.2.4 NAS level congestion control optimization
#### 6.2.4.1 Use case
The current NAS level congestion control as described in TS 23.501 [13] uses
back-off timer to avoid AMF receiving large amounts of NAS messages from UEs
including Mobility Registration Update request. The AMF will accept or rejects
the request depending on various aspects. In virtualized environment, the
Mobility Registration Update request may be rejected due to inadequacy of
available resources with the target AMF. The resource may include virtual
resource (e.g., compute, memory and disk). If the Mobility Registration Update
request is rejected by the AMF, UE will receive the reject message with a
Mobility Management back off time and send a new Mobility Registration Update
request after the timer. It is possible that the new Mobility Registration
Update request will be rejected again because of the load of the AMF. This
mechanism results in wastage of UE and network resources. It also brings
inefficiency in network procedures.
It is desirable to use MDAS (Management data analytic service) to assist
congestion control in order to avoid too many rejections of NAS messages at
the AMF. MDAS producer provides analytics report containing the current and
future/predicted resource consumption status for the target AMF. The analytics
report also provides recommended actions to optimize the target AMF for
congestion control. Based on the report MDAS consumer adjusts (e.g., scale-
out/up the virtual resource, set the suitable timer) the resources before
continuing processing the received messages.
#### 6.2.4.2 Potential requirements
**REQ-NAS_OPT_CON-1** MDAS producer should have the capability to provide
analytics report describing the NAS level congestion issue at AMF.
**REQ-NAS_OPT_CON-2** The analytics report describing the NAS level congestion
issue should contain the following information:
\- The identifier of the NAS level congestion issue described in the analytics
report;
\- The start time and end time of the NAS level congestion issue;
\- Affected AMF;
\- Root cause of the NAS level congestion issue;
\- The recommended actions to solve NAS level congestion issue.
#### 6.2.4.3 Possible solutions
##### 6.2.4.3.1 Solution Description
The MDAS producer correlates and analyses the management data described in the
following subclause to provide AMF resource consumption
statistics/predictions, identification of NAS level congestion issues and the
root cause as the table in clause 6.2.4.3.3 shows. This procedure may be
triggered by the request or periodically
##### 6.2.4.3.2 Data required for NAS level congestion control analysis
The management data required to analyse the NAS level congestion issue are
defined as the following table.
+--------------------------+------------------------------------------+ | Data category | Required data | +==========================+==========================================+ | Performance Measurements | Performance measurement for AMF: | | | registration and service related | | | measurement for AMF as defined in clause | | | 5.2 of TS 28.552 [8]. | | | | | | Virtual resource usage measurement for | | | AMF as defined in clause 5.7 of TS | | | 28.552 [8]. | +--------------------------+------------------------------------------+ | | | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. | +--------------------------+------------------------------------------+
##### 6.2.4.3.3 Analytics report
The NAS level congestion issue analytics report contains the following
information.
Analytics Report of NAS level congestion issue Attribute Name Description
* * *
                                                   Issue identifier      The identifier of the NAS level congestion.
                                                   Start Time            The start time of the issue happened.
                                                   Stop Time             The end time of the issue has been solved.
                                                   Affected AMF          The MOI of the AMF that affected by the NAS level congestion issue.
                                                   Root cause            The root cause of the NAS level congestion issue, e.g. increased resource consumption.
                                                   Recommended actions   Recommendation for AMF in order to make it optimal for NAS level congestion control e.g. scale-out AMF, Increase or adjust back-off time to avoid large amounts of UEs initiate deferred requests simultaneously.
#### 6.2.4.4 Evaluation
The solution described in clause 6.2.4.3 requires the analytics inputs as
described in in clause 6.2.4.3.2, wherein
\- the performance measurements for AMF are available in TS 28.552 [8].
\- the configuration data are available in TS 28.541 [20].
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.2.4.3.3 can be
derived.
Therefore, this solution is a feasible candidate for NAS level congestion
control analysis.
## 6.3 SLS analysis related issues
### 6.3.1 E2E latency analysis
#### 6.3.1.1 Use case
Latency is one of the SLA parameters for URLLC services. User data packets
should be successfully delivered within certain time constraints to satisfy
the end users requirements.
Latency could be impacted by the network capability and network
configurations, e.g. configuration of service priority, RAN capacity, network
load, number of re-transmissions, Wireless channel environment and the
processing time of the network functions, etc. These factors may be the root
cause if the latency requirements cannot be achieved. Packet transmission
latency may dynamically change if one or multiple of these factors change. The
latency requirement should be assured even if some of the network conditions
may degrade. There are some mechanisms to assure latency, e.g. to upgrade the
service priority, allocate or reserve more network resource, prepare backups.
With regard to latency analysis for URLLC services, the performance data and
fault data are required to be collected, reported and analysed in near real
time. Distributed MDAS deployment architecture should be applied for this
scenario. The domain MDAS producers located in the edge network provides
latency analysis or predictions for local services in near real time. E.g. for
latency and other related QoS evaluation or prediction for V2X application,
user location and user trajectory may need to be analysed in near-real time.
The analytics report can be consumed by edge AFs to perform actions in time.
The centralized MDAS producers analyses integrate latency performance for
cross domain. It may provide more comprehensive analytics report to the
centralized AF. AI/ML models or analytics data may need to be exchanged
between the neighbouring domain MDAS producers, or between domain MDAS
producers and the centralized MDAS producers.
From the management perspective, resource configuration and allocation
algorithms or policies should support latency assurance. E2E latency is the
latency across multiple domains, e.g. RAN domain, core network domain and
transport network domain, each domain should ensure its own latency
requirement to achieve the total E2E latency goal. The domain specific MDAS
can be utilized to provide the domain specific latency analysis, and together
with the cross-domain MDAS to provide E2E latency analysis to support SLS
assurance.
#### 6.3.1.2 Potential requirements
**REQ-LATENCY_ASS-CON-1** : MDAS producer should have the capability to
provide analytics report describing the latency problem.
**REQ-LATENCY_ASS-CON-2** : The MDAS producer should have the capability to
provide the latency analytics report in time for URLLC services according to
the corresponding latency requirement.
**REQ-LATENCY_ASS-CON-3** : Distributed MDAS deployment and exchange of
analytics data between MDAS producers should be supported.
**REQ-LATENCY_ASS-CON-4** : The analytics report describing the latency
problem should include the following information:
\- The identifier of the latency issue;
\- Indication of latency issue type;
\- The start time and end time of the latency issue;
\- The geographical area and location where the latency issue exists;
\- Root cause of the latency issue;
\- The objects affected by the latency issue;
\- The severity level of the latency issue;
\- The recommended actions to solve the latency issue.
#### 6.3.1.3 Possible solutions
##### 6.3.1.3.1 Solution description
_The performance measurements, e.g., network latency, UE throughput, network
resource utilization and packet loss can be utilized for E2E latency analysis.
To support E2E latency assurance_ in order to satisfy the latency requirements
from the vertical users, MDAS producer is able to provide the analytics report
as defined in clause 6.3.1.3.5 related with E2E latency analytics triggered by
event or periodically.
##### 6.3.1.3.2 Required data for latency analysis for RAN domain
The management data required to analyse the latency are defined as the
following table.
+------------------------+------------------+------------------------+ | Input data | Data type | Description | +========================+==================+========================+ | S-NSSAI | Service data | \"S-NSSAI\" as defined | | | | in clause 5.15.2, TS | | | | 23.501 [13]. MDAS | | | | may derive network | | | | topology information | | | | according to S-NSSAI | +------------------------+------------------+------------------------+ | Performance | Measurement data | Packet delay: \"packet | | measurement | | delay\" measurement as | | | | defined in clause | | | | 5.1.1.1, clause | | | | 5.1.3.3, TS 28.552 | | | | [8]; | | | | | | | | IP Latency | | | | measurements: \"IP | | | | Latency measurements\" | | | | as defined in clause | | | | 5.1.3.4, TS 28.552 | | | | [8]; | | | | | | | | UE throughput: The IP | | | | throughput of end | | | | users, see clause | | | | 5.1.1.3 of TS 28.552 | | | | [8]; | | | | | | | | RAN UE throughput: A | | | | KPI that shows how | | | | NG-RAN impacts the | | | | service quality | | | | provided to an | | | | end-user, see clause | | | | 6.3.6 of TS 28.554 | | | | [7]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput at | | | | N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | | | | | | | | Radio resource | | | | utilization: The usage | | | | of physical radio | | | | resource utilization | | | | of the network, see | | | | clause 5.1.1.2 of TS | | | | 28.552 [8]; | | | | | | | | CQI related | | | | measurements: the | | | | distribution of | | | | Wideband CQI (Channel | | | | Quality Indicator) | | | | reported by UEs in the | | | | cell, see clause | | | | 5.1.1.11 of TS 28.552 | | | | [8]; | | | | | | | | MCS related | | | | Measurements: the | | | | distribution of the | | | | MCS scheduled for | | | | PDSCH RB by NG-RAN, | | | | the distribution of | | | | the MCS scheduled for | | | | PUSCH RB by NG-RAN, | | | | see clause 5.1.1.12 in | | | | TS 28.552 [8]; | +------------------------+------------------+------------------------+ | MDT Data | Measurement data | UE measurements | | | | related to RSRP, RSRQ, | | | | SINR and UE location | | | | information, see TS | | | | 37.320 [12]. | +------------------------+------------------+------------------------+ | QoE Data | Measurement data | The details | | | | information of QoE | | | | data required by this | | | | case is FFS. | +------------------------+------------------+------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.1.3.3 Required data for latency analysis for CN domain
The management data required to analyse the latency are defined as the
following table.
+------------------------+------------------+------------------------+ | Input data | Data type | Description | +========================+==================+========================+ | S-NSSAI | Service data | \"S-NSSAI\" as defined | | | | in clause 5.15.2, TS | | | | 23.501 [13]. MDAS | | | | may derive network | | | | topology information | | | | according to S-NSSAI | +------------------------+------------------+------------------------+ | Performance | Measurement data | Round-trip GTP Data | | measurement | | Packet Delay: | | | | \"Round-trip GTP Data | | | | Packet Delay\" as | | | | defined in clause | | | | 5.4.1.9, TS 28.552 | | | | [8]; | | | | | | | | GTP packets delay in | | | | UPF: \"GTP packets | | | | delay in UPF\" as | | | | defined in clause | | | | 5.4.5, TS 28.552 | | | | [8]; | | | | | | | | Round-trip GTP Data | | | | Packet Delay on N9 | | | | interface: | | | | \"Round-trip GTP Data | | | | Packet Delay on N9 | | | | interface\" as defined | | | | in clause 5.4.4.1, TS | | | | 28.552 [8]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput at | | | | N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | +------------------------+------------------+------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.1.3.4 Required data for E2E latency analysis for cross domain
For cross domain analysis, the RAN and CN domain required data as described in
clauses 6.3.1.3.2 and 6.3.1.3.3 may be also needed, as well as the potential
data described in the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Service data | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [13]. MDAS may | | | | derive network | | | | topology information | | | | according to S-NSSAI | +----------------------+----------------------+----------------------+ | Performance | Measurement data | End-to-end Latency | | measurement | | of 5G Network: | | | | \"End-to-end Latency | | | | of 5G Network\" as | | | | defined in clause | | | | 6.3.1, TS 28.554 | | | | [7]; | | | | | | | | Throughput for | | | | network slice | | | | instance: | | | | Upstream/Downstream | | | | throughput for | | | | network and Network | | | | Slice Instance, see | | | | clause 6.3.2 and | | | | clause 6.3.3 of TS | | | | 28.554 [7]; | +----------------------+----------------------+----------------------+ | QoE Data | Measurement data | The details | | | | information of QoE | | | | data required by | | | | this case is FFS. | +----------------------+----------------------+----------------------+ | \"Required data for | Measurement data | Raw data in | | latency analysis for | and/or analytics | \"6.3.1.3.2 Required | | RAN domain\" or RAN | data | data for latency | | domain \"Analytics | | analysis for RAN | | report for latency | | domain\", or | | analysis\" | | analytics data of | | | | RAN domain in | | | | \"6.3.1.3.5 | | | | Analytics report for | | | | latency analysis\" | +----------------------+----------------------+----------------------+ | \"Required data for | Measurement data | Raw data in | | latency analysis for | and/or analytics | \"6.3.1.3.3 Required | | CN domain\" or CN | data | data for latency | | domain \"Analytics | | analysis for CN | | report for latency | | domain\", or | | analysis\" | | analytics data of CN | | | | domain in | | | | \"6.3.1.3.5 | | | | Analytics report for | | | | latency analysis\" | +----------------------+----------------------+----------------------+
Note: The above parameters may not be the complete list.
##### 6.3.1.3.5 Analytics report for latency analysis
The following table shows the potential information of the domain specific or
cross domain E2E analytics report for latency analysis based on the required
data received as described in clauses 6.3.1.3.2, 6.3.1.3.3 and 6.3.1.3.4.
Analytics Report of E2E latency analysis Attribute Name Description
* * *
                                             SLS assurance issue Identifier           The identifier indicates the SLS assurance issue
                                             Indication of SLS assurance issue type   Indicates the type of the SLS assurance issue, e.g., RAN latency issue, CN latency issue, TN latency issue.
                                             Location                                 The geographical area and location where the latency issue exists
                                             Start time                               The start time of the latency issue
                                             Stop time                                The stop time of the latency issue
                                             Root cause                               The root cause of the E2E latency degradation, e.g. coverage issue, load issue in RAN, load issue in CN, low throughput in RAN, low throughput in CN etc.
                                             Affected objects                         The MOIs of cells, subnetwork or network slices affected by the latency issue;
                                             Severity level                           The severity level (e.g., critical, medium, not important) of the latency issue;
                                             Recommended actions                      The recommended actions to solve the latency issue, e.g., resource radio resources re-allocation in gNB, scaling the VNF.
#### 6.3.1.4 Evaluation
Based on the potential solutions, the required inputs are listed in clause
6.3.1.3.2, clause 6.3.1.3.3 and clause 6.3.1.3.4, where:
\- The S-NSSAI as defined in clause 5.15.2, TS 23.501 [2] and TS 28.531 [33],
can be acquired along with the performance measurements in TS 28.552 [8], TS
28.554 [7] and configuration data in TS 28.531 [33].
\- The performance measurements and KPIs is defined in TS 28.552 [8] and TS
28.554 [7].
\- The MDT Data defined in TS 37.320 [12] can be acquired in procedures
defined in TS 32.422 [25].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
Therefore, MDA is feasible to support the analyses and predictions of E2E
latency analysis related factors for cross domain MDAS, RAN domain and CN
domain respectively.
### 6.3.2 Network slice load analysis
#### 6.3.2.1 Use case
Network slice load may vary over time. Therefore, network resources allocated
initially could not always satisfy the traffic requirements, for example, the
network slice may be overloaded or underutilized. Various factors may impact
the network slice load, e.g. number of UEs accessing the network, number of
PDU sessions, service types and the end users distribution. Overload of
signalling in control plane and/or user data congestion in user plane will
lead underperforming network. Besides, allocating excessive resources for
network slice with light load will decrease resource efficiency.
The analysis of network slice load should consider the load of services with
different characteristics (e.g., QoS information, service priority), load
distribution to derive the corresponding resource requirements. Load level and
detailed load distribution analytic result may be provided, e.g. load
distribution for different applications (probably coming from respective AFs),
different frequency layers, different RATs, different locations and/or time
periods, different NFs etc. Proportion of high loaded/light loaded cells,
NSSI, NFs etc can also be provided.
The analysis of network slice load should consider the network slice capacity,
so that the number of users and the numbers of PDU sessions setup can be
further supported by certain network slice could be analysed and predicted.
_\"maxNumberofUEs\"_ and _\"maxNumberofConns\"_ are two capacity related
requirements in the ServiceProfile in [20]. In addition, from end to end
network slice perspective, the capacity related requirements and the actual
network performance measurements also help to reflect the slice load and slice
capacity headroom.
Traffics and resources related performance measurements and UE measurements
can be utilized by MDAS producer to identify degradation of the performance
measurements and KPI documented in an SLS due to load issues and provide the
analytic reports of load analysis. For example, the analytic report for a RAN
NSSI, slice load related information may include load of DRBs and SRBs, radio
resource utilization and availability, virtual resources of RAN CU, etc. MDAS
may provide statistic or predictive results of the above load related
information and their correlations for multiple cells in certain coverage
areas and time periods. MDAS may further provide recommendations of slice RRM
Policy ratio for the analysed cells. In addition, MDAS may utilize the
analytics data of slice load analysis (as appropriate), NF load analysis, UE
mobility statistics/predictions and UE communication statistics/predictions
from NWDAF [18] to assist its network slice load analysis. The producer of
MDAS is able to, from the perspective of the management aspects, provide the
network slice load analytics report related to an NSI, a specific RAN NSSI, a
specific CN NSSI. This analytics report can be considered as an input to
support SLA assurance to perform further evaluation.
#### 6.3.2.2 Potential requirements
**REQ-NS_LOAD_ANA-1** : MDAS producer should have the capability to provide
analytics report describing the network slice load problem.
**REQ-NS_LOAD_ANA-2** : MDAS producer should have the capability to analyse
and predict the network slice load based on the slice capacity including the
number of users and the numbers of PDU sessions setup supported by certain
network slice.
**REQ-NS_LOAD_ANA-3** : The analytics report describing the network slide load
problem should include the following information:
\- The identifier of the network slice load problem;
\- Indication of the network slice load problem;
\- The start time and end time of the network slice load problem;
\- The type of network slice load problem (overloaded or underutilized);
\- The geographical area, location (UE, gNB, or UPF), domain (RAN, CN) where
the network slice load problem exists;
\- The cause(es) of network slice problem (e.g., number of UEs accessing the
network, number of PDU sessions, service types and the end user\'s
distribution);
\- Root cause of the network slice load problem;
\- The applications affected by the network slide load problem
\- The severity level of the network slice load problem;
\- The recommended actions to solve the network slice load problem.
#### 6.3.2.3 Possible solutions
##### 6.3.2.3.1 Solution description
_The MDAS_ producer correlates and analyses the ongoing and/or potential
network slice load problem based on the current and historical performance
data related to resource usage and network traffic for the target network
slice. The slice load is a comprehensive result. Various factors may impact
the network slice load, e.g. number of UEs accessing the network, number of
QoS flows, the resource utilizations of different NFs which is related with
the network slice instance. The analysis of network slice load should consider
the load of services with different characteristics (e.g., QoS information,
service priority), load distribution to derive the corresponding resource
requirements. To identify degradation of the KPIs documented in an SLS due to
load issues, traffics and resources related performance measurements and UE
measurements can be utilized. Note that this solution covers both domain
specific and cross-domain load problem analysis scenarios. Based on the
analysis above, the MDAS producer is able to provide the analytics report as
defined in clause 6.3.2.3.5 related with the network slice load analytics
triggered by event or periodically.
##### 6.3.2.3.2 Required data for network slice load analysis for cross domain
For cross domain analysis, the RAN and CN domain required data as described in
clauses 6.3.2.3.3 and 6.3.2.3.4 may be also needed, as well as the potential
data described in the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Service data | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [2]. MDAS uses | | | | this information to | | | | identify target | | | | network slices for | | | | network slice load | | | | analytics and may | | | | derive network | | | | topology information | | | | according to | | | | S-NSSAI. | +----------------------+----------------------+----------------------+ | Performance | Measurement data | End-to-end Latency | | measurement | | of 5G Network: | | | | \"End-to-end Latency | | | | of 5G Network\" as | | | | defined in clause | | | | 6.3.1, TS 28.554 | | | | [7]; | | | | | | | | UE throughput: The | | | | IP throughput of end | | | | users, see clause | | | | 5.1.1.3 of TS 28.552 | | | | [8]; | | | | | | | | Throughput for | | | | network slice | | | | instance: | | | | Upstream/Downstream | | | | throughput for | | | | network and Network | | | | Slice Instance, see | | | | clause 6.3.2 and | | | | clause 6.3.3 of TS | | | | 28.554 [7]; | | | | | | | | Virtualised resource | | | | utilization of | | | | Network Slice | | | | Instance, see clause | | | | 6.4.2 of TS 28.554 | | | | [7] | +----------------------+----------------------+----------------------+ | MDT Data | Measurement data | UE measurements | | | | related to RSRP, | | | | RSRQ, SINR and UE | | | | location | | | | information, see TS | | | | 37.320 [12]. | +----------------------+----------------------+----------------------+ | Capacity planning | Use case and | Capacity management | | data | procedures | use case and | | | | procedure, see | | | | clause 5.4.15 of TS | | | | 28.530 [5] and | | | | clause 7.15 of TS | | | | 28.531 [33]. | +----------------------+----------------------+----------------------+ | Network topology | Network topology | The topology of the | | | data | network for network | | | | slice load | | | | analytics. | +----------------------+----------------------+----------------------+
Note: The above parameters may not be the complete list.
##### 6.3.2.3.3 Required data for network slice load for RAN domain
The management data required to analyse the network slice load for RAN domain
are defined as the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Service data | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [2]. MDAS uses | | | | this information to | | | | identify target | | | | network slices for | | | | network slice load | | | | analytics and may | | | | derive network | | | | topology information | | | | according to | | | | S-NSSAI. | +----------------------+----------------------+----------------------+ | Performance | Measurement data | RAN domain | | measurement | | measurement data: | | | | | | | | RAN UE throughput: A | | | | KPI that shows how | | | | NG-RAN impacts the | | | | service quality | | | | provided to an | | | | end-user, see clause | | | | 6.3.6 of TS 28.554 | | | | [7]; | | | | | | | | Radio resource | | | | utilization: The | | | | usage of physical | | | | radio resource | | | | utilization of the | | | | network, see clause | | | | 5.1.1.2 of TS 28.552 | | | | [8]; | | | | | | | | Performance | | | | measurement data for | | | | gNB as defined in | | | | clause 5.1 of TS | | | | 28.552 [8]; | +----------------------+----------------------+----------------------+ | MDT Data | Measurement data | UE measurements | | | | related to RSRP, | | | | RSRQ, SINR and UE | | | | location | | | | information, see TS | | | | 37.320 [12]. | +----------------------+----------------------+----------------------+ | Capacity planning | Use case and | Capacity management | | data | procedures | use case and | | | | procedure, see | | | | clause 5.4.15 of TS | | | | 28.530 [5] and | | | | clause 7.15 of TS | | | | 28.531 [33]. | +----------------------+----------------------+----------------------+ | Network topology | Network topology | The topology of the | | | data | network for network | | | | slice load | | | | analytics. | +----------------------+----------------------+----------------------+
Note: The above parameters may not be the complete list.
##### 6.3.2.3.4 Required data for network slice load for CN domain
The management data required to analyse the network slice load for CN domain
are defined as the following table.
+----------------------+----------------------+----------------------+ | Input data | Data type | Description | +======================+======================+======================+ | S-NSSAI | Service data | \"S-NSSAI\" as | | | | defined in clause | | | | 5.15.2, TS 23.501 | | | | [2]. MDAS uses | | | | this information to | | | | identify target | | | | network slices for | | | | network slice load | | | | analytics and may | | | | derive network | | | | topology information | | | | according to | | | | S-NSSAI. | +----------------------+----------------------+----------------------+ | Performance | Measurement data | CN domain | | measurement | | measurement data | | | | | | | | User subscription | | | | data by performance | | | | measurement for AMF | | | | as defined in clause | | | | 5.2 of TS 28.552 | | | | [8]; | | | | | | | | Performance | | | | measurement data for | | | | SMF as defined in | | | | clause 5.3 of TS | | | | 28.552 [8]; | | | | | | | | Performance | | | | measurement data for | | | | UPF as defined in | | | | clause 5.4 of TS | | | | 28.552 [8]; | | | | | | | | Performance | | | | measurement data for | | | | NF as defined in | | | | clause 5.7 of TS | | | | 28.552 [8]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput | | | | at N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | +----------------------+----------------------+----------------------+ | Capacity planning | Use case and | Capacity management | | data | procedures | use case and | | | | procedure, see | | | | clause 5.4.15 of TS | | | | 28.530 [5] and | | | | clause 7.15 of TS | | | | 28.531 [33]. | +----------------------+----------------------+----------------------+ | Network topology | Network topology | The topology of the | | | data | network for network | | | | slice load | | | | analytics. | +----------------------+----------------------+----------------------+
Note: The above parameters may not be the complete list.
##### 6.3.2.3.5 Analytics report for network slice load analysis
The following table provides the potential information of the domain specific
or cross domain analytics report for network slice load analysis based on the
required data received as described in clauses 6.3.2.3.2, 6.3.2.3.3 and
6.3.2.3.4.
Analytics Report of resource utilization analysis Attribute Name Description
* * *
                                                      Network slice load problem identifier     The identifier indicates the network slice load problem
                                                      Type of network slice load problem        Indicates the type of the network slice load problem, e.g., ongoing/potential or overload/underutilized network slice load problem
                                                      Time period                               Describes the time period(s) during which the network slice load problem has happened or is going to happen
                                                      Slice load level and distribution         Describes the detailed load distribution e.g., load distribution for different applications (probably coming from respective AFs), different frequency layers, different RATs, different locations and/or time periods, different NFs etc.
                                                      Slice load information                    Describes the load of the following resources in the network slice: DRBs and SRBs, radio resource utilization and availability, virtual resources of RAN CU, etc
                                                      Slice load statistics                     Describes the load statistics of the above resources and their correlated results in terms of coverage and time periods
                                                      Slice load predictive results             Describes the load predictive results of the above resources and their correlations in terms of coverage and time periods
                                                      Indication of network slice load demand   \"Indicates, for each time period, that the network slice is overutilized (load is too high) or underutilized (load is too low)Â 
                                                      Percentage of network slice load demand   Describes how much of the slice load capacity is either overloaded or underutilized in each time period
                                                      List of network entities                  Lists the network entities involved in the network slice load problem
                                                      Recommended actions                       Describes the recommended actions to solve the network slice load problem
#### 6.3.2.4 Evaluation
Based on the potential solutions, the required inputs are listed in clause
6.3.2.3.2, clause 6.3.2.3.3 and clause 6.3.2.3.4, where:
\- The S-NSSAI as defined in clause 5.15.2, TS 23.501 [2] and TS 28.531 [33],
can be acquired along with the performance measurements in TS 28.552 [8], TS
28.554 [7] and configuration data in TS 28.531 [33].
\- The performance measurements and KPIs is defined in TS 28.552 [8] and TS
28.554 [7].
\- The MDT Data defined in TS 37.320 [12] can be acquired in procedures
defined in TS 32.422 [25].
\- The capacity planning data is available as the configuration data defined
in TS 28.541 [20].
\- The topology data is available as the NRM defined in TS 28.541 [20].
\- User service experience data can be acquired from NWDAF or AF as defined in
clause 6.4 of TS 23.288 [18].
Therefore, MDA is feasible to support the analyses and predictions of load
related factors for cross domain MDAS, RAN domain and CN domain respectively.
### 6.3.3 Service experience analysis
#### 6.3.3.1 Use case
The variety of the services provided to the verticals is an important feature.
The network requirements which support different services may vary a lot. For
example, the users who require URLLC service have high requirements on the
network latency and reliability, while the users who require NB-IoT service
considers the maximum access UE numbers as the first priority for network
requirement. Furthermore, for services supported by different QoS flows, the
requirements of different QCIs/5QIs should be assured to satisfy the end user
experience, e.g., the voice service which is critical on the users'
experience. The analysis of service experience of different vertical consumers
and services types is important to support the SLA assurance from the network
aspect.
The UE level\'s experience analysis based on the information from AF provided
by NWDAF, e.g., the service MOS may be utilized by MDAS producer as one of the
service performance inputs. The network performance measurements from RAN,
e.g., the UE IP throughput, coverage, latency, successful handover ratio, etc.
and network performance measurements from core network could also be used by
MDAS producer for the analysis of service fulfilment by the network. Resource
utilization efficiency may impact the user quality of service experience as
described in clause 6.2.2. The analytics information from combining the user
quality of service experience and, network performance and network resource
utilisation analysis could be made available to operators or verticals for
further action and analysis. For example, root cause for service deteriorating
and recommendation actions may be provided by MDAS producer if some of the SLA
requirements cannot be achieved, operators could perform the network
optimization actions when needed. The MDAS producer may also coordinate with
other management services producer to perform close loop SLA assurance of the
network under the operator configured policies.
#### 6.3.3.2 Potential requirements
**REQ-Ser_Exp_CON-1** The MDAS producer should have a capability to provide
the analytics report on service experience analysis
**REQ-Ser_Exp_CON-2** The analytics report describing the service experience
should contain the following information describing the service experience
aspects and potentially future prediction:
> \- The predictive service experience or observed service experience
> statistics, may split into subcounters in different levels, e.g. per
> S-NSSAI, per 5QI, per UE, etc.
>
> \- Service experience indication and root cause analysis.
#### 6.3.3.3 Possible solutions
##### 6.3.3.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to provide service experience analysis and identify the
root cause. As the table in clause 6.3.3.3.3 shows, the analytics report is
able to be provided by the MDAS producer to describe the service experience
issue, root causes and recommendations. This procedure may be triggered by the
request or periodically.
##### 6.3.3.3.2 Data required for service experience analysis
The following table shows the potential data required to analyse the service
experience analysis.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Packet delay: The average and | | | distribution of DL/UL packet delay | | | between UPF and UE, see clause 5.4.9 of | | | TS 28.552 [8] | | | | | | UE throughput: Average and distribution | | | of DL/UL UE throughput in gNB, see | | | clause 5.1.1.3 of TS 28.552 [8]; | | | | | | RAN UE throughput: A KPI that shows how | | | NG-RAN impacts the service quality | | | provided to an end-user, see clause | | | 6.3.6 of TS 28.554 [7]. | | | | | | Throughput for network slice instance: | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.2 and clause 6.3.3 of TS | | | 28.554 [7]; | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR, packet delay and UE location | | | information. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | | | | | | The App data which shows the end user | | | experience. | +--------------------------+------------------------------------------+ | Analytics Data | NWDAF analytics data: Slice QoE, see | | | clause 6.3 of TS 23.288 [18] | | | | | | NWDAF analytics data: Observed Service | | | Experience and/or observed Service MoS, | | | see clause 6.4 of TS 23.288 [18] | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.3.3.3 Analytics report for service experience analysis
The following table shows the potential information of the domain specific or
cross domain analytics report for service experience analysis based on the
required data received as described in clause 6.3.3.3.2.
Analytics Report of service experience Attribute Name Description
* * *
                                           Service experience identifier           The identifier indicates the analytics report is related with service experience analysis.
                                           Managed Objects of service experience   The object instances where the service experience is applicable, e.g., SubNetwork Instance, NetworkSlice Instance, S-NSSAI.
                                           Service experience level                The level of service experience, e.g. there are five levels which are represented by 1, 2, 3, 4, 5 where level 1 represents the users are endures bad experience while level 5 represents the users' requirements are perfectly satisfied.
                                           Info of service experience              Statistics or predictions of the service experience, may be analysed in different levels, e.g. per S-NSSAI, per network slice, per network slice subnet, per 5QI, per UE etc.
                                           Root cause                              The root cause of the service experience issues, e.g., unstable handover successful rate, high latency, low QoS retainability etc.
#### 6.3.3.4 Evaluation
Based on the potential solutions, the required inputs are listed in clause
6.3.3.3.2, where:
\- The performance measurements and KPIs is defined in TS 28.552 [8] and TS
28.554 [7].
\- The MDT Data defined in TS 37.320 [12] can be acquired in procedures
defined in TS 32.422 [25].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
\- User service experience data can be acquired from NWDAF or AF as defined in
clause 6.4 of TS 23.288 [18].
Therefore, MDA is feasible to support the analyses and predictions of service
experience.
### 6.3.4 Network slice throughput analysis
#### 6.3.4.1 Use case
Throughput is of great importance which represents the end users\' experiences
and also reflects the network problems, e.g., low UE throughput may be caused
by the resource shortage.
In order to satisfy the requirements of _dL/ulThptPerSlice_ in the
_ServiceProfile_ , MDAS may be utilized for throughput related
analysis/predictions for, network slice instance. The related performance
measurements in 28.552 [2] and KPIs in 28.554 [8] are utilized as the input
data for analysis.
MDAS producer should have the capability to detect potential throughput issues
and identify the root cause to assist throughput assurance. Network slice
throughput analysis can be for a specific domain or for cross-domain. It is
expected that 3GPP Cross Domain MDAS producer provides the federated
throughput analytics report and Domain MDAS producer provides the domain
specific throughput analysis. These two levels of MDAS producers worked in a
coordinated way to assure the throughput performance.
The CSC concerns the statistics or predictions of subscribers\' DL/UL
throughput for the network slice, e.g. average percentage of users, for which
the required SLS throughput is met or the average percentage of time, during
which the required SLS throughput is, or could be, met. The 3GPP Cross Domain
MDAS producer may indicate whether the problem is caused by NG-RAN or CN. In
general, NG-RAN is the bottleneck because of the limited radio resources and
complicated wireless radio conditions. In this case, resource reconfigurations
may be needed to resolve the throughput degradation issue. To identify the
root causes, RAN Domain MDAS Producer may collect radio resource related
configurations and measurements, radio condition related measurements and
throughput related measurements.
The producer of MDAS is able to, from the perspective of the management
aspects, provide the network slice throughput analytics report. This analytics
report can be considered as an input to support SLS assurance to perform
further evaluation.
#### 6.3.4.2 Potential requirements
**REQ-THP_CON-1** The MDAS producer should have a capability to provide the
analytics report on network slice throughput.
**REQ-THP_CON-2** The analytics report of the network slice throughput should
contain the following information:
\- Network slice throughput statistics/predictions;
\- Root cause analysis of network slice throughput degradation;
#### 6.3.4.3 Possible solutions
##### 6.3.4.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to provide network slice throughput
statistics/predictions, identification of throughput degradation issues and
the root cause as the table in clause 6.3.4.3.5 shows. This procedure may be
triggered by the request or periodically.
##### 6.3.4.3.2 Data required for network slice throughput
statistics/predictions for RAN domain
The following table shows the potential data required to analyse the network
slice throughput for RAN domain.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Packet Delay, see clause 5.1.1.1.1 in TS | | | 28.552 [8]; | | | | | | UE throughput, see clause 5.1.1.3 of TS | | | 28.554 [7]; | | | | | | RAN UE throughput, KPI shows how NG-RAN | | | impacts the service quality provided to | | | an end-user, see clause 6.3.6 of TS | | | 28.554 [7]; | | | | | | Throughput at N3 interface, | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | | | | | | Radio resource utilization, the usage of | | | physical radio resource utilization of | | | the network, see clause 5.1.1.2 of TS | | | 28.552 [8]. | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR and UE location information. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. The | | | data comes from the provisioning MnS | | | producer, e.g., the configured | | | ServiceProfile. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.4.3.3 Data required for network slice throughput
statistics/predictions for CN domain
The following table shows the potential data required to analyse the network
slice throughput for CN domain.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Throughput at N3 interface, | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | | | | | | Number of incoming/outgoing GTP data | | | packets on the N3 interface, see clauses | | | 5.4.1.1 and 5.4.1.2 of TS 28.552 [8]. | | | | | | Number of octets of incoming/outgoing | | | GTP data packets on the N3 interface, | | | see clauses 5.4.1.3 and 5.4.1.4 of TS | | | 28.552 [8]. | | | | | | Virtualised resource usage measurement, | | | see clause 6.2 of TS 28.552 [8]. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. The | | | data comes from the provisioning MnS | | | producer, e.g., the configured | | | ServiceProfile. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.4.3.4 Data required for network slice throughput
statistics/predictions for cross domain
For cross domain analysis, the RAN and CN domain required data as described in
6.3.4.3.2 and 6.3.4.3.3 may be also needed, as well as the potential data
described in the following table.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Latency of 5G Network, see clause 6.3.1 | | | of TS 28.554 [7]; | | | | | | Throughput for network slice instance, | | | see clause 6.3.2 of TS 28.554 [7]; | | | | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.3 of TS 28.554 [7]. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. The | | | data comes from the provisioning MnS | | | producer, e.g., the configured | | | ServiceProfile. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.4.3.5 Analytics report for network slice throughput analysis
The following table shows the potential information of the domain specific or
cross domain analytics report for network slice throughput analysis based on
the required data received as described in clauses 6.3.4.3.2, 6.3.4.3.3 and
6.3.4.3.4.
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | network slice | | | | throughput | | | +======================+======================+======================+ | | Network slice | The identifier of | | | throughput issue | the network slice | | | identifier | throughput issues; | +----------------------+----------------------+----------------------+ | | Network slice | Average percentage | | | throughput | of users, for which | | | st | the required SLS | | | atistics/predictions | throughput is, or | | | | could be, met. | | | | | | | | Average percentage | | | | of time, during | | | | which the required | | | | SLS throughput is, | | | | or could be, met. | +----------------------+----------------------+----------------------+ | | Root cause | The root cause of | | | | the network slice | | | | throughput | | | | degradation issues, | | | | e.g. PRB resource | | | | shortage; | +----------------------+----------------------+----------------------+ | | Recommended actions | The recommend | | | | actions to solve the | | | | network slice | | | | throughput | | | | degradation issues. | +----------------------+----------------------+----------------------+
#### 6.3.4.4 Evaluation
Based on the potential solutions, the required inputs are listed in clause
6.3.4.3.2, clause 6.3.4.3.3 and clause 6.3.4.3.4, where:
\- The performance measurements and KPIs is defined in TS 28.552 [8] and TS
28.554 [7].
\- The MDT Data defined in TS 37.320 [12] can be acquired in procedures
defined in TS 32.422 [25].
\- The configuration data is available as the NRM defined in TS 28.541 [20].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
Therefore, MDA is feasible to support the analyses and predictions of network
slice throughput related factors for cross domain MDAS, RAN domain and CN
domain respectively.
### 6.3.5 Uplink/downlink throughput per UE in network slice analysis
#### 6.3.5.1 Use case
Uplink/downlink throughput per UE in network slice is one of the SLA
parameters. Data rate in uplink and downlink should be guaranteed to assure
quality experience of UE using specific network slice.
UE uplink/downlink throughput could be impacted by the network capability and
network configurations, e.g. configuration of service priority, RAN capacity,
network load, number of UE in one TA, Wireless channel environment and the
processing time of the network functions, etc. These factors may be the root
cause if the UE uplink/downlink throughput requirements cannot be achieved.
There are some mechanisms to assure UE uplink/downlink throughput, e.g. to
upgrade the service priority, allocate or reserve more network resource.
From the management perspective, resource configuration and allocation
algorithms or policies should support UE uplink/downlink throughput assurance.
MDAS can be utilized to provide UE uplink/downlink throughput analysis to
support SLS assurance.
#### 6.3.5.2 Potential requirements
**REQ-UETHROUGHPUT_ASS-CON-1** : MDAS producer should have the capability to
provide analytics report describing the UE uplink/downlink throughput problem.
**REQ-UETHROUGHPUT_ASS-CON-2** : The analytics report describing the UE
uplink/downlink throughput problem should include the following information:
\- The identifier of the UE uplink/downlink throughput issue;
\- Indication of UE uplink/downlink throughput issue type;
\- The start time and end time of the UE uplink/downlink throughput issue;
\- The geographical area and location where the UE uplink/downlink throughput
issue exists;
\- Root cause of the UE uplink/downlink throughput issue;
\- The objects affected by the UE uplink/downlink throughput issue;
\- The severity level of the UE uplink/downlink throughput issue;
\- The recommended actions to solve the UE uplink/downlink throughput issue.
#### 6.3.5.3 Possible solutions
##### 6.3.5.3.1 Solution description
_The performance measurements, e.g., UE throughput, network resource
utilization can be utilized for_ UE uplink/downlink throughput _analysis. To
support_ UE uplink/downlink throughput _assurance_ in order to satisfy the
throughput per UE requirements from the vertical users, MDAS producer is able
to provide the analytics report as defined in 6.3.5.3.3 related with UE
uplink/downlink throughput analytics triggered by event or periodically.
##### 6.3.5.3.2 Required data for UE uplink/downlink throughput analysis
The management data required to analyse the UE uplink/downlink throughput are
defined as the following table.
+------------------------+------------------+------------------------+ | Input data | Data type | Description | +========================+==================+========================+ | S-NSSAI | Service data | \"S-NSSAI\" as defined | | | | in clause 5.15.2, TS | | | | 23.501 [13]. MDAS | | | | may derive network | | | | topology information | | | | according to S-NSSAI | +------------------------+------------------+------------------------+ | Performance | Measurement data | UE throughput: The IP | | measurement | | throughput of end | | | | users, see clause | | | | 5.1.1.3 of TS 28.552 | | | | [8]; | | | | | | | | RAN UE throughput: A | | | | KPI that shows how | | | | NG-RAN impacts the | | | | service quality | | | | provided to an | | | | end-user, see clause | | | | 6.3.6 of TS 28.554 | | | | [7]; | | | | | | | | Throughput for network | | | | slice instance: | | | | Upstream/Downstream | | | | throughput for network | | | | and Network Slice | | | | Instance, see clause | | | | 6.3.2 and clause 6.3.3 | | | | of TS 28.554 [7]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput at | | | | N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | | | | | | | | Radio resource | | | | utilization: The usage | | | | of physical radio | | | | resource utilization | | | | of the network, see | | | | clause 5.1.1.2 of TS | | | | 28.552 [8]; | | | | | | | | CQI related | | | | measurements: the | | | | distribution of | | | | Wideband CQI (Channel | | | | Quality Indicator) | | | | reported by UEs in the | | | | cell, see clause | | | | 5.1.1.11 of TS 28.552 | | | | [8]; | | | | | | | | MCS related | | | | Measurements: the | | | | distribution of the | | | | MCS scheduled for | | | | PDSCH RB by NG-RAN, | | | | the distribution of | | | | the MCS scheduled for | | | | PUSCH RB by NG-RAN, | | | | see clause 5.1.1.12 in | | | | TS 28.552 [8]; | +------------------------+------------------+------------------------+ | MDT Data | Measurement data | UE measurements | | | | related to RSRP, RSRQ, | | | | SINR and UE location | | | | information, see TS | | | | 37.320 [12]. | +------------------------+------------------+------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.5.3.3 Analytics report for UE uplink/downlink throughput analysis
The following table provides the potential contents of the analytics report of
UE uplink/downlink throughput analysis.
Analytics Report of UE uplink/downlink throughput analysis Attribute Name
Description
* * *
                                                               SLS assurance issue Identifier           The identifier indicates the SLS assurance issue
                                                               Indication of SLS assurance issue type   Indicates the type of the SLS assurance issue, e.g.,
                                                               Location                                 The geographical area and location where the UE uplink/downlink throughput issue exists
                                                               Start time                               The start time of the UE uplink/downlink throughput issue
                                                               Stop time                                The stop time of the UE uplink/downlink throughput issue
                                                               Root cause                               The root cause of the UE uplink/downlink throughput degradation, e.g. coverage issue, load issue in RAN, load issue in CN, low throughput in RAN, low throughput in CN etc.
                                                               Affected objects                         The MOIs of cells, subnetwork or network slices affected by the UE uplink/downlink throughput issue;
                                                               Severity level                           The severity level (e.g., critical, medium, not important) of the UE uplink/downlink throughput issue;
                                                               Recommended actions                      The recommended actions to solve the UE uplink/downlink throughput issue, e.g.,
#### 6.3.5.4 Evaluation
The solution described in clause 6.3.5.3 requires the analytics inputs as
described in in clause 6.3.5.3.2, wherein
\- the S-NSSAI can be provided along with the performance measurements, NRM
and planning data.
\- the performance data (measurements and KPIs) are available in TS 28.552 [8]
and TS 28.554 [7].
\- the MDT data are available in TS 32.422 [25].
With these analytics inputs which are already defined, the analytics output as
described in 6.3.5.3.3 can be derived. Therefore, this solution is a feasible
candidate for analysis of Uplink/downlink throughput per UE in network slice.
### 6.3.6 KPI anomaly analysis
#### 6.3.6.1 Use case
KPI(s) are of great importance for network operators to monitor the key
performance of the network. For 5G and beyond, a large amount of KPIs are
defined in TS 28.554 [7]. The correlations between different KPIs are
complicated and it is hard to monitor the KPI(s) manually. Also, how to assign
each KPI threshold is a big challenge for network operators, since each KPI
threshold should not be a fixed value considering many factors such as network
capacity, service types, end user\'s experiences, etc. In addition, the
criteria to determine whether a KPI is anomalous also depends on a variety of
requirements.
MDAS is expected to have the capability to analyse KPI(s) for both cross
domain and single domain. 3GPP Cross Domain MDAS producers may coordinate with
domain MDAS producer to identify the anomalous KPI(s) and the corresponding
root cause(s). The KPI anomaly analysis should consider both a single KPI and
KPI correlations of different domains. For cross domain KPI anomaly analysis,
KPIs or KPI analytics report from each domain should be collected. The MDAS
producer should also be able to identify anomalous network situations. A
network situation can be characterized as a combination of KPIs with respect
to a predefined context that includes time, location, amount of traffic, user
characteristics, etc.
MDAS is also expected to have the capability to detect and predict the
anomalous KPI(s) and anomalous network situations in different levels e.g. per
S-NSSAI, per NSI or per NSSI, etc. The detection and prediction of single KPI
anomaly and in relation with multiple correlated KPIs anomaly may be involved.
The detection and prediction may also involve a single or multiple KPIs in
relation with a network situation. By utilizing Machine learning technology,
historical KPI data and performance data may also be used as the input to
perform the ML model training. Besides, along with the KPI or network
situation anomaly identification and root cause analysis, MDAS may also
recommend more appropriate network configurations to optimize and resolve the
KPI anomaly issue and improve the slice QoE.
The KPI anomaly analysis is also expected to have the capability to analyse
the correlations between SLS and KPIs or network situations according to
service model and identify the most relevant anomalous KPIs and network
situations, which cause the SLS degradation, this corresponding analytics
report can be considered as an input to support further SLS assurance.
#### 6.3.6.2 Potential requirements
**REQ-KPI_ANOMALY_CON-1** The MDAS producer should have a capability to
provide the analytics report on KPIs anomaly analysis.
**REQ- KPI_ANOMALY_CON-2** The analytics report describing the KPIs anomaly
should contain the following information describing the KPI anomaly aspects
and potentially future prediction:
\- The predictive anomaly KPI(s) or observed anomaly KPI(s), may split into
subcounters in different levels, e.g. per S-NSSAI, per NSI, per NSSI, per 5QI,
per UE etc;
\- KPI anomaly indication and root cause analysis;
\- Cross domain and domain KPI anomaly analysis;
\- The recommendations for the configurations of network resource and KPI
threshold.
**REQ- KPI_ANOMALY_CON-3** The MDAS producer should have a capability to
provide the analytics report that identifies a possible anomalous KPIs with
respect to network situations for a particular network location and network.
#### 6.3.6.3 Possible solutions
##### 6.3.6.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to provide KPI anomaly analysis and identify the root
cause. The required data can be from RAN domain or CN domain or both. As the
table in 6.3.6.3.5 shows, the analytics report is able to be provided by the
MDAS producer to describe the KPI anomaly issue, root causes and
recommendations. This procedure may be triggered upon request or periodically.
##### 6.3.6.3.2 Data required for KPI anomaly analysis for RAN domain
The following table shows the potential data required to analyse the RAN
domain KPI anomaly.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Radio resource utilization: The usage of | | | physical radio resource utilization of | | | the network, see clause 5.1.1.2 of TS | | | 28.552 [8]; | | | | | | Performance Measurements for gNB: for | | | example, for RRC connection related KPI | | | anomaly analysis, see clause 5.1, TS | | | 28.552 [8], e.g., RRC connection | | | number, RRC connection establishment, | | | RRC connection re-establishment, RRC | | | connection resuming; | | | | | | RAN UE throughput: A KPI that shows how | | | NG-RAN impacts the service quality | | | provided to an end-user, see clause | | | 6.3.6 of TS 28.554 [7]. | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR and UE location information. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration data | The execution data including the changes | | | or the configuration of the MOIs. | +--------------------------+------------------------------------------+ | Context data | The information on the conditions | | | applicable to the data considered for | | | analytics, e.g. time of day, season or | | | event in relation to location. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.6.3.3 Data required for KPI anomaly analysis for CN domain
The following table shows the potential data required to analyse the CN domain
KPI anomaly.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Performance Measurements for AMF: for | | | example, for number of registered | | | subscribers related KPI anomaly | | | analysis, see clause 5.2.1, TS 28.552 | | | [8]; | | | | | | Performance Measurements for SMF: for | | | example, for PDU session management | | | related KPI anomaly analysis, see clause | | | 5.3.1, TS 28.552 [8]; | | | | | | Throughput at N3 interface: KPI related | | | to Upstream/Downstream GTP data | | | throughput at N3 interface, see clause | | | 6.3.4 and clause 6.3.5 of TS 28.554 | | | [7]; | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.6.3.4 Data required for KPI anomaly analysis for cross domain
For cross domain analysis, the RAN domain and CN domain required data as
described in clauses 6.3.6.3.2 and 6.3.6.3.3 may be needed, as well as the
potential data described in the following table.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Throughput for network slice instance: | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.2 and clause 6.3.3 of TS | | | 28.554 [7]; | | | | | | NWDAF analytics data: Slice QoE, see | | | clause 6.4 of TS 23.288 [18]. | +--------------------------+------------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +--------------------------+------------------------------------------+ | Configuration data | The execution data including the changes | | | or the configuration of the MOIs. | +--------------------------+------------------------------------------+ | Context data | The information on the conditions | | | applicable to the data considered for | | | analytics, e.g. time of day, season or | | | event in relation to location. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.3.6.3.5 Analytics report for KPI anomaly analysis
The following table shows the potential information of the domain specific or
cross domain analytics report for KPI anomaly analysis based on the required
data received as described in clauses 6.3.6.3.2, 6.3.6.3.3 and 6.3.6.3.4.
Analytics Report of network slice KPI anomaly Attribute Name Description
* * *
                                                  KPI anomaly identifier             The identifier of the KPI anomaly;
                                                  Anomalous KPI Name                 The name of the KPI(s) which is identified or predicted as anomalous, the KPI name refers to bullet a) in TS28.554 (7);
                                                  Managed Objects of anomalous KPI   The object instances where the KPI is applicable, e.g., SubNetwork Instance, NetworkSlice Instance
                                                  Info of KPI anomaly                Statistics or predictions of the anomalous KPIs, may concern single KPI or multiple correlated KPIs, and may split into subcounters at different levels, e.g. per S-NSSAI, per NSI, per NSSI, per 5QI, per UE etc.
                                                  Root cause                         The root cause of the network slice KPI anomaly issues, e.g., unstable handover successful rate, low PRB utilization, low QoS retainability.
##### 6.3.6.3.6 Analytics report for network situation analysis
The following table shows the potential information of the domain specific or
cross domain analytics report based on the required data received as described
in clauses 6.3.6.3.2, 6.3.6.3.3, and 6.3.6.3.4.
Analytics Report on network **situations** Attribute Name Description
* * *
                                               Network situation identifier           The identifier of the possible network situations.
                                               Network situation description          The network performance, QoE and UE location and context data that describe a network situation.
                                               Network situations anomaly status      The status of a network situation. This may be a simple binary function that indicates a normal or anomalous status.
                                               Severity level                         Describes the degree of anomaly (low, medium, high).
                                               Location                               Geographical location of anomalous network situations
                                               Managed Objects of network situation   The object instances, e.g., cell, carrier.
                                               Type of analytics                      Statistics or predictions of the anomalous situation.
#### 6.3.6.4 Evaluation
The solution described in clause 6.3.6.3.1 requires the analytics inputs as
described in clause 6.3.6.3.2, 6.3.6.3.3 and 6.3.6.3.4 wherein:
\- Input PM related to RAN utilization, gNB measurements, AMF and SMF are
specified in TS 28.552 [8].
\- Input KPIs related to RAN UE throughput and throughput N3 are specified in
TS 28.554 [7].
\- Input MDT data are specified in TS 37.320 [12].
\- Input related to service experience specified in TS 37.320 [12].
\- QoE data is optional.
\- Configuration data is available.
\- Context data that relates to information on the conditions applicable to
the data considered for analytics need to be specified as NRM.
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.3.6.3.5 and 6.3.6.3.6 can be derived for
RAN domain, CN domain and Cross domain respectively.
Therefore, the solution is feasible.
### 6.3.7 Jitter analysis
#### 6.3.7.1 Use case
In addition to E2E latency, some new network services have extra new demands
for SLA parameters. In extreme business scenarios such a V2X and telemedicine,
network jitter also needs to be satisfied. Jitter assurance with high
certainty becomes a new dimension of end-to-end delay-sensitive service
quality.
The analysis of jitter does not cover the entire network, but the analysis
should consider the whole process of data frame and also the definition of
different network layers. It could be impacted by the configuration of clock
synchronization, traffic forwarding, bandwidth reservation and latency etc. In
some circumstances, it will also be affected by the external environment,
which can only be resolved non-automatically.
The performance measurements and measurements related to data forwarding
mechanisms can be used by the MDAS producer to generate a jitter analysis
report. For example, in some network node, one particular data flow can have
bandwidth competition with normal data flow which causes high network jitter.
The MDAS will first analyse the bandwidth resource and provide bandwidth
reservation modification so that it can satisfy network performance
requirement of this particular data flow. Jitter is also related to E2E
latency parameter, the jitter analysis will then need to be cross-domain, e.g.
RAN domain, core network domain transport network domain. The jitter analysis
from the multiple domains may be combined to calculate the end-to-end jitter,
which can be checked against end-to-end SLA requirements. The jitter analysis
will reference the E2E analysis result to support SLS assurance. In case of
excessive end-to-end jitter, the jitter analysis from each domain may be used
to identify the likely source of problems.
#### 6.3.7.2 Potential requirements
**REQ-JITTER_ASS-CON-1** : MDAS producer should have the capability to provide
analytics report describing the jitter problem.
**REQ-JITTER_ASS-CON-2** : The analytics report describing the jitter problem
should include the following information:
\- The identifier of the jitter issue;
\- Indication of jitter issue type;
\- The start time and end time of the jitter issue;
\- The geographical area and location where the jitter issue exists;
\- Root cause of the jitter issue;
\- The objects affected by the jitter issue;
\- The severity level of the jitter issue;
\- The concrete parameters of jitter value;
\- The recommended actions to solve the jitter issue.
#### 6.3.7.3 Possible Solutions
##### 6.3.7.3.1 Solution description
The solution requires MDAS producer to collect various data and provide jitter
analytics report. To support jitter assurance in order to satisfy the
customer's requirements, the MDAS producer is required to provide the
analytics report as defined in 6.3.7.3.3. This procedure may be triggered by
the request or periodically
##### 6.3.7.3.2 Data required
The management data required to analyze the jitter are defined as the
following table.
+----------------------+----------------------+----------------------+ | **Input data** | **Data type** | **Description** | +----------------------+----------------------+----------------------+ | S-NSSAI | Service data | "S-NSSAI" as defined | | | | in clause 5.15.2, TS | | | | 23.501 [13]. MDAS | | | | may derive network | | | | topology information | | | | according to S-NSSAI | +----------------------+----------------------+----------------------+ | Performance | Measurement data | Packet delay: | | measurement | | "packet delay" | | | | measurement as | | | | defined in clause | | | | 5.1.1.1, clause | | | | 5.1.3.3, TS 28.552 | | | | [8]; | | | | | | | | Round-trip GTP Data | | | | Packet Delay: | | | | "Round-trip GTP Data | | | | Packet Delay" as | | | | defined in clause | | | | 5.4.1.9, TS 28.552 | | | | [8]; | | | | | | | | GTP packets delay in | | | | UPF: "GTP packets | | | | delay in UPF" as | | | | defined in clause | | | | 5.4.5, TS 28.552 | | | | [8]; | | | | | | | | Round-trip GTP Data | | | | Packet Delay on N9 | | | | interface: | | | | "Round-trip GTP Data | | | | Packet Delay on N9 | | | | interface" as | | | | defined in clause | | | | 5.4.4.1, TS 28.552 | | | | [8]; | | | | | | | | One way packet delay | | | | between NG-RAN and | | | | PSA UPF as defined | | | | in clause 5.4.7, TS | | | | 28.552 [8]; | | | | | | | | Round-trip packet | | | | delay between PSA | | | | UPF and NG-RAN as | | | | defined in clause | | | | 5.4.8, TS 28.552 | | | | [8]; | | | | | | | | One way packet delay | | | | between PSA UPF and | | | | UE as defined in | | | | clause 5.4.9, TS | | | | 28.552 [8]; | | | | | | | | Throughput at N3 | | | | interface: | | | | Upstream/Downstream | | | | GTP data throughput | | | | at N3 interface, see | | | | clause 6.3.4 and | | | | clause 6.3.5 of TS | | | | 28.554 [7]; | | | | | | | | End-to-end Latency | | | | of 5G Network: | | | | "End-to-end Latency | | | | of 5G Network" as | | | | defined in clause | | | | 6.3.1, TS 28.554 | | | | [7]; | | | | | | | | Throughput for | | | | network slice: | | | | Upstream/Downstream | | | | throughput for | | | | network and Network | | | | Slice Instance, see | | | | clause 6.3.2 and | | | | clause 6.3.3 of TS | | | | 28.554 [7]; | | | | | | | | New measurements on | | | | UL and DL jitter for | | | | network slice in | | | | NG-RAN, on N3/N9 | | | | interface, between | | | | PSA UPF and UE, | | | | between PSA UPF and | | | | NG-RAN. | +----------------------+----------------------+----------------------+ | QoE Data | Measurement data | The details | | | | information of QoE | | | | data required by | | | | this case is FFS. | +----------------------+----------------------+----------------------+ | Service experience | Network analytics | The analysis result | | data | data | regarding observed | | | | service experience | | | | from the NWDAF | | | | defined in TS 23.288 | | | | [18]. | +----------------------+----------------------+----------------------+
Note: The above parameters may not be the complete list.
##### 6.3.7.3.3 Analytics report
The jitter analytics report contains the following information.
Jitter Analytics Report Attribute Name Description
* * *
                            Jitter Identifier      Identification of the jitter issue.
                            Jitter Issue Type      Indication of jitter issue type, e.g. RAN jitter issue, CN jitter issue.
                            Start Time             The start time of the jitter issue.
                            End Time               The end time of the jitter issue.
                            Location Information   The geographical area and location where the jitter issue exists.
                            Root cause             Root cause of the jitter issue.
                            Affected objects       The MOIs of cells, subnetwork or network slices affected by the jitter
                            Severity level         The severity level (e.g., critical, medium, not important) of the jitter issue.
                            Jitter Value           The average network latency with delay variation range
                            Recommended actions    The recommended actions to solve the jitter issue.
#### 6.3.7.4 Evaluation
The solution described in clause 6.3.7.3 requires the analytics inputs as
described in in clause 6.3.7.3.2, wherein
\- the S-NSSAI can be provided along with the performance measurements, NRM
and planning data.
\- the delay related performance measurements and KPIs are available in TS
28.552 [8] and TS 28.554 [7], and the jitter related measurements can be
defined in TS 28.552 [8].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
\- The analysis results regarding observed service experience from the NWDAF
are defined in TS 23.288 [18].
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.3.7.3.3 can be
derived.
Therefore, this solution is a feasible candidate for jitter analysis.
### 6.3.8 Network slice traffic projection
#### 6.3.8.1 Use case
Some of the requirements captured in SliceProfile need to be translated into
configurable parameter for various network entities including entities in 5G
Core Network (5GC), Radio Access Network (RAN) and Transport Network. One of
the example would be: the GST attribute Downlink throughput per slice
(dLThptPerSlice) can be translated into maximum downlink throughput per slice
(maxDlThptPerSlice) as a configuration parameter, for UPF. However, as one
slice may have multiple UPF instances, dividing the total quota available
among each UPF instance is critical. **The given requirements can be divided
among all the targeted NF instance based on the slice traffic analytics.** The
value for a particular configurable parameter (translated from a particular
ServiceProfile attribute) for a particular NF is crucial, especially if
multiple instance of that NFs is available in the network slice instance.
**It is desirable to use uses MDAS to get the network slice traffic
projections including individual traffic projections on each of the
constituent network functions instances present in the slice. The individual
traffic projections can be used to divide total available quota among the
constituent network functions instances which can then be configured for
network function(s), as required. For example, MDAS can provide total number
of projected terminal or subscription for each AMF instance in the slice.
Based on the projections the total available quota can be divided among the
multiple AMF instances in the slice. The AMF instance serving more users or
required to serve more users in future will have more quota then other AMF
instances in the slice.**
#### 6.3.8.2 Potential Requirements
**REQ-TRA_CON_CON-1: The MDAS producer should have a capability allowing the
authorized consumer to request the slice traffic analytics report describing
traffic projection of the slice including its constituent network functions.**
**REQ-TRA_CON_CON-2: The MDAS producer should have a capability to provide the
slice traffic analytics report describing the traffic projections for each
constituent network function instance in the slice.**
**REQ-TRA_CON_CON-3:** **The slice traffic analytics report providing traffic
projection for the slice may include the following information:**
\- Projected uplink and downlink throughput on each User Plane Function
instance (UPF) present in the slice.
\- Projected number of Packet Data Unit (PDU) session for each Session
Management Function (SMF) instance present in the slice.
\- Projected number of UE or Registered subscriptions for each AMF instance
present in the slice.
\- Projected maximum packet size for each UPF instance present in the slice.
\- Projected UE uplink and downlink throughput on each gNodeB (gNB) instance
present in the slice.
\- Projected number of UE for each gNB/NR cell instance present in the slice.
#### 6.3.8.3 Possible Solutions
##### 6.3.8.3.1 Solution description
The solution requires MDAS producer to produce slice traffic analytics report
providing traffic projections on NFs involved in the slice. Based on the
report, configuration for the NFs can be decided.
##### 6.3.8.3.2 Data required
The following table shows the data required to generate the traffic analytics
report.
+-------------------------------+-------------------------------------+ | Data Category | Required Data | +===============================+=====================================+ | Performance Measurements/KPIs | | +-------------------------------+-------------------------------------+ | | From each UPF instance in the slice | | | | | | - Current Uplink throughput. See | | | 5.4.1.3 in TS 28.552. | | | | | | - Current Downlink throughput. See | | | 5.4.1.4 TS 28.552 | | | | | | - Current Maximum packet size | +-------------------------------+-------------------------------------+ | | From each gNB instance in the slice | | | | | | - Current Uplink UE throughput. | | | See 5.1.1.3 in TS 28.552. | | | | | | - Current Downlink UE throughput. | | | See 5.1.1.3 in TS 28.552. | +-------------------------------+-------------------------------------+ | | From each SMF instance in the slice | | | | | | - Current Number of PDU session. | | | See 5.2.1 in TS 28.552. | +-------------------------------+-------------------------------------+ | | From each AMF instance in the slice | | | | | | - Current Number of registered | | | subscriptions. See 5.2.1 in TS | | | 28.552. | +-------------------------------+-------------------------------------+ | MDT Data | UE measurements related to RSRP, | | | RSRQ, SINR and UE location | | | information, see TS 37.320 [12]. | +-------------------------------+-------------------------------------+
##### 6.3.8.3.3 Analytics report
The slice traffic analytics report contains the following:
Analytics Report of slice traffic projection Attribute Name Description
* * *
                                                 Slice Identifier            Identifier of the slice for which the report is provided
                                                 Projection Timestamp        Provide a particular time stamp for which the projections are provided
                                                 Projection Duration         Provides a time duration during which the average projections are provided
                                                 For each UPF in the slice   Projected Uplink throughput
                                                                             Projected Downlink throughput
                                                                             Projected Maximum packet size
                                                 For each gNB in the slice   Projected Uplink UE throughput.
                                                                             Projected Downlink UE throughput
                                                                             Projected Number of UE
                                                 For each SMF in the slice   Projected Number of PDU session
                                                 For each AMF in the slice   Projected Number of registered subscriptions
#### 6.3.8.4 Evaluation
Based on the potential solutions, the required inputs are listed in clause
6.3.8.3.2, where:
\- The performance measurements and KPIs are defined in TS 28.552 [8] and TS
28.554 [7].
\- The MDT Data defined in TS 37.320 [12] can be acquired in procedures in TS
32.422 [25].
With these analytics inputs which are already defined, the analytics output as
described in 6.3.8.3.3 can be derived.
Therefore, MDA is feasible to support the analyses and predictions of traffic
projection.
## 6.4 Fault management related issues
### 6.4.1 Alarm incident analysis
#### 6.4.1.1 Use case
In 5G network, millions of alarms are generated due to the network complexity.
Since the topological relations between different network elements and logical
relations between different generated alarms, a series of alarms caused by a
same root cause should be correlated with each other. In addition, the same
root causes may give rise to the network performance deterioration. For
example, an alarm in a lower layer of the communication protocol stack or an
alarm related to virtualized resource in the virtualization deployment
environment may cause multiple alarms in higher layers. Sequence of alarms may
be generated in multiple domains along a communication link if one fault in
the source domain occurs.
Large amount of alarms brings difficulties in network operation and
maintenance. Therefore, the alarms and deteriorated performance measurements
of same root cause should be correlated and analysed. Some ML models and
algorithms may be used to group or filter the correlated alarms and indicate
the root cause. Also, the historical alarms, performance measurements and
network topology data can be utilized as the foreknowledge.
To improve the efficiency of network operation and maintenance, the MDAS
producer is able to provide the analytics result including the root alarm or
root cause by correlate and group the related alarms and performance
measurements into an alarm incident.
#### 6.4.1.2 Potential requirements
**REQ-ALARM_MDA-01:** The MDAS producer should have a capability to provide
the analytics report describing the alarm incident analysis.
**REQ-ALARM_MDA-02:** The analytics report describing the alarm incident
should include the following information:
\- Alarm incident Identifier
\- List of Correlated Alarms, performance measurements
\- The start time and end time of the Alarm incident
\- The root cause or root alarm of the Alarm incident
\- Severity level
\- Affected objects and object relationships
\- Recommended actions
#### 6.4.1.3 Possible Solutions
##### 6.4.1.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to identify the alarm incident. The required data can be
from RAN domain or CN domain or both. As the table in clause 6.4.1.3.4 shows,
the analytics report is able to be provided by the MDAS producer to describe
the root causes and recommendations of identified alarm incident. It can be a
domain specific or cross domain analytics report. This procedure may be
triggered by the request or periodically.
##### 6.4.1.3.2 Data required for alarm incident analysis for RAN domain
Alarm data and performance measurements from correlated logical and physical
resources are able to be utilized to perform the alarm incident analysis. The
data listed in following table are the potential management data for RAN
domain used to construct the alarm incident.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Alarm Data | The alarm information, e.g., the alarm | | | of NG-U, the alarm of radio frequency | | | unit, the alarm of cell outage, as | | | specified in TS 28.532 [28]. | +--------------------------+------------------------------------------+ | Performance measurements | The deteriorated performance or the | | | abnormal performance measurements based | | | on certain performance monitoring | | | threshold. Following performance | | | measurements may be used. | | | | | | UE throughput: The IP throughput of end | | | users, see clause 5.1.1.3 of TS 28.552 | | | [8]; | | | | | | PDCP Data Volume: The transmitted PDCP | | | data volume, see clause 5.1.2.1 and | | | 5.1.3.6 of TS 28.552 [8]; | | | | | | TB related measurements: The TB | | | transmitted in a cell, see clause | | | 5.1.1.7 of TS 28.552 [8]; | | | | | | CQI related measurements: the | | | distribution of Wideband CQI (Channel | | | Quality Indicator) reported by UEs in | | | the cell, see clause 5.1.1.11 of TS | | | 28.552 [8]; | | | | | | MCS related Measurements: the | | | distribution of the MCS scheduled for | | | PDSCH RB by NG-RAN, the distribution of | | | the MCS scheduled for PUSCH RB by | | | NG-RAN, see clause 5.1.1.12 in TS 28.552 | | | [8]; | | | | | | RAN UE throughput: A KPI that shows how | | | NG-RAN impacts the service quality | | | provided to an end-user, see clause | | | 6.3.6 of TS 28.554 [7]; | | | | | | Throughput for network slice instance: | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.2 and clause 6.3.3 of TS | | | 28.554 [7]; | | | | | | Throughput at N3 interface: | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | | | | | | Inter-gNB handovers: Number of | | | requested/successful handover/failed | | | handover preparations/resource | | | allocations/executions, see clause | | | 5.1.1.6.1 of TS 28.552 [8]. | | | | | | Intra-gNB handovers: Number of | | | requested/successful handover | | | executions, see clause 5.1.1.6.2 of TS | | | 28.552 [8]. | | | | | | RRC connection establishment related | | | measurements: Attempted/ successful RRC | | | connection establishments, see clause | | | 5.1.1.15 of TS 28.552 [8]. | | | | | | RRC connection Re-establishment and RRC | | | connection Resuming related measurement, | | | see clause 5.1.1.17 and 5.1.1.18 of TS | | | 28.552 [8]. | | | | | | Packet loss rate, Packet drop rate and | | | Packet delay including the average delay | | | and distribution of delay in CU-UP, | | | F1-U, gNB-DU, CU-UP both of downlink and | | | uplink. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. | +--------------------------+------------------------------------------+ | Network topology | The topology of the network deployment. | | | The topology can be reflected by the | | | NG-RAN NRMs as defined in TS 28.541 | | | [20]. | +--------------------------+------------------------------------------+
##### 6.4.1.3.3 Data required for alarm incident analysis for CN domain
Alarm data and performance measurements from correlated logical and physical
resources are able to be utilized to perform the alarm incident analysis. The
data listed in following table are the potential management data for CN domain
used to construct the alarm incident.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Alarm Data | The alarm information, e.g., the alarm | | | of AMF, the alarm of UPF, as specified | | | in TS 28.532 [28]. | +--------------------------+------------------------------------------+ | Performance measurements | The deteriorated performance or the | | | abnormal performance measurements based | | | on certain performance monitoring | | | threshold. Following performance | | | measurements may be used. | | | | | | Mobility related measurements for AMF: | | | Number of requested/failed PDU | | | sessions/QoS flows for inter-AMF | | | handovers and number of | | | attempted/successful/failed handover | | | between 5GS and EPC, see clause 5.2.5.1, | | | 5.2.5.3 and 5.2.5.4 of TS 28.552 [8]. | | | | | | Session related measurements for SMF: | | | Number of successful/failed PDU session | | | creation and number of | | | requested/successful/failed PDU session | | | modifications, see clause 5.3.1 of TS | | | 28.552 [8]. | | | | | | QoS flow monitoring for SMF: Number of | | | requested/successful/failed QoS flows to | | | create/modify, see clause 5.3.2 of TS | | | 28.552 [8]. | | | | | | Session related measurements for UPF: | | | Number of requested/failed N4 session | | | establishments, see clause 5.4.3.1 of TS | | | 28.552 [8]. | | | | | | Throughput for network slice instance: | | | Upstream/Downstream throughput for | | | network and Network Slice Instance, see | | | clause 6.3.2 and clause 6.3.3 of TS | | | 28.554 [7]; | | | | | | Throughput at N3 interface: | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | | | | | | Virtualised resource usage measurement, | | | see clause 6.2 of TS 28.552 [8]. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs. | +--------------------------+------------------------------------------+ | Network topology | The topology of the network deployment. | | | The topology can be reflected by the 5GC | | | NRMs as defined in TS 28.541 [20]. | +--------------------------+------------------------------------------+
##### 6.4.1.3.4 Analytics report for alarm incident analysis
The following table provides the potential information of the domain specific
or cross domain analytics report for alarm incident analysis based on the
required data received as described in clauses 6.4.1.3.2 and 6.4.1.3.3.
Analytics Report of alarm incident Information Description
* * *
                                       Alarm Incident Identifier                          The alarm incident id or name for correlated alarms and performance measurements, e.g., NgU transmission alarm incident, Xn transmission alarm incident.
                                       List of Correlated AlarmInfo                       List of Alarm name or alarm ID, e.g., alarm of NgU setup, alarm of user plane link failure, alarm of user plane failure, alarm of cell outage.
                                       List of Correlated performance measurements info   Performance measurements and the corresponding value, e.g., NgU handover failure rate, NgU setup failure rate.
                                       Location                                           The geographical area or the cells where the alarm incident exists
                                       Start Time                                         The start time of alarm incident
                                       Stop Time                                          The end time of alarm incident
                                       Affected objects and object relations              The affected objects (e.g., the MOIs of cells or subnetworks or network slices affected by the alarm incident) and the (list of) relationship(s) between the objects, e.g., if two affected cells are connected with each other through Xn interface, the relationship between these two objects may be noted as neighbouring.
                                       Root cause or Root alarm                           Root alarm identified or predicted by root cause decision model, e.g. alarm of NgU setup, alarm of virtualized resource failure.
                                       Severity level                                     The severity level (e.g., critical, medium, not important) of the alarm incident
                                       Recommended actions                                The recommend actions to clear the alarm incident. The recommended actions could be to replace the hardware unit, reconfigure the protocol, e.g., Xn application protocol
#### 6.4.1.4 Evaluation
It is important to identify the root cause and resolve network malfunctions
for the network to remain healthy.
The solution described in clause 6.4.1.3.4 requires the analytics inputs as
described in in clause 6.4.1.3.2 and 6.4.1.3.3, wherein
\- the alarm data are available in TS 28.532 [28].
\- the performance measurements are available in TS 28.552 [8].
\- the network topology can be reflected by the NRMs defined in TS 28.541
[20].
\- the configuration data for NG-RAN and 5GC have been defined in TS 28.541
[20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.4.1.3.4 can be derived.
Therefore, this solution is a feasible candidate for Alarm incident analysis
for RAN domain, CN domain and cross domain network.
### 6.4.2 Fault prediction analysis
#### 6.4.2.1 Use case
In 5G network, millions of alarms are generated every day. The causes of these
alarms are usually the faults or abnormal states of the network. These alarms
could be caused by for example the multiple managed entities (MEs) such as NSI
(Network slice instance), NSSI (network slice subnet instance), NFs (network
functions) (that could be run over Virtual NFs (VNFs) or as Physical NFs
(PNFs)) that may be subject to risks e.g. inaccurate programming, unforeseen
software errors, incorrect or incompatible version of software or hardware
being used, vulnerabilities in design, unforeseen protocol errors, security
errors and so on. The current treatment method is generally based on the alarm
information analysis, to find out the cause of the faults or abnormal states,
and then determine the fault repair method and solve the problems, so as to
eliminate the alarms. Because the amount of alarms is so large, it is a big
challenge to deal with these alarms in a timely and efficient manner.
One the other hand, if the network can be maintained very well so that it has
fewer faults and abnormal states, then there will be fewer alarms. This
requires the system to be able to predict the potential faults or abnormal
states before they occur, and to recommend appropriate handling actions to
prevent the fault or abnormal state really occur. For example, often long
before failures manged entities (particularly software entities) tend to
exhibit strange _outlying_ behaviour in comparison with their previous
operation (example: sudden increase in time taken to process requests, high
compute or memory use, slower response to heartbeat and so forth). In 5G
network, MDAS is adopted, which is in conjunction with AI and ML techniques.
The MDAS producer may train the ML model of the MDAS by using the historical
alarms, performance measurements, configuration data and configuration
notifications, and network topology information to obtain the basic health
maintenance knowledges (e.g. the relationship between the faults or potential
faults and the related maintenance actions).
The MDAS producer monitors and analyses the performance measurements and KPIs
continuously, and provides the analytics report which includes the predictive
information of potential faults or abnormal states and corresponding
recommendation of maintenance actions to prevent the fault or abnormal state
really occur, so that the MDAS consumer can execute the recommended actions
accordingly or by taking the recommended actions into account.
The MDAS producer is informed when the recommended actions are taken by the
MDAS consumer to maintain the network health, so that the MDAS producer can
evaluate the result of the executed actions and update its basic health
maintenance knowledges.
The MDAS producer also periodically does the ML training based on the new
collected alarms, performance measurements and KPIs, configuration data and
configuration notifications, and updates its basic health maintenance
knowledges.
#### 6.4.2.2 Potential requirements
**REQ-HEALTH_MDA-01:** The MDAS producer should have a capability to provide
the analytics report describing the fault prediction analysis results.
**REQ-HEALTH_MDA-02:** The analytics report describing the results from the
fault prediction analysis should include the detailed advice on how to
eliminate the cause of potential fault(s).
Note: The detailed advice described in the fault prediction analytics report
may include but not limited the follows:
\- List of potential faults and severity levels
\- Affected objects' identifiers
\- Timestamp: Time at which the report is generated
\- Relevant performance measures
\- The start time and end time of the Outlying ME analysis
\- Recommended actions
#### 6.4.2.3 Possible solutions
##### 6.4.2.3.1 Solution description
The MDAS producer analyses the management data described in the following
subclauses to identify the potential faults or abnormal states and
corresponding recommended actions. The required data can be from RAN domain or
CN domain or both.
As the table in clause 6.4.2.3.4 shows, the analytics report is able to be
provided by the MDAS producer to describe the analytics result and
recommendations of network health maintenance. It can be a domain specific or
cross domain analytics report. This procedure may be triggered by the request
or periodically.
##### 6.4.2.3.2 Data required for fault prediction analysis for RAN domain
The following table shows the potential data required to perform the fault
prediction analysis for RAN domain.
+----------------------------------+----------------------------------+ | Data Category | Required Data | +==================================+==================================+ | Performance Measurements | RAN related performance | | | measurements and KPIs, see TS | | | 28.552 [8] and TS 28.554 | | | [7]; | | | | | | The detailed types of | | | performance measurements and | | | KPIs are FFS | +----------------------------------+----------------------------------+ | Configuration Data | The execution data including the | | | changes or the configuration of | | | the MOIs. | +----------------------------------+----------------------------------+ | Network topology | The topology of the network | | | deployment. | +----------------------------------+----------------------------------+ | Versioning and vendor | The versioning and vendor | | information | information of the managed | | | entities | +----------------------------------+----------------------------------+
Note: The above parameters may not be the complete list.
##### 6.4.2.3.3 Data required for fault prediction analysis for CN domain
The following table shows the potential data required to perform the fault
prediction analysis for CN domain.
+----------------------------------+----------------------------------+ | Data Category | Required Data | +==================================+==================================+ | Performance Measurements | CN related performance | | | measurements and KPIs, see TS | | | 28.552 [8] and TS 28.554 | | | [7]; | | | | | | The detailed types of | | | performance measurements and | | | KPIs are FFS | +----------------------------------+----------------------------------+ | Configuration Data | The execution data including the | | | changes or the configuration of | | | the MOIs. | +----------------------------------+----------------------------------+ | Network topology | The topology of the network | | | deployment. | +----------------------------------+----------------------------------+ | Versioning and vendor | The versioning and vendor | | information | information of the managed | | | entities | +----------------------------------+----------------------------------+
Note: The above parameters may not be the complete list.
##### 6.4.2.3.4 Analytics report for fault prediction analysis
The following table shows the potential information carried in the analytics
report of fault prediction analysis.
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | fault prediction | | | +======================+======================+======================+ | | Fault prediction | The identifier of | | | analytics report | the fault prediction | | | identifier | analytics report | +----------------------+----------------------+----------------------+ | | List of potential | List of potential | | | faults | faults, including: | | | | | | | | Fault type | | | | | | | | Location | | | | | | | | Severity level | | | | | | | | Affected objects | +----------------------+----------------------+----------------------+ | | Root cause or Root | Root alarm | | | alarm | identified or | | | | predicted by root | | | | cause decision | | | | model, e.g. | | | | incompatible | | | | software versions | +----------------------+----------------------+----------------------+ | | Recommended actions | The recommend | | | | actions to eliminate | | | | the causes of the | | | | potential faults | +----------------------+----------------------+----------------------+
Note: The above parameters carried in the analytics report may not be the
complete list.
#### 6.4.2.4 Evaluation
The solution described in clause 6.4.2.3 requires the analytics inputs as
described in clause 6.4.2.3.2 and 6.4.2.3.3, wherein
\- the performance measurements and KPIs are available in TS 28.552 [8] and TS
28.554 [7].
\- the configuration data for NG-RAN have been defined in TS 28.541 [20].
\- the network topology can be reflected by the NRMs defined in TS 28.541
[20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.4.2.3.4 can be derived.
Therefore, this solution is a feasible candidate for fault prediction
analytics.
### 6.4.3 Alarm malfunction analytics
#### 6.4.3.1 Use case
Fault management involves a large amount of alarms at different levels, which
are inter-related based on topology and logical paradigms. Hence, a single
cause may trigger several alarms across multiple domains. To identify the root
cause, alarm correlation is adopted leveraging also the benefits of historical
data and performance measurements.
However, a major assumption in alarm correlation is the fact that the
source(s) of the corresponding alarm(s) are not malfunctioning, i.e. producing
false alarms. In fact, there are cases where alarms are triggered in
situations where there is no need, (i.e. false positive alarms), as well as
cases where no alarm is triggered although it should have been triggered, i.e.
false negative alarms. Also, there are cases where an alarm malfunction is
caused, e.g. by operational errors, device malfunctions, manmade
interruptions, etc., and in different stages, e.g. during proceed alarm
subscription, generate alarm, generate alarm notification, report alarm,
proceed alarm acknowledge or clear, etc.
To resolve the problem of alarm malfunction, the MDAS producer can assess the
risk of a faulty alarm, recommending methods to suppress other related alarms
if a faulty alarm is identified in order to avoid large amounts of alarms,
which may overload the network. **The MDAS producer may correlate alarms and
performance measurements** in order to check or predict the consistency, e.g.
an alarm of abnormal handover failures towards a specific cell need to be
consistent with the number of newly residing users within a certain time
window. In addition, certain performance measurements need to be consistent,
e.g. reports on high mobility towards a specific cell need to be consistent
with the user density of that cell. Otherwise, a malfunction risk exists,
which needs to be analysed via the corresponding MDAS producer. Such a
consistency check can also take place across different automation levels, e.g.
considering NE level and domain level or domain level and cross-domain level.
The MDAS producer shall analyse the malfunction issue to identify the root
case and recover erroneous data based on historical data in conjunction with
"neighboring" data, using e.g. missing value estimation mechanisms. In
addition, to track down malfunction across different automation levels, the
source of alarms needs to be visible across different automation levels, e.g.
via the means of meta-data, to enable backtracking.
#### 6.4.3.2 Potential requirements
**REQ-ALR_MUL -1** The MDAS producer should have a capability to provide the
analytics report describing alarm malfunction analytics information to
authorized consumers.
**REQ-ALR_MUL -2** The analytics report describing alarm malfunction
information should contain the following information:
> \- the alarm malfunction incident ID;
>
> \- the location related to the alarm malfunction or faulty performance
> measurement(s);
>
> \- the affected object(s) related to malfunction alarm or faulty performance
> measurement(s);
>
> \- the time period(s) related to malfunction alarm or faulty performance
> measurement(s);
>
> \- the root cause that identifies the source malfunction alarm or the source
> faulty performance measurement(s);
>
> \- recommended actions involving malfunction alarm isolation and potential
> data recovery based on history data and "neighboring" data.
#### 6.4.3.3 Possible solutions
##### 6.4.3.3.1 Solution description
_For this Solution, MDAS producer provides information related to the
assessment of alarm_ **malfunctions to** identify the type of malfunction,
location, management objects and recommendation actions, e.g. to supress false
alarms and recover erroneous data.
##### 6.4.3.3.2 Data required
The following data is required to do the required analysis.
+------------------------------+--------------------------------------+ | **Data category** | **Required data** | +==============================+======================================+ | **Alarm Data** | Fault supervision data report | | | services as defined in TS 28.545 | | | [22]. | +------------------------------+--------------------------------------+ | **Service Data** | S-NSSAI as defined in clause 5.15.2, | | | TS 23.501 [13]. | +------------------------------+--------------------------------------+ | **Performance Measurements** | Performance Measurements for gNB (TS | | | 28.552 [8]) | | | | | | > \- Radio resource utilization | | | > (UL/DL), total/available/used PRBs | | | > | | | > \- RRC mean/max number of | | | > connections | | | > | | | > \- PDU session management, | | | > mean/max request/successful/failed | | | > to set up | | | > | | | > \- Inter/intra-gNB mobility, | | | > number of request/successful | | | > /failed preparations, resource | | | > availability, executions, failed | | | > | | | > \- TB/DRB/CQI/QoS flow related | | | > measurements | | | | | | Performance Measurements for 5GC (TS | | | 28.552 [8]) | | | | | | > \- Number of UEs/periodic | | | > registration updates (AMF) | | | > | | | > \- Number of service requests | | | > (AMF) | | | > | | | > \- Mobility related measurements | | | > (AMF) | | | > | | | > \- Session management, number of | | | > PDU sessions/modifications/QoS | | | > flows (request, successful, | | | > failed) (SMF) | | | > | | | > \- Performance measurements N4 | | | > (SMF) | | | > | | | > \- N4/N6/N9 measurements traffic | | | > volume, link usage, packet loss | | | > (UPF) | | | > | | | > \- QoS flow related measurements | | | > (UPF) | | | | | | E2E KPIs (TS 28.554 [7]) | | | | | | > \- Registered subscribers (AMF) | | | > | | | > \- Upstream/downstream throughput | | | > for network and NSI | | | > | | | > \- QoS flow/DRB retainability | +------------------------------+--------------------------------------+ | **Configuration Data** | NRM attributes affecting the NF | | | resource allocation and | | | configuration | | | | | | NRM update reports (notification and | | | log) containing the creation or | | | changes of the MOIs affecting the | | | NFs | +------------------------------+--------------------------------------+ | **Network Topology** | Topology of the network | +------------------------------+--------------------------------------+
##### 6.4.3.3.3 Analytics report
The network congestion analytics report contains the following information.
+-------------------------+-------------------------------------------+ | **Information** | **Description** | +=========================+===========================================+ | **Incident Identifier** | Identifier that indicates the incident | | | e.g. alarm malfunction, faulty | | | performance measurement, etc. | +-------------------------+-------------------------------------------+ | **Location** | Affected geographical location related to | | | e.g. malfunction alarm or faulty | | | performance measurement, etc. | +-------------------------+-------------------------------------------+ | **Affected Objects** | NF, MnS, PDU session, QoS Flow, Slice, | | | etc. | +-------------------------+-------------------------------------------+ | **Start/Stop Time** | Starts/stop time of the malfunction issue | +-------------------------+-------------------------------------------+ | **Root Cause** | The source alarm or performance | | | measurements related to the identified | | | malfunction. | +-------------------------+-------------------------------------------+ | **Severity Level** | The severity level (e.g. critical, | | | medium, not important) of the malfunction | +-------------------------+-------------------------------------------+ | **Recommended Actions** | Recommendation actions to resolve | | | malfunction issues: | | | | | | > \- Isolate/terminate malfunction e.g. | | | > alarm or performance measurement, etc. | | | > | | | > \- Suppress alarms triggered by a | | | > faulty alarm. | | | > | | | > \- Recover erroneous data based on | | | > "neighbor" data. | +-------------------------+-------------------------------------------+
#### 6.4.3.4 Evaluation
The solution described in clause 6.4.3.3.1 requires the analytics inputs as
described in clause 6.4.3.3.2, wherein
\- the alarm data and S-NSSAI can be provided along with PMs and NRM data.
\- input PM for gNB and 5C are specified in TS 28.552 [8].
\- input E2E KPIs are specified in TS 28.554 [7].
\- input network topology, provision operations and configuration data can be
reflected by the NRMs defined in TS 28.541 [20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.4.3.3.3 can be derived.
Therefore, this solution is a feasible candidate for alarm malfunction
analytics.
## 6.5 Mobility management related issues
### 6.5.1 Handover optimization
#### 6.5.1.1 Use case
Current handover procedures are mainly based on radio conditions for selecting
the target gNB upon a handover. The target gNB accepts or rejects the handover
(HO) request depending on various conditions. In virtualized environment, the
HO may be rejected due to inadequate available resources within the target
gNB. The notion of resources may include virtual resources (e.g., compute,
memory) and/or radio resources (e.g., PRB, RRC connected users). If the HO
request is rejected, a UE will try to connect to a different gNB until the
request is successfully accepted. Several target gNBs can be tried until the
request is successfully accepted. This process can result in wastage of UE and
network resources, while it may also introduce service disruption due to
increased latency and radio link failures (RLFs). It also introduces
inefficiency in the HO or other network procedures.
To address this handover optimization issue, it is desirable to use MDAS
(Management data analytic service) to provision and/or select a particular
target gNB for handover in order to reduce or even avoid HO rejections. The
MDAS producer provides a HO optimization analytics report containing the
current and future/predicted resource consumption, resources capabilities and
other KPIs\' status for the available target gNB(s). The analytics report also
provides recommended actions to optimize the target gNB for handover. This may
include resource re-configuration or the updated selection criteria for target
gNB. Based on the report, the MDAS consumer adjusts (e.g., scale-out/up the
virtual resource, re-schedule/optimize radio resource) the resources before
continuing with the handover and/or adjusts the selection criteria of the
target gNB by also considering the overlapping coverages of inter-frequency
and inter-RAT deployments.
#### 6.5.1.2 Potential requirements
**REQ-HO_OPT_CON-1** The MDAS producer should have a capability to provide the
analytics report describing the resource consumption to authorized consumers
based on the current and future virtual resource consumption of gNB.
**REQ-HO_OPT_CON-2** The MDAS producer should have a capability to provide the
analytics report describing the resource consumption to authorized consumers
based on the current and future radio resource consumption of gNB.
**REQ-HO_OPT_CON-3** The analytics report describing the resource consumption
should contain the following information describing the current and future
resource consumption:
\- Assigned virtual, radio, and transport resources for target gNB.
\- Consumed virtual, radio, and transport resources for target gNB.
\- Projected virtual, radio and transport resource usage in near future for
target gNB.
\- Indication on whether the target gNB is optimal for handover.
\- Recommended action to optimize the target gNB and/or the selection of the
target gNB for handover.
**REQ-HO_OPT_CON-4** The MDAS producer should have a capability to provide an
analytics report indicating a selection priority for the target cell, among a
set of candidate inter-frequency cells.
**REQ-HO_OPT_CON-5** The MDAS producer should have a capability to provide an
analytics report indicating a list of target cells to spare, i.e. avoid, a
handover for an indicated time period.
**REQ-HO_OPT_CON-6** The analytics report describing inter-frequency target
cell selection for handover may provide information for provisioning or
selecting a target gNB with respect to a specific service or slice, if the
same Network Slice Instance (NSI) is available in both the current and target
gNB.
**REQ-HO_OPT_CON-7** The analytics report describing inter-frequency target
cell selection for handover should provide indication of current and expected
QoE (for the UE) at the current and target gNB.
#### 6.5.1.3 Possible solutions
##### 6.5.1.3.1 Solution description
The solution considers resource consumption both in terms of virtual and radio
resource for the target gNB. The current resource consumption is analysed with
the future/predicative resource consumption to decide if the target gNB is
optimal for handover or not.
_The MDAS producer can correlate and analyse_ the ongoing and/or potential
handover optimization issues based on the current and historical performance
data related to handover performance considering intra-gNB and inter-gNB
handover measurements as well as other performance measurements including
network load, E2E latency, retainability and radio conditions, UE measurements
including MDT, location and QoE for the network or network slices. The MDAS
producer can provide the analytics report as defined in clause 6.5.1.3.3
related with resource utilization analytics triggered by an event or
periodically.
##### 6.5.1.3.2 Data required
The following data is required to do the required analysis.
+----------------------------+----------------------------------------+ | Data category | Required data | +============================+========================================+ | Performance Measurements | Average/distribution of UE reported | | | RSRPs/RSRQs/SINRs of each neighbour | | | cell; | | | | | | Packet delay related to neighbour | | | cells as defined in clauses | | | 5.1.1.1/5.1.3.3, TS 28.552 [8]; | | | | | | IP Latency to neighbour cells as | | | defined in clause 5.1.3.4, TS 28.552 | | | [8]; | | | | | | Round-trip GTP Data Packet Delay to | | | neighbour cells as defined in clause | | | 5.4.1.9, TS 28.552 [8]; | | | | | | End-to-end Latency of 5G Network to | | | neighbour cells as defined in clause | | | 6.3.1, TS 28.554 [7]; | | | | | | CQI related measurements: The | | | distribution of Wideband CQI reported | | | by UEs, clause 5.1.1.11 of TS 28.552 | | | [8]; | | | | | | Intra-gNB handovers: Number of failed | | | handovers in terms of handover | | | preparation/resource | | | allocation/execution and the mean time | | | of handover execution, clause 5.1.1.6 | | | of TS 28.552 [8]; | | | | | | Inter-gNB handovers: Number of failed | | | handovers in terms of handover | | | preparation/resource preparation | | | clause 5.1.1.6.2, TS 28.552 [8]; | | | | | | Frequency Priority Information (i.e., | | | based on deployment) set by the MNOs: | | | Absolute priorities for different NR | | | frequencies or inter-RAT frequencies, | | | clause 5.2.4.1, TS 38.304 [21]; | | | | | | Throughput at N3 interface: | | | Upstream/Downstream GTP data | | | throughput at N3 interface, clause | | | 6.3.4/6.3.5 of TS 28.554 [7]; | | | | | | Data packet loss: Data volume of | | | outgoing GTP data packets per QoS | | | level on the N3 interface, from UPF to | | | (R)AN and via versa clause 5.4.1.6 TS | | | 28.552 [8] | +----------------------------+----------------------------------------+ | Allocated Virtual Resource | Allocated Compute: This describes the | | | number of vCPUs allocated to the | | | virtual machine on which the gNB VNF | | | is hosted. | | | | | | Allocated Memory: This describes the | | | number of vMemory allocated to the | | | virtual machine on which the gNB VNF | | | is hosted. | | | | | | Allocated Storage: This describes the | | | number of vStorage allocated to the | | | virtual machine on which the gNB VNF | | | is hosted. | +----------------------------+----------------------------------------+ | Consumed Virtual Resource | Consumed Compute: This describes the | | | number of total aggregated compute | | | resource consumption at a particular | | | point of time. | | | | | | Consumed Memory: This describes the | | | number of total aggregated memory | | | consumption at a particular point of | | | time. | | | | | | Consumed Storage: This describes the | | | number of total aggregated storage | | | consumption at a particular point of | | | time. | +----------------------------+----------------------------------------+ | Consumed Radio Resource | Radio resource utilization: The | | | physical radio resource utilization of | | | the target gNB, see clause 5.1.1.2 of | | | TS 28.552 [8]; | +----------------------------+----------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR (serving cell and neighbour | | | cells) and UE location information, TS | | | 37.320 [12]. | +----------------------------+----------------------------------------+ | UE location reports | UE location information provided by | | | the LCS with the anonymous ID, which | | | can be used to correlate with MDT | | | reports. | +----------------------------+----------------------------------------+ | QoE Data | Detailed measurements are FFS. | +----------------------------+----------------------------------------+ | S-NSSAI | S-NSSAI as defined in clause 5.15.2, | | | TS 23.501 [13]. MDAS uses this | | | information to identify target gNBs or | | | inter-RAT cells associated with a | | | network slice performing handover | | | optimization and may derive resource | | | utilization and network performance | | | analytics. | +----------------------------+----------------------------------------+ | Configuration Data | Resource configuration data including | | | RAN and virtualized NFs. | | | | | | The current policy configured in the | | | RAN related to the handover | | | optimization. | +----------------------------+----------------------------------------+
##### 6.5.1.3.3 Analytics report on gNB resource consumption
The resource analytics report contains the following information per gNB.
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | gNB resource | | | | consumption | | | +======================+======================+======================+ | | Allocated Virtual | Allocated Compute: | | | Resource | This describes the | | | | number of vCPUs | | | | allocated to the | | | | virtual machine on | | | | which the gNB VNF is | | | | hosted. | | | | | | | | Allocated Memory: | | | | This describes the | | | | number of virtual | | | | vMemory allocated to | | | | the virtual machine | | | | on which the gNB VNF | | | | is hosted. | | | | | | | | Allocated Storage: | | | | This describes the | | | | number of vStorage | | | | allocated to the | | | | virtual machine on | | | | which the gNB VNF is | | | | hosted. | +----------------------+----------------------+----------------------+ | | Consumed Virtual | Consumed Compute: | | | Resource | This describes the | | | | number of total | | | | aggregated compute | | | | resource consumption | | | | at a particular | | | | point of time. | | | | | | | | Consumed Memory: | | | | This describes the | | | | number of total | | | | aggregated memory | | | | consumption at a | | | | particular point of | | | | time. | | | | | | | | Consumed Storage: | | | | This describes the | | | | number of total | | | | aggregated storage | | | | consumption at a | | | | particular point of | | | | time. | +----------------------+----------------------+----------------------+ | | Projected Virtual | Projected Compute: | | | Resource consumption | This describes the | | | | number of total | | | | projected compute | | | | resource consumption | | | | at a particular | | | | point of time. | | | | | | | | Projected Memory: | | | | This describes the | | | | number of total | | | | projected memory | | | | consumption at a | | | | particular point of | | | | time. | | | | | | | | Projected Storage: | | | | This describes the | | | | number of total | | | | projected storage | | | | consumption at a | | | | particular point of | | | | time. | | | | | | | | Timestamp: Time for | | | | which the projection | | | | is made. | +----------------------+----------------------+----------------------+ | | Assigned radio | The physical radio | | | resources | resource assignment | | | | to the target gNB. | +----------------------+----------------------+----------------------+ | | Consumed radio | The physical radio | | | resource | resource utilization | | | | of the target gNB. | +----------------------+----------------------+----------------------+ | | Projected radio | The physical radio | | | resource | resource projected | | | | utilization of the | | | | target gNB. | +----------------------+----------------------+----------------------+ | | isOptimal | Indication on | | | | whether the target | | | | gNB is optimal for | | | | handover. This will | | | | include: | | | | | | | | isOptimal: | | | | TRUE/FALSE | | | | indication if it is | | | | optimal. | | | | | | | | Network slice | | | | Identifier: | | | | Indication of the | | | | target slice (or | | | | same slice type) at | | | | the target gNB. | +----------------------+----------------------+----------------------+ | | isFutureOptimal | Indication on | | | | whether the target | | | | gNB is optimal for | | | | handover at a future | | | | point of time | | | | (Timestamp). This | | | | will include: | | | | | | | | isFutureOptimal: | | | | TRUE/FALSE | | | | indication if it is | | | | optimal. | | | | | | | | TimeStamp: | | | | Indicating the | | | | timestamp at which | | | | the target gNB will | | | | be optimal | | | | | | | | Network slice | | | | Identifier: | | | | Indication of the | | | | target slice (or | | | | same slice type) at | | | | the target gNB. | +----------------------+----------------------+----------------------+ | | Priority | Priority of the | | | | target gNB for | | | | optimal HO, in case | | | | of multiple targets. | +----------------------+----------------------+----------------------+ | | Remedial Action | Recommendation for | | | | gNB modification in | | | | order to make it | | | | optimal for handover | | | | e.g., scale-out gNB, | | | | increase radio | | | | resource. | +----------------------+----------------------+----------------------+
#### 6.5.1.4 Evaluation
The solution described in clause 6.5.1.3.1 requires the analytics inputs as
described in clause 6.5.1.3.2 wherein
\- Input PMs on Packet delay, IP latency, Round-trip GTP data packet delay,
CQI measurements, Intra-gNB handover, Inter-gNB handover, data packet loss,
virtual resource usage and RAN utilization are specified in TS 28.552[8].
\- Input KPIs on E2E latency of 5G and throughput at N3 are specified in TS
28.554 [7].
\- Input MDT data is specified in TS 37.320 [12].
\- Frequency Priority Information is specified in TS 38.304 [21].
\- Input QoE can be available from service experience analytics from NWDAF TS
23.288 [18].
\- UE location reports can be accessed by consuming the LCS via service-based
interfaces as defined in TS 23.273 [27].
\- Input Configuration data and S-NSSAI are available and have been defined in
TS 28.541[20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.5.1.3.3 can be derived.
Therefore, this solution is a feasible candidate for handover optimization.
### 6.5.2 Inter-gNB Beam Selection Optimization
#### 6.5.2.1 Use case
The handover procedure specified in 5G is triggered based on UE measurements,
specifically considering the cell level radio quality of the source and target
cell(s). In case of beamformed access, one cell can make use of several beams
for serving residing users (SSB or CSI-RS) with each user served by a single
beam at a time. The cell level quality can be represented as an aggregated
metric over one or more beams. So, although handover is performed between two
5G cells, the granularity of handover can be further broken down to beam
level.
The target cell provides RACH resources to the UE, which are linked with
specific beams. Currently, the target cell can do this, based on beam level
measurements performed and reported by the UE. Picking the wrong beam for
performing RACH on the target cell could easily result in RLF for the UE and
should be avoided.
To address this beam level handover optimization issue, it is desirable to use
MDAS to prioritize and/or select a beam in case of handover for a specific
target cell, in order to minimize or even avoid RLF.
The MDAS producer provides a beam level HO optimization analytics report
considering information about the handover performance of different beam
combinations between specific source and target cell pairs. In particular, the
MDAS producer considers the beam of the current cell and the selected beam
(out of several available beams) of the target cell to keep statistics
regarding the handover performance in order to avoid RLF. Beams of the target
cell with a successful handover are preferred in the selection.
In other words, the analytics report also provides recommended actions to
optimize the beam selection at the target gNB as a function of serving beam on
the source cell side. Based on the recommended actions, the MDAS consumer
adjusts the priorities for the beam selection at HO, i.e. the beam
combinations that are likely to succeed are prioritized, less optimal beam
combinations are down prioritized. The analytics report may also be used to
aid the target cell to allocate RACH resources in a way that ensures HO
success.
#### 6.5.2.2 Potential requirements
**REQ-HO_BEAM_OPT_CON-1** The MDAS producer should have a capability to
provide the analytics report describing the handover performance of beam
combinations between cell pairs to authorized consumers based on previously
recorded HOs.
**REQ-HO_BEAM_OPT_CON-2** The analytics report describing the performance of
beam combinations between cell pairs should contain the following information:
\- Number of successful HOs between a given beam pair
\- Number of failed HOs between a given beam pair
\- Indication if a beam pair is to be prioritized or down prioritized
#### 6.5.2.3 Possible solutions
##### 6.5.2.3.1 Solution description
_The MDAS producer can analyse_ the ongoing and/or potential beam handover
optimization based on the current and historical beam selection
success/failure rate for an inter-gNB handover. The MDAS producer can provide
the analytics report as defined in clause 6.5.2.3.3 triggered by an event or
periodically.
##### 6.5.2.3.2 Data required
The following data is required to do the required analysis.
+--------------------------+------------------------------------------+ | Data category | Required data | +==========================+==========================================+ | Performance Measurements | Inter-gNB handover: Number of | | | success/failed handovers in terms of | | | handover preparation/resource | | | preparation; | | | | | | Beam level measurements: CSI-RS, SSB | | | beam related measurements clause | | | 5.1.1.28, TS 28.552 [8]; | | | | | | Success/failure handover rate per beam | | | pair, i.e. serving beam Id in source gNB | | | and selected beam Id in target gNB. | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to radio | | | conditions and UE location information. | +--------------------------+------------------------------------------+ | Beam selection policy | The current policy configured in the RAN | | | related to beam selection for inter-gNB | | | handover optimization. | +--------------------------+------------------------------------------+
##### 6.5.2.3.3 Analytics report
The gNB resource analytics report contains the following information.
Analytics Report of gNB resource consumption Attribute Name Description
* * *
                                                 Beam level Inter-gNB Handover performance   Handover success rate perÂ beam ID pair. This can be quantified as high, medium or low success rate.Â 
                                                 Time period                                 Time period, in the future, for the handover success rate perÂ beam ID pair
                                                 List of gNBs                                Objects involved: gNB(s) and cells of gNBs,
#### 6.5.2.4 Evaluation
The solution described in clause 6.5.2.3.1 requires the analytics inputs as
described in clause 6.5.2.3.2, wherein
\- Input PMs on Inter-gNB handover and beam level measurements are specified
in TS 28.552 [8].
\- Input MDT data is specified in TS 37.320 [12].
\- Input PM that need to be specified:
> a. success/failure handover rate per beam pair, i.e. serving beam Id in
> source gNB and selected beam Id in target gNB;
>
> b. current configuration related to beam selection for inter-gNB handover
> optimization.
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.5.2.3.3 can be derived.
Therefore, this solution is a feasible candidate for Inter-gNB Beam Selection
Optimization provided that Input PM related to success/failure handover rate
per beam pair and current configuration related to beam selection are
specified.
### 6.5.3 Load Balancing optimization
#### 6.5.3.1 Use case
The rapid traffic growth and multiple frequency bands utilized in a commercial
network make it challenging to steer the traffic in a balanced distribution.
To address the problem, load balancing had been proposed. The objective of
load sharing and load balancing is to distribute cell load evenly among cells
or to transfer part of the traffic from congested cell, or to offload users
from one cell or carrier or RAT to improve the network resource utilization
efficiency and achieve network energy saving. This can be done by means of
optimization of cell reselection/handover parameters and handover actions.
To ensure the service performance and user experience, the load balancing
action based on handovers highly depends on the measurement report (MR) from
the UE. For example, the inter-frequency scenarios with the deployment of
multiple different frequency bands, the MR configuration and UE MR reports may
cause amount of signalling overhead over Uu interface. The frequent inter-
frequency measurement will cause huge UE power consumption and severely impact
on running service by the data interruption for inter-frequency measurement
gap, e.g. the gap time in LTE is number of frequency*60ms per 480ms period and
the gap time in NR also depends on SMTC period. The gap assistant inter-
frequency measurements mechanism will bring delay of the measurement and
decrease the data transmission rate. Solutions are desired to improve the
effectiveness of the MR configuration and report, which may help to greatly
reduce the MR signalling overhead, UE power consumption and data interruption
of running service, and improve the convergence speed of the load balancing.
The MDA can help to predict the measurement results of cell on neighboring
frequencies for each UE without the GAP assisted measurement. Via analysing
the historical intra-frequency and inter-frequency measurement from both the
serving cell and the neighbour cell, the MDA can construct the network \"radio
finger print\", which characterize the network intra-frequency and inter-
frequency coverage quality. The \"radio finger print\" information is composed
of multiple virtual grid. The grid index is to identify a specific virtual
grid and this index consists of cell ID and corresponding coverage quality,
e.g., RSRP, of at least three intra-frequency cells. The attributes of the
grid are used to describe the wireless characteristics of the grid, such as
coverage of inter-frequency neighbor cells, including RSRP, reference signal
receiving quality (RSRQ), received signal strength indication (RSSI), channel
quality indicator (CQI), modulation and coding scheme (MCS), beam ID, etc.
The MDAS producer provides the analytics report on \"radio finger print\"
information to the gNB, gNB can directly predict the measurement values of
cells on neighboring frequencies for each UE based on the well-constructed
\"radio finger print\" and the real-time intra-frequency measurement. In this
case, the GAP assisted inter-frequency measurement is avoided, and the gNB can
make proper load balancing actions based on the predictions, which helps to
reduce the data interruption of running services and improve the load
balancing speed. Moreover, the MDA producer can provide authorised consumers
with radio configuration options to perform MLB based handover according to
the Service Experience Type, e.g. on voice, video, other, associated with the
offloaded UE from one cell to another to ensure the desired service
experience.
The MDAS producer may also provide the traffic load prediction report to the
authorized consumers, e.g., gNB, to enable the proactive load balancing
actions. This would help to prevent the user experience degradation in advance
compared to the reactive optimizations based on the delayed load information
measurement and exchange.
The Centralized SON can have a bigger picture of the Network Congestion, by
looking at a Cluster of cells. This can complement the load balancing at
Distributed SON and make it possible to completely mitigate the risks of
service degradations due to network congestions. Moreover, predictive
analytics can make it possible to take anticipatory steps towards mitigating
network congestions. The MDAS producer can predict the cells that will be
congested in a near future, in a cluster of NR Cells, by performing data
analytics on Cell Level Performance Measurements: DL PRB
Utilization, Latency related Performance Measurements, Average RRC Connected
Users, DL Packet Drop Rate, DL PDCP SDU Drop Rate, Average Delay in DL in CU-
UP, Average Delay on F1-U, IP Latency. Network Congestions can be predicted at
Cell level as well as group of neighbouring cells. Based on this, a list of
congested cells can be provided to the MDAS Consumer. Also, a list of cells
for which it is predicted that the congestion will ease out, can be provided
to the MDAS Consumer.
#### 6.5.3.2 Potential requirements
**REQ-MLB_OPT_CON-1** The MDAS producer should have a capability to provide
the analytics report describing the radio measurement information to
authorized consumers, e.g., gNB.
**REQ-MLB_OPT_CON-2** The analytics report describing the radio measurement
information should contain the following information:
\- the applied cell ID;
\- the time period(s) of the original data used for deriving the analytics
report;
\- the serving cell and its inter-frequency/intra-frequency neighboring
cell\'s cell ID and corresponding radio measurement information, e.g., CSI-
RSRP, SS-RSRP, etc;
\- Indication on whether the gNB is suitable to be selected as the target gNB
for the MLB based handover based on the radio signal qualities.
**REQ-MLB_OPT_CON-3** The analytics report describing the predicted resource
utilization status of gNB should contain the following information:
\- predicted virtual, radio, and transport resources utilizations for
potential MLB source and target gNBs in the near future;
\- Indication on whether the gNB is needed to activate the MLB operation;
\- Indication on whether the gNB is suitable to be selected as the target gNB
for the MLB based handover.
**REQ-MLB_OPT_CON-4** The MDAS producer should have the capability to provide
authorized consumers, e.g. gNB, with the analytics report describing the
service specific radio configuration options needed to perform MLB based
handover.
The solution considers resource consumption both in terms of virtual and radio
resource for the target gNB. The current resource consumption is analysed with
the future/predicative resource consumption to decide if the target gNB is
optimal for handover or not.
#### 6.5.3.3 Possible solutions
##### 6.5.3.3.1 Solution description
*The solution considers resource consumption both in terms of virtual and radio resource for the target gNB. The current resource consumption is analysed with the future/predicative resource consumption to decide if the target gNB is optimal for handover or not. *
_For this solution, MDAS Consumer shall be C-SON. The MDAS producer can
collect Performance Measurement data as described in Clause 6.5.3.3.2.
Performance Measurement data shall be captured for a cluster of NR cells in
Operator's network. The MDAS Consumer can provide a mechanism for selecting a
cluster of cells, depending upon geographic region, or, based on the network
density. The periodicity of the Performance Measurement collections and the
storage duration can be configurable. The prediction is required for a time
period in advance, defined as Prediction Horizon, which can be a configurable
parameter. The MDAS Producer can collect historic Performance Measurements in
order to arrive at the congestion predictions._
Prediction shall be made well in advance, in order to compensate for possible
latencies in collecting Performance Measurements as well as latencies in
applying the mitigation actions to the network. The MDAS consumer can prevent
Network congestion by applying cell reselection/handover parameter changes in
advance, based on the Prediction Report from the MDAS producer.
##### 6.5.3.3.2 Data required
The following data is required to do the required analysis.
+----------------------------------+----------------------------------+ | **Data category** | **Required data** | +==================================+==================================+ | Virtual Resource Measurements | For every Cell in the selected | | | Cluster, Common performance | | | measurements for NFs including: | | | Virtualised resource usage, | | | Virtual memory usage, Virtual | | | disk usage, clause 5.7, TS | | | 28.552 [8] | +----------------------------------+----------------------------------+ | Resource Utilization | Radio resource utilization: The | | Measurements | physical radio resource | | | utilization of the source and | | | target gNB, see clause 5.1.1.2 | | | of TS 28.552 [8]; | | | | | | For every Cell in the selected | | | Cluster, DL Total PRB Usage, | | | clause 5.1.1.2.1, TS 28.552 | | | [8] | +----------------------------------+----------------------------------+ | Average RRC Connection | For every Cell in the selected | | Measurements | Cluster, **Mean number of RRC | | | Connections, clause 5.1.1.4.1,** | | | TS 28.552 [8] | +----------------------------------+----------------------------------+ | Packet Drop Measurements | For every Cell in the selected | | | Cluster, DL Packet Drop Rate in | | | gNB-DU, clause 5.1.3.2.2, TS | | | 28.552 [8] | | | | | | For every Cell in the selected | | | Cluster, DL PDCP SDU Drop rate | | | in gNB-CU-UP, clause 5.1.3.2.1, | | | TS 28.552 [8] | +----------------------------------+----------------------------------+ | Delay Measurements | For every Cell in the selected | | | Cluster, Average delay DL in | | | CU-UP, clause 5.1.3.3.1, TS | | | 28.552 [8] | | | | | | For every Cell in the selected | | | Cluster, Average delay on F1-U, | | | clause 5.1.3.3.2, TS 28.552 | | | [8] | +----------------------------------+----------------------------------+ | MDT data | UE measurements related to RSRP, | | | RSRQ, SINR as specified in TS | | | 37.320 [12] | +----------------------------------+----------------------------------+ | Service experience data | Service experience statistics | | analytics | and service in use predictions | | | provided by NWDAF per UE | | | (source/target cell) | +----------------------------------+----------------------------------+ | NR Cell ID | Unique NR Cell ID For every Cell | | | in the selected Cluster | +----------------------------------+----------------------------------+ | Timestamp | Timestamp for each Performance | | | Measurement reported | +----------------------------------+----------------------------------+
##### 6.5.3.3.3 Analytics report
The gNB resource analytics report contains the following information for the
source and target gNBs.
+----------------------+----------------------+----------------------+ | **Analytics Report |** Attribute Name**|** Description**| | of gNB resource | | | | consumption** | | | +======================+======================+======================+ | | Predicted Virtual | Predicted Compute: | | | Resource consumption | This describes the | | | | average predicted | | | | compute resource | | | | consumption. | | | | | | | | Predicted Memory: | | | | This describes the | | | | average predicted | | | | memory consumption. | | | | | | | | Predicted Storage: | | | | This describes the | | | | average predicted | | | | storage consumption. | | | | | | | | Timestamp: Time for | | | | which the prediction | | | | is made. | +----------------------+----------------------+----------------------+ | | Predicted radio | The physical radio | | | resource | resource predicted | | | | utilization of the | | | | target gNB. | +----------------------+----------------------+----------------------+ | | Prediction Horizon | Prediction is | | | | performed for a time | | | | window in advance. | +----------------------+----------------------+----------------------+ | | isMLBNeeded | Indication on | | | | whether the target | | | | gNB needs to | | | | activate the MLB | | | | operation. | +----------------------+----------------------+----------------------+ | | List of Predicted | List of cells or | | | Congested Cells | list of cells in the | | | | selected cluster, | | | | for which it is | | | | predicted that those | | | | will be congested at | | | | the Prediction | | | | Horizon. | +----------------------+----------------------+----------------------+ | | List of possible | Indication on | | | target cells | whether the target | | | | gNB is suitable to | | | | be selected as the | | | | target gNB for the | | | | MLB based handover. | | | | | | | | List of cells in the | | | | selected cluster, | | | | that are suitable | | | | for MLB based | | | | handover. | +----------------------+----------------------+----------------------+ | | List of Predicted | List of cells or | | | Congestion Easing | list of cells in the | | | out Cells | selected cluster, | | | | for which it is | | | | predicted that the | | | | congestion shall | | | | ease out at the | | | | Prediction Horizon. | +----------------------+----------------------+----------------------+ | | Prediction | The Prediction | | | Confidence | Confidence shall | | | | define the measured | | | | prediction accuracy. | +----------------------+----------------------+----------------------+ | | Service list | List of supported | | | | service classes or | | | | corresponding slice | | | | IDs at the List of | | | | possible target | | | | cells. | +----------------------+----------------------+----------------------+
#### 6.5.3.4 Evaluation
The solution described in clause 6.5.3.3.1 requires the analytics inputs as
described in clause 6.5.3.3.2, wherein:
\- Input PM data: Virtual Resource measurements, RAN utilization, Average RRC
Connection, Packet Drop Measurements and Delay Measurements are specified in
TS 28.552 [8].
\- MDT data is specified in TS 37.320 [12].
\- Service experience analytics is specified in TS 23.288 [18].
\- NR Cell ID and timestamp are available.
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.5.3.3.3 can be derived.
Therefore, this solution is a feasible candidate for load balancing
optimization.
### 6.5.4 Mobility performance analysis
#### 6.5.4.1 Use case
The mobility performance related problems may be resulted from different
causes, e.g., too long mobility interruption time for latency sensitive
services, low handover successful rate due to poor coverage of the cell-edge,
handover failure due to lack of handover resources, too-early/too-late/ping-
pong handovers due to inappropriate handover parameters.
In addition, there are different handover mechanisms, e.g. Dual Active
Protocol Stack (DAPS) to reduce service interruption time during handover,
Conditional Handover (CHO) to improve handover robustness, RACH-less handover
to reduce handover latency etc. SON MRO solutions can handle multiple handover
robustness issues such as too early handover, too late handover, handover to a
wrong cell etc. Furthermore, handover mechanisms are also related with NSA and
SA deployment architecture. In different scenarios, handover solutions will
have different impacts on the mobility performance. The analytics report to
identify the most optimal handover mechanism may be provided by MDAS producer.
For example, to satisfy the requirements of 0ms mobility interruption time for
some URLLC services, MDAS may propose to prioritize the usage of DAPS. If
handover successful rate is low due to coverage issue in the cell edge, MDAS
may recommend CHO instead of gNB triggered handover mechanisms. To provide
optimal handover mechanisms and the corresponding handover related parameters,
MDAS may consider multiple factors, e.g. radio conditions, cell load, service
requirements, handover successful rate, history performance data around
handover, UE RF finger prints etc. MDAS may compare performance of different
handover mechanisms and propose optimal handover mechanisms, e.g. the
prioritization of different handover mechanisms in different conditions.
MDAS can be used to analyse service experience and network performance during
handover period in different mobility scenarios. It may also be able to
provide the recommendations of optimal handover parameters, resource
configurations and mobility related policies. Mobility performance analysis
should cover the following aspects:
\- Mobility scenarios in NSA and SA deployment architectures;
\- Optimal handover mechanisms, e.g. DAPS, CHO, RACH-less handover etc;
\- Optimal handover parameter and resource configurations;
\- Coordination with SON MRO mechanisms to improve handover robustness;
\- Mechanisms for fast handover failure recovery.
The MDAS producer is able to, from the perspective of the management aspects,
provide the mobility performance analytics report. This analytics report can
be considered as an input to support SLS assurance to perform further
evaluation.
#### 6.5.4.2 Potential requirements
**REQ-MOB_PMF_CON-1** The MDAS producer should have a capability to provide
the analytics report of mobility performance.
**REQ-MOB_PMF_CON-2** The analytics report describing the mobility performance
should contain the following information describing the mobility related
performance aspects:
\- Optimal handover mechanism and the corresponding parameters for DAPS, CHO,
RACH-less handover and NR SON MRO scenarios etc.
#### 6.5.4.3 Possible solutions
##### 6.5.4.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to provide optimal handover mechanisms and the
corresponding configurations regarding parameters and resources. As the table
in 6.5.4.3.3 shows, the analytics report is able to be provided by the MDAS
producer to describe the optimal handover mechanisms. This procedure may be
triggered by the request or periodically.
##### 6.5.4.3.2 Data required for mobility performance analysis
The following table shows the potential data required to analyse the mobility
performance.
+--------------------------+------------------------------------------+ | Data Category | Required Data | +==========================+==========================================+ | Performance Measurements | Inter-gNB handovers: see clause | | | 5.1.1.6.1 of TS 28.552 [8]; | | | | | | Intra-gNB handovers: see clause | | | 5.1.1.6.2 of TS 28.552 [8]; | | | | | | Handovers between 5GS and EPS: see | | | clause 5.1.1.6.3 of TS 28.552 [8]; | | | | | | RRC Connection Re-establishment: see | | | clause 5.1.1.17 of TS 28.552 [8]; | | | | | | Inter-AMF handovers: see clause 5.2.5.1 | | | of TS 28.552 [8]; | | | | | | Handovers from 5GS to EPS: see clause | | | 5.2.5.3 of TS 28.552 [8]; | | | | | | Handovers from EPS to 5GS: see clause | | | 5.2.5.4 of TS 28.552 [8]; | | | | | | Number of handover events, Number of HO | | | failures, Number of too early HO | | | failures, Number of too late HO | | | failures, Number of HO failures to wrong | | | cell, Number of unnecessary HOs to | | | another RAT: see clause 4.3.5 of TS | | | 28.628 [34]; | | | | | | Radio resource utilization: The usage of | | | physical radio resource utilization of | | | the network, see clause 5.1.1.2 of TS | | | 28.552 [8]; | | | | | | RAN UE throughput: A KPI that shows how | | | NG-RAN impacts the service quality | | | provided to an end-user, see clause | | | 6.3.6 of TS 28.554 [7]; | | | | | | Throughput at N3 interface: | | | Upstream/Downstream GTP data throughput | | | at N3 interface, see clause 6.3.4 and | | | clause 6.3.5 of TS 28.554 [7]; | | | | | | NWDAF analytics data: UE mobility | | | analytics, UE Communication Analytics; | | | see clauses 6.7.2 and 6.7.3 of TS 23.288 | | | [18]; | | | | | | Measurements related to DAPS and CHO. | +--------------------------+------------------------------------------+ | Identifier | sNSSAIList that are analyzed. | +--------------------------+------------------------------------------+ | MDT Data | UE measurements related to RSRP, RSRQ, | | | SINR and UE location information. | +--------------------------+------------------------------------------+ | QoE Data | The QoE data as defined in TS 26.247 | | | [29] and TS 26.114 [30]. | +--------------------------+------------------------------------------+ | Configuration Data | The execution data including the changes | | | or the configuration of the MOIs related | | | with mobility. | +--------------------------+------------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.5.4.3.3 Analytics report for mobility performance analysis
The following table shows the potential information carried in the analytics
report of mobility performance analysis.
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | mobility performance | | | +======================+======================+======================+ | | Mobility performance | The identifier of | | | issue identifier | the mobility | | | | performance issue | | | | analysis; | +----------------------+----------------------+----------------------+ | | Root cause of | The root cause of | | | mobility performance | mobility performance | | | issue | issues, e.g., too | | | | long mobility | | | | interruption time | | | | for latency | | | | sensitive services, | | | | low handover | | | | successful rate due | | | | to poor coverage of | | | | the cell-edge, | | | | too-earl | | | | y/too-late/ping-pong | | | | handovers due to | | | | in | | | | appropriate handover | | | | parameters | +----------------------+----------------------+----------------------+ | | Recommended handover | Recommended handover | | | mechanisms | mechanisms according | | | | to network | | | | conditions, e.g., | | | | DAPS, CHO, RACH-less | | | | handover; | | | | | | | | See note. | +----------------------+----------------------+----------------------+ | | Recommended handover | Corresponding | | | related parameters | configurations of | | | | handover related | | | | parameters, e.g., | | | | the range of | | | | handover offset. | +----------------------+----------------------+----------------------+ | | Time duration | The time duration | | | | the identified | | | | handover mechanism | | | | or handover related | | | | parameters are | | | | recommended to | | | | apply. | +----------------------+----------------------+----------------------+ | | Location | The geographical | | | | area or the cells | | | | where the identified | | | | handover mechanism | | | | and handover related | | | | parameters are | | | | applied. | +----------------------+----------------------+----------------------+ | Note: The DAPS and | | | | CHO mechanism are | | | | mutually exclusive. | | | +----------------------+----------------------+----------------------+
#### 6.5.4.4 Evaluation
The solution described in clause 6.5.4.3 requires the analytics inputs as
described in in clause 6.5.4.3.2, wherein
\- the performance data (measurements and KPIs) are either already available
in TS 28.552 [8] and TS 28.554 [7] or can be defined in TS 28.552 [8].
\- the MDT data are available in TS 32.422 [25].
\- QoE data as defined in TS 26.247 [29] and TS 26.114 [30] can be acquired
through the procedures defined in TS 28.405 [31].
\- Configuration Data are defined in TS 28.541 [20].
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.5.4.3.3 can be
derived.
Therefore, this solution is a feasible candidate for mobility performance
analysis.
### 6.5.5 Handover optimization based on UE trajectory
#### 6.5.5.1 Use case
Handover optimization can benefit from knowledge about the trajectory on which
the user may be moving. A trajectory here is a sequence of location
coordinates, i.e. a vector that captures the sequences of coordinates within a
certain time interval, derived from historical UE location data, that
identifies the different directions which users may take starting at a given
point. At a city junction, for example, the probability of a handover success
for a user moving straight through the junction may be different from that for
a user who is turning left or turning right. So, at that junction, the
straight, left and right direction indicate three possible trajectories. The
MDAS producer should be able to analyse historical handover performance data
in combination with historical UE location and the radio characteristics like
RSRP and SINR considering the possible user trajectories to identify the
optimal handover configurations and target cell prioritization.
#### 6.5.5.2 Potential requirements
REQ-HO_OPT_TR_CON-1 The MDAS producer should have a capability to provide an
analytics report indicating the possible candidate user trajectories across
the cell boundaries and provide radio configuration and target gNB selection
specific to each trajectory.
#### 6.5.5.3 Possible solutions
##### 6.5.5.3.1 Solution description
The solution considers UE trajectories and radio conditions for selecting the
target gNB. The current resource consumption is analysed with the
future/predicative resource consumption to decide if the target gNB is optimal
for handover or not.
##### 6.5.5.3.2 Data required
The following data is required to do the required analysis.
Data category Required data
* * *
Consumed Radio Resource Radio resource utilization: The physical radio
resource utilization of the target gNB, see clause 5.1.1.2 of TS 28.552 [8];
MDT data UE measurements related to RSRP, RSRQ, SINR time-stamped and
annotated with UE location information.
##### 6.5.5.3.3 Analytics report on user trajectory-based handover
optimization
The gNB trajectory-based handover optimization analytics report contains the
following information.
Analytics Report of user trajectory-based handover Attribute Name Description
* * *
                                                       User trajectory                            The predicted UE location that identifies a trajectory across a cell boundary.
                                                       Radio characteristics on user trajectory   Radio parameters e.g. RSRP, SINR, on the predicted user trajectory across a cell boundary.
                                                       Recommended actions                        Recommendation for optimal gNB configuration and/or target gNB selection/prioritization based on the user trajectory information. [ ]{.underline}
#### 6.5.5.4 Evaluation
The solution described in clause 6.5.5.3.1 requires the analytics inputs as
described in clause 6.5.5.3.2, wherein
\- Input PM on RAN utilization is specified in TS 28.552 [8].
\- Input MDT data is specified in TS 37.320 [12].
\- To specify MDAS report.
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.5.5.3.3 can be derived.
Therefore, this solution is a feasible candidate for Handover optimization
based on UE trajectory.
### 6.5.6 Handover optimization based on UE Load
#### 6.5.6.1 Use case
The target node, eNB, may not have adequate resources to accept certain
handover requests. In the context of network virtualization, these resources
may include not only legacy radio resources, but also virtual resources such
as processor and memory. Handover optimization can benefit from knowledge
about the projected UE load on the target cell including additional radio and
virtual resources.
#### 6.5.6.2 Potential requirements
**REQ-HO_OPT_TR_CON-1** The MDAS producer should have a capability to provide
an analytics report indicating the projected UE load with respect to virtual
resource and radio resource on the target cell.
#### 6.5.6.3 Possible solutions
##### 6.5.6.3.1 Solution description
The MDAS producer will periodically collect UE Load statistics from gNBs. Data
collected over a period of time is analysed to predict aggregated UE resource
requirements on the target gNB. MDAS can utilize historical data and AI/ML
(for example, time series based) algorithm to predict UE requirements of the
gNB. The target cell, on receiving, a handover request will request UE load
analytics report (ULAR) for the migrating UE. The ULAR will provide
predictions for the additional radio and virtual resource requirements for the
UE on the target cell in near future. The target cell will use the ULAR to
decide whether to accept or reject the UE handover.
##### 6.5.6.3.2 Data required
The following data is required to do the required analysis.
**Data category** **Required data**
* * *
UE Load Statistics UE Identifier: Anonymous id e.g. C-RNTI Cell ID: Cell ID
represents the cell id of the cells to which the UE (identified by anonymous
id e.g. C-RNTI) has been previously connected. DL load: DL loads are the loads
that the UE imposed on previously connected cells. These loads are represented
by the number of assigned DL PRBs, DL traffic volume and the amount of virtual
resource consumed within the collection period. UL load: UL loads are the
loads that the UE imposed on previously connected cells. These loads are
represented by the number of assigned UL PRBs, UL traffic volume and the
amount of virtual resource consumed within the collection period. Load on
virtual processors: Virtual processor usage for the UE Load on memory: Virtual
memory usage for the UE Load on storage: Virtual storage usage for the UE
Record time: Recording time is the time instance when the statistics were
gathered Collection Period: elapsed time for which the statistics are
collected, e.g. last 10000 seconds.
Editor's Note: The availability mechanisms of the Input data defined above is
FFS.
##### 6.5.6.3.3 Analytics report on UE Load
The UE load analytics report contains the following information.
+----------------------+----------------------+----------------------+ | UE load analytics | **Attribute Name** | **Description** | | report | | | +======================+======================+======================+ | | Projected UE virtual | Projected | | | resource | requirement for | | | requirements | virtual processor | | | | usage for the UE. | | | | | | | | Projected | | | | requirement for | | | | virtual memory usage | | | | for the UE. | | | | | | | | Projected | | | | requirement for | | | | virtual storage | | | | usage for the UE. | +----------------------+----------------------+----------------------+ | | Projected UE radio | Projected number of | | | resource | required DL and UL | | | requirements | PRBs, DL and UL | | | | traffic volumes | | | | within the | | | | collection period. | +----------------------+----------------------+----------------------+ | | Projection Time | The future timestamp | | | | to which the | | | | projections applies. | +----------------------+----------------------+----------------------+
#### 6.5.6.4 Evaluation
The solution described in clause 6.5.6.3 requires the analytics inputs as
described in in clause 6.5.6.3.2, wherein
\- The UE Identifier and the Cell ID can be accessed from MDT reports.
\- The required UE radio and virtual resource load can be/should be defined as
part of MDT data.
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.5.6.3.3 can be
derived.
Therefore, this solution is a feasible candidate for HO optimization based on
UE Load.
## 6.6 Energy efficiency related issues
6.6.1 MDA assisted energy saving
6.6.1.1 Use case
Energy saving is a critical issue for the 5G operators. Energy saving is
achieved by activating the energy saving mode of the NR capacity booster cell
or 5GC NF (e.g. UPF etc.), and the energy saving activation decision making
may be based on the various information such as load information of the
related cells/UPFs, the energy saving policies set by operators as specified
in TS 28.310 [14] and the energy saving instructions provided by MDAS producer
as described in TR 28.813 [23].
As the conclusion from clause 7.2 of the TR 21.866 [15], \"The EE Control and
Coordination Function: a self-managed automated process to control and
coordinate system wide power saving operations including the access networks,
core network, backhaul/fronthaul transmission networks, backbone networks and
other subsystems\", the management system has the overall view of network load
information and it could also take the inputs from the control plane analysis
(e.g. the analytics provided by NWDAF). The management system may provide the
network wide analytics and cooperate with Core and RAN domains and decide on
which cell/UPF should move into energy saving mode in a coordinated manner.
There are various performance measurements could be used as inputs by MDA for
energy saving analysis, for example, the EE related performance measurements,
(e.g. PDCP data volume of cells, PNF temperature, and PNF power consumption
etc.) for the gNBs, and the data volume, number of PDU sessions with SSC mode
1 (see TS 23.501 [13]), delay related measurements, and VR usage for UPFs, and
the traffic load variation related performance measurements, (e.g. the PRB
utilization rate, RRC connection number).
The composition of the traffic load could be also considered as inputs for
energy saving analysis. (e.g., the percentage of high-value traffic in the
traffic load). The variation of traffic load may be related to the network
data (e.g., historical handover information of the UEs or network congestion
status, packet delay). Collecting and analysing the network data with machine
learning tools may provide predictions related to the trends of traffic load.
The composition and the trend of the traffic load may be used as references
for making decision on energy saving.
There are many prediction data models which may use machine learning tools for
predicting the energy saving related information, such as traffic load. MDAS
may also take these prediction data models as input, make analysis and select
the optimal prediction data models to provide more accurate prediction results
as references for making energy saving decision. The more accurate the
prediction results are, the better the energy-saving decision based on the
prediction results will be. The prediction data models are related to services
(e.g., traffic load, resource utilization, service experience), which can be
provided by consumer.
MDAS may also obtain NF location or other inventory information such as energy
efficiency and the energy cost of the data centers, while analysing historical
network information. Based on the collected information, MDAS producer makes
analysis and gives suggestions to network management in optimization
suggestion for 5G Core NF deployment options in high-value traffic region
(e.g. location of VNF in context of energy saving). The information from
control plane data analysis from NWDAF, such as UE Communication analytics
(see TS 23.288 [18]), may also be used as input for energy saving analysis and
instruction.
The decision of core NF and RAN node energy saving should be coordinated by
management system to guarantee the overall network and service performance are
not affected as much as possible.
To achieve an optimized balance between the energy consumed and the
performance provided by the network, MDAS can be used to provide an analytics
report by analysing the above information comprehensively to assist the energy
saving.
6.6.1.2 Potential requirements
**REQ-MDA_ES-CON-1** The MDAS producer should have a capability to provide the
analytics report describing the energy saving instruction.
**REQ-MDA_ES-CON-2** The analytics report describing the energy saving
instruction should contain the following information:
> \- The identifier of the energy saving instruction described in the
> analytics report;
>
> \- Location of geographical area where the unreasonable energy consumption
> exists or optimization recommended;
>
> \- Root cause of the energy consumption issue;
>
> \- Recommended optimal prediction data models to assist the energy saving
> related decision making;
>
> \- Recommended NR Cells (ES-Cell) to enter energySaving state;
>
> \- Recommended candidate cells with precedence for taking over the traffic
> of each ES-Cell.
>
> \- Recommended UPFs (ES-UPF) to enter energySaving state;
>
> \- Recommended candidate UPFs with precedence for taking over the traffic of
> each ES-UPF.
>
> \- Predictions related to the trends of traffic load which could be used as
> references for making decision on energy saving.
>
> \- Recommended 5GC NF deployment options in high-value traffic region (e.g.
> location of VNF in context of energy saving).
**REQ-MDA_ES-CON-3** The MDAS producer should have a capability to update the
recommended decisions and fine-tune the accuracy of the analytics result after
being informed of the service experience changes and traffic load changes
since the recommended decisions have been are taken to make energy saving
decisions.
6.6.1.3 Possible solutions
6.6.1.3.1 Solution description
The MDAS producer correlates and analyses the management data described in the
following subclause to assist the management energy saving function to make
energy efficiency decisions. As the table in clause 6.6.1.3.3 shows, the
analytics report is able to be provided by the MDAS producer to describe the
analytics result and recommendations of energy saving. This procedure may be
triggered by the request or periodically.
Energy saving activation decision making may be based on the various
information such as load information. The prediction result of these
information can be used by operators to make energy-saving policies. There are
many prediction data models which may use ML algorithms for predicting these
information, such as energy-saving scenarios prediction data models and
traffic load prediction data models.
The prediction data models are trained to be able to produce the expected
training output from the training input. The data for models training may
include the traffic load variation (e.g., RRC connection number, PRB
utilization etc.), the network data related to the traffic load variations
(e.g., UE throughput, rate of successful handover executions, air-interface
delay, CQI related measurements etc.), energy consumption, service experience,
etc. After the training process, the prediction data models for predicting
information used to make energy-saving policies can be obtained.
The more accurate the information prediction results are, the better the
energy-saving policies based on the information prediction results will be.
According to the result of information prediction and the energy-saving
benefit, the MDA service can assist the energy saving policy decision making
by recommending the optimal prediction data models which can provide more
accurate information prediction results for consumers.
The MDAS producer is informed when the recommended decisions are taken by the
MDAS consumer to make energy saving decisions, so that the MDAS producer can
get the service experience changes and traffic load changes to evaluate
energy-saving benefits. By maximizing the expected sum of energy-saving
benefits, the MDAS producer may update recommended decisions and fine-tune the
accuracy of the analytics result.
For example, the MDAS producer may take base station energy saving scenarios
prediction data model and the traffic load prediction data model together as
input. Based on the results of energy saving scenarios prediction and traffic
load prediction, the MDAS producer may use the ML algorithms to calculate the
energy-saving benefit based on base station related information (e.g., service
experience changes), the traffic load changes as well as the prediction of the
energy saving scenarios. And by maximizing the expected sum of energy-saving
benefits, the MDAS producer may provide the optimal prediction data models for
making recommendation of base station energy saving policies accordingly.
6.6.1.3.2 Data required for MDA assisted energy saving
The following table shows the potential data required to analyse the energy
saving issue.
+--------------------------+------------------------------------------+ | **Data Category** | **Required Data** | +==========================+==========================================+ | Performance Measurements | PNF Power Consumption: Power consumed | | | over the measurement period, see clause | | | 5.1.1.19.2 of TS 28.552 [8]; | | | | | | PNF Energy consumption: The energy | | | consumed, see clause 5.1.1.19.3 of TS | | | 28.552 [8]; | | | | | | PNF Energy Temperature: The temperature | | | over the measurement period, see clause | | | 5.1.1.19.4 of TS 28.552 [8]; | | | | | | PNF Voltage: The voltage, see clause | | | 5.1.1.19.5 of TS 28.552 [8]; | | | | | | PNF Current: The current, see clause | | | 5.1.1.19.6 of TS 28.552 [8]; | | | | | | PNF humidity consumption: The percentage | | | of humidity during the measurement | | | period, see clause 5.1.1.19.7 of TS | | | 28.552 [8]; | | | | | | PDCP Data Volume of NR cells: The | | | transmitted PDCP data volume, see clause | | | 5.1.2.1 and 5.1.3.6 of TS 28.552 [8]; | | | | | | Virtual resource usage of NF: The | | | resource usage of virtual network | | | functions, see clause 5.7.1 of TS 28.552 | | | [8]. | | | | | | Data volume of UPF: see clause 5.4 of TS | | | 28.552 [8].\ | | | Delay related measurements of UPF: see | | | clause 5.4 of TS 28.552 [8].\ | | | Number of PDU sessions with SSC mode 1 | | | on UPF. | | | | | | Traffic load variation: | | | | | | PRB utilization rate, see clause 5.1.1.2 | | | of TS 28.552[2] | | | | | | RRC connection number, see clause | | | 5.1.1.4 of TS 28.552[8] etc | | | | | | Network data: | | | | | | UE throughput, see clause 5.1.1.3 of TS | | | 28.552[2] | | | | | | the mobility management data (e.g. rate | | | of successful handover executions), see | | | clause 5.1.1.6 of TS 28.552[2] | | | | | | packet delay data (e.g., air-interface | | | delay), see clause 5.1.1.1 of TS | | | 28.552[2] etc. | +--------------------------+------------------------------------------+ | QoE Data | The measurements that are collected are | | | DASH (see TS 26.247 [16]) and MTSI | | | (see TS 26.114 [17]) measurements (see | | | TS 28.406 [24]). | +--------------------------+------------------------------------------+ | NRM | MOIs of the cells, UPFs and SMFs, see TS | | | 28.541 [20]. | +--------------------------+------------------------------------------+ | Alarm information | The alarm information of the cells, UPFs | | | and SMFs. | +--------------------------+------------------------------------------+ | Network analytics data | The control plane analysis result from | | | the NWDAF defined in TS 23.288 [18], | | | e.g., observed service experience | | | related network data analytics. | +--------------------------+------------------------------------------+ | Prediction Data Models | The prediction data models, which may be | | | based on ML algorithms and trained to be | | | able to produce the expected training | | | output for consumers, e.g., base station | | | energy-saving scenarios prediction data | | | models, traffic load prediction data | | | models. | | | | | | The prediction data models, including | | | the model related information (e.g., | | | including Model Id), are related to | | | service (e.g., traffic load, resource | | | utilization, service experience), which | | | can be provided by consumer. | | | | | | NOTE: The model related information may | | | include but not only Model Id. Maybe | | | more work on the model related | | | information is needed. | +--------------------------+------------------------------------------+
6.6.1.3.3 Analytics report for MDA assist energy saving
The following table shows the potential information carried in the analytics
report of MDA assist energy saving.
+----------------------+----------------------+----------------------+ | **\Analytics Report |** Attribute Name**|** Description**| | of MDA assisted | | | | energy saving** | | | +======================+======================+======================+ | | Energy saving | The identifier of | | | analytics identifier | the MDA assisted | | | | energy saving | +----------------------+----------------------+----------------------+ | | Location | The geographical | | | | area, UPFs or the | | | | cells where the | | | | unreasonable energy | | | | consumption exists | | | | or recommended 5GC | | | | NF deployment | | | | options or NR cells | | | | in high-value | | | | traffic geographical | | | | area. | +----------------------+----------------------+----------------------+ | | Root cause | The root cause of | | | | the part of the | | | | energy consumption | | | | that may be | | | | conserved, e.g., | | | | ultra-low traffic | | | | load area with | | | | energy consumption, | | | | excessive energy | | | | consumption | +----------------------+----------------------+----------------------+ | | Trends of the | Predictions related | | | traffic load | to the trends of | | | | traffic | | | | load(e.g.,PRB | | | | utilization rate, | | | | RRC connection | | | | number, etc.)which | | | | could be used as | | | | references for | | | | making decision on | | | | energy saving | +----------------------+----------------------+----------------------+ | | Recommended | The optimal | | | prediction data | prediction data | | | models | models which can be | | | | selected from the | | | | prediction data | | | | models (e.g. | | | | including Model Id) | | | | to provide more | | | | accurate information | | | | prediction results | | | | to assist the energy | | | | saving related | | | | decision making. | +----------------------+----------------------+----------------------+ | | Current Energy | The information of | | | Saving State of the | the current energy | | | cells | saving state of the | | | | cells, e.g. the | | | | cells which are | | | | currently in the | | | | energy saving state. | +----------------------+----------------------+----------------------+ | | Energy saving | For ES on NR cells. | | | recommendations | It may contain a set | | | | of | | | | | | | | > \- recommended NR | | | | > Cell (ES-Cell) to | | | | > enter energySaving | | | | > state. | | | | > | | | | > \- recommended | | | | > candidate cells | | | | > with precedence | | | | > for taking over | | | | > the traffic of the | | | | > ES-Cell. | | | | > | | | | > \- a time period, | | | | > during which | | | | > energy saving is | | | | > or not allowed. | | | | > | | | | > \- predictions | | | | > related to the | | | | > trends of traffic | | | | > load which could | | | | > be used as | | | | > references for | | | | > making decision on | | | | > energy saving of | | | | > NR cells. | | | | | | | | For ES on UPFs. It | | | | contains a set of | | | | | | | | > \- recommended UPF | | | | > (ES-UPF) to | | | | > conduct energy | | | | > saving; | | | | > | | | | > \- recommended | | | | > candidate UPFs | | | | > with precedence | | | | > for taking over | | | | > the traffic of the | | | | > ES-UPF. | | | | > | | | | > \- a time period, | | | | > during which | | | | > energy saving is | | | | > or not allowed. | | | | > | | | | > \- predictions | | | | > related to the | | | | > trends of traffic | | | | > load which could | | | | > be used as | | | | > references for | | | | > making decision on | | | | > energy saving of | | | | > UPFs. | +----------------------+----------------------+----------------------+
6.6.1.4 Evaluation
The solution described in clause 6.6.1.3 requires the analytics inputs as
described in in clause 6.6.1.3.2, wherein
> \- the performance measurements on number of PDU sessions with SSC mode 1 on
> UPF can be defined in TS 28.552 [8], while other measurements are already
> available in TS 28.552 [8].
>
> \- Traffic load variation and network data are all available in TS 28.552
> [8].
>
> \- QoE data are available in TS 28.406 [24].
>
> \- NRMs are available in TS 28.541 [20].
>
> \- the inventory information can be defined in NRMs.
>
> \- the alarm information is supported by fault supervision MnS (see TS
> 28.545 [22] and TS 28.541 [20]).
>
> \- the network analytics data can be accessed from NWDAF according to TS
> 23.288 [18].
>
> \- the prediction data models are a kind of data provided by consumer, which
> include the model related information to be used for identifying the existed
> ML model. The identifier of existed ML models can be defined in ML model
> training for MDA.
With these analytics inputs which either are already defined or can be defined
in the normative work, the analytics output as described in 6.6.1.3.3 can be
derived.
## 6.7 Software management related issues
### 6.7.1 RAN Node Software Upgrade
#### 6.7.1.1 Use case
As per the current mechanism of software upgrade at RAN node results in
service disruption or huge operational cost. Consider a scenario, when a RAN
Node is required to shut down manually to undergo critical maintenance for a
very short duration of time. Software upgrade can be one such critical
maintenance scenario. In such cases, all the resources (bearer, security
functions, mobility management) that are managed by this RAN Node need to be
purged and reconfigured at another RAN Node (standby RAN Node) or if another
RAN Node is not available then resources will be reconfigured again when
former RAN Node comes up after software upgrade. Both the situations lead to
additional operational expenses and data loss. Operational expense in terms of
all the resources to be released/attached again and data loss for all GBR
sessions/bearer.
It is expected to use MDAS to optimize the procedure of software upgrade at
RAN Node. The software upgrade should be automatically initiated by the OAM
system, once configured, at the time when the expected impacts are minimum
i.e. at the Optimal Time when there would be minimum expected operational cost
and data loss. The Optimal Time (current or futuristic) can be derived by
collecting and analysing the data related to DRBs including GBR/non-GBR,
state, modification count, ongoing handover etc. MDAS can utilize historical
data and AI/ML (e.g., time series based) algorithm to derive the future
optimal time for software upgrade.
Note: RAN Node above refers to CU-CP in case of gNB split case.
#### 6.7.1.2 Potential Requirements
**REQ-SW_UPG_CON-1:** The MDAS producer should have a capability allowing the
authorized consumer to get the DRB info analytics report describing the DRBs
info at a particular RAN Node(s).
**REQ-SW_UPG_CON-2:** The MDAS producer should have a capability to provide
the DRB info analytics report describing the DRB info based on DRB
characteristics including GBR/non-GBR, state, modification count, handover
etc.
**REQ-SW_UPG_CON-3:** The DRB info analytics report describing the DRB info
should contain the following information:
\- Timestamp: Time at which the report is generated
\- CurrentUpgradeOptimal: Whether RAN Node is optimal for upgrade at present
\- DRB status: Total number of GBR and non-GBR DRBs at present
\- FutureUpgradeOptimal: Whether RAN Node will be optimal for upgrade at a
future point of time. This will also provide a future timestamp.
\- DRB status: Total number of GBR and non-GBR DRBs at future point of time.
This will also provide a future timestamp.
#### 6.7.1.3 Possible Solutions
##### 6.7.1.3.1 Solution description
The solution requires MDAS producer to collect current DRB info as defined in
clause 6.7.1.3.2 and produce the report, as defined in clause 6.7.1.3.3,
providing the optimal time for software upgrade i.e. the time at which the
impact is minimal.
##### 6.7.1.3.2 Data required
The following table shows the data required to generate the DRB info analytics
report. The data is collected per DRB from gNB.
+-------------------+-------------------------------------------------+ | Data Category | Required Data | +===================+=================================================+ | Bearer Statistics | QCI: Indicates resource bearer type (GBR, | | | non-GBR). | +-------------------+-------------------------------------------------+ | | Radio bearer State: Radio bearer State (Idle, | | | Active) | | | | | | _The state indicates the status of bearer | | | connection. The state will be used to deduce | | | total number of idle/active connection. Based | | | on the number of idle/active connections | | | probable time for SW Upgrade can be decided._ | +-------------------+-------------------------------------------------+ | | Bearer Modification Count: This count indicates | | | number of times, this bearer has gone for | | | modification since its creation. | | | | | | _This count will be used to detect how frequent | | | this session has undergone bearer modification. | | | With history database, if this session is | | | vulnerable to many bearer modifications, the | | | upgrade can be differed._ | +-------------------+-------------------------------------------------+ | | Handover In Progress: This flag indicates | | | whether the bearer is undergoing handover or | | | not. | | | | | | _This flag will help to deduce number of | | | sessions which are undergoing handover. MDAS | | | may choose to defer SW upgrade based on number | | | of sessions which are undergoing handover. The | | | best practise is to go for SW upgrade, when | | | active handovers are minimum._ | +-------------------+-------------------------------------------------+ | | GTP Error Indication: This flag indicates GTP | | | Path has gone to error state and system is | | | waiting for recovery. | | | | | | _This flag will help to deduce number of | | | sessions which are recovering due to error in | | | GTP Path. MDAS may choose to defer SW upgrade | | | based on number of sessions which are in error | | | state. The best practise is to go for SW | | | upgrade, when number of sessions suffering from | | | this error are minimal._ | +-------------------+-------------------------------------------------+ | | Timestamp: This parameter indicates timestamp | | | during which this information has been | | | collected | +-------------------+-------------------------------------------------+
##### 6.7.1.3.3 Analytics report
The DRB info analytics report contains the following
DRB info Analytics Report Attribute Name Description
* * *
                              Timestamp               Time at which the report is generated
                              CurrentUpgradeOptimal   Boolean attribute indicating whether RAN Node can be upgrade at present.
                              No. of GBR DRB          Total number of GBR bearer
                              No. of Non-GBR DRB      Total number of non-GBR bearer
                              FutureUpgradeOptimal    Boolean attribute indicating whether RAN Node can be upgrade in future. This will provide the time duration in future.
                              No. of GBR DRB          Total number of GBR bearer
                              No. of Non-GBR DRB      Total number of non-GBR bearer
#### 6.7.1.4 Evaluation
The solution described in clause 6.7.1.3 requires the analytics inputs as
described in in clause 6.7.1.3.2. All the required bearer related inputs can
be defined as part of DRB related measurements in TS 28.552 [8].
With these analytics inputs which can be defined in the normative work, the
analytics output as described in 6.7.1.3.3 can be derived.
Therefore, this solution is a feasible candidate for RAN node software
upgrade.
## 6.8 MDA assisted SON coordination
### 6.8.1 SON conflict prevention and resolution
#### 6.8.1.1 Use case
Some SON functions, such as the MRO function and the MLB function, may modify
the same parameters of an NR cell and potentially cause conflict. For
instance, the MRO function may need to modify the HO parameters causing a
handover to occur later (i.e., when the signal strength of the neighbour cell
become stronger). However, the MLB function may need to modify the same HO
parameters causing the HO to occur sooner, in order to offload some traffic
towards the same neighbour cell. Similarly, the conflict may arise between a
SON function and a non-SON function (e.g., eMIMO).
The potential SON conflict (which could be between SON functions or between a
SON function and a non-SON function) should be prevented from happening as
much as possible, and if the conflict happens it should be resolved as soon as
possible. The SON conflict prevention and resolution can be assisted by MDA.
The MDA could analyse the following data for identifying the potential SON
conflict or detecting whether a SON conflict occurred:
\- historical and the most recent changes made by the SON functions and non-
SON functions;
\- the current network configurations;
\- historical and current network performance data related to the SON
function(s) and the relevant non-SON functions. For instance, load information
of the NR cells, handover performance measurements (too early HOs, too late
HOs, etc.);
\- Policies and targets for the SON functions.
\- historical and current MDT/RLF data.
\- for a SON function or a non-SON function, considering potential affected
parameters.
If a potential SON conflict is identified, the MDAS producer provides an
analytics report to describe the potential conflict and the recommended
actions to prevent such conflicts from happening.
If a SON conflict is detected, the MDAS producer provides an analytics report
to describe the conflict including the recommended actions to resolve it.
The recommended actions for SON conflict prevention and resolution may be one
or more of the following:
\- modify the policies and targets for the SON function(s);
\- change the priority of the SON function(s);
\- set or change the range of the attributes value that the SON function(s)
are allowed to modify;
\- update the attributes value to correct the conflict (if already occurred);
\- temporarily switch off one or more SON function(s);
\- undo the most recent configuration undertaken by the SON function;
\- provide a report that describes how SON function(s) can potentially affect
the common parameters.
#### 6.8.1.2 Potential requirements
**REQ-MDA_SONCO-CON-1** The MDAS producer should have a capability to provide
the analytics report describing the identified potential conflict between SON
functions or between a SON function and a non-SON function with recommended
actions to prevent the conflict from happening.
**REQ-MDA_SONCO-CON-2** The MDAS producer should have a capability to provide
the analytics report describing the detected conflict between SON functions or
between a SON function and a non-SON function with recommended actions to
resolve the conflict.
**REQ-MDA_SONCO-CON-3** The MDAS producer should have a capability to provide
the analytics report that describes the relations (i.e. whether certain
conditions trigger conflicts) between observed conflicts and observed
conditions.
#### 6.8.1.3 Possible solutions
##### 6.8.1.3.1 Solution description
The MDAS producer correlates and analyses the data described in the following
subclause within a time period on a regular basis or trigged by events (e.g.,
the RLF report) to identify a potential SON conflict or a SON conflict already
occurred.
Once a potential or already occurred SON conflict is identified, the MDAS
producer provides the analytics report to describe the SON conflict as shown
in subclause 6.8.1.3.3.
##### 6.8.1.3.2 Data required for SON conflict analysis
The following table describes the data required for SON conflict analysis:
+-------------------------------+-------------------------------------+ | Data Category | Required Data | +===============================+=====================================+ | Performance Measurements | Radio Measurements for gNB: RRC | | | connection, see clause 5.1, TS | | | 28.552 [8], e.g., RRC connection | | | number, RRC connection | | | establishment, RRC connection | | | re-establishment, RRC | | | connection resuming; | | | | | | RAN UE throughput: A KPI that shows | | | how NG-RAN impacts the service | | | quality provided to an | | | end-user, see clause 6.3.6 of TS | | | 28.554 [7]. | | | | | | Throughput for network slice | | | instance: Upstream/Downstream | | | throughput for network and Network | | | Slice Instance, see clause 6.3.2 | | | and clause 6.3.3 of TS 28.554 | | | [7]; | | | | | | Performance measurements related to | | | the SON functions, see TS 28.313 | | | [19]. | +-------------------------------+-------------------------------------+ | CM notification | Notifications of the NRM updates | | | made by SON functions and non-SON | | | functions. | | | | | | The notification needs to include | | | the Id (e.g., DN) of the SON | | | function or non-SON function who | | | made the NRM updates. | +-------------------------------+-------------------------------------+ | MDT/RLF Data | UE measurements related to RLF | | | containing RSRP, RSRQ, of the | | | serving cell and neighbour cells, | | | SINR with anonymous id (e.g., | | | C-RNTI) and UE location | | | information. | +-------------------------------+-------------------------------------+ | QoE Data | The details information of QoE data | | | required by this case is FFS. | +-------------------------------+-------------------------------------+ | Configuration Data (NRM) | The execution data including the | | | changes or the configuration of the | | | MOIs. This should include | | | historical UE configurations | | | | | | The attributes of the MOIs to be | | | analysed, see TS 28.541 [20]. | | | | | | The policy and targets of the SON | | | functions, see TS 28.313 [19]. | +-------------------------------+-------------------------------------+ | Potential affected parameters | SON functions parameters affected | | | including the degree of impact | | | (low, medium, high). | +-------------------------------+-------------------------------------+
Note: The above parameters may not be the complete list.
##### 6.8.1.3.3 Analytics report SON conflict prevention and resolution
The following table provides the potential contents of the analytics report
for SON conflict prevention and resolution:
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | SON conflict | | | | prevention and | | | | resolution | | | +======================+======================+======================+ | | Conflicting | Indicates the Id of | | | Functions Id | the conflicting SON | | | | functions or non-SON | | | | functions. | +----------------------+----------------------+----------------------+ | | Conflict type | Indicates the | | | | conflict is a | | | | potential conflict | | | | or an already | | | | occurred conflict. | +----------------------+----------------------+----------------------+ | | Conflicting | The MOI and | | | attributes | attributes where the | | | | conflict occurred or | | | | is potentially to | | | | occur. | +----------------------+----------------------+----------------------+ | | Conflict reason | The description of | | | | the reason for the | | | | conflict. | +----------------------+----------------------+----------------------+ | | Conflicting metrics | The performance | | | | measurements and | | | | KPI(s) over which | | | | the SON or non-SON | | | | Functions conflict. | +----------------------+----------------------+----------------------+ | | Attribute conflict | The relative level | | | status | (low, medium, high) | | | | of impact of each | | | | conflicting | | | | attribute on the SON | | | | or non-SON | | | | functions. | +----------------------+----------------------+----------------------+ | | Recommended actions | The MOI and | | | | attributes | | | | recommended to be | | | | configured/changed: | | | | | | | | - The policy and | | | | targets of the SON | | | | functions, see TS | | | | 28.313 [19]; | | | | | | | | - The range of | | | | attributes value | | | | each conflicting SON | | | | function can change; | | | | | | | | - The priority of | | | | each conflicting SON | | | | function; | | | | | | | | - Value of the | | | | attributes of the | | | | affected MOIs (e.g., | | | | NRCellCU) where the | | | | conflict already | | | | occurred; | | | | | | | | - Switch off the | | | | conflicting SON | | | | functions, see TS | | | | 28.313 [19]. | +----------------------+----------------------+----------------------+
#### 6.8.1.4 Evaluation
The solution described in clause 6.8.1.3.1 requires the analytics inputs as
described in clause 6.8.1.3.2 wherein:
\- PM related to Radio Measurements for gNB are specified in TS 28.552 [8].
\- KPI related to RAN UE throughput, Throughput for network slice instance are
specified in TS 28.554 [7].
\- PM related to the SON functions, see TS 28.313 [19].
\- CM notifications made by SON functions as specified in TS 28.532.
\- MDT data is available in TS 37.320 [12].
\- QoE can be provided by the service experience analytics by NWDAF TS 23.288
[18].
\- Configuration Data (NRM) TS 28.541 [20].
\- Potential affected SON parameters are available. NRM extension are also
needed to reflect the degree of impact (low, medium, high).
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.8.1.3.3 can be derived.
Therefore, this solution is a feasible candidate for SON conflict prevention
and resolution.
## 6.9 Security related issues
### 6.9.1 Security risk assessment
#### 6.9.1.1 Use case
Security risk assessment can detect anomalies in managed objects, e.g. NF,
identify potential security risks and propose countermeasures to mitigate
them. Security risks may originate from a variety of different sources and
target distinct network objects resulting in different abnormal behaviours,
e.g. sudden increase in computing and storage in a virtualization environment,
sudden increase of load in network links, abnormal communication patterns
between NFs, excessive latency in accessing a NF, relocating a NF in an
unexpected location or interrupting the relocation of a NF.
To assess security risks, the management plane can leverage the benefits of
MDAS. An MDAS producer can identify a security risk issue by correlating
different performance measurements and alarms, i.e. performing a root cause
analysis to locate the malicious source and the network objects affected. Such
activity can be triggered when certain performance measurements and KPIs
indicate an abnormal network behaviour, which is not related to another known
faults.
_The MDAS producer should correlate the usage, e.g. considering the NF
procedures for supporting a number of UEs or other events, with the
corresponding resource usage, e.g. in terms of CPU, storage and disk. If no
other fault alarms indicate another reason, an unexpected resource usage
should trigger a security risk analysis to identify the issue based on a
private security risk database that resides on the MDAS producer._ The MDAS
producer can notify the MDAS consumer, e.g. MnF or NE, and provide
recommendations to mitigate the identified security risk. The MDAS consumer
based on the recommendations can isolate the malicious network objects, e.g.
NF, and can also provide recommendations to harden the network in order to
avoid similar security risks in the future.
#### 6.9.1.2 Potential requirements
**REQ-SEC_CON-1** The MDAS producer should have a capability to provide the
analytics report describing the security risk to authorized consumers based on
the correlation of current and predicted performance measurements and alarms.
**REQ-SEC_CON-2** The analytics report describing the security issue should
contain the following information describing the current and future security
risk issue:
\- Identify the type of security risk.
\- Location and network objects affected by the security risk.
\- Root cause analysis of the security risk issue.
\- Recommended action to isolate and/or restrict the security risk issue and
harden the network security.
#### 6.9.1.3 Possible solutions
##### 6.9.1.3.1 Solution description
The solution considers security risk assessment related to network. The MDAS
producer correlates and analyses performance measurements and alarm data
considering the network topology and configuration data related to statistics
or predictions to identify the type of security risk.
##### 6.9.1.3.2 Data required
The following data is required to perform the corresponding security risk
assessment analysis.
+--------------------------+------------------------------------------+ | Data category | Required data | +==========================+==========================================+ | Alarm Data | Alarm information - types of alarms | +--------------------------+------------------------------------------+ | Service Data | S-NSSAI as defined in clause 5.15.2, TS | | | 23.501 [13]. MDAS may derive network | | | topology information | +--------------------------+------------------------------------------+ | Performance Measurements | Failures and disruptions: | | | | | | - Number of abnormal releases: DRB, QoS | | | flows, PDU sessions, UE context in | | | serving cells as per TS 28.552 [8] | | | | | | - Disruption measurements: CQI/MCS as | | | per TS 28.552 [8] | | | | | | - Excessive delay in accessing NFs | | | (e.g. AMF or SMF) | | | | | | - NF abnormal and excessive | | | communication, e.g. AMF uses a different | | | SMF without performing the expected | | | selection process or it overloads an SMF | | | unexpectedly. | | | | | | - Intra/inter-gNB handover: failures - | | | long handover time as per TS 28.552 | | | [8] | | | | | | Virtualized resources/behaviour: | | | | | | - Virtual resource usage of NF: The | | | resource usage of virtual network | | | functions, see clause 5.7.1 of TS 28.552 | | | [8] | | | | | | - Virtual NF Re-location: | | | Timing/duration and success rate | | | | | | - Frequency of virtual NF Re-location: | | | Rate of relocation | | | | | | - Virtual NF location: NF location with | | | respect to a data network | | | | | | NF context information as per TS 28.552 | | | [8]: | | | | | | - Number of UEs/periodic registration | | | updates (AMF) | | | | | | - Number of PDU sessions/modifications, | | | QoS flows (SMF) | | | | | | - N4/N6/N9 measurements or packet | | | delay, traffic volume, link usage, | | | packet loss (UPF) | | | | | | - Number of application trigger | | | requests rejected (NEF) | | | | | | - Number of failed NF service | | | registration/update, discoveries due to | | | unauthorized NF/error (NRF) | +--------------------------+------------------------------------------+ | Configuration Data | NRM attributes affecting the location | | | and virtual NF resource allocation and | | | configuration | | | | | | NRM update reports (notification and | | | log) containing the creation or changes | | | of the MOIs affecting the virtual NFs | +--------------------------+------------------------------------------+ | Network Topology | Topology of the network | +--------------------------+------------------------------------------+
##### 6.9.1.3.3 Analytics report
The MDAS producer offers a new Security Analytics service to the MDAS
consumer, which supports security risk assessment related providing the
following analytics results:
+----------------------+----------------------+----------------------+ | Analytics Report of | Attribute Name | Description | | Security risk | | | | assessment | | | +======================+======================+======================+ | | **Security Incident | Identifier that | | | Identifier** | indicates the | | | | security risk (e.g. | | | | DDoS, malicious NF, | | | | etc.) | +----------------------+----------------------+----------------------+ | | **Type of | Statistics or | | | Analytics** | Prediction of | | | | security risks | +----------------------+----------------------+----------------------+ | | **Location** | Geographical | | | | location that the | | | | security risk | | | | affects | +----------------------+----------------------+----------------------+ | | **Affected Objects** | NF, PDU session, QoS | | | | Flow, Slice | +----------------------+----------------------+----------------------+ | | **Start/Stop Time** | Starts/stop time of | | | | the security risk | | | | issue | +----------------------+----------------------+----------------------+ | | **Root Cause** | The originator of | | | | security issue to | | | | isolate fast the | | | | problem. | +----------------------+----------------------+----------------------+ | | **Severity Level** | The severity level | | | | (e.g. critical, | | | | medium, not | | | | important) of the | | | | security risk issue | +----------------------+----------------------+----------------------+ | | **Recommended | Recommendation | | | Actions** | actions to resolve | | | | the security risk | | | | issue: | | | | | | | | - Isolate/terminate | | | | NF, terminate PDU | | | | session, throttle | | | | signalling from NF | | | | or UE, block UE, | | | | etc. | | | | | | | | - Harden security | | | | on specific NF, | | | | firewall update, | | | | scaling resources, | | | | load balancing, | | | | admission control, | | | | etc. | +----------------------+----------------------+----------------------+
#### 6.9.1.4 Evaluation
The solution described in clause 6.9.1.3.1 requires the analytics inputs as
described in clause 6.9.1.3.2, wherein
\- Input related to alarm data, configuration data and network topology are
available.
\- Input PMs: Number of abnormal releases, Disruption measurements,
Intra/inter-gNB handover and Excessive delay in accessing NFs (e.g. AMF or
SMF) are specified in TS 28.552[8].
\- Virtualized resources/behaviour are specified in TS 28.552[8].
\- IP address of virtual NF is available.
\- NF context information for AMF, SMF, UPF, NEF, NRF are specified in TS
28.552 [8].
\- The following PMs related to 5GC NFs shall be specified:
> \- NF abnormal and excessive communication.
\- The following PMs related to virtual NFs shall be specified:
> \- Virtual NF Re-location: Timing/duration and success rate.
>
> \- Frequency of virtual NF Re-location: Rate of relocation.
\- the network topology can be reflected by the NRMs defined in TS 28.541
[20].
With these analytics inputs which are already defined or accessible, the
analytics output as described in 6.9.1.3.3 can be derived.
Therefore, this solution is a feasible candidate for security risk assessment.
## 6.10 MDA management aspects
### 6.10.1 ML model training for MDA
#### 6.10.1.1 Use case
The MDA process may rely on ML technologies. To optimize the accuracy of MDA
result, the ML model of the MDA process may need to be trained.
For training the ML model of the MDA process, the consumer provides the
training data including training input and the desired output to the MDAS
producer. The MDAS producer uses the training input and the desired output to
train the ML model, i.e., to train the algorithm of the ML model to generate
the desired output as accurately as possible by analysis of the training
input. The MDAS producer provides an ML model training report to the consumer.
With a trained ML model for MDA, the MDAS producer can analyse the analytics
input and generate the analytics report as output data of the analysis to the
consumer.
The consumer may validate the output data provided by the MDAS producer. The
output data to be validated may be the analytics report and/or the ML model
training report as described above. The consumer may provide the validation
data as feedback to the MDAS producer, and the MDAS producer will use the
validation data for further ML model training for MDA with the input data that
were used to generate the validated report. As a result of validation, the
consumer may also provide the training data and request the MDAS producer to
train the ML model.
#### 6.10.1.2 Potential requirements
**REQ-MDA_MGMT-CON-1** The MDAS producer should have a capability allowing the
consumer to provide the data for training the ML model for MDA.
**REQ-MDA_MGMT-CON-2** The MDAS producer should have a capability to provide
ML model training report to the consumer.
**REQ-MDA_MGMT-CON-3** The MDAS producer should have a capability to receive
the validation data from the consumer and train the ML model for MDA based on
the received validation data.
#### 6.10.1.3 Possible solutions
##### 6.10.1.3.1 Solution description
The MDAS producer trains the ML model with the training data or the validation
data received from consumer. The ML model training should have no or minimal
impact to the normal data analytics.
For the ML model training with the training data, the training data (see
subclause 6.10.1.3.2) should include the training Id, the training input and
expected training output:
> \- The training Id is used to identify the ML model training request, and to
> associate with the training report.
>
> \- The training input is a set of training input data with the indicated
> data type (e.g., performance measurements, MDT report, NRM, etc.).
>
> \- The expected training output specifies the analysis result that the ML
> model should aim to achieve based on the training input.
For the ML model training with the validation data, the validation data (see
subclause 6.10.1.3.2) should include the validation report Id, analytics
report Id that was validated, and the validated information including the data
that are rectified. The MDAS producer looks up the historical data that are
associated with the validated analytics report and trains the ML model with
the historical data and the validation data.
The MDAS producer provides a training report (as shown in subclause
6.10.1.3.3) to the consumer, with indication of whether the training
(identified by the training Id or validation Id) is successful and possibly
the failure cause if the training is not fully successful.
##### 6.10.1.3.2 Data required for ML model training for MDA
The following table describes the data required for ML model training for MDA:
+-------------------+-------------------------------------------------+ | **Data category** | **Required data** | +===================+=================================================+ | Training data | The training data include a training | | | identifier, the training input and the expected | | | training output. | | | | | | Training Id: Identifier of the training. | | | | | | The training input contains the same types of | | | information that should be included in a normal | | | analytics input. | | | | | | The expected training output contains the same | | | types of information that should be included in | | | a normal analytics report. | +-------------------+-------------------------------------------------+ | Validation data | The training data include a validation report | | | Id, analytics report Id that was validated, and | | | the data that are rectified. Example of the | | | rectified data is the originally reported root | | | cause is not accurate by validation and should | | | be updated to another. | +-------------------+-------------------------------------------------+
##### 6.10.1.3.3 ML model training report
Following table provides the potential contents of the ML model training
report.
**ML model training report** **Attribute Name** **Description**
* * *
                                 Training Id            The training Id that the training report is associated to.
                                 Validation report Id   The validation report Id that the training report is associated to.
                                 Training result        Indication of whether the training is successful, partially successful or failed.
                                 Failure cause          The cause of the failure if the training is not fully successful.
#### 6.10.1.4 Evaluation
The solution described in clause 6.10.1.3 requires the input s described in in
clause 6.10.1.3.2, wherein
\- the training data are fully aligned with the analytics input and output of
the concrete analytics use cases.
\- and validation data can be provided by the consumer.
Therefore, this solution is a feasible candidate for ML model training for the
use cases that are concluded with a feasible solution.
### 6.10.2 Requesting and reporting of Management Data Analytics Reports
#### 6.10.2.1 Use case
A MDAS Producer may provide several management data analysis reports. Multiple
consumers may wish to receive a selection of these reports.
The consumer submits a request to MDAS producer to request MDA reports. This
request may include a filter to specify the scope of MDA reports (e.g., type
of analytics report such as coverage issue analysis, resource utilization
analysis, the managed objects to be analysed, etc.). The MDAS producer
activates the data collection if it is not already active. In the request, the
consumer may indicate the method that the MDA reports are to be reported.
For all reports, the MDAS producer collects data, analyses the data, and
generates the analytics report.
The MDAS producer provides the MDA reports based on the reporting method
designated by the consumer.
The consumer may send a request to MDAS producer to modify, i.e. adjust the
filter scope of the MDA reporting, or terminate the MDA reporting request.
#### 6.10.2.2 Potential requirements
**REQ-MDA_SUB-1** The MDAS producer should have a capability to allow an MDAS
consumer to request an analytics report. The reporting request should
optionally allow the MDAS consumer to filter the scope of data in the
analytics report.
**REQ-MDA_SUB-2** The MDAS producer should have a capability to provide the
analytics report to the requesting consumers.
**REQ-MDA_SUB-3** The MDAS producer should have a capability to allow an MDAS
consumer to terminate an analytics reporting request.
#### 6.10.2.3 Possible solutions
The MDAS consumer sends the MDA reporting request to MDAS producer, with the
following information included:
\- identifier of the reporting request;
\- reporting method, i.e., file reporting or streaming based reporting or
notification based reporting;
\- streaming target if the reporting method is designated to streaming based
reporting;
\- file information (e.g., file location, ready time, expiration time, size,
compression and format) if the reporting method is designated to file
reporting;
\- notification target if the reporting method is designated to provide
notification based reporting;
\- filter for the scope of the MDA analytics report (e.g., type of analytics
report, start/stop time, managed objects to be analysed, etc.).
The MDA reporting request may be modelled as an IOC and managed via
provisioning related operations (such as CreateMOI, ModifyMOI, DeleteMOI).
The MDAS producers provides a response indicating the status of the request.
For the MDA reporting request designating the reporting method of file based
reporting and streaming based reporting are already defined in TS 28.532 [28],
while notification based reporting needs to be further investigated during the
normative phase. Figure 6.10.2.3-1 shows MDA reporting with the multiple
reporting options.
{width="6.320895669291339in" height="6.603529090113736in"}
Figure 6.10.2.3-1: MDA reporting with multiple options
> NOTE: The file server where MDA report is fetched from in steps a5 and a6
> can be deployed separately from the MDAS producer.
#### 6.10.2.4 Evaluation
The solution described in clause 6.10.2.3 include multiple reporting options:
  * the generic streaming based reporting service and file based reporting service as defined in TS 28.532 [28] and can be reused for MDA data reporting,
  * notification based reporting, for which the detailed solution can be further investigated and determined during the normative phase.
Therefore, this solution is a feasible candidate for MDA reporting request and
reporting.
### 6.10.3 Retrieval of historical Management Data Analytics Reports
#### 6.10.3.1 Use case
Besides the subscription and reporting of the MDA reports, the consumer may
need to retrieve some historical MDA reports which were produced by MDAS
producer in the past. The consumer may retrieve the historical MDA reports for
some reason, for example the reports have been received but lost by the
consumer, or the reports have been received but could not be successfully
parsed by the consumer, or even the reports were not successfully received by
the consumer.
The consumer submits a request to MDAS producer to retrieve the historical MDA
reports. This request may include a time frame and a filter to specify the
scope of MDA reports to be retrieved (e.g., type of analytics report such as
coverage issue analysis, resource utilization analysis, the managed objects
analysed, etc.).
The MDAS producer retrieves the historical MDA reports, and sends the results
to the consumer.
#### 6.10.3.2 Potential requirements
The handling of historical management data in being addressed in some other
work items, such as MADCOL (UID: 880028), it is assumed that the generic
requirements on historical management data handling are applicable to MDA
reports. No specific potential requirements are identified for retrieval of
historical MDA reports.
#### 6.10.3.3 Possible solutions
Handling of historical management data in being addressed in some other work
items, such as MADCOL (UID: 880028), and the solutions derived from those work
items can be applied to retrieval of historical MDA reports.
> NOTE: How to apply the solutions on historical management data handling made
> by these work items to MDA reports is not addressed in this study.
#### 6.10.3.4 Evaluation
The retrieval and reporting of historical MDA reports can reuse the solutions
for handling historical management data being addressed in other work items,
such as MADCOL (UID: 880028). This study does not address how to apply these
solutions to MDA reports.
### 6.10.4 Confidence indicator in analysis results
A _consumer of MDAS should treat the results of Management Data Analytics with
caution. A decision based on analytics should take into account the degree of
confidence of the analysis result, especially in the case when multiple
analysis results are based on different models or different source data._
_Therefore it is proposed that the result of Management Data Analytics should
contain an attribute which indicates the degree of confidence of the
analysis._
NOTE: How to evaluate a degree of confidence and how it should be expressed as
an attribute are not addressed in this study. More work is needed in this
area.
# 7 Conclusions and recommendations
The technical report described the MDA functionality and service framework
(see clause 4), MDA process, MDA role in the management loop, and interactions
between MDAS producer and NWDAF and gNB (see clause 5).
The technical report identified and documented a wide range of use cases for
management data analytics and the MDA management aspects, derived the
corresponding potential requirements, and developed and evaluated the possible
solutions (see clause 6).
For enhancement of MDAS in normative work in 3GPP Rel-17, it is recommended:
To describe:
> > MDA service framework illustrating the service relations between MDAS
> producer, potential MDAS consumers and other network/management
> functions/services, including:
>
> >> relation of MDAS producer with NWDAF;
>
> >> relation of MDAS producer with other management functions/services;
>
> >> relation between cross domain MDAS producer and domain specific MDAS
> producer, including 3GPP RAN/CN domain and non-3GPP management domains
> (e.g., transport, virtualized resources);
>
> >> other relations related to MDA, where necessary.
>
> > MDA role in the management loop (including open control loop and closed
> control loop supporting Closed Loop SLS Assurance (COSLA) and eCOSLA).
>
> > How the MDA output is aligned to Analytics Insights defined in COSLA and
> eCOSLA work.
And, to specify:
> > MDA input and report (output):
>
> >> Common information elements of MDA report (output);
>
> >> 3GPP domain specific MDA input data (including new performance
> measurements, NRMs, MDT data, etc.) and MDA output attributes (as extension
> to the common attributes of MDA report) pertaining to the specific MDA use
> cases, based on the use cases studied, evaluated, concluded and recommended
> in TR 28.809 with the following issues prioritized for Rel-17:
>
> **\- Alarm incident analysis**
>
> **-** SLS **analysis** related issues
>
> **\- MDA assisted** Energy **Saving**
>
> **-** Coverage **related issues**
>
> **\- Mobility** management **related issues**
>
> >> Mechanisms to address multiple relevant use cases by a consolidated MDA
> report.
>
> > Requirements and mechanisms on MDA report request and reporting;
>
> > Requirements on retrieval of historical data related to MDA (including MDA
> input and output), and mechanisms by reusing the solutions for historical
> data handling defined by work item MADCOL (UID: 880028) and possibly other
> work items if relevant;
>
> > Support for ML model training for MDA, where ML can be applicable without
> introducing tight coupling and dependencies on MDAS.
#