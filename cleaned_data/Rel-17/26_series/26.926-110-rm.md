# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
In the present document, modal verbs have the following meanings:
**shall** indicates a mandatory requirement to do something
**shall not** indicates an interdiction (prohibition) to do something
The constructions \"shall\" and \"shall not\" are confined to the context of
normative provisions, and do not appear in Technical Reports.
The constructions \"must\" and \"must not\" are not used as substitutes for
\"shall\" and \"shall not\". Their use is avoided insofar as possible, and
they are not used in a normative context except in a direct citation from an
external, referenced, non-3GPP document, or so as to maintain continuity of
style when extending or modifying the provisions of such a referenced
document.
**should** indicates a recommendation to do something
**should not** indicates a recommendation not to do something
**may** indicates permission to do something
**need not** indicates permission not to do something
The construction \"may not\" is ambiguous and is not used in normative
elements. The unambiguous constructions \"might not\" or \"shall not\" are
used instead, depending upon the meaning intended.
**can** indicates that something is possible
**cannot** indicates that something is impossible
The constructions \"can\" and \"cannot\" are not substitutes for \"may\" and
\"need not\".
**will** indicates that something is certain or expected to happen as a result
of action taken by an agency the behaviour of which is outside the scope of
the present document
**will not** indicates that something is certain or expected not to happen as
a result of action taken by an agency the behaviour of which is outside the
scope of the present document
**might** indicates a likelihood that something will happen as a result of
action taken by some agency the behaviour of which is outside the scope of the
present document
**might not** indicates a likelihood that something will not happen as a
result of action taken by some agency the behaviour of which is outside the
scope of the present document
In addition:
**is** (or any other verb in the indicative mood) indicates a statement of
fact
**is not** (or any other negative verb in the indicative mood) indicates a
statement of fact
The constructions \"is\" and \"is not\" do not indicate requirements.
# Introduction
Media services in general, but also in particular eXtended Reality (XR) and
Cloud Gaming are some of the most important 5G media applications under
consideration in the industry. XR is an umbrella term for different types of
realities and refers to all real-and-virtual combined environments and human-
machine interactions generated by computer technology and wearables. It
includes representative forms such as Augmented Reality (AR), Mixed Reality
(MR) and Virtual Reality (VR) and the areas interpolated among them.
**On XR and Cloud Gaming traffic with high throughput, low latency and high
reliability requirements, it is important to consider system aspects of such
services. If the traffic requirements of the XR and Cloud Gaming service are
flexible (e.g., the underlying architecture allows adaptation of content),
then the capacity of the service can be studied by assessing the delay,
throughput and reliability variations with increasing number of users in the
system. In order to properly study this, detailed traffic characteristics are
necessary.**
Based on this, the present provides traffic models and quality evaluation
methods for different media and eXtended Reality (XR) Services. In order to
address this, generic modelling considerations are introduced and for
different services reference designs, simulation models and suitable quality
metrics are reported. This information permits to obtain accurate information
on exact bitrate and delay requirements in uplink and downlink, develop
detailed traffic traces, develop suitable statistical models for media and XR
traffic and to evaluate the expected media quality of such services. The
information may be used by other 3GPP groups in order to assess media quality
for different configuration of 5G System parameters, as well as for evaluating
the requirements in terms of QoS for XR and media services.
# 1 Scope
The present document provides traffic models and quality evaluation methods
for different media and eXtended Reality (XR) Services. In order to address
this, generic modelling considerations are introduced and for different
services reference designs, simulation models and suitable quality metrics are
reported. This information permits to obtain accurate information on exact
bitrate and delay requirements in uplink and downlink, develop detailed
traffic traces, develop suitable statistical models for media and XR traffic
and to evaluate the expected media quality of such services. The information
may be used by other 3GPP groups in order to assess media quality for
different configuration of 5G System parameters, as well as for evaluating the
requirements in terms of QoS for XR and media services.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TR 26.925, \"Typical traffic characteristics of media services on
3GPP networks\"
[3] 3GPP TR 26.928, \"Extended Reality (XR) in 5G\"
[5.1i] 3GPP TS 26.501, \"System architecture for the 5G System (5GS)\"
# 3 Definitions of terms, symbols and abbreviations
## 3.1 Terms
For the purposes of the present document, the terms given in 3GPP TR 21.905
[1] and the following apply. A term defined in the present document takes
precedence over the definition of the same term, if any, in 3GPP TR 21.905
[1].
**trace:** a well-defined format to describe a sequence of timed data units
together with relevant metadata for system simulation.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
\ \
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
\ \
# 4 Overview and Scope
Editor's Note: add the agreed information from permanent document
# 5 General Design and Modelling
## 5.1 Introduction
The system and RAN model used in this Technical Report for XR Traffic analysis
follows the 5G System architecture as defined in TS 23.501 [5.1i]. In
particular, the user plane aspects are considered as showing in Figure 5.1-1
for which an XR Server is in the external DN or in a trusted DN, and the XR
Device is a 5G UE. The exchange of data is assumed to be carried out using the
5G System. Interfaces to the 5G Core functions for charging, QoS control, etc.
are not considered in the below diagram, for details refer to TS 23.501.
Figure 5.1-1 Simplified Architecture for XR end-to-end services
In order to identify the traffic characteristics and the impacts of media and
XR applications on 5G Systems and radio access networks, an appropriate
modelling of both, the XR traffic as well as of the 5G System is desirable,
also to identify the interaction of the two components. Note that the XR
Server may be hosted in the cloud or may be hosted on an edge.
Details on the modelling of the 5G System and RAN as well as simulation
parameters are provided in clause 5.2.
## 5.2 End-to-end Modelling of XR Traffic
### 5.2.1 Architecture and System Model
In order to support modelling of end-to-end XR Traffic, an abstracted
architecture and system model is introduced in Figure 5.2.1-1. This system is
used as a baseline and may be refined for specific traffic classes. The system
follows the basic building blocks from clause 5.1 but provides details for
each of those aspects. In order to support simpler system modelling and
simulation, the interfaces are supported by _traces_. Traces define a well-
defined format to describe a sequence of timed data units together with
relevant metadata for system simulation. An overview of traces is introduced
in clause 5.2.2, details are provided in each of the functions.
In order to split the tasks across different 3GPP working groups and
companies, an approach as introduced in Figure 5.2.1-1 is considered:
  * A content model based on traces may be provided based on company input providing sufficient information on a rendered application, e.g. a game or a scene.
  * Based on these content model traces, codec centric experts defines a content encoding and delivery model taking into account a system design, for example based on TR26.928 [3]. The system model includes encoding, delivery, decoding and also a quality definition, providing packet traces.
  * 5G System and RAN simulations may be carried out by the 3GPP radio experts using the packet traces to simulate different traffic characteristics and evaluate different performance options.
An overview of the functions is provided below:
1) Content Model: Provides typical characteristics for the XR content model
for video, audio and potentially other data. This content is rendered for
XR/CG consumption -- for details refer to clause 5.5.
2) Content Encoding Model: Provides the details for the content encoding in
order to meet certain objectives. This includes the generation of sequences of
application data units (slices, video frames, audio frames, etc.) and the
incurred timestamp of each of the units is available. For details refer to
clause 5.6.
3) Content Delivery Model: provides details on content delivery, for example
packetization, delay jitter, but possibly also more sophisticated models such
as retransmission, TCP operations and so on. This also includes emulation of
5G Core Network. It produces packet traces (timestamp and size of each packet)
based on traces of application data units. For details refer to clause 5.7.
4) Core Network Model: This model emulates the core network behaviour in terms
of delay, latency and packet losses in the core network, i.e. the interface
between the UPF and gNB. For details refer to clause 5.4.
5) Packet Radio Model: The packet radio model receives sequences/traces of
packets at a given time and of a specified size. The packets may have
additional metadata assigned that can potentially be used by the radio
simulator. The packet traces are provided for multiple users and reflect
typical traffic characteristics. The RAN simulator provides packet traces
after delivery that reflect the occurred delays and losses for each user. For
details refer to clause 5.4.
6) Content Receiver Model: The content receiver converts the packet traces
into application units taking into account delays and losses occurred. It may
also model additional functions such as retransmissions or FEC, if applicable.
For details refer to clause 5.7.
7) Content Decoding Model: The decoding model uses the received application
units to model the reconstruction of timed data (video frames, audio)
considering delays, losses and also content model properties (error
propagation, refresh data and so on). For details refer to clause 5.6.
8) Quality Evaluation: A quality evaluation tool is provided that takes into
account traces to compute different quality metrics based on packet,
application unit and media quality. For details refer to clause 5.8.
9) Uplink Model: A model for the uplink traffic in a similar fashion also
providing packet traces. For details refer to clause 5.9.
Several of the functions are initialized by appropriate configurations. An
overview of configurations is introduced in each of the functions, if needed.
Figure 5.2.1-1 General architecture and system model for XR Traffic
### 5.2.2 Interfaces and Traces
#### 5.2.2.1 Introduction
Generally, traces include the following information:
\- Time series of data units
\- Size of each data units
\- Association to the application
\- Additional \"metadata\"
Traces can also directly be fed to a quality evaluation tool obtain the
application quality based on the simulation as shown in clause 5.8. The
benefit of traces compared to first order statistical model are as follows:
  * traces to represent time correlations.
  * traces can be used to assign additional metadata in the time series
  * statistical models can be developed based on time series
  * traces can be directly fed to quality evaluation tool
The trace formats are defined as CSV files and follow the recommendation in
RFC4180 \"Common Format and MIME Type for CSV Files\". For CSV files, headers
are added to identify the data types. From existing CSV types, the online tool
https://csv.openbridge.dev/ has been used to
1) Validate the csv file
2) Create a schema for the files.
#### 5.2.2.2 Overview of Traces
Figure 5.2.1-1 shows different traces, associated to different interfaces. The
following traces are defined:
1) V-Traces: Video traces provide a time series of the statistics/complexity
of a video frame. Such a V-Trace is expected to provide sufficient information
such that a coding model can create suitable statistics statics for encoded
video streams at the output. For details refer to clause 5.5.
2) S-Traces: Slice traces provide a time series of encoded video application
data units, referred to as slices as known from H.264/AVC and H.265/HEVC
suitable application data units as outputs from a video codecs. S-Traces are
expected to provide sufficient information to the content delivery module to
create IP packets. For details refer to clause 5.6.
3) P-Traces: P Traces provide a time series of IP packets, possibly associated
with different application flows and/or QoS Flows. In addition, they provide
metadata information that may be beneficial for advanced delivery. For details
refer to clause 5.3.
4) P'-Traces have the same format as P-Traces, but in addition may include
losses and delays observed due to the delivery over a network.
5) S'-Traces are have the same format as S-Traces, but in addition may include
information on losses and distortions due to the delivery over a network. For
details refer to clause 5.7.
6) V'-Traces: Video receive traces provide a time series of received video
frames together with an associated encoding and delivery quality. For details
refer to clause 5.6.
### 5.2.3 Benefits and Limitations
Tbd
## 5.3 P-Traces
Packet Traces (P-Traces) are defined to emulate the arrival of IP packets with
associated metadata at certain network nodes. A P-Trace format together with
semantics is defined in Table 5.3-1.
The first row just keeps an index of the packets.
The second to fourth data row information is considered to be available in a
general delivery environment and are summarized as baseline parameters.
  * availability_time: A relative time from the start of the trace when this packet is available in the delivery function
  * size: the size of the packet in octets
  * flow_id: an id to a flow in order to differentiate different QoS flows to identify a PDU session aligned with the QFI.
Table 5.3-1 P-Trace format
+-----------------------+--------+-----------------------------+ | Name | Type | Semantics | +=======================+========+=============================+ | number | BIGINT | Unique packet number in the | | | | delivery | +-----------------------+--------+-----------------------------+ | Baseline parameters | | | +-----------------------+--------+-----------------------------+ | availability_time | BIGINT | Availability time of packet | | | | for next processing step | | | | relative to start time 0 in | | | | microseconds (0 means | | | | lost). | +-----------------------+--------+-----------------------------+ | size | BIGINT | packet size in bytes. | +-----------------------+--------+-----------------------------+ | flow_id | BIGINT | an id to the flow in order | | | | to differentiate different | | | | QoS flows | +-----------------------+--------+-----------------------------+ | Cross-Layer Parameter | | | +-----------------------+--------+-----------------------------+ | buffer | BIGINT | The associated eye buffer | | | | 1=left 2=right | | | | | | | | In general, differentiates | | | | application traffic for | | | | different buffers, for | | | | example audio, video, left | | | | eye, right eye. | +-----------------------+--------+-----------------------------+ | delay | BIGINT | Delay observed of the | | | | packet in the last | | | | processing step (-1 means | | | | lost) | +-----------------------+--------+-----------------------------+ | render_timing | BIGINT | the rendering generation | | | | timing associated to the | | | | media included in the | | | | packet. | +-----------------------+--------+-----------------------------+ | number_in_unit | BIGINT | The number of the packet | | | | within the unit (slice), | | | | start at 1 | +-----------------------+--------+-----------------------------+ | last_in_unit | BIGINT | Indicates if this is the | | | | last packet in the | | | | slice/unit 0=no, 1=yes | +-----------------------+--------+-----------------------------+ | type | BIGINT | The data type of the unit | | | | | | | | 0 unknown | | | | | | | | For video 1=intra 2=inter | +-----------------------+--------+-----------------------------+ | importance | BIGINT | assigned relative | | | | importance information | | | | (higher number means higher | | | | importance) | +-----------------------+--------+-----------------------------+
In addition, it is considered that an application may provide additional
metadata per packet that if present, may be used by the delivery system.
Different definitions are provided in the P-Trace format. The signaling of
this data a 5G System is for further study. This information would not be
available to RAN delivery in the incoming IP packets. These parameters are
summarized as cross-layer parameters.
5GS through the QoS model may provide the ability that application streams are
separated into different streams or the streams are at least marked
accordingly. Details would need further discussion with SA2 on how application
traffic and packet properties or stream properties can be matched to the 5G
System architecture. This aspect may be considered, once it is identified that
certain "cross-layer" markings make sense.
Based on this, it is suitable to use the basic packet trace structure as
defined in Table 5.3-1 and that RAN1 can assume that RAN1 may have access to
the above packet information. The P-Trace may be extended based on new
findings.
For any system simulations, it is important that simulation are carried out
only using the baseline parameters. Additional cross-layer parameters may be
used as defined above. New packet-based markers may be added.
It is also considered that realization in the 5G System is only addressed once
the benefits of an approach are identified.
## 5.4 5G System Model and Simulation
### 5.4.1 5G System Model
In order to further model the 5G system, an alignment with the 5GS QoS model
as defined in clause 5.7 of TS 23.501 is considered appropriate. The interface
between the application domain and the 5G System is assumed to be based on QoS
Flows. A QoS Flow is the finest granularity of QoS differentiation in the PDU
Session. A QoS Flow ID (QFI) is used to identify a QoS Flow in the 5G System.
User Plane traffic with the same QFI within a PDU Session receives the same
traffic forwarding treatment **_(e.g._** scheduling, admission threshold).
The QFI is carried in an encapsulation header on N3 without any changes to the
e2e packet header and is used to uniquely identify a PDU Session. The
principle for classification and marking of User Plane traffic and mapping of
QoS Flows to AN resources is illustrated in Figure 5.7.1.5-1 of TS 23.501 and
repeated in Figure 5.1-2.
Figure 5.2.1-1: The principle for classification and User Plane marking for
QoS Flows and mapping to AN Resources (see TS 23.501 [5.1i], Figure 5.7.1.5-1)
In the downlink, incoming data packets are classified by the UPF based on the
Packet Filter Sets of the DL PDRs in the order of their precedence. The UPF
conveys the classification of the User Plane traffic belonging to a QoS Flow
through an N3 User Plane marking using a QFI. The AN binds QoS Flows to AN
resources (i.e. Data Radio Bearers of in the case of 3GPP RAN).
In UL, for relevant PDU Session of Type IP, the UE evaluates UL packets
against the UL Packet Filters in the Packet Filter Set in the QoS rules based
on the precedence value of QoS rules in increasing order until a matching QoS
rule is found. The UE uses the QFI in the corresponding matching QoS rule to
bind the UL packet to a QoS Flow. The UE then binds QoS Flows to AN resources.
TS 23.501 defines the following QoS characteristics:
  * Clause 5.7.3.2: Resource type (Non-GBR, GBR, Delay-critical GBR)
\- A GBR QoS Flow uses either the GBR resource type or the Delay-critical GBR
resource type. The definition of PDB and PER are different for GBR and Delay-
critical GBR resource types, and the MDBV parameter applies only to the Delay-
critical GBR resource type.
\- A Non-GBR QoS Flow uses only the Non-GBR resource type.
\- Clause 5.7.3.3: Priority Level;
\- The Priority Level associated with 5G QoS characteristics indicates a
priority in scheduling resources among QoS Flows. The lowest Priority Level
value corresponds to the highest priority.
\- Clause 5.7.3.4: Packet Delay Budget (including Core Network Packet Delay
Budget);
\- The Packet Delay Budget (PDB) defines an upper bound for the time that a
packet may be delayed between the UE and the N6 termination point at the UPF.
For a certain 5QI the value of the PDB is the same in UL and DL. In the case
of 3GPP access, the PDB is used to support the configuration of scheduling and
link layer functions (e.g., the setting of scheduling priority weights and
HARQ target operating points).
\- For GBR QoS Flows using the Delay-critical resource type, a packet delayed
more than PDB is counted as lost if the data burst is not exceeding the MDBV
within the period of PDB and the QoS Flow is not exceeding the GFBR. For GBR
QoS Flows with GBR resource type not exceeding GFBR, 98 percent of the packets
shall not experience a delay exceeding the 5QI\'s PDB.
\- Clause 5.7.3.5: Packet Error Rate;
\- The Packet Error Rate (PER) defines an upper bound for the rate of PDUs
(e.g. IP packets) that have been processed by the sender of a link layer
protocol (e.g. RLC in RAN of a 3GPP access) but that are not successfully
delivered by the corresponding receiver to the upper layer (e.g. PDCP in RAN
of a 3GPP access).
\- Thus, the PER defines an upper bound for a rate of non-congestion related
packet losses. The purpose of the PER is to allow for appropriate link layer
protocol configurations (e.g. RLC and HARQ in RAN of a 3GPP access). For every
5QI the value of the PER is the same in UL and DL. For GBR QoS Flows with
Delay-critical GBR resource type, a packet which is delayed more than PDB is
counted as lost, and included in the PER unless the data burst is exceeding
the MDBV within the period of PDB or the QoS Flow is exceeding the GFBR.
\- Clause 5.7.3.6: Averaging window (for GBR and Delay-critical GBR resource
type only);
\- Each GBR QoS Flow shall be associated with an Averaging window. The
Averaging window represents the duration over which the GFBR and MFBR shall be
calculated (e.g. in the (R)AN, UPF, UE).
\- Clause 5.7.3.7: Maximum Data Burst Volume (for Delay-critical GBR resource
type only).
\- Each GBR QoS Flow with Delay-critical resource type shall be associated
with a Maximum Data Burst Volume (MDBV).
\- MDBV denotes the largest amount of data that the 5G-AN is required to serve
within a period of 5G-AN PDB.
\- Every standardized 5QI (of Delay-critical GBR resource type) is associated
with a default value for the MDBV (specified in QoS characteristics Table
5.7.4.1). The MDBV may also be signalled together with a standardized 5QI to
the (R)AN, and if it is received, it shall be used instead of the default
value.
\- The MDBV may also be signalled together with a pre-configured 5QI to the
(R)AN, and if it is received, it shall be used instead of the pre-configured
value
### 5.4.2 Core Network Model
The core network is expected to operate on constant bitrate channel from the
UPF to the gNB.
In the model showing in Figure 5.4.2-1, the core network model gets as input a
P-Trace and delivers a P-Trace. Packets provided to the core network are
expected to be delivered from the UPF to the gNB at a constant bitrate of
bitrate _R_ , whereby each packet is delivered at a delivery time being the
later of the two
1) The arrival time at the UPF
2) The delivery end time of the previous packet
The arrival time at the gNB is modelled as the sum of the delivery time and
the packet size divided by the bitrate _R_.
The core network delivery is considered to be error-free.
Figure 5.4.2-1 Core Network Model
The modelling of such a delay is for example show in Figure 5.4.2-2 for
different packet size models.
{width="2.9895833333333335in"
height="1.5833333333333333in"}{width="3.5104166666666665in"
height="1.78125in"}
Figure 5.4.2-2 Packet size and latency addition in core network model
The configuration parameters for the core network model are
\- the input packet trace
\- the core network bitrate
\- the output packet trace
### 5.4.3 RAN Model
The RAN delivery may be modelled by a function that receives P-Traces as input
for each simulated user as shown in Figure 5.4.3-1 and produces a P'-Trace at
the output for the primary user. The other users are only used from
statistical competing traffic. Configuration parameters for the simulation are
left to RAN experts.
Figure 5.4.3-1 Packet Radio Network Model
However, the usage of traces for RAN simulations is complex and there is a
preference to operate with statistical models that emulate the arrival time
and size of the packets. The main reason is for example, that RAN1 needs to
simulate multiple cells at the same time, for example 63, for interference
considerations. A statistical model is preferred.
### 5.4.4 Test Channels
P-Traces as produced by XR applications may be used by RAN for complex RAN
simulations. However, at the same time it is important to evaluate different
XR application and encoding configurations that result in a P-Trace and then
compare different settings. For this purpose, test channels are defined
addressing two aspects:
\- Permit to emulate typical radio conditions in terms delays and losses.
\- Permit to evaluate the application quality for different representative
radio conditions.
The test channel aligns with the 5G System and QoS Model as introduced in
clause 5.4.1.
A test channel is defined by the following models:
\- Packet Error Rate:
\- Definition: the rate of PDUs (e.g. IP packets) that have been processed by
the sender of a link layer protocol (e.g. RLC in RAN of a 3GPP access) but
that are not successfully delivered by the corresponding receiver to the upper
layer (e.g. PDCP in RAN of a 3GPP access).
\- Model: iid loss model independent of the packet size with parameter _PLR_
\- Packet Delay:
\- Definition: the time that a packet may be delayed between the UE and the N6
termination point at the UPF.
\- Model: iid distributed latency between 0 and a max_delay
Other parameters and modelling aspects for test channels are for further
study.
## 5.5 Content Modelling
### 5.5.1 Introduction
In order to evaluate the traffic characteristics and the quality of XR
applications, it is relevant to operate with realistic source data.
Preferably, the traffic of existing services is evaluated, for example taking
P-Traces from existing XR services. However, in doing so, there are
limitations
1) in accessing the data
2) in terms of legal restrictions on providing such data
3) in terms of making use of such data for quality evaluation
4) in terms of understanding the structure and details of the contained
traffic
5) in terms of understanding the options and impact of different system
configurations
6) and for other reasons
Based on this, an approach is proposed in this report as shown in Figure
5.5.1-1.
Figure 5.5.1-1 Content Modelling Approach
It is assumed that the raw media data is generated based on an XR/game session
for one or several popular games, using typical pose traces and interactions,
for example by a human playing the game. This information may be running over
several minutes or even hours in order to create sufficiently representative
statistics.
The output of the XR rendering engine is video representing texture, primitive
and eye buffers.
For service and applications not relying on XR engines, other approaches may
be taken. This is for further study and discussed as part of the specific
traffic characteristics.
### 5.5.2 V-Trace Generation
The basic idea behind the generation of V-Traces is motivated by two main
aspects:
1) Handling of several minutes or hours of raw data from a game engine is too
significant. A statistical version is preferred in order to properly handle
the amount of data
2) Representative games are typically attached with copyright and licensing
terms and hence, the raw media data cannot be shared publicly.
Based on this V-Trace generation has been initiated by
\- using a representative game
\- using a repeatable pose and interaction trace for the game
\- using a high-quality output of a game engine for each eye buffer, encoded
for example in H.264(AVC)
\- decode these video sequences and store this raw data for proper model
encoding to generate a trace. As an example, the decoded video sequence is 2K
x 2K at 120 fps for each source buffer.
A shorter version of the raw data is then encoded in with different parameters
to create some statistical output that allows to estimate the performance of a
video codec when used with different configurations. For this purpose,
initially a set of encoding parameters are used to understand the impact of
each of the parameters for the resulting bitrate and quality. This information
is then used for the modelling.
Based on the above model findings, it considered to only generate two V-Traces
for the entire sequence to keep data handling manageable as shown in Figure
5.5.2-1. One V-Trace is generated for Intra coding only, the other one is
generated for predictive coding.
The V-Traces are combined to document relevant statistics for each frame.
Figure 5.5.2-1 V-Trace Generation
For the purpose of this TR, a modelling is provided Annex A.
### 5.5.3 V-Trace Format
For each frame in the video sequence, the information as documented in Table
5.5.3-1 is provided.
Table 5.5.3-1 V-Trace Format
Name Type Semantics
* * *
time_stamp_in_micro_s BIGINT Associated rendering time of the frame
encode_order BIGINT The display order of the frames i_qp BIGINT Quantization
Parameter decided for the I frame. i_bits BIGINT Number of bits consumed by
the I frame. i_y_psnr BIGINT Peak signal to noise ratio for Y planes in dB and
multiplied by 1000 for I frame. i_u_psnr BIGINT Peak signal to noise ratio for
U planes in dB and multiplied by 1000 for I frame. i_v_psnr BIGINT Peak signal
to noise ratio for V planes in dB and multiplied by 1000 for I frame.
i_yuv_psnr BIGINT Peak signal to noise ratio for weighted Y, U and V planes in
dB and multiplied by 1000 for I frame. i_ssim BIGINT Quality metric that
denotes the structural similarity between frames for I frame. i_ssim_d_b
BIGINT Quality metric that denotes the structural similarity between frames in
dB for I frame. i_total_frame_time_ms BIGINT Total time spent to encode the
frame for I frame. p_poc BIGINT The display order of the frames for P p_qp
BIGINT Quantization Parameter decided for the P frame. p_bits BIGINT Number of
bits consumed by the P frame. p_y_psnr BIGINT Peak signal to noise ratio for Y
planes in dB and multiplied by 1000 for P frame. p_u_psnr BIGINT Peak signal
to noise ratio for U planes in dB and multiplied by 1000 for P frame. p_v_psnr
BIGINT Peak signal to noise ratio for V planes in dB and multiplied by 1000
for P frame. p_yuv_psnr BIGINT Peak signal to noise ratio for weighted Y, U
and V planes in dB and multiplied by 1000 for P frame. p_ssim BIGINT Quality
metric that denotes the structural similarity between frames for P frame.
p_ssim_d_b BIGINT Quality metric that denotes the structural similarity
between frames in dB for P frame. p_total_frame_time_ms BIGINT Total time
spent to encode the frame for P frame. intra DOUBLE PRECISION Percentage of
intra Coding Units in P frame merge DOUBLE PRECISION Percentage of merge
Coding Units in P frame skip DOUBLE PRECISION Percentage of skip Coding Units
in P frame inter DOUBLE PRECISION Percentage of inter Coding Units in P frame
### 5.5.4 Other media types
For other media types, no dedicated trace format is defined.
## 5.6 Media Coding and Decoding Modelling
### 5.6.1 General
Media may be encoded in many different ways in order to meet application,
service and delivery requirements. Whereas decoders in media coding are
typically fully specified, there typically exist significant options in the
encoding, in particular for video, but also in general, apply different
encoding methods and functionalities.
These functionalities may impact:
  * Bitrate of the media stream, both in terms of short-term (for example over a window of 1 second) as well as average bitrate of the media stream. The bitrate may also be dynamically adjusted, for example based on content properties and/or network conditions. Changes in bitrate impact the quality of the media stream.
  * Delays in encoder or decoder due to processing or algorithmic functionalities. For example the time when captures/generated media sample hits encoder and the time when it leaves the decoder may be configurable. Permitting longer delays typically results in better quality.
  * Output data unit properties determining the size of the generated data units and packets. Adding constraints typically results in additional bitrates
  * Error resilience and random access includes functionalities in the media stream that support mitigation of potential losses of data. Adding such functionalities typically increases the bitrate or reduces the quality.
More details for video media type is provided in clause 5.6.2. Other media
types are for further study.
### 5.6.2 Video Coding and Decoding
Figure 5.6.2-1 provides an overview of content encoding and decoding models
with focus on video. Encoding makes use of V-Traces and based on
configurations and possibly feedback from the decoder, encoding of individual
media samples/frames is carried out. The encoder produces a sequence of slices
according to the format defined in clause 5.6.3. After delivery, a content
decoding model generates a sequence of V'-Traces that can be used by a Quality
evaluation tool. The decoding model may also provide feedback to the encoder,
for example on lost or late frames.
Figure 5.6.2-1 Media Coding and Decoding Modelling
For video encoding, among others the following parameters may be considered:
  * Codec in use, for example H.264/AVC or H.265/HEVC
  * Encoding patterns:
    * Usage of B and P pictures
    * Latency settings: P pictures only, look-ahead units only 0 (for minimum latency)
  * Rate control to be used, for example
    * CBR with bitrate
    * Capped VBR with bitrate
    * constant rate factor (CRF), i.e. constant quality with weighted rate adjustments
    * constant QP (CQP), i.e. constant quantization parameters, or
    * feedback-based using information on loss an delay statistics
  * Slice settings, examples
    * 1 slice per frame,
    * 1 slice per every 16 pixel row,
    * 8 slices per video frame
  * Intra settings and error resilience:
    * Regular IDR Patterns, e.g. 16-th frame,
    * Regular Gradual Decoder Refresh Pattern,
    * adaptive Intra,
    * feedback based Intra,
    * feedback based predication and ACK-based,
    * feedback-based prediction and NACK based
  * Complexity settings for encoders
  * Pre-Encoding delays: generation timestamp of the rendered frame until this frame is encoded
  * Encoding delays: the time of encoder from receiving the frame until a slice is produced.
Based on these settings, a slice trace S-Trace can be generated based on the
format in clause 5.6.3. A slice delivery unit now uses a slice trace and
provides as output an S'-Trace as also provided in clause 5.6.3. The delivery
impacts the slice delivery in one or multiple of the following ways:
1) The slice is delayed, i.e. a delivery timestamp is assigned to the slice in
the S'-Trace. This can be used to determine if the slice is received in time
2) The slice is corrupted, i.e. the slice is entirely, only a correct prefix
of the slice available or the slice was fully received for the decoding model.
A decoding model now takes into account the S'-Trace as well as the V-Trace in
order to generate a decoding model. For this, coding units in a video frame
may be viewed individually and get assigned a state:
  * A coding unit spanning a certain area of the video frame is _correct_ if and only if it is received correctly and it predicts for a non-damaged coding unit. Predicting from non-damaged units means that both spatial prediction as well as temporal prediction coding units are correct
  * If any of the two conditions do not hold, the coding unit state is _damaged_.
Note that only non-predicted coding units will eventually remove damaged
areas.
An important factor for the quality of the video is the percentage of lost
area.
A specific algorithm to model the generation of slices as well as the decoding
of received slices is provided in Annex X.
### 5.6.3 S-Trace and S'-Trace Format
For each generated slice, an S-Trace format is provided according to Table
5.6.4-1.
Table 5.6.4-1 S-Trace Format
+---------------------------+--------+-----------------------------+ | Name | Type | Semantics | +===========================+========+=============================+ | index | BIGINT | Unique index increased by 1 | | | | and indexing this row in | | | | the S-Trace file. | +---------------------------+--------+-----------------------------+ | time_stamp_in_micro_s | BIGINT | Availability time of slice | | | | after encoder relative to | | | | start time 0 in | | | | microseconds. | +---------------------------+--------+-----------------------------+ | size | BIGINT | Slice size in bytes. | +---------------------------+--------+-----------------------------+ | render_timing | BIGINT | the rendering generation | | | | timing associated to the | | | | frame | +---------------------------+--------+-----------------------------+ | buffer | BIGINT | The associated eye buffer | | | | 1=left 2=right | | | | | | | | In general, differentiates | | | | application traffic for | | | | different buffers, for | | | | example audio, video, left | | | | eye, right eye. | +---------------------------+--------+-----------------------------+ | frame_index | BIGINT | Frame index to identify the | | | | frame buffer | +---------------------------+--------+-----------------------------+ | type | BIGINT | The slice type 1=intra | | | | 2=inter | +---------------------------+--------+-----------------------------+ | importance | BIGINT | assigned relative | | | | importance information | | | | (higher number means higher | | | | importance) | +---------------------------+--------+-----------------------------+ | start_address_cu | BIGINT | start address of CU in | | | | slice | +---------------------------+--------+-----------------------------+ | number_cus | BIGINT | total number of CUs in | | | | slice | +---------------------------+--------+-----------------------------+ | frame_file | STRING | Reference to frame file | | | | containing information for | | | | each CU including index and | | | | eyebuffer | +---------------------------+--------+-----------------------------+
For each frame, an entry into a binary file is generated that documents the
information according to Table 5.6.4-2.
Table 5.6.4-2 Frame-related metadata from encoding
Name Type Semantics
* * *
address BIGINT Address of CU in frame. size BIGINT CU size in bits mode BIGINT
The mode of the CU 1=intra, 2=merge, 3=skip, 4=inter reference BIGINT The
reference frame of the CU 0 n/a, 1=previous, 2=2 in past, etc. qpnew BIGINT
the QP decided for the CU psnr_y BIGINT the estimated Y-PSNR for the CU in dB
multiplied by 1000 psnr_yuv BIGINT the estimated weighted YUV-PSNR for the CU
dB multiplied by 1000
The S'-Trace format after the delivery unit is shown in Table 5.6.4-3.
Table 5.6.4-3 S'-Trace Format
Name Type Semantics
* * *
index BIGINT Unique slice index increased by 1 and indexing this row in the
S-Trace file. time_stamp_in_micro_s BIGINT Availability time of slice at
decoder relative to start time 0 in microseconds. recovery_position BIGINT
Recovered unit position (0 => lost, in between, full) size BIGINT Original
size of slice/data unit in bytes. render_timing BIGINT the rendering
generation timing associated to the frame start_address_cu BIGINT start
address of CU in slice number_cus BIGINT total number of CUs in slice
frame_file STRING Reference to frame file containing information for each CU
### 5.6.4 Other media types
For other media types, no dedicated modelling is defined.
## 5.7 Content Delivery Modelling
Editor's Note: add the agreed information from permanent document
## 5.8 Quality Metrics and Computation
Editor's Note: add the agreed information from permanent document
In summary, for proper quality evaluation, SA4 needs P'-traces to identify
application quality.
Agreement 2: It is agreement
\- that SA4 develops quality models based on P'-traces
Test Channel Results
{width="5.8125in" height="3.1458333333333335in"}
## 5.9 Uplink Modeling
Editor's Note: add the agreed information from permanent document
# 6 XR Split Rendering
## 6.1 Introduction
Editor's Note: add the agreed information from permanent document
## 6.2 Reference System Design
Editor's Note: add the agreed information from permanent document clause
6.2.1, 6.2.2 and 6.2.3
## 6.3 Simulation System
Editor's Note: add the agreed information from permanent document
## 6.4 Recommended Configurations
Editor's Note: add the agreed information from permanent document
## 6.5 Traces and Statistical Models
Editor's Note: add the agreed information from permanent document
## 6.6 Test Channel Results
Editor's Note: add the agreed information from permanent document
# 7 Cloud Gaming
## 7.1 Introduction
Editor's Note: add the agreed information from permanent document
## 7.2 Reference System Design
Editor's Note: add the agreed information from permanent document
## 7.3 Simulation System
Editor's Note: add the agreed information from permanent document
## 7.4 Recommended Configurations
Editor's Note: add the agreed information from permanent document
## 7.5 Traces and Statistical Models
Editor's Note: add the agreed information from permanent document
## 7.6 Test Channel Results
Editor's Note: add the agreed information from permanent document
# 8 AR Conversational
Editor's Note: add the agreed information from permanent document
# 9 Viewport Dependent VR Streaming
Editor's Note: add the agreed information from permanent document
# 10 XR Distributed Computing
Editor's Note: add the agreed information from permanent document
###### ## Annex A: Example Content Coding Modelling
# A.1 Introduction
as follows:
./x265 --input input.yuv --input-res 2048x2048 --preset medium --fps
**[30,60]** \--crf 28 --keyint **[-1,1]** \--slice **1** \--csv
/csv_analfiles/**[i,p]_[60]_[22,28]_[1,8]**.csv \--csv-log-level 2 --log-level
full --rc-lookahead 0 --no-deblock \--bframes 0 --frames 480 --psnr --ssim
--output /output/xxxx.h265
#### 5.5.2.2 Impact of encoding parameters
A shorter version of the sequence is sent through a model encoder for
identifying the impacts of different parameter settings. The following x.265
parameters are used resulting in total with 24 different configurations. The
configurations take into account different frame rates, different quality
factors, an all intra and an all inter predicted sequence, as well as no
slices or 8 slices, as follows:
./x265 --input input.yuv --input-res 2048x2048 --preset medium --fps
**[30,60]** \--crf **[22, 28,34]** \--keyint **[-1,1]** \--slice **[1,8]**
\--csv /csv_analfiles/**[i,p]_[30,60]_[22,28,34]_[1,8]**.csv \--csv-log-level
2 --log-level full --rc-lookahead 0 --no-deblock \--bframes 0 --frames 480
--psnr --ssim --output /output/xxxx.h265
A summary of the bitrates and resulting quality is provided in Table 5.5.2-1
for each of the 24 parameter settings.
Table 5.5.2-1 Bitrates and resulting quality for different configurations
* * *
**Mode** **Frame\** CRF**** Slice**** Data\ **Average\** Average\ **Y\** U\
**V\** YUV\ **SSIM** **SSIM(dB)** **Frames** rate**Rate\ QP** Rate\
PSNR**PSNR** PSNR**PSNR**  
in Mbit/s**Factor**
* * *
**I** 60 22 1 43.77 27.29 19.09 42.58 46.58 46.12 43.52 0.979 16.82 480
**I** 60 22 8 44.16 27.29 19.09 42.57 46.57 46.11 43.51 0.979 16.77 480
**P** 60 22 1 14.14 23.82 21.99 43.46 48.27 47.83 44.60 0.983 17.80 480
**P** 60 22 8 29.64 23.71 21.99 43.37 47.94 47.49 44.45 0.983 17.57 480
**I** 60 28 1 21.56 33.30 25.09 39.07 44.11 43.64 40.27 0.960 14.00 480
**I** 60 28 8 21.85 33.30 25.09 39.06 44.11 43.64 40.26 0.960 13.98 480
**P** 60 28 1 3.86 30.17 27.99 40.36 46.12 45.57 41.73 0.972 15.51 480
**P** 60 28 8 12.33 29.91 27.99 40.13 45.48 44.96 41.40 0.969 15.07 480
**I** 60 34 1 10.55 39.35 31.09 35.68 42.62 42.16 37.36 0.928 11.44 480
**I** 60 34 8 10.76 39.34 31.09 35.65 42.63 42.17 37.34 0.928 11.43 480
**P** 60 34 1 1.30 36.75 33.99 37.16 43.84 43.32 38.77 0.949 12.94 480
**P** 60 34 8 5.59 36.20 33.99 36.85 42.99 42.47 38.32 0.942 12.40 480
**I** 30 22 1 29.43 24.88 19.28 44.08 47.70 47.27 44.93 0.984 17.99 250
**I** 30 22 8 29.66 24.87 19.28 44.07 47.70 47.26 44.92 0.984 17.91 250
**P** 30 22 1 12.78 21.87 22.18 44.43 48.95 48.51 45.50 0.986 18.41 250
**P** 30 22 8 20.59 21.82 22.18 44.42 48.72 48.29 45.44 0.985 18.25 250
**I** 30 28 1 14.54 30.88 25.27 39.07 44.11 43.64 40.27 0.960 14.00 250
**I** 30 28 8 14.71 30.88 25.27 39.06 44.11 43.64 40.26 0.960 13.98 250
**P** 30 28 8 3.82 28.07 28.17 41.23 46.69 46.19 42.53 0.976 16.11 250
**P** 30 28 8 8.34 27.94 28.17 41.09 46.17 45.67 42.30 0.974 15.80 250
**I** 30 34 1 7.14 36.91 31.26 37.07 43.19 42.73 38.54 0.943 12.44 250
**I** 30 34 8 7.26 36.91 31.26 37.05 43.20 42.74 38.53 0.943 12.43 250
**P** 30 34 1 1.29 34.38 34.16 38.09 44.36 43.83 39.59 0.957 13.62 250
**P** 30 34 8 3.71 34.11 34.16 37.86 43.57 43.12 39.23 0.952 13.21 250
* * *
An analysis of the bitrate decrease per CRF/QP increase is shown in Table
5.5.2-2.
Table 5.5.2-2 Bitrate decrease per CRF/QP increase
**1 Slices**
* * *
I only P only  
22->28/60 28->34/60 22->28/30 28->34/30 22->28/60 28->34/60 22->28/30
28->34/30  
2.03 2.04 2.02 2.04 3.85 3.23 3.46 3.18 8 Slices  
I only P only  
22->28/60 28->34/60 22->28/30 28->34/30 22->28/60 28->34/60 22->28/30
28->34/30  
2.02 2.04 2.02 2.04 3.85 3.23 3.46 3.18
The bitrate increase for I-frames vs P-frames for different configurations is
shown in Table 5.5.2-3.
Table 5.5.2-3 Bitrate decrease per CRF/QP increase
Frame I/P 22/60/1 I/P 28/60/1 I/P 34/60/1 I/P 22/60/8 I/P 28/60/8 I/P 34/60/8
* * *
Average 3.17 6.10 10.03 1.51 1.81 1.81
Finally, the bitrate increase for 60fps vs 30fps is shown in Table 5.5.2-4.
Table 5.5.2-4 Bitrate increase for 60 fps vs. 30fps
Frame 22-1 28-1 34-1 22-8 28-8 34-8
* * *
Average 1.83 2.13 2.28 1.39 1.38 1.36
From these results some initial conclusions:
  * Slices as available in x265 being constrained across pictures are very costly if used without prediction across slice boundaries
  * Prediction over 2 P-frames results roughly in factor 2
  * Rate decrease per CRF/QP for I is consistently 2 for QP increase 6
  * Rate decrease per CRF/QP for P is consistently between 3 and 4 for QP increase 6
  * Bitrate increase for I frame coding when doubling frame rate is typically in the range of 2.
  * Bitrate increase for P frame coding when doubling frame rate is typically in the range of 1.4.
#### 7.2.3.3 Global Configuration (each of the following):
  * Bitrate Control (one of the following):
    * Constant Bitrate -- bitrate & buffer
    * Feedback-based Variable Bitrate - dynamic
    * _Constant Rate Factor -- rate factor with CRF (default: CRFref is used)_
  * Slice Setting (one of the following)
    * _Default -- no slices_
    * _number of slices -- number of slices (typical numbers are 4, 8, 16)_
    * ~~maximum slice size -- number in bytes~~
  * Error Resilience (one of the following)
    * _Intra-refresh frame parameter would be the period (default is no intra refresh)_
    * _Intra-refresh slice: period (1 1 slice every 1 frame with the slice being picked as POC mod #slices_ ) ~~2 1 slice every 2 frames~~)
    * Feedback-based
      * Mode:
        * _intra refresh: add an intra for the lost slice_
        * ACK-mode: only use acknowledge slices in prediction
        * _NACK-mode: use an old reference frame or intra in case of loss_
#### 7.2.3.4 Dynamic status:
  * Max number of bits for next frame (external rate control)
  * frame Number, slice number ACK/NACK/unknown
#### 7.2.3.5 Content Encoding Modelling
The content encoding is modelled as follows:
  * Create a map of slices, CTU maps (64 x 64) and reference frames
    * Example: 2048 x 2048, 8 slices, 3 reference frames
      * Addresses for 2048 / (8 * 64) = 4 rows with 32 CTUs for 8 slices in 3 frames maintained.
      * For each CTU store encoding mode:
        * intra/inter+merge/skip
        * largest reference frame (only previous one is used in simple config)
  * For frame i from V-Trace (based on sample time)
    * Read frame i from V-Trace (timestamp)
    * Read latest dynamic information from dynamic status info, if applicable
    * Do a model encoding
      * Input parameters: V-Trace, global configuration, dynamic status (if applicable)
      * Output: _s_ slices with parameters
```{=html}
``` \- Based on the map of intra and inter for each slice/MB
    -   Constant CRF
        -   Re-use the CRFref for which the Trace was generated.
        -   For every CTU
            -   Take P-frame intra/inter/merge/skip percentage and
                decide intra/inter/merge/skip randomly. This is done
                that the number of MBs is matching the trace entry.
        -   For every slice,
            -   apply intra/(inter + reference) decision as follows
                -   *Periodic: slice mode follows pattern, other inter +
                    reference 1*
                -   *Feedback intra: slice was lost =\> intra, else
                    inter + reference 1*
                -   Feedback ack: only ACK slices =\> reference inter +
                    reference backward (typically more than 1), such
                    that the slice was acked.
                -   Feedback NACK: slice nack =\> reference inter +
                    reference backward (typically more than 1) prior to
                    NACK or intra.
            -   For intra slices,
                -   overwrite the above CTU decision and make it an
                    intra
            -   draw a number of bits for the slice
                -   that takes into account
                    -   the type of the CTU
                    -   the information in the trace file
                -   the total amount of the bits for this slice by the
                    following modelling
                    -   intra size:
                        -   take total intra size and divide by number
                            of CTUs as medium value
                        -   apply Gaussian drawing with 10% variance of
                            the total number of bits
                        -   Use the CRF and apply adjustment as follows
                            -   FinalBits = Bits \* pow(2, (CRFref --
                                CRF)/6)
                            -   QPnew = QPref - (CRF -- CRFref)
                    -   skip:
                        -   1 byte
                    -   merge + inter
                        -   *compute medium value as total inter size
                            (total P-size - intra-percentage\*intra-size
                            -- skip-percentage) and divide by number of
                            inter CTUs (total CTUs\*(1 --
                            intra\_percentage -- skip\_percentage))*
                        -   *reference frame 1*
                            -   *apply Gaussian drawing with 20%
                                variance of the total number of bits
                                with medium value*
                            -   *Use the CRF and apply adjustment as
                                follows*
                                -   *FinalBits = Bits \* pow(2, (CRFref
                                    -- CRF)/6)*
                                -   *QPnew = QPref - (CRF -- CRFref)*
                        -   reference frame more than 1, it is X
                            -   take total inter size and divide by
                                number of CTUs as medium value
                            -   multiply the medium value with X
                            -   apply Gaussian drawing with 20% variance
                                of the total number of bits
                            -   do also intra test as above, if intra is
                                lower, apply intra, else this inter
                                mode.
                            -   Use the CRF and apply adjustment as
                                follows
                                -   FinalBits = Bits \* pow(2, (CRFref
                                    -- CRF)/6)
                                -   QPnew = QPref - (CRF -- CRFref)
                -   sum up the size of each CTU for the slice
            -   Dump the following information:
                -   Slice Timing/frame count
                -   Left or right eye
                -   Slice availability (after the slice timing) relative
                    to 0.
                    -   Without encoding delay this is the same as the
                        slice. time.
                -   Quality/QPnew
                -   New PSNR -- add a function
                -   Slice size
                -   Slice type
                -   CTU types
    -   Bitrate constrained -- a total max of bits available max\_bits
        -   Do the same as for constant CRF
        -   Iterate to the smallest CRF that fulfill max\_bits
The following issues are not yet included:
  * Modelling of encoding times -- no priority
    * Model encoding delay.
  * We also need to address the two independent buffers for left and right eye.
    * Can we for now create the same process
      * Option 1: Same timing
      * Option 2: staggered left right at half the frame rate
  * Can we derive a new PSNR for the updated QP?
    * (Thomas) check for model
    * In the absence of a model, we just make 1 QP step up reduces PSNR by 1dB (linear)
  * What about slice modelling and all the issues that we saw
    * Ignore slice modelling for now, no bitrate change compared to not using slices.
  * We need to also address ACK/NACK based feedback
    * NACK already addressed above.
    * ACK you only take acknowledged slice/frames for references -- this basically extends the reference frame the feedback delay per slice.
#### 7.2.3.6 Rate Control
  * Parameters
    * Total average bit budget per frame: Tbits
    * Window size is W
    * Allocated bits to frame i: Bits[i]
    * Allocated bits in model encoding (from V-Trace): A[i]
    * Forced intra period -- intra_period = 1, 2, ... (0 means no forced intra)
  * Algorithm
    * Available Bits B[i] = Tbits*W -- sum (j=i-W+1) (i-1) Bits[j]
    * If A[i] \ 0: Bits[i] \ {width="4.873611111111111in" height="4.215972222222222in"}
Video decoding is based on input from S'-Trace and S-Trace, resulting in
V'-Trace.
The following simulation is proposed for identifying damaged CUs:
  * Keep a state for each CU
    * Damaged
    * Correct
  * CU is damaged
    * If it is part of a slice that is lost for this transmission
    * If it is correctly received, but it predicts from a wrong CU
  * CU is correct
    * If it is received correctly and it predicts for a non-damaged CU
  * Predicting from non-damaged CU means
    * Spatial prediction is correct
    * Temporal prediction is correct
  * Recovering CU done by Intra Refresh and predicting from correct CUs again.
Detailed modelling is provided in clause 7.2.7.3.
#### 7.2.7.2 Configuration
No configuration is applied, but the input parameters are the S and S'-Trace.
#### 7.2.7.3 Modelling
Input information
  * S-Trace
    * Slice index
    * Slice metadata for reconstruction
      * For every CU
        * Size
        * Mode
        * Reference
        * Quality/QPnew
        * PSNR/PSNRnew
  * S'-Trace
    * Slice index
    * Slice availability time
    * Recovered slice position (0 => lost, in between, full)
Run the following algorithm:
  * Input parameters.
    * Parameters of source
      * Resolution
      * reference frames
    * S'-Trace
  * Create a map of slices, CU maps (64 x 64) and reference frames
    * Example: 2048 x 2048, 8 slices, 3 reference frames
      * Addresses for 2048 / (8 * 64) = 4 rows with 32 CUs for 8 slices in 3 frames maintained.
      * For each CU of each frame, store mode:
        * Correct
        * Damaged
        * Unavailable
      * Initialize all CUs as unavailable
  * For each frame i
    * Get all slices from trace for the frame
    * For all slices that are lost
      * Mark all CUs as unavailable
      * Indicate the slice loss for feedback
    * For all slices are received
      * Indicate the slice received for "feedback"
      * If it is an intra CU, mark it correct
      * If it is an inter CU and it _references_ a damaged or unavailable CU, mark it as damaged, otherwise mark it as correct
        * _Referencing_ is determined as follows (note a better model may be developed in the future)
          * The CU in the new frame references the CU at the same position in the referencing frame is 100%
          * The probability of referencing a neighbouring CU top/bottom/left/right is 50%
          * The probability of referencing a neighbouring CU is 50%, if one of the two top/botton/left/right is referenced, and 100% if both are referenced, and is 0% if none are referenced.
    * Compute the totally unavailable and damaged CUs in this frame
    * Compute the average PSNR for this frame
      * avPSNR = PSNR * correctCUs/totalCUs + PSNRwrong (1- correctCUs/totalCUs) with PSNRwrong = 0
  * Run this independently for each eye buffer
  * Dump this information into a V'-Trace according to format in 7.2.7.4.
  * Add the availability time stamp
{width="5.604166666666667in" height="2.466666666666667in"}
{width="5.877777777777778in" height="2.58125in"}
#### 7.2.7.4 V'-Trace
For each eye, provide the following information in the V-Trace
  * **Presentation Time Stamp**
  * **Availability Time Stamp**
  * **POC** Picture Order Count - The display order of the frames.
  * **QPnew** Quantization Parameter decided for the frame.
  * **Bits** Number of bits consumed by the frame.
  * **PSNRnew** Peak signal to noise ratio for Y, U and V planes.
  * **Percentage Correct**
  * **Percentage Lost**
  * **Percentage Damaged**
  * **Total delay** Total delay spent to deliver the frame.
  * **Percentage CU Intra**
  * **Percentage CU Merge**
  * **Percentage CU Skip**
  * **Percentage CU Inter**
**For details see S4aV200627.**
###### ##
#