# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
In the present document, modal verbs have the following meanings:
**shall** indicates a mandatory requirement to do something
**shall not** indicates an interdiction (prohibition) to do something
The constructions \"shall\" and \"shall not\" are confined to the context of
normative provisions, and do not appear in Technical Reports.
The constructions \"must\" and \"must not\" are not used as substitutes for
\"shall\" and \"shall not\". Their use is avoided insofar as possible, and
they are not used in a normative context except in a direct citation from an
external, referenced, non-3GPP document, or so as to maintain continuity of
style when extending or modifying the provisions of such a referenced
document.
**should** indicates a recommendation to do something
**should not** indicates a recommendation not to do something
**may** indicates permission to do something
**need not** indicates permission not to do something
The construction \"may not\" is ambiguous and is not used in normative
elements. The unambiguous constructions \"might not\" or \"shall not\" are
used instead, depending upon the meaning intended.
**can** indicates that something is possible
**cannot** indicates that something is impossible
The constructions \"can\" and \"cannot\" are not substitutes for \"may\" and
\"need not\".
**will** indicates that something is certain or expected to happen as a result
of action taken by an agency the behaviour of which is outside the scope of
the present document
**will not** indicates that something is certain or expected not to happen as
a result of action taken by an agency the behaviour of which is outside the
scope of the present document
**might** indicates a likelihood that something will happen as a result of
action taken by some agency the behaviour of which is outside the scope of the
present document
**might not** indicates a likelihood that something will not happen as a
result of action taken by some agency the behaviour of which is outside the
scope of the present document
In addition:
**is** (or any other verb in the indicative mood) indicates a statement of
fact
**is not** (or any other negative verb in the indicative mood) indicates a
statement of fact
The constructions \"is\" and \"is not\" do not indicate requirements.
# 1 Scope
The present document investigates specific use cases and service requirements
for 5GS support of enhanced XR-based services, (as XR-based services are an
essential part of \"Metaverse\" services considered in this study,) as well as
potentially other functionality, to offer shared and interactive user
experience of local content and services, accessed either by users in the
proximity or remotely. In particular, the following areas are studied:
\- Support of interactive XR media shared among multiple users in a single
location, including:
\- performance (KPI) aspects; e.g. latency, throughput, connection density
\- efficiency and scalability aspects, for large numbers of users in a single
location.
\- the combination of haptics type of XR media and other non-haptics types of
XR media.
\- Identification of users and other digital representations of entities
interacting within the Metaverse service.
\- Acquisition, use and exposure of local (physical and digital) information
to enable Metaverse services, including:
\- acquiring local spatial/environmental information and user/UE(s)
information (including viewing angle, position and direction);
\- exposing local acquired spatial, environmental and user/UE information to
3^rd^ parties to enable Metaverse services.
\- Other aspects, such as privacy, charging, public safety and security
requirements.
The study also investigates gaps between the identified new potential
requirements and the requirements already specified for the 5G system.
It is acknowledged that there are activities related to the topic Metaverse
outside of 3GPP, such as the W3C Open Metaverse Interoperability Group (OMI).
These activities may be considered in the form of use cases and related
contributions to this study, but there is no specific objective for this study
to consider or align with external standardization activities.
A difference between this study and TR 22.847 \" Study on supporting tactile
and multi-modality communication services\" is that Metaverse services would
involve coordination of input data from different devices/sensors from
different users and coordination of output data to different devices at
different destinations to support the same task or application.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TS 22.228: \"Service requirements for the Internet Protocol (IP)
Multimedia core network Subsystem (IMS)\".
[3] 3GPP TS 22.173: \"IP Multimedia Core Network Subsystem (IMS) Multimedia
Telephony Service and supplementary services\".
[4] 3GPP TS 22.101: \"Service principles\".
[5] 3GPP TS 22.261: \"Service requirements for the 5G system\".
[6] Talaba D, Horvath I, Lee K H, \"Special issue of Computer-Aided Design on
virtual and augmented reality technologies in product design,\" Computer-Aided
Design 42:361 - 363.
[7] Wang X, Tsai J J-H, \"Collaborative Design in Virtual Environments,\" In:
ISCA 48:1-2.
[8] Benford S, Greenhalgh C, Rodden T, \"Collaborative virtual environments.\"
Communications of the ACM 44: 79--85.
[9] M. Eid, J. Cha, and A. El Saddik, \"Admux: An adaptive multiplexer for
haptic-audio-visual data communication\", IEEE Tran. Instrument. and
Measurement, vol. 60, pp. 21--31, Jan 2011.
[10] K. Iwata, Y. Ishibashi, N. Fukushima, and S. Sugawara, \"QoE assessment
in haptic media, sound, and video transmission: Effect of playout buffering
control\", Comput. Entertain., vol. 8, pp. 12:1--12:14, Dec 2010.
[11] N. Suzuki and S. Katsura, \"Evaluation of QoS in haptic communication
based on bilateral control\", in IEEE Int. Conf. on Mechatronics (ICM), Feb
2013, pp. 886--891.
[12] E. Isomura, S. Tasaka, and T. Nunome, \"A multidimensional QoE monitoring
system for audiovisual and haptic interactive IP communications\", in IEEE
Consumer Communications and Networking Conference (CCNC), Jan 2013, pp. 196--
202.
[13] A. Hamam and A. El Saddik, \"Toward a mathematical model for quality of
experience evaluation of haptic applications\", IEEE Tran. Instrument. and
Measurement, vol. 62, pp. 3315--3322, Dec 2013.
[14] O. Holland et al., \"The IEEE 1918.1 \"Tactile Internet\" Standards
Working Group and its Standards,\" Proceedings of the IEEE, vol. 107, no. 2,
Feb. 2019.
[15] Lee, Donghwan et. al., \"Large-scale Localization Datasets in Crowded
Indoor Spaces, \" NAVER LABS, NAVER LABS Europe, CVPR 2021.
[16] 3GPP TR 22.837: \" Study on Integrated Sensing and Communication.\"
[17] Alriksson, F., Kang, D.H., Phillips, C., Pradas, J.L. and Zaidi, A.,
2021. XR and 5G: Extended reality at scale with time-critical communication.
Ericsson Technology Review.
[18] 3GPP TR 26.928: \"Extended Reality (XR) in 5G.\"
[19] Ge, Y., Wen, F., Kim, H., Zhu, M., Jiang, F., Kim, S., Svensson, L. and
Wymeersch, H., 2020. 5G SLAM using the clustering and assignment approach with
diffuse multipath. Sensors, 20(16), p.4656.
[20] Kim, H., Granström, K., Gao, L., Battistelli, G., Kim, S. and Wymeersch,
H., 2020. 5G mmWave cooperative positioning and mapping using multi-model PHD
filter and map fusion. IEEE Transactions on Wireless Communications, 19(6),
pp.3782-3795.
[21] Liu, A., Huang, Z., Li, M., Wan, Y., Li, W., Han, T.X., Liu, C., Du, R.,
Tan, D.K.P., Lu, J. and Shen, Y., 2022. A survey on fundamental limits of
integrated sensing and communication. IEEE Communications Surveys & Tutorials,
24(2), pp.994-1034.
[22] Dwivedi, S., Shreevastav, R., Munier, F., Nygren, J., Siomina, I.,
Lyazidi, Y., Shrestha, D., Lindmark, G., Ernström, P., Stare, E. and Razavi,
S.M., 2021. Positioning in 5G networks. IEEE Communications Magazine, 59(11),
pp.38-44.
[23] Suhr, C. \"S.L.A.M. and Optical Tracking for XR\",
https://medium.com/desn325-emergentdesign/s-l-a-m-and-optical-tracking-for-xr-
cfabb7dd536f \
[24] Kim, H., Granstrom, K., Svensson, L., Kim, S. and Wymeersch, H., 2022\.
PMBM-based SLAM Filters in 5G mmWave Vehicular Networks. IEEE Transactions on
Vehicular Technology.
[25] 5GAA \"C-V2X Use Cases Volume II: Examples and Service Level
Requirements\", 5G Automobile Assocaition White Paper, https://5gaa.org/wp-
content/uploads/2020/10/5GAA_White-Paper_C-V2X-Use-Cases-Volume-II.pdf
\
[26] A. Ebrahimzadeh, M. Maier and R. H. Glitho, \"Trace-Driven Haptic Traffic
Characterization for Tactile Internet Performance Evaluation,\" 2021
International Conference on Engineering and Emerging Technologies (ICEET),
2021, pp. 1-6.
[27] Lee, L.-H., Braud, T., Zhou, P., Wang, L., Xu, D., Lin, Z., Kumar, A.,
Bermejo, C., and Hui, P. All One Needs to Know about Metaverse: A Complete
Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda,
2021.
[28] Halbhuber, David & Henze, Niels & Schwind, Valentin. (2021). Increasing
Player Performance and Game Experience in High Latency Systems. Proceedings of
the ACM on Human-Computer Interaction. 5. 1-20. 10.1145/3474710.
[29] ITU-T Recommendation Y.3090 (02/22): \"Digital twin network -
Requirements and architecture\"
(https://www.itu.int/rec/T-REC-Y.3090-202202-I).
[30] Skalidis, I., Muller, O. and Fournier, S., 2022. CardioVerse: The
Cardiovascular Medicine in the Era of Metaverse. Trends in Cardiovascular
Medicine.
[31] Koo, H., 2021. Training in lung cancer surgery through the metaverse,
including extended reality, in the smart operating room of Seoul National
University Bundang Hospital, Korea. Journal of educational evaluation for
health professions, 18.
[32] Mozumder, M.A.I., Sheeraz, M.M., Athar, A., Aich, S. and Kim, H.C., 2022,
February. Overview: technology roadmap of the future trend of metaverse based
on IoT, blockchain, AI technique, and medical domain metaverse activity. In
2022 24th International Conference on Advanced Communication Technology
(ICACT) (pp. 256-261). IEEE.
[33] Ning, H., Wang, H., Lin, Y., Wang, W., Dhelim, S., Farha, F., Ding, J.
and Daneshmand, M., 2021. A Survey on Metaverse: the State-of-the-art,
Technologies, Applications, and Challenges. arXiv preprint arXiv:2111.09673.
[34] https://www.xrtoday.com/virtual-reality/ukrainian-doctors-perform-first-
vr-surgery/
[35] https://www.cryptotimes.io/surgeon-performed-surgery-remotely-through-
metaverse/
[36] Senk, S., Ulbricht, M., Tsokalo, I., Rischke, J., Li, S.C., Speidel, S.,
Nguyen, G.T., Seeling, P. and Fitzek, F.H., 2022. Healing Hands: The Tactile
Internet in Future Tele-Healthcare. Sensors, 22(4), p.1404.
[37] Chen, D. and Zhang, R., 2022. Exploring Research Trends of Emerging
Technologies in Health Metaverse: A Bibliometric Analysis. Available at SSRN
3998068.
[38] https://www.yankodesign.com/2022/05/15/the-metaverse-has-the-power-to-
improve-healthcare-and-it-has-already-begun/
[39] 3GPP TS 23.501: \"System architecture for the 5G System (5GS)\".
[40] 3GPP TS 38.300: \"NR; NR and NG-RAN Overall description; Stage-2\".
[41] 3GPP TS 22.826: \"Study on communication services for critical medical
applications\".
[42] Tataria, H., Shafi, M., Molisch, A.F., Dohler, M., Sjöland, H. and
Tufvesson, F., 2021. 6G wireless systems: Vision, requirements, challenges,
insights, and opportunities. Proceedings of the IEEE, 109(7), pp.1166-1199.
[43] 3GPP TS 23.682: \"Architecture enhancements to facilitate communications
with packet data networks and applications\".
[44] 3GPP TS 23.273: \"5G System (5GS) Location Services (LCS); Stage 2\".
[45] \"MPEG-4 Face and Body Animation\",
https://visagetechnologies.com/mpeg-4-face-and-body-animation/ (accessed
2.11.22).
[46] 3GPP TS 22.105: \"Services and service capabilities\".
[47] Virtual humans: https://en.wikipedia.org/wiki/Virtual_humans
[48] Kwang Soon Kim, et al., \"Ultrareliable and Low-Latency Communication
Techniques for Tactile Internet Services\", PROCEEDINGS OF THE IEEE, Vol. 107,
No. 2, February 2019
[49] I.-J. Hirsh and C. E. J. Sherrick, \"Perceived order in different sense
modalities, \" Journal of Experimental Psychology, vol. 62, no. 5, pp. 423--
432, 1961.
[50] VESA Compression Codecs - vesa.org/vesa-display-compression-codecs
[51] 3GPP TR 23.700-80: \"Study on 5G System Support for AI/ML-based
Services\".
[52] EU data protection rules \| European Commission (europa.eu)
(https://ec.europa.eu/info/law/law-topic/data-protection/eu-data-protection-
rules_en)
[53] California Consumer Privacy Act (CCPA) \| State of California -
Department of Justice - Office of the Attorney General
(https://oag.ca.gov/privacy/ccpa)
[54] Y. Sun, Z. Chen, M. Tao, and H. Liu, \"Communications, caching, and
computing for mobile virtual reality: Modelingand trade off, \" IEEE Trans.
Commun., vol. 67, no. 11, pp. 7573--7586, Nov. 2019.
[55] Y. Cai, J. Llorca, A. M. Tulino and A. F. Molisch, \"Compute- and Data-
Intensive Networks: The Key to the Metaverse,\" 2022 1st International
Conference on 6G Networking (6GNet), 2022.
[56] 3GPP TS 22.226: \"Global Text Telephony\"
[57] ITU-T SG16 \"ITU-T SG 16 Work on Accessibility - How ITU is Pioneering
Telecom Accessibility for All\",
https://www.itu.int/en/ITU-T/studygroups/com16/accessibility/Pages/telecom.aspx,
accessed 30.1.23.
[58] Ankit Ojha, Ayush Pandey, Shubham Maurya, Abhishek Thakur, Dr. Dayananda
P, \"Sign Language to Text and Speech Translation in Real Time Using
Convolutional Neural Network,\" INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH
& TECHNOLOGY (IJERT) NCAIT -- 2020 (Volume 8 -- Issue 15).
[59] R. Kazantsev and D. Vatolin, \"Power Consumption of Video-Decoders on
Various Android Devices,\" 2021 Picture Coding Symposium (PCS), Bristol,
United Kingdom, 2021, pp. 1-5, doi: 10.1109/PCS50896.2021.9477481.
[60] glTF 2.0 specification,
https://registry.khronos.org/glTF/specs/2.0/glTF-2.0.html, accessed
02/02/2023..
[61] B. Egger, W. A. P. Smith, A. Tewari, S. Wuhrer, M. Zollhoefer, T. Beeler,
F. Bernard, T. Bolkart, A. Kortylewski, S. Romdhani, C. Theobalt, V. Blanz,
and T. Vetter. \"3D Morphable Face Models---Past, Present, and Future\". ACM
Trans. Graph. 39, 5, Article 157 (Jun 2020)
[62] M. Zollhöfer, J. Thies, P. Garrido, D. Bradley, T. Beeler, P. Pérez, M.
Stamminger, M. Nießner, and C. Theobalt. \"State of the Art on Monocular 3D
Face Reconstruction, Tracking, and Applications\". Computer Graphics Forum
(2018).
[63] 3GPP TS 23.503, \" Policy and charging control framework for the 5G
System (5GS); Stage 2\".
# 3 Definitions of terms, symbols and abbreviations
## 3.1 Terms
For the purposes of the present document, the terms given in 3GPP TR 21.905
[1] and the following apply. A term defined in the present document takes
precedence over the definition of the same term, if any, in 3GPP TR 21.905
[1].
**avatar:** a digital representation specific to media that encodes facial
(possibly body) position, motions and expressions of a person or some software
generated entity.
**Conference** : An IP multimedia session with two or more participants. Each
conference has a \"conference focus\". A conference can be uniquely identified
by a user. Examples for a conference could be a Telepresence or a multimedia
game, in which the conference focus is located in a game server.
NOTE 1: This definition was taken from TS 22.228 [2].
**Conference Focus** : The conference focus is an entity which has abilities
to host conferences including their creation, maintenance, and manipulation of
the media. A conference focus implements the conference policy (e.g. rules for
talk burst control, assign priorities and participant's rights).
NOTE 2: This definition was taken from TS 22.228 [2].
**digital asset** : digitally stored information that is uniquely identifiable
and can be used to realize value according to their licensing conditions and
applicable regulations. Examples of digital assets include digital image
(avatar), software licenses, gift certificates and files (e.g. music files)
that have been purchased under a license that allows resale.
**digital representation:** the mobile metaverse media associated with the
presentation of a particular virtual or physical object. The digital
representation could present the current state of the object. One example of a
digital representation is an avatar, see Annex A.
**digital twin:** A real-time representation of physical assets in a digital
world.
NOTE 3: This definition was taken from ITU-T Recommendation Y.3090 [29].
**gesture:** a change in the pose that is considered significant, i.e. as a
discriminated interaction with a mobile metaverse service.
**immersive:** a characteristic of a service experience or AR/MR/VR media,
seeming to surround the user, so that they feel completely involved.
**localization** : A known location in 3 dimensional space, including an
orientation, e.g. defined as pitch, yaw and roll.
**location related service experience:** user interaction and information
provided by a service to a user that is relevant to the physical location in
which the user accesses the service.
**location agnostic service experience:** user interaction and information
provided by a service to a user that has little or no relation to the physical
location in which the user accesses the service. Rather the service provides
interaction and information concerning either a distant or a non-existent
physical location.
**mobile metaverse media:** media communicated or enabled using the 5G system
including audio, video, XR (including haptic) media, and data from which media
can be constructed (e.g. a \'point cloud\' that could be used to generate XR
media.)
**mobile metaverse:** the user experience enabled by the 5G system of
interactive and/or immersive XR media, including haptic media.
**mobile metaverse server:** an application server that supports one or more
mobile metaverse services to a user access by means of the 5G system.
**mobile metaverse service:** the service that provides a mobile metaverse
experience to a user by means of the 5G system.
**pose:** the relative location, orientation and direction of the parts of a
whole. The pose can refer the user, specifically used in terms of identifying
the position of a user\'s body. The pose can also also refer to an entity or
object (whose parts can adopt different locations, orientations, etc.) that
the user interacts with by means of mobile metaverse services.
**predictive digital representation model:** a model used for creating a
digital representation (e.g. avatar) of a user or object in AR/MR/VR
communication that helps to compensate for communication latency and/or
conceal negative effects of high communication latency between the users by
predicting for example events, changes or outcomes that impact the digital
representation, such as predicting the current position or pose of a remote
user.
**service information** : this information is out of scope of standardization
but could contain, e.g. a URL, media data, media access information, etc. This
information is used by an application to access a service.
**spatial anchor** : an association between a location in space (three
dimensions) and service information that can be used to identify and access
services, e.g. information to access AR media content.
**spatial map** : A collection of information that corresponds to space,
including information gathered from sensors concerning characteristics of the
forms in that space, especially appearance information.
**spatial mapping service:** A service offered by a mobile network operator
that gathers sensor data in order to create and maintain a Spatial Map that
can be used to offer customers Spatial Localization Service.
**spatial localization service:** A service offered by a mobile network
operator that can provide customers with Localization.
**User Identifier:** a piece of information used to identify one specific User
Identity in one or more systems.
NOTE 4: This definition was taken from TS 22.101 [4].
**User Identity** : information representing a user in a specific context. A
user can have several user identities, e.g. a User Identity in the context of
his profession, or a private User Identity for some aspects of private life.
NOTE 5: This definition was taken from TS 22.101 [4].
**User Identity Profile:** A collection of information associated with the
User Identities of a user.
NOTE 6: This definition was taken from TS 22.101 [4].
**digital wallet: one type of digital asset container, also known as an
e-wallet or mobile wallet, is a software application that securely stores
digital credentials, such as payment information, loyalty cards, tickets, and
other digital assets. It allows users to make electronic transactions, such as
payments and transfers, conveniently and securely using their digital
credentials.**
NOTE 7: Digital wallets typically employ encryption and authentication
mechanisms to protect the stored information and ensure the security of
transactions.
## 3.2 Symbols
Void.
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
AI Artificial Intelligence
CCTV ClosedCircuit TeleVision
DoF Degrees of Freedom
DVE Distributed Virtual Environment
FACS Facial Action Coding System
FOV Field Of View
LiDAR Light Detection And Ranging
VRU Vulnerable Road User
# 4 Overview
Mobile metaverse services are discussed in this technical report both in the
abstract and concrete. Specific services mentioned in the TR include:
\- Situational awareness for drivers, pedestrians, cyclists, to increase
safety and efficiency of transport (see 5.2).
\- XR enabled collaborative and concurrent engineering, to enable local and
remote collaboration (see 5.3).
\- Participatory in and passive observation of virtual reality events, e.g.
basketball (see 5.6).
\- Presentation of AR content, e.g. a feature length movie, on a virtual
screen (see 5.7).
\- Remote critical health care, including surgery and treatment (see 5.10).
The study also considers a number of use cases that feature new service
enablers, including:
\- Providing users with information and services that are of local relevance
(see 5.1).
\- Enhancements to IMS to support multiple users and multi-modal XR
communication (see 5.3).
\- Support for spatial anchors to link service information to specific
locations (see 5.4).
\- Support for spatial localization and mapping services, and enablers for
them in the 5GS (see 5.5).
\- Support for multi-service coordination for different input and output
devices and diverse services (see 5.8).
\- Support for synchronization of different data streams and predicted network
conditions (especially latency) to enable immersive remote collaboration
despite significant distance and therefore communication delay between
participants (see 5.9).
# 5 Use Cases
## 5.1 Use Case on Localized Mobile Metaverse Service
### 5.1.1 Description
This use case will consider the potential service opportunities that arise
when advanced location information is available to trigger AR based services.
A precursor to this use case is briefly considered: i-mode service, introduced
by NTT DOCOMO in 1999. The discussion of i-mode serves as an inspiration. This
service was extremely successful, was one of the early mobile services beyond
messaging and voice, and has many potential similarities with metaverse
services. This service in many ways preceded and foresaw mobile internet
services that would arise 10 years later. Users could access data on-line
concerning weather, traffic, etc. While there were many revolutionary aspects
to i-mode, three are particularly relevant for this use case:
\- **i-area** \-- a location information service that enabled the user to
identify _locally relevant information_ \-- concerning traffic, maps and
retail store information for business in the user's proximity.
\- **i-channel** \-- _a distribution service of 'latest information'_ that the
user could further investigate (through interaction) and _whose display was
user configurable_.
\- **a fully decentralized content and service creation framework** allowing
third parties to easily provide content, especially relevant: _location
specific content_. This made it possible for small businesses to provide
information to potential customers in the proximity such as opening hours,
special offers, etc. It was even possible for those in the same location to
meet and join \'virtually,\' e.g. to play a computer game with other
passengers in the same train car or bus.
In this use case, analogously, services that are locally relevant can be
accessed, with relevant information retried. The user will have the ability to
selectively control which of this information to display. The content that is
obtained comes from decentralized sources - in this case different individual
merchants will provide this information.
The use case described in this clause does not rely on or recreate the i-mode
service. Rather, some of the ideas in i-mode service are carried forward given
the new opportunities enabled by localized 3D interactive media. Localized
mobile metaverse services creates exciting new opportunities to receive
locally relevant content and interact with services.
These capabilities taken to a much greater level will form the essence of the
coming mobile metaverse user experience. What distinguishes this service most
is that it will provide the user with information services _integrated into
their ordinary experience._ Consider an example of a commuter navigating an
underground passage. Diverse relevant information is integrated into the
user\'s field of view, as shown in Figure 5.1.1-1 below.
{width="5.179245406824147in" height="3.8695264654418198in"}
Figure 5.1.1-1: Localized Mobile Metaverse Services offering relevant
information
Here, the AR annotation provides much more than an augmented map. The user is
going to catch a train. (a) The path to the platform is shown without
obstructing the user\'s perception of their proximity, where the contrast is
good and no distractions appear. The (b) store on the right can provide
content that may be relevant to the potential shopper, here the store\'s
opening hours. Further along, (c) a restaurant provides a personalized
message, reminding the user that they ate there in the past and ordered soup.
These services are linked to the space that the user is in. See Figure
5.1.1-2, below.
{width="2.7075470253718286in" height="1.8952832458442694in"}
Figure 5.1.1-2: Services offering relevant information are anchored in space
The three information augmentations that are displayed are the result of
different source of information. The path information (a) can be presented
anywhere that it fits into the scene, where the information for (b) and (c)
are anchored in space. The information depends on interaction between the user
(or the user\'s preferences, application context, etc.): the content shown
depends on the user\'s interest: (a) they are travelling, and the navigation
app knows what information the user needs to see. (b) The user has sought Blue
Lotion and it is available here - the user\'s \'persistent search\' shows
local results. (c) At this time of day, the user often eats, so the restaurant
on the left\'s reminder is welcome.
The 5G system enables this \'access to local services\' in a number of ways.
### 5.1.2 Pre-conditions
Ulysses uses his AR capable glasses while travelling. These are tethered to a
UE that he carries. This UE receives 5G service from the mobile network
operator he has a subscription with, M. Ulysses is interested in using his AR
capable glasses to receive relevant information, so he has selected _which
applications_ are \'relevant.\' These applications are therefore configured
with the operator M to be \'active.\' The purpose of this configuration will
be described below.
Local services, that is services that are localized, have anchors that can be
relevant to applications. A pre-condition of this use case is that such
services exist and have _spatially defined_ access. This is considered in this
use case as an \'anchor.\' The local service provider associates a service
with an anchor, potentially as well as metadata concerning the service (e.g.
\'is a restaurant\'.) This information is available to M, either because the
local services have been registered with M directly, or are available in maps,
registries, or other information sources that M has access to (e.g.
information that is provided by the UE).
### 5.1.3 Service Flows
Ulysses transfers at the Osaka train station. He has some time before he
catches his connecting train. As he is hungry, he activates a \'persistent
search\' on his mobile device to inform him of opportunities to eat as he
traverses the station. His mobile device begins to collect information about
his surroundings. The collected information can include information that is
obtained by interacting with nearby devices (e.g. sensors and other mobile
devices). He also has a shopping list (a set of items of interest) from retail
stores. He makes use of a navigation facility so that he will neither lose his
way nor lose track of time.
This has the following consequences:
a) As a result of the applications activated by the user, and the user\'s
preferences, the UE requests the \'localized mobile metaverse\' service
enabler offered by M, providing a list of \'persistent search\' information
and the information that was collected from the user's surroundings.
b) M receiving the persistent search and the information that was collected
from the user's surroundings, engages localized service activation. The
location of the UE is compared against the search criteria, the information
that was collected from the user's surroundings and information available to M
of spatially defined access points.
c) M identifies a match - essentially a \'JOIN\' of user preferences /
application persistent search criteria \"restaurants\" AND location (in the
user\'s field of view) AND local spatially defined access exists.
d) This match is provided to the UE, for further processing by the
application.
Example 1: The application [service] associated with \'restaurants\' has
stored information the user has eaten there and indicated it was \'good.\'
Thus the annotation \'Soup you liked last time\' is displayed.
Example 2: The application [service] associated with \'shopping\' has stored a
shopping list. When a local \'shopping service access point\' has been
identified, the shopping application queries the service provider directly. If
there is success, the result is displayed. Here: \"Blue Lotion you want
¥2000.\"
e) The UE is able to interact with the application to obtain information
associated with the service.
### 5.1.4 Post-conditions
Spatial information has successfully been employed to allow a user to identify
services. Please see 5.5 for further information on how spatial information is
obtained.
The user\'s location has been applied.
The information output of different applications are integrated into the media
that the user sees through the AR display device. The information associated
with the services is displayed in the proper location in the user\'s field of
view.
Ulysses may choose to eat soup or buy Blue Lotion.
M may charge Ulysses for this service, e.g. for the use of a persistent search
and for each successful result provided.
### 5.1.5 Existing feature partially or fully covering use case functionality
Location based services exist, to identify the position of the UE.
The 5GS supports means to expose the UE\'s location to a 3rd party service
provider.
Location Services for CIoT in 23.682, 4.5.19 [43], allow an LCS client to
obtain the location of a CIoT UE location, either periodically or in a
triggered manner, as well as the last known location of a UE that is
unreachable for an extended time.
Location services, as specified in 23.273, 5.5 [44] support exposure to
authorized third parties through a CAPIF API between NEF and the AF. The
information that can be exposed includes the target UE identity and parameters
for location events, e.g. the trigger (the UE enters, leaves or is within a
target area), the time between reports if multiple reports are requested, and
of course the precise location in three dimensions up to the maximum
horizontal and vertical accuracy supported.
Effectively, there are also numerous \'over the top\' mechanisms that
essentially communicate GPS or other location information that can be accessed
by the application from the mobile OS and terminal equipment. The client
application on the UE supplies this location information to an AS \'opaque\'
to the 5G system (that is, in application traffic that is not visible to the
5G system.) In this sense, the 5G system supports location services in that it
provides a UE with the mobile broadband services.
Using the location information obtained by any of these three mechanisms
described above, a service provider can identify appropriate content for or
interaction with the user, by means that are out of scope of 3GPP. This could
include the services introduced in 5.1.1 in the example of i-mode. The
application service can be configured or used in such a way that the user\'s
interest is known, and location-relevant information can be \'pushed\' to the
client application.
There is a significant gap between this support and the functionality
described in this use case. The Localized Mobile Metaverse Services unlike
e.g. i-mode and similar location service enabled applications **_does not
assume_** the use of a specific mediating application that organizes and
delivers information. Rather, this use case features and motivates a service
enabler that allows the user to **_discover_** different locally relevant
services and content, so that any available application service can be
accessed by the UE, e.g. through a web browser or other interactive-media
capable general purpose application.
### 5.1.6 Potential New Requirements
[PR 5.1.6-1] The 5G system shall enable third parties to make known the
availability of application services (i.e. provided by Application Servers)
associated with a precise location.
[PR 5.1.6-2] The 5G system shall provide suitable exposure mechanisms for
application services (i.e. provided by Application Servers) associated with a
precise location available in the user\'s proximity (e.g. within line of
sight), such that the mobile metaverse services can conform to specific
service constraints.
NOTE: The term \'service constraints\' implies that certain targets of service
discovery are supported, e.g. to find \'restaurants.\'
[PR 5.1.6-3] The 5G system shall provide suitable means to discover mobile
metaverse services (i.e. provided by mobile metaverse servers) associated with
a precise location available in the user\'s proximity.
[PR 5.1.6-4] The 5G system shall enable the rendering of diverse media, from
one or more mobile metaverse services associated with a single location, to be
combined to form a single location related service experience.
## 5.2 Use Case on Mobile Metaverse for 5G-enabled Traffic Flow Simulation and
Situational Awareness
### 5.2.1 Description
Smart transport is a very important area for 5G system as well as an important
mobile metaverse service. To reduce traffic jam and minimize traffic accident,
5G, including cellular based V2X technologies becomes more and more essential.
The 5G system can be utilized to support real-time information & data delivery
for the traffic participants including pedestrians, bicycle riders, and
vehicles with or without autonomous driving mode. As shown in Figure 5.2.1-1,
the physical objects including road infrastructure and vehicles including cars
and trucks in each lanes, will have a corresponding digital twin in the
virtual world, and the virtual and physical objects form the mobile metaverse.
In this use case, there are virtual objects which actually represent the
physical objects including vehicle, road and also pedestrians. This is the
basis to enable smart transport applications like traffic flow simulation and
situational awareness.
{width="6.704166666666667in" height="3.479861111111111in"}
Figure 5.2.1-1 Example of Smart Transport Metaverse
With the support of 5GS, real-time information and data about the physical
objects can be delivered and the virtual objects of the road infrastructure
and traffic participants including vulnerable road users can form a smart
transport mobile metaverse service as shown in Figure 5.2.1-2. Then real-time
processing& computing can be conducted to support traffic simulation,
situational awareness, and real time path guidance. Real-time safety or
security alerts can be generated for vehicles as well as the driver and
passengers.
{width="6.4219652230971125in" height="3.519458661417323in"}
Figure 5.2.1-2 Scenario of 5G-enabled Traffic Flow Simulation and Situational
Awareness
In order to support traffic flow simulation and situational awareness service,
the 5G network need to provide low latency, high data rate and high
reliability transmission, handover procedures that minimize service
disruptions, and in addition, the 5G network may also need to be further
enhanced to meet the service requirements for 5G-enabled traffic flow
simulation and situation awareness. Meanwhile, in addition to the physical
objects which may use UEs for telecommunication services, their corresponding
virtual objects are also capable of interacting with each other and also
interact with physical objects via 5GS.
This use case employs terminology defined in Table 5.2.1-1.
Table 5.2.1-1: Traffic Flow Simulation and Situational Awareness Terminology
* * *
Situational Awareness The ability to know and understand what is going on in
the surroundings, e.g. the perception of environmental elements and events
* * *
### 5.2.2 Pre-conditions
1\. Traffic participants may or may not be equipped with 5G-enabled terminal
equipment. The 5G-enabled terminal equipment can send and receive data via 5G
network.
2\. Computing and storage resources are provided for the mobile metaverse
servers deployed locally or over the data network (in a centralized cloud) to
allow real-time processing of huge amount of data produced by human, vehicle,
camera, radar etc.
3\. Wired or wireless communication resources are configured among the road
infrastructure and mobile metaverse server so that the server has real time
information of traffic participants perceived by these sensors. The road
infrastructure include camera, radar/lidar and also other devices, e.g. for
traffic control and guidance etc. The infrastructure equipment may have wired
connection e.g. fiber or ethernet or any other wired connection if available.
The cellular wireless network can be used to provide a more flexible way to
obtain communication services, especially when a wired connection is not
available.
### 5.2.3 Service Flows
1\. Wired or wireless communication path are configured among the road
infrastructure and mobile metaverse server so that the server has real time
information of traffic participants perceived by these sensors.
2\. Sensors deployed in roadside are initialized and enter normal working mode
which means the sensors can start to capture the traffic participants.
3\. Data connections between the vehicle driver's UE and server are
established with 5G UE module being registered with the server. Vehicles
without driver are equipped with a 5G UE module.
4\. The traffic participants including pedestrians, bicycle riders, and
vehicles, send real-time information towards the server by means of 5G UEs.
The real-time status information includes position, speed, direction, brake
status etc. Other related status information can be, but is not limited to,
sensors for XR (haptic, audio, video, etc) or smart transport (traffic light,
camera, radar/LiDAR, etc). These traffic participants are physical objects in
physical world which send their property and status information to the
corresponding digital-twins objects, that is, virtual objects. This real-time
information may include structured and unstructured data. Structured data
normally means data which has been processed and thus formatted in a certain
way, can be easily stored in database and when transmitted via 5G wireless
network, normally less transmission resource (e.g. lower data rate) is needed.
Unstructured data are not formatted in a certain and pre-defined way and is
not easy to store in database and when transmitting over 5G network, more
transmission resource would be needed.
5\. Within the mobile metaverse service, for 5G-enabled traffic flow
simulation and situation awareness, real-time information of the physical
objects including the road infrastructure, the traffic participants as well as
other information from traffic light signal, camera, radar, etc, are
synchronized to the virtual objects and real-time simulation are conducted.
The virtual objects can also interact with each other within virtual world and
interact with physical objects via 5G system.
Having interaction among virtual and physical objects is essential and
beneficial to support traffic simulation and situational awareness as the
virtual objects can also have their intelligence which allow them to request
telemetry from physical objects or other virtual objects. Thus, not only the
physical but also virtual objects need to interact via 5G system and they need
to be identified. Interaction means the physical objects deliver real-time
status data to digital twin i.e. virtual objects in one direction, and the
virtual objects can also send alert or other information to physical objects.
Such interaction needs to be realized via 5GS because the physical objects
like vehicles or humans need to use a 5G UE to access the 5G system and use 5G
resources to establish a connection with 5G system QoS support. Identification
of these physical and virtual objects by 5GS and thus associating them is
needed to support such interaction, meanwhile, from 5G system perspective,
this make it easier for the 5G system, based on operator policy with 3rd
party, to provide bearer services with corresponding QoS guarantee for both
physical and virtual objects as they can be seen as UE for 5GS.
It is noted that physical objects may include static and dynamic. Some static
objects deployed along road side like light poles may not move but their
properties can be synchronized with their virtual objects if such properties
may impact traffic simulation and situational awareness and also visualization
processing of the physical world. Theses physical objects may or may not have
dedicated UEs. Pedestrian and vehicles are equipped with UEs but the sensors
like camera, LiDAR and radar may share a common communication module to
establish connection with mobile metaverse services. The communication module
can be wired node or wireless UE.
6\. Traffic flow simulation is conducted which can predict whether there will
be traffic jam and which path is optimal for a certain vehicle. The simulation
can generate traffic assistance or guidance in a real-time manner towards the
traffic participants.
7\. The mobile metaverse server sends the traffic assistance or guidance
information to the UE which serves the pedestrian, bicycle rider, vehicle
driver or autonomous vehicle.
### 5.2.4 Post-conditions
1\. The mobile metaverse server conducts big data analysis to further refine
the accuracy of traffic flow simulation and situation awareness.
2\. Both UEs serving (e.g. providing sensing data for) the physical objects
and digital twins objects can be identified by the 5G system.
### 5.2.5 Existing features partly or fully covering the use case
functionality
The QoS framework of 5G system supports low latency, high reliability or high
data rate transmission of data traffic.
### 5.2.6 Potential New Requirements needed to support the use case
[PR 5.2.6-1] The 5G system shall be able to support the following KPIs for
transmission for traffic between a large number of UEs and application server
(e.g. mobile metaverse server).
+-------+-------+-------+-------+-------+-------+-------+-------+ | **Use | * | * | | | | | | | Ca |_Char |_ Infl | | | | | | | ses** | acter | uence | | | | | | | | istic | quant | | | | | | | | para | ity**| | | | | | | | meter | | | | | | | | | (K | | | | | | | | | PI)** | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | | **Max |** Se | **Rel | * |** Me | **Tra |** Se | | | al | rvice | iabil | _Area | ssage | nsfer | rvice | | | lowed | bit | ity_ _| Tr | Data | Inter | A | | | end-t | rate: | | affic | Vol | val_ _| rea_ _| | | o-end | u | | capac | ume_ _| | | | | late | ser-e | | ity_ _| | | | | | ncy_ _| xperi | | | | | | | | | enced | | | | | | | | | data | | | | | | | | | r | | | | | | | | | ate_ * | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | 5G-en | [5 | [10 | > | [\ | Ty | 2 | City | | abled | -20] | \~100 | 99.9% | ~39.6 | pical | 0-100 | or | | Tr | ms | Mbi | | Tb | data: | ms | Co | | affic | (NOTE | t/s] | | it/s/ | | | untry | | Flow | 1) | \ | | km^2^ | Ca | (NOTE | wide | | Simul | | [25] | | ] | mera: | 3) | | | ation | | (NOTE | | | 10M | | (NOTE | | and | | 6) | | (NOTE | b | | 4) | | S | | | | 5) | its/s | | | | ituat | | | | | per | | | | ional | | | | | s | | | | Awar | | | | | ensor | | | | eness | | | | | (uns | | | | | | | | | truct | | | | | | | | | ured) | | | | | | | | | | | | | | | | | | L | | | | | | | | | iDAR: | | | | | | | | | 90M | | | | | | | | | b | | | | | | | | | its/s | | | | | | | | | per | | | | | | | | | s | | | | | | | | | ensor | | | | | | | | | (uns | | | | | | | | | truct | | | | | | | | | ured) | | | | | | | | | | | | | | | | | | R | | | | | | | | | adar: | | | | | | | | | 10M | | | | | | | | | per | | | | | | | | | s | | | | | | | | | ensor | | | | | | | | | (uns | | | | | | | | | truct | | | | | | | | | ured) | | | | | | | | | | | | | | | | | | Real | | | | | | | | | -time | | | | | | | | | S | | | | | | | | | tatus | | | | | | | | | i | | | | | | | | | nform | | | | | | | | | ation | | | | | | | | | incl | | | | | | | | | uding | | | | | | | | | Tele | | | | | | | | | metry | | | | | | | | | data: | | | | | | | | | | | | | | | | | | [\ Table 5.3.3-1 Typical QoS requirements for multi-modal streams [9] [10] [11]
> [12] [13]
                        **Haptics**   **Video**      **Audio**
* * *
Jitter (ms) ≤ 2 ≤ 30 ≤ 30 Delay (ms) ≤ 50 ≤ 400 ≤ 150 Packet loss (%) ≤ 10 ≤ 1
≤ 1 Update rate (Hz) ≥ 1000 ≥ 30 ≥ 50 Packet size (bytes) 64-128 ≤ MTU 160-320
Throughput (kbit/s) 512-1024 2500 - 40000 64-128
4\. The haptic information, video and voice are generated at one party and
distributed to all other parties continuously. Note that based on the company
security policy, some information is shielded before being distributed to
certain participants. For example, George joins the meeting from the factory,
which is considered less secure according to the company policy. Consequently
some sensitive information is filtered before being distributed to George.
Information filtering is typically done at the conference centre (i.e.
conference focus).
5\. George travels back to office while staying connected on the conference.
The connection quality of George's devices has deteriorated sharply, and the
5G network triggers the codec re-negotiation to maintain the reasonable
quality of experience for all participants.
### 5.3.4 Post-conditions
The 5G system enables efficient communication, with enhanced security and
identity management, in support of DVEs for the collaborative and concurrent
engineering.
### 5.3.5 Existing features partly or fully covering the use case
functionality
The service requirements on the support of multimedia communication among
multiple users have been captured in TS 22.228 [2] with the following key
definitions:
> _**Conference:** An IP multimedia session with two or more participants.
> Each conference has a \"conference focus\". A conference can be uniquely
> identified by a user. Examples for a conference could be a Telepresence or a
> multimedia game, in which the conference focus is located in a game server._
>
> _**Telepresence:** A conference with interactive audio-visual communications
> experience between remote locations, where the users enjoy a strong sense of
> realism and presence between all participants by optimizing a variety of
> attributes such as audio and video quality, eye contact, body language,
> spatial audio, coordinated environments and natural image size._
>
> _**Telepresence System:** A set of functions, devices and network elements
> which are able to capture, deliver, manage and render multiple high quality
> interactive audio and video signals in a Telepresence conference. An
> appropriate number of devices (e.g. cameras, screens, loudspeakers,
> microphones, codecs) and environmental characteristics are used to establish
> Telepresence._
>
> _**Conference Focus:** The conference focus is an entity which has abilities
> to host conferences including their creation, maintenance, and manipulation
> of the media. A conference focus implements the conference policy (e.g.
> rules for talk burst control, assign priorities and participant's rights)._
Support of Multi-device and Multi-Identity in IMS MMTEL service is captured in
TS 22.173 clause 4.6 [3]:
> _The support of multiple devices is inherent in IMS. In addition, a service
> provider may allow a user to use any public user identities for its outgoing
> and incoming calls. The added identities can but do not have to belong to
> the served user. Identities may be part of different subscriptions and
> different operators._
In addition, TS 22.101 [4] has specified in clause 26a a set of service
requirements on User Identity:
> _Identifying distinguished user identities of the user (provided by some
> external party or by the operator) in the operator network enables an
> operator to provide an enhanced user experience and optimized performance as
> well as to offer services to devices that are not part of a 3GPP network.
> The user to be identified could be an individual human user, using a UE with
> a certain subscription, or an application running on or connecting via a UE,
> or a device ("thing") behind a gateway UE._
>
> _Network settings can be adapted and services offered to users according to
> their needs, independent of the subscription that is used to establish the
> connection. By acting as an identity provider, the operator can take
> additional information from the network into account to provide a higher
> level of security for the authentication of a user._
>
> _The 3GPP System shall support to authenticate a User Identity to a service
> with a User Identifier._
The functional requirement and performance KPIs in support of XR applications
are mainly captured in TS 22.261:
\- clause 7.6.1 AR/VR;
\- clause 6.43 Tactile and multi-modal communication service
\- clause 7.11 KPIs for tactile and multi-modal communication service
Clause 8 of TS 22.261 specifies the security related requirements covering
aspects such as authentication and authorization, identity management, and
data security and privacy.
Additional consideration need to be given to allow multiple users from
different geographical locations to interact using XR techniques.
The 5G system is able to collect charging information per UE or per
application for the use of IMS based conferencing services.
### 5.3.6 Potential New Requirements needed to support the use case
#### 5.3.6.1 KPIs for the collaborative and concurrent engineering in product
design
[PR 5.3.6.1-1] The 5G System shall provide the appropriate connectivity KPIs
for the use case of collaborative and concurrent engineering in product
design, see table 5.3.6.1-1.
**Table 5.3.6.1-1 -- Potential key performance requirements for collaborative
and concurrent engineering in product design**
+-------+-------+-------+-------+-------+-------+-------+-------+ | **Use | * | * | | | | | | | Ca |_Char |_ Infl | | | | | | | ses** | acter | uence | | | | | | | | istic | quant | | | | | | | | para | ity**| | | | | | | | meter | | | | | | | | | (K | | | | | | | | | PI)** | | | | | | | +=======+=======+=======+=======+=======+=======+=======+=======+ | | **Max |** Se | **Rel | * |** Me | **UE |** Se | | | al | rvice | iabil | _Area | ssage | Sp | rvice | | | lowed | bit | ity_ _| Tr | size | eed_ _| A | | | end-t | rate: | | affic | (by | | rea_ _| | | o-end | u | | capac | te)__| | | | | late | ser-e | | ity_ _| | | | | | ncy_ _| xperi | | | | | | | | | enced | | | | | | | | | data | | | | | | | | | r | | | | | | | | | ate_ _| | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Col | [ | [1- | [ > | [3. | Ty | Stati | typi | | labor | ≤10] | 100] | 99 | 804] | pical | onary | cally | | ative | ms | M | .9%] | Tb | h | or | | | and | | bit/s | ([ | it/s/ | aptic | Pedes | \ | | DoFs: | | | | | (note | | 99 | | 6-24 | | | | | 1) | | .9%] | | | | | | | | | (wi | | 6 | | | | | | | thout | | DoFs: | | | | | | | co | | 12-48 | | | | | | | mpres | | | | | | | | | sion) | | V | | | | | | | | | ideo: | | | | | | | Typi | | 1500 | | | | | | | cally | | | | | | | | | for | | A | | | | | | | Ha | | udio: | | | | | | | ptic: | | 100 | | | | | | | [> | | | | | | | | | 99.9 | | ([ | | | | | | | 99%] | | 14]) | | | | | | | (with | | | | | | | | | c | | | | | | | | | ompre | | | | | | | | | ssion | | | | | | | | | (note | | | | | | | | | 4)) | | | | | | | | | | | | | | | | | | \ | | | | | | | | | [26] | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | NOTE | | | | | | | | | 1: | | | | | | | | | The | | | | | | | | | ne | | | | | | | | | twork | | | | | | | | | based | | | | | | | | | confe | | | | | | | | | rence | | | | | | | | | focus | | | | | | | | | is | | | | | | | | | ass | | | | | | | | | umed, | | | | | | | | | which | | | | | | | | | rec | | | | | | | | | eives | | | | | | | | | data | | | | | | | | | from | | | | | | | | | all | | | | | | | | | the | | | | | | | | | par | | | | | | | | | ticip | | | | | | | | | ants, | | | | | | | | | per | | | | | | | | | forms | | | | | | | | | rend | | | | | | | | | ering | | | | | | | | | ( | | | | | | | | | image | | | | | | | | | s | | | | | | | | | ynthe | | | | | | | | | sis), | | | | | | | | | and | | | | | | | | | then | | | | | | | | | d | | | | | | | | | istri | | | | | | | | | butes | | | | | | | | | the | | | | | | | | | re | | | | | | | | | sults | | | | | | | | | to | | | | | | | | | all | | | | | | | | | par | | | | | | | | | ticip | | | | | | | | | ants. | | | | | | | | | The | | | | | | | | | la | | | | | | | | | tency | | | | | | | | | r | | | | | | | | | efers | | | | | | | | | to | | | | | | | | | the | | | | | | | | | tr | | | | | | | | | ansmi | | | | | | | | | ssion | | | | | | | | | delay | | | | | | | | | be | | | | | | | | | tween | | | | | | | | | a UE | | | | | | | | | and | | | | | | | | | the | | | | | | | | | a | | | | | | | | | pplic | | | | | | | | | ation | | | | | | | | | se | | | | | | | | | rver. | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | 2: To | | | | | | | | | su | | | | | | | | | pport | | | | | | | | | at | | | | | | | | | least | | | | | | | | | 15 | | | | | | | | | users | | | | | | | | | pr | | | | | | | | | esent | | | | | | | | | at | | | | | | | | | the | | | | | | | | | same | | | | | | | | | loc | | | | | | | | | ation | | | | | | | | | (e.g. | | | | | | | | | in an | | | | | | | | | area | | | | | | | | | of | | | | | | | | | 20m\ | | | | | | | | | _20m) | | | | | | | | | to | | | | | | | | | act | | | | | | | | | ively | | | | | | | | | enjoy | | | | | | | | | imme | | | | | | | | | rsive | | | | | | | | | Meta | | | | | | | | | verse | | | | | | | | | se | | | | | | | | | rvice | | | | | | | | | con | | | | | | | | | curre | | | | | | | | | ntly, | | | | | | | | | the | | | | | | | | | area | | | | | | | | | tr | | | | | | | | | affic | | | | | | | | | cap | | | | | | | | | acity | | | | | | | | | is | | | | | | | | | calcu | | | | | | | | | lated | | | | | | | | | c | | | | | | | | | onsid | | | | | | | | | ering | | | | | | | | | per | | | | | | | | | user | | | | | | | | | cons | | | | | | | | | uming | | | | | | | | | non-h | | | | | | | | | aptic | | | | | | | | | XR | | | | | | | | | media | | | | | | | | | (e.g. | | | | | | | | | for | | | | | | | | | video | | | | | | | | | per | | | | | | | | | s | | | | | | | | | tream | | | | | | | | | up to | | | | | | | | | 40000 | | | | | | | | | kb | | | | | | | | | it/s) | | | | | | | | | and | | | | | | | | | co | | | | | | | | | ncurr | | | | | | | | | ently | | | | | | | | | 60 | | | | | | | | | h | | | | | | | | | aptic | | | | | | | | | se | | | | | | | | | nsors | | | | | | | | | (per | | | | | | | | | h | | | | | | | | | aptic | | | | | | | | | s | | | | | | | | | ensor | | | | | | | | | gene | | | | | | | | | rates | | | | | | | | | data | | | | | | | | | up to | | | | | | | | | 1024 | | | | | | | | | kbi | | | | | | | | | t/s). | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | 3: In | | | | | | | | | prac | | | | | | | | | tice, | | | | | | | | | the | | | | | | | | | se | | | | | | | | | rvice | | | | | | | | | area | | | | | | | | | de | | | | | | | | | pends | | | | | | | | | on | | | | | | | | | the | | | | | | | | | a | | | | | | | | | ctual | | | | | | | | | d | | | | | | | | | eploy | | | | | | | | | ment. | | | | | | | | | In | | | | | | | | | some | | | | | | | | | cases | | | | | | | | | a | | | | | | | | | local | | | | | | | | | app | | | | | | | | | roach | | | | | | | | | (e.g. | | | | | | | | | the | | | | | | | | | a | | | | | | | | | pplic | | | | | | | | | ation | | | | | | | | | se | | | | | | | | | rvers | | | | | | | | | are | | | | | | | | | h | | | | | | | | | osted | | | | | | | | | at | | | | | | | | | the | | | | | | | | | ne | | | | | | | | | twork | | | | | | | | | edge) | | | | | | | | | is | | | | | | | | | pref | | | | | | | | | erred | | | | | | | | | in | | | | | | | | | order | | | | | | | | | to | | | | | | | | | sa | | | | | | | | | tisfy | | | | | | | | | the | | | | | | | | | re | | | | | | | | | quire | | | | | | | | | ments | | | | | | | | | of | | | | | | | | | low | | | | | | | | | la | | | | | | | | | tency | | | | | | | | | and | | | | | | | | | high | | | | | | | | | re | | | | | | | | | liabi | | | | | | | | | lity. | | | | | | | | | | | | | | | | | | NOTE | | | | | | | | | 4: | | | | | | | | | The | | | | | | | | | ar | | | | | | | | | rival | | | | | | | | | int | | | | | | | | | erval | | | | | | | | | of | | | | | | | | | compr | | | | | | | | | essed | | | | | | | | | h | | | | | | | | | aptic | | | | | | | | | data | | | | | | | | | us | | | | | | | | | ually | | | | | | | | | f | | | | | | | | | ollow | | | | | | | | | some | | | | | | | | | s | | | | | | | | | tatis | | | | | | | | | tical | | | | | | | | | dist | | | | | | | | | ribut | | | | | | | | | ions, | | | | | | | | | such | | | | | | | | | as | | | | | | | | | g | | | | | | | | | enera | | | | | | | | | lized | | | | | | | | | P | | | | | | | | | areto | | | | | | | | | dis | | | | | | | | | tribu | | | | | | | | | tion, | | | | | | | | | and | | | | | | | | | E | | | | | | | | | xpone | | | | | | | | | ntial | | | | | | | | | di | | | | | | | | | strib | | | | | | | | | ution | | | | | | | | | [ | | | | | | | | | 26]. | | | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+
#### 5.3.6.2 Service requirements for collaborative and concurrent engineering
in product design
[PR 5.3.6.2-1] The 5G system shall enhance the interaction between IMS CN and
5G CN to allow 5G CN to provide the IMS CN with real-time feedback in support
of XR communication among multiple users simultaneously.
NOTE: The feedback can include information such as network condition, achieved
QoS. Such information can be used by the IMS CN, for example, to trigger the
codec negotiation.
[PR 5.3.6.2-2] Subject to regulatory requirements, operator policies and user
consent, the 5G system shall be able to support mechanisms to expose to a
trusted third party (e.g. the conference focus) the result of authenticating a
user identity to a UE.
NOTE: Authenticating a user identity to a UE at the terminal side is out of
3GPP scope.
[PR 5.3.6.2-3] The 5G system shall provide a means to synchronize multiple
data flows from multiple UEs associated with one user.
## 5.4 Use Case on Spatial Anchor Enabler
### 5.4.1 Description
In use case 5.1 \"Localized Mobile Metaverse Service Use Case,\" we introduced
the term spatial anchor to describe an association between space and service
information. This use case elaborates the concept of the spatial anchor to
enable diverse mobile metaverse services, including those described in use
case 5.1. This use case focuses on the creation, management and use of spatial
anchors.
The overall purpose of this enabler is to make it possible to create AR
content and share it with others. The \'spatial anchor producer\' determines
what to share and its location, and any constraints (e.g. who to share the
spatial anchor with) and additional information, most importantly the
\'resource\' associated with the anchor (e.g. AR media, mobile metaverse
service to access, etc.).
The \'spatial anchor consumer\' is able to recognize anchors associated with
locations, and use the spatial anchor to obtain the associated information.
Functionally, a spatial anchor has the following model:
\- **Spatial Anchor** : information that can be provided by a content producer
to a content consumer. How this is done is out of scope of this use case.
\- **Precise spatial location information** : where the produced content is
located, including the content\'s extent, orientation, etc.
\- **Service information** : this information is out of scope of
standardization but could contain, e.g. a URL, media data, media access
information, etc. This information is used by an application to access a
service.
The spatial anchor enabler will benefit retail environments. Here, a cheese
seller has extensive information about her wares that she will share with
customers.
{width="2.7559055118110236in" height="4.123183508311461in"}
Figure 5.4.1: Spatial Anchors - created by a user to share with other users
In this use case there are several actors that are relevant.
\- Leka the cheese seller is the content producer. She creates content and
also anchors them to her wares. (She is very adept at putting the cheese back
in the same places, and moving the anchors when this is not possible.)
\- I am the customer.
\- Warez is the mobile metaverse service provider stores that updates Leka\'s
content, generates its presentation (that is, the AR content that is presented
to customers), supports any interactive features, etc.
\- FineNet is the network operator that enables anchored services for any
content producer, customer and mobile metaverse service provider.
### 5.4.2 Pre-conditions
Leka makes use of a UE that has a subscription with FineNet. She has a sensor
that can be used in combination with the UE to indicate precise locations. In
figure 5.4.1-1, the sensor can identify the location of the tip of the cheese
fork she holds.
Leka obtains services from Warez, where she stores information related to her
inventory. She also has arranged in advance what information to display to
customers and in what format.
I wear glasses that provide AR experience and communicate by means of my UE. I
have a mobile subscription with FineNet also.
### 5.4.3 Service Flows
Leka places wares on display. She indicates the location of wares that are
associated with inventory information so that the association of location
(listed below as \'pose\' including position in 3 dimensional space,
orientation and possibly more spatial data) and service information are
captured by the 5GS. This inventory information is captured also by the mobile
network operator as **spatial anchors**. This is shown on the top half of
Figure 5.4.3-1.
{width="6.695138888888889in" height="1.3583333333333334in"}
Figure 5.4.3-1: Spatial Anchor Enabler Service Flow
NOTE: The text in parentheses in the figure above are examples.
**1\. Creating, modifying, removing spatial anchors**
To add new wares to display, Leka captures the location of the item with her
cheese for (which includes sensors.) She associates a new spatial anchor with
this location and product information (e.g. by scanning the bar code on the
cheese). The Warez inventory management system also generates AR content on
demand (mobile metaverse media): this application is _external_ to 3GPP
standards. Leka\'s UE accesses the mobile metaverse service that establishes
the association that comprises the spatial anchor, the physical location and
the service information.
When Leka moves wares, Leka can adjust the spatial location of items or remove
them entirely (e.g. when the cheese has been sold) by registering the new
location or removal with the Warez inventory management system. The
\'location\' and \'service information\' can be updated over time.
Leka can also update the information that will be displayed as AR (mobile
metaverse media) to the customer by the mobile metaverse server offered by the
Warez inventory management system, for example the description of the cheese,
its price, etc. This interaction is out of scope of 3GPP standards.
**2\. Accessing and using Spatial Anchors**
I enter the store and examine what is on display. I capture the scene with
sensors and share this information with the Spatial Mapping and Localization
Service Enabler (5.5). This allows me to identify my localization information,
including orientation, precisely.
I put on (activating) my AR glasses. By means of my UE with access over
FineNet, share the location and orientation information (the area of interest)
with the 5G system.
The 5G system uses the localization information to identify all applicable
spatial anchors in the area of interest. These are returned to the UE and the
AR glasses. This function is illustrated in the lower half of Figure 5.4.3-1.
The service information suffices to access the media server offered by Warez.
The location information indicates the location of each spatial anchor.
I now perceive the spatial anchors as shown in Figure 5.4.1.
As Leka indicates the Halloumi in her counter, and my gaze focuses on that
location (known to a very high degree of precision), the AR glasses use the
service information associated with the spatial anchor to activate the
Halloumi cheese media.
I now perceive the AR information panel associated with the Halloumi cheese.
### 5.4.4 Post-conditions
I am able to observe the AR content (mobile metaverse media) associated with
the wares on display, as shown in Figure 5.4.1-1. As wares are moved or
removed from the display, the content shifts as well. The display of AR
content is the result of the service information (i.e. how to access the
media) and the localization information (i.e. where the media is placed,
oriented, etc.)
At any time Leka can add new wares and associated AR content, update the
content that is displayed, etc.
I perceive AR content associated with the items in the shop and happily buy
the cheese that meets my needs.
### 5.4.5 Existing feature partly or fully covering use case functionality
### 5.4.6 Potential New Requirements needed to support the use case
[PR 5.4.6-1] Subject to operator policy, the 5G system shall enable an
authorized third party to establish an association between a physical location
(in three dimensional space, an orientation, etc.) and service information,
where the service information is provided to the 5G system and the spatial
anchor is either provided or determined by the 5G system.
[PR 5.4.6-2] Subject to operator policy, the 5G system shall be able to
establish an association between a physical location (in three dimensional
space, an orientation, etc.) and service information, where the service
information is provided to the 5G system and the spatial anchor is either
provided or determined by the 5G system.
[PR 5.4.6-3] Subject to operator policy, the 5G system shall enable an
authorized third party to obtain all of the spatial anchors located in a given
three dimensional area.
NOTE 1: How the authorized third party identifies which three dimensional area
to request spatial anchors in is not in scope of the 3GPP standard. Spatial
localization and mapping information could be used to identify areas of
interest.
[PR 5.4.6-4] Subject to operator policy, the 5G system shall enable a third
party to request the service information associated with the precise location
of a specific spatial anchor. Making use of this service and location
information, the third party can access a mobile metaverse server to obtain AR
media.
NOTE 2: How the service and location information is used by the third party to
access a mobile metaverse server and the AR media itself is out of scope of
this requirement.
[PR 5.4.6-5] Subject to operator policy, the 5G system shall provide an
authorized third party a means to manage the spatial anchor(s), e.g. add,
remove or modify spatial anchors, determine privacy and security aspects, and
specifically to enable the third party to define which spatial anchors they
manage have restricted access conditions.
[PR 5.4.6-6] The 5G system shall be able to collect charging information for
the establishment or management of an association between a physical location
and service information, where a third party creates, deletes or changes a
spatial anchor or associated service information.
[PR 5.4.6-7] The 5G system shall be able to collect charging information
associated with the network operator exposure of spatial anchors to authorized
third parties, and of service information associated with spatial anchors.
NOTE: The preceding requirements assumes that exposure of network anchors and
associated service information can be a service provided by a network operator
to third parties.
## 5.5 Use Case on Spatial Mapping and Localization Service Enabler
### 5.5.1 Description
Spatial mapping is constructing or updating a map of an unknown location and
localization is tracking an object to identify its location and orientation
over time.
For the localized mobile metaverse use case 5.1, the service provider or
operator needs to provide and use spatial map information, i.e. a 3D map of
indoor or outdoor environment. This use case considers how a spatial map can
be created and employed, both as service enablers. The creation and
maintenance of the spatial map is referred to as Spatial Mapping Service and
the employment of the map to identify the customer\'s Localization is termed
Spatial Localization Service.
Spatial mapping will classify objects into modelling and tracking of
stationary and moving objects. For stationary object, spatial mapping has to
estimate the number of objects, type of object and position. Whereas for
moving objects, spatial mapping has to determine the position, type of object,
direction, speed. Once the spatial mapping service has sufficient information,
it has to map all the stationary and moving objects related to UE's
environment. This information can be provided to the UE, service providers and
surrounding subscribed users as well [17,19,24]
{width="6.695138888888889in" height="1.867361111111111in"}
Figure 5.5.1-1: Spatial Location Services Enabler
Specifically, this use case proposes two spatial localization service
enablers.
1) Spatial Mapping
Sensing data gathered transparently is processed in order to identify the
static and transient forms. For example, in Figure 5.5.1-1
(A), the Rubens room in the Louvre, sensing data captures information.
(B) This information is processed to identify the static and transient
information, to establish an up-to-date spatial map.
(C) In combination with location information (for the sensors) and other
information (e.g. architectural specifications of the Louvre), a spatial map
can be achieved in which not only the forms but also their locations in 3D
space are known.
2) Localization
Given that a spatial mapping exists,
(A) sensing data for a user (from devices communicating by means of a UE) can
be captured,
(B) compared with the spatial map,
(C) used to identify in 3D space the position, viewing direction, angle, etc.
of the user.
**Spatial Mapping Considerations**
Examples when spatial mapping could be useful:
\- A government conducting a digital city project can build a 3D spatial map
of outdoor environment of an entire city or public spaces such as outdoor
parks or indoor offices of their government building.
\- A navigation service provider (or Spatial Localization Service) can build a
3D spatial map of outdoor environment of entire roads or public spaces. An
operator's partner or operator also can build a 3D spatial map of outdoor
environment.
\- A customer wanting mapping of their indoor environment, e.g. the interior
of a commercial space such as the cheese shop as described in 5.4.
It is a hard task to perform the mapping of the entire city using a vehicle by
traversing various roads and spaces. It also requires lots of time and effort
(for data conversion, etc.) if the work is performed in offline. If the
multiple capturing devices are used in parallel, the spatial map data in the
same location could be synthesized over the different cameras and input
devices to generate the spatial map.
The mobile capturing device, vehicle or robot equipped with multiple
stereo/mono RGB cameras and multiple LiDAR to capture various qualities of
images and depth information of the environments. As an example in [15], a
mobile indoor robot is equipped with two LiDARs, 6 industrial cameras and 4
smartphone cameras. Based on the example, we can derive the uplink bandwidth
requirements for one mobile indoor mapping robot. There is a corresponding use
case for \'transparent sensing\' in TR 22.837.
**Localization Considerations**
The environment mapping can be used for providing a visual positioning
service, for enhancing the accuracy of location service or for helping the
metaverse contents management system for spatial internet.
In this use case, the UE provides uplink sensor information that can be
interpreted, along with a spatial map, to identify Localization. This is
analogous to the process used by the UE to provide sensor data that the 5G
system can use to determine UE location for Location Services.
### 5.5.2 Pre-conditions
**For Spatial Mapping**
Mobile operator or service provider can determine the target area for the
spatial mapping. The target area can be divided into multiple areas where each
can be mapped with multiple capturing devices (e.g. capturing indoor robot,
vehicles).
Each capturing device is equipped with multiple sensors, e.g. mono or stereo
RGB camera or one or more multiple LiDAR cameras. The intrinsic parameter of
cameras equipped with the device are pre-configured in the device.
Each capturing device has a capability with high resolution positioning
system, the positioning technique also can derive the altitude information.
An indoor robot can be equipped with non-3GPP based high resolution
positioning system such as UWB. To derive the exact the position of the
capturing device, each capturing device also can utilize the structure-from-
motion technologies.
The capturing devices communicate via a UE that can access the mobile network
of the MNO that supports spatial mapping. The capturing devices can either
sense the spatial area to be mapped, or move about sufficiently that the
capturing devices can be used to acquire sensing data corresponding to the
area.
**For Localization**
A set of sensors are accessible by a UE. These could be built into the
terminal equipment or communicate with it in some way that is out of scope of
this use case, e.g. using a cable, a personal area network, etc.
The UE can access a mobile network of MNO M that offers localization services.
In 3GPP-R16, bistatic localization is performed on both downlink and uplink
using time-difference-of-arrival (TDoA), angle-of-arrival (AoA), and angle-of-
departure (AoD) measurements both at the base station. Both bistatic and
monostatic have limitation with their accuracies, with the advent of high BW
mmWave brings high resolution in both time-delay and angle domains. The
spatial mapping problem can be divided into front and a back-end problem. In
front-end, the problem is to determine the data association - between
landmarks and measurement directions. The back-end problem is to find the
probabilistic SLAM density for a given data association- determined in the
front end. Most of the back-end algorithms are Kalman filter (EKF), Fast-SLAM
and Graph-SLAM, which are based on Bayesian method [21, 22, 23].
5.5.3 Service Flows
**For Spatial Mapping**
1\. A capturing mobile device attaches to the mobile network and becomes
authorized to deliver sensing data for the purposes of spatial mapping.
2\. The capturing mobile device starts the mapping operation.
a. The capturing mobile device arranges for its sensors to provide information
as needed by the 5G system. This could require some configuration of the
capturing mobile device, e.g. the sensors or to control the uplink
communication of sensing data.
3\. The capturing mobile device navigates, e.g. with a pre-defined route,
within the selected target region in order to provide a sufficiently complete
set of sensor information for the space to be mapped.
4\. The capturing mobile device uploads the captured RGB camera images and
LIDAR depth images to the 5G system with the positioning information of the
mobile device.
5\. The mapping server collects and analyzes the information provided to
cumulative create or update a spatial map.
**For Localization**
1\. A mobile device requiring localization attaches to a mobile network and
becomes authorized to obtain Localization services.
2\. The mobile device uses sensors to capture information corresponding to the
point and direction that has to be localized.
3\. The mobile device sends via uplink communication the sensing data to the
5G system.
4\. The 5G system uses the sensing data and a spatial map to determine the
localization, that is, the corresponding positioning information and sensor
pose.
5.5.4 Post-conditions
**For Spatial Mapping**
The spatial mapping enabler re-structures the data provided by mobile capture
devices to create a spatial map. This information could be combined with other
information available, e.g. floor plans of buildings, survey data, satellite
images, etc. Over time sufficient information will be captured to allow the
spatial mapping enabler to distinguish between static and dynamic objects in
the environment.
**For Localization**
The result of the localization service is available as a service enabler. This
could be provided to the UE for use by applications or exposed to a third
party by means of an API by the 5GS. This information can be used for many
purposes, especially for media based applications that require localization to
control the rendering of AR or MR content. Localization can be done over time,
e.g. to track a user\'s movement.
The localization enabler can enhance location services as it can identify with
some degree of precision a location by means of diverse sensors compared with
known location data in the spatial map.
### 5.5.5 Existing feature partially or fully covering use case functionality
Location Services provide location information which can be used to support
services that are triggered or informed by the user\'s location. This
information is not at the degree of accuracy needed for some applications,
e.g. AR. Location Services also do not work in all environments, e.g. indoor.
The 5GS supports uplink media and uplink sensor data communication.
In clause 4.1.4 of TR 26.928 [18] - XR spatial mapping and localization - has
been proposed. This use case specifies the key areas of XR and AR are spatial
mapping, which is creating a map of the surrounding areas, and localization -
positioning the user in that map. This use case depends on multiple external
sensors such as monocular/stereo/depth cameras, radio beacons, GPS, inertial
sensors, etc., There is no requirement on how 5G sensing can be used for
spatial mapping and localization services.
Positioning in 5G Networks been proposed in 3GPP release-16, it specifies
positioning signals and measurements for the 5G NR. In release-16, 5G
Positioning architecture extends 4G positioning architecture by adding
Location Management Function (LMF) and Transmission reception points (TRP).
5G- along with enables- provides new positioning methods based on multi-cell
round-trip time measurements, multiple antenna beam measurements, multiple to
enable downlink angle of departure (DL-AoD) and uplink angle of arrival (UL-
AoA) [M, 21, 22]. Current 5G system supports positioning of the device-based
but not device-free -- Objects that do not radiate EM signals. We propose to
add new requirement in section- 5.5.6 to extend this architecture to support
spatial mapping and localization service enablers.
### 5.5.6 Potential New Requirements needed to support the use case
#### 5.5.6.1 Requirements for Spatial Mapping
[PR 5.5.6.1-1] Subject to operator policy and relevant regional and national
regulation, the 5GS shall support mechanisms for an authorized UE to provide
sensing data that can be used to produce or modify a spatial map.
[PR 5.5.6.1-2]
Subject to operator policy, user consent and relevant regional and national
regulation, the 5GS shall support mechanisms to receive and process sensing
data to produce or modify a spatial map.
[PR 5.5.6.1-3]
Subject to operator policy and relevant regional and national regulation, the
5GS shall support mechanisms to expose a spatial map or derived localization
information from that map to authorized third parties.
NOTE 1: The spatial map and derived localization information supports services
that produce AR and MR media, e.g. as described in clause 5.1.
NOTE 2: The precision of spatial positioning of sensors that provide sensing
data used to create or modify the spatial map is not specified. This could be
revisited in future as more experience accumulates with spatial mapping
services.
[PR 5.5.6.1-4] The 5G system shall support the collection of charging
information associated with the exposure of a spatial map or derived
localization information to authorized third parties.
[PR 5.5.6.1-5] The 5G system shall support the collection of charging
information associated with the production or modification of a spatial map on
behalf of an authorized third party.
#### 5.5.6.2 Requirements for Localization
[PR 5.5.6.2-1] Subject to operator policy and relevant regional and national
regulation, the 5GS shall support mechanisms to authorize Spatial Localization
Service.
[PR 5.5.6.2-2] Subject to operator policy and relevant regional and national
regulation, the 5GS shall support mechanisms for an authorized UE to provide
sensor data that can be used to for Spatial Localization Service.
[PR 5.5.6.2-3] Subject to operator policy and relevant regional and national
regulation, the 5GS shall support mechanisms to expose Spatial Localization
Service information to authorized third parties.
NOTE 2: The Spatial Localization Service information supports services that
produce AR and MR media, e.g. as described in clause 5.1.
[PR 5.5.6.2-4] The 5G system shall support the collection of charging
information associated with exposing spatial location service information to
authorized third parties.
## 5.6 Use Case on Mobile Metaverse for Immersive Gaming and Live Shows
#### 5.6.1 Description
The mobile metaverse combines the physical and digital world. Mobile metaverse
services have already gained significant attention and will benefit multiple
areas, such as gaming, social, medical, industry, transport, and so on [27].
Mobile metaverse services will bring more immersive user experience, which
will bring more potential requirements to 5G systems. Among these fields,
gaming is considered a pioneer in the development of mobile metaverse
services. This use case aims to discuss mobile metaverse services for
immersive gaming and live shows.
With the support of 5GS, game players can interact with each other on the
cloud or edge server, which may form a digital world we term a mobile
metaverse. Figure 5.6.1-1 shows the general idea of this use case. The mobile
metaverse service may be deployed at the cloud or edge server for immersive
gaming and live shows. When the players are playing a basketball game, they
may achieve an immersive experience with their avatars, and the avatars can
interact with each other, whether the players are in proximity or non-
proximity. Meanwhile, other players in the metaverse can join in this digital
world as spectators to watch the live show.
The sensor data obtained by the cloud or edge server may perform coding and
rendering to generate the digital representation for immersive gaming and live
shows, which may be displayed (as if) on a big screen, and the interactive
service data could be exchanged among the players, avatars. Here, sensing data
include the physical pose and gestures including movement. For a basketball
game, the court and surrounding facilities also can have sensor. The sensor
data obtained from these sensors is useful for the metaverse to determine how
to perform 3D digital representation of the participants and setting. An
immersive user experience could be provided for the players and their
audience. The major impact on 3GPP is whether and how 5GS can be used to
better utilize the sensor data and achieve immersive experiences of the
multiple players.
{width="4.738990594925634in" height="2.697579833770779in"}
Fig 5.6.1-1 Mobile Metaverse for Immersive Gaming and Live Shows
### 5.6.2 Pre-conditions
The following are pre-conditions for this use case:
1) The computing resource used for game design and real-time processing such
as game development library and rendering tools could be provided for mobile
metaverse on the cloud or edge server.
2) 5GS is capable of transporting the uplink/downlink service data.
The VR/AR/MR/Cloud Gaming mobile devices, such as mobile headsets or other
haptic mobile devices, could be connected to the cloud or edge server for
supporting the mobile metaverse immersive game and live show via 5GS.
### 5.6.3 Service Flows
The following are service flows for this use case:
1) Player A (a lady who is a fan of the gaming mobile metaverse service)
configures her smartphone (a UE.) One path is established between the cloud or
edge server and the UE. The UE sends a request to the 5GS, and the 5GS
authorizes the request and exposes the capability of mobile metaverse game
production and game development (e.g. expose APIs) to the UE. Player A
controls her avatar and performs game production and game development based on
the rules made by her and the game development or material library stored on
the cloud or edge server. She creates a basketball game venue, game rules, NFT
characters (with player attributes), supermarkets, etc., all stored and run on
the cloud or edge server.
NOTE: The edge may be a burden when it comes to long-term services, such as
mobile metaverse service data storage and large-scale computing. In such a
case, cloud service is essential for being a centralized node to maintaining
shared space for thousands or even millions of concurrent users in such a
large scale mobile metaverse service, and cloud-edge interaction via 5GS is
necessary.
2) Player A invites seven players (B, C, D, E, F, G, H) who are participating
in the mobile metaverse service to join the game venue created by herself.
These players form two teams to play a 3v3 basketball game match. Among them,
B, C, and D are one group, E, F, and G are another group, and H is the game
referee. Each member of each group chooses their own digital representatoin.
Then, player A publishes the game match information, and other players as
spectators in the metaverse can enter the game venue to watch the match.
NOTE: Players A, B, C, D, E, F, G, and H can be located in different areas in
the physical world.
3) The game starts. When the players are playing in a venue realized as a
mobile metaverse service. 3D positioning accuracy is required for the digital
representations (avatars) that represent the players' location and also
gestures in a basketball game. The team members in the physical world control
the (digital representation of the) basketball through 5GS in the uplink
direction by means of a typical mobile input device, e.g., VR headset, VR
glasses. At the same time, the players can interact with each other and pass
the basketball, etc.; though the player has no actual contact with a
basketball in the physical world, he can get some haptic experience of the
basketball. The team members both in the physical world and digital
representation can interact with each other via 5GS anytime, anywhere for an
immersive experience.
4) Spectators can watch the game match through 5G system by a typical mobile
device. At the same time, the spectators can view diverse content such as the
game attributes, including game rules and player information, by switching the
viewing direction. Multiple mobile metaverse media can be provided to
spectators an immersive live show experience through 5G system. The spectators
can interact with each other via 5G system for an immersive experience.
NOTE: During the running of the game, UE can access mobile metaverse services
with low power consumption to reduce the metaverse game interruption. The
cloud or edge server is used for coding, rendering, and generating the mobile
metaverse media for immersive gaming and live show mobile metaverse services.
5) Player A terminates the game application.
### 5.6.4 Post-conditions
The following are post-conditions for this use case:
The players in the game match achieved an immersive game experience by means
of a mobile metaverse serivce enabled by 5GS.
The spectators in the game match had an immersive live show.
The 5GS can address and meet the mobile metaverse service game requirements
with the cloud or edge side.
Players A, B, C, D, E, F, G, and H may earn money from the game, a mobile
metaverse service.
### 5.6.5 Existing features partly or fully covering the use case
functionality
In clause 6.43.2 of 3GPP TS 22.261, there are the following requirements:
The 5G system shall enable an authorized 3rd party to provide policy(ies) for
flows associated with an application. The policy may contain e.g. the set of
UEs and data flows, the expected QoS handling and associated triggering
events, and other coordination information.
The 5G system shall support a means to apply 3rd party provided policy(ies)
for flows associated with an application. The policy may contain e.g. the set
of UEs and data flows, the expected QoS handling and associated triggering
events, and other coordination information.
NOTE: The policy can be used by a 3rd party application for the coordination
of the transmission of multiple UEs' flows (e.g., haptic, audio, and video) of
a multi-modal communication session.
5.6.6 Potential New Requirements needed to support the use case [PR 5.6.6-1]
The 5G System shall support the transmission of uplink sensor data
transmission and downlink feedback information with stringent requirements on
packet delay and bandwidth for real-time interaction.
[PR.5.6.6-2] The 5G System shall support a mechanism to obtain the location
and gestures of the players with stringent requirements on 3D positioning
accuracy.
KPI requirements related to the potential requirements:
**Table 5.6.6-1 -- Potential key performance requirements for immersive gaming
and live shows**
* * *
**Use Case** **Characteristic parameter (KPI)**  
**End-to-end latency** **Service bit rate: user-experienced data rate**
**Reliability** **Positioning accuracy** Mobile Metaverse for immersive gaming
and live shows [5\~20] ms [1\~1000] Mbit/s [>99.99%] [\ 2\. Aphrodite calls automated customer service. Aphrodite calls a customer
> service of company \"Inhabitabilis\" to initiate a video call.
ii. Inhabitabilis customer service employs a \'receptionist\' named Nemo, who
is actually not a person at all. He is a software construct. There is an
artificial intelligence algorithm that generates his utterances. At the same
time, an appearance is generated as a set of code points using a FACS system,
corresponding to the dialog and interaction between Aphrodite and Nemo.
iii. Aphrodite is able to get answers to her questions and thanks Nemo. In all
the above scenarios, the following applies.
> 3\. Aphrodite uses a terminal device without cameras, or whose cameras are
> insufficient and/or Adonis uses a terminal device without avatar codec
> support
>
> In this scenario, the UE used by either calling party is not able to support
> an IMS 3D avatar call. Through the use of transcoding, this lack of support
> can be overcome. In the service flow shown below, as an example,
> Aphrodite\'s UE cannot capture her visually so as to generate an avatar
> encoding, so she expresses herself in text.
i. Aphrodite calls Adonis and wants to share an avatar call. She cannot
however be captured via FACS due to a lack of sufficient camera support on her
UE. Instead she uses _text-based avatar media_.
ii. The text-based avatar media is transported to the point at which this
media is _rendered_ as a 3D avatar media codec.
{width="5.118319116360455in" height="2.3195636482939634in"}
Figure 5.11.3-3: Example of a text-based Avatar Media enables avatar call
without camera, etc. support on a UE
The transcoding rendering of the avatar media to 3D avatar media could be at
any point in the system - Aphrodite\'s UE, the network, or Adonis\' UE.
iii. Adonis\' UE is able to display an avatar version of Aphrodite and hear it
speak (text to voice.) To the extent that the avatar configuration and voice
generation configuration is well associated with Aphrodite, Adonis can hear
and see her speaking, though Aphrodite only provides text as input to the
conversation.
Other examples (not further described here) could, for example, transcode the
media provided by Aphrodite (e.g. text, binary, avatar encoding, etc.) and
transcode it to video for presentation to Adonis. This would be useful if
Adonis\' UE did not have support for avatar encoding.
### 5.11.4 Post-conditions
In each of the scenarios above, avatar media provides an acceptable
interactive choice for a video call experience. The advantages are privacy,
efficiency and ease of integration with computer software to animate a
simulated conversational partner.
### 5.11.5 Existing feature partly or fully covering use case functionality
TS 22.228 define the service requirements for IMS. IMS supports different IMS
multimedia applications. IMS supports a wide range of services, notably voice
and video calls. There is extensive support for services, tightly integrated
with the 3GPP system, with extensive support for roaming and integration with
both PSTN and ISDN telephony, emergency services and more. The requirements
for a 3D avatar application are largely covered by existing requirements in
the 5G standard for IMS.
TS 22.173 defines the media handling capabilities of the IMS Multimedia
Telephony service
The specific gaps that are addressed in 5.A.6 include: extended feature
negotiation, enabling the user to decide whether to present video or avatar
communication, the ability to support Avatar communication and content
efficiently, the ability to support standardized Avatar media in the 5G
system.
The following KPIs are easily supported by the 5G system. They are included in
order to contrast the requirements of an avatar call with a video call.
+----------------------+----------------------+----------------------+ | **Use Case** | **Characteristic | | | | parameter (KPI)** | | +----------------------+----------------------+----------------------+ | | **End-to-end |** Service bit rate: | | | latency**| user-experienced | | | | data rate** | +----------------------+----------------------+----------------------+ | Avatar call | [NOTE1] | \ 1) The body motion or facial expressions of Humphrey are captured at UE1,
> which is transmitted to the network.
>
> 2) With the received information about the user's motion or facial
> expressions, the network renders the avatar (the dynamic 3D object).
>
> 3) The media data (converted from the 3D object) is then transmitted to the
> recipient, UE2 of Mrs. Dursley.
>
> 4) The video image (with the rendered avatar) is displayed at the screen of
> Mrs. Dursley's terminal.
{width="4.8875in" height="1.9256944444444444in"}
Figure 5.16.3-1: An example of avatar call functional flow (image rendering at
the network)
NOTE: it is also possible for UE1 to send video stream to the network, with
which the network can render the avatar (the dynamic 3D object). This is
particularly useful for UEs with limited capability.
3.2 Mrs. Dursley downloads, from the network, one of her registered digital
representations to be used for the XR communication. During the session, the
rendering is done at the terminal side. An example of the functional flow from
Mrs. Dursley to Humphrey is illustrated in figure 5.16.3-2. Note that in this
option the required 3D avatar model needs to be made available at all the
recipients (UE1 in this option).
> 1) The body motion or facial expressions of Mrs. Dursley are captured at
> UE2, which is transmitted to the recipient via the network.
>
> 2) With the received information about Mrs. Dursley's motion or facial
> expressions, UE1 renders the avatar (the dynamic 3D object). The video image
> (with the rendered avatar) is displayed at the screen of Humphrey's
> terminal.
{width="4.711805555555555in" height="1.9145833333333333in"}
Figure 5.16.3-2: An example of avatar call functional flow (image rendering at
the receiving side)
> NOTE: this use case shows the example of rendering of the video-based avatar
> media to 3D avatar. It is also possible to render other media to 3D avatar.
### 5.16.4 Post-conditions
The 3GPP system with a combination of various technologies offers the users an
immersive shopping experience equivalent to a face-to-face purchase in a
crowded market.
### 5.16.5 Existing features partly or fully covering the use case
functionality
The service requirements on IMS Multimedia Telephony Service and supplementary
services are well documented in TS 22.173 since Rel-7, many of which have been
implemented in stage-2 and stage-3 WGs. The requirements on 3GPP IMS
Multimedia Telephony Service are captured in TS 22.261 [5] clause 6.39 as a
result of Rel-18 work.
On the user identity related aspects, there are several features defined
including:
\- Support of Multi-device and Multi-Identity in IMS MMTEL service is captured
in TS 22.173 clause 4.6 [3]:
The support of multiple devices is inherent in IMS. In addition, a service
provider may allow a user to use any public user identities for its outgoing
and incoming calls. The added identities can but do not have to belong to the
served user. Identities may be part of different subscriptions and different
operators.
\- TS 22.101 [4] has specified in clause 26a a set of service requirements on
User Identity:
_Identifying distinguished user identities of the user (provided by some
external party or by the operator) in the operator network enables an operator
to provide an enhanced user experience and optimized performance as well as to
offer services to devices that are not part of a 3GPP network. The user to be
identified could be an individual human user, using a UE with a certain
subscription, or an application running on or connecting via a UE, or a device
("thing") behind a gateway UE._
_Network settings can be adapted and services offered to users according to
their needs, independent of the subscription that is used to establish the
connection. By acting as an identity provider, the operator can take
additional information from the network into account to provide a higher level
of security for the authentication of a user._
_The 3GPP System shall support to authenticate a User Identity to a service
with a User Identifier._
\- Clause 8 of TS 22.261 [5] specifies the security related requirements
covering aspects such as authentication and authorization, identity
management, and data security and privacy.
The functional requirement and performance KPIs in support of XR applications
are mainly captured in TS 22.261 [5]:
\- clause 7.6.1 AR/VR;
\- clause 6.43 Tactile and multi-modal communication service;
\- clause 7.11 KPIs for tactile and multi-modal communication service
In support of metaverse services, additional considerations need to be given
on the following aspects:
\- securely register and store the digital representations (e.g. avatars) for
the users. The user could be an individual human user using a UE with a
certain subscription, or an application running on or connecting via a UE, or
a device behind a gateway UE. The user could also be a third party, which is
typically an enterprise customer having service level agreement with the
operator and interacting with the 3GPP network via an application server.
\- assist the authorization of the use of third party's digital assets (e.g.
the digital representations (e.g. avatars) in the XR communication. The third
party is also involved in the procedure to certify the user identity (e.g. an
employee of the company).
\- when required render the digital representations (e.g. avatars) based on
the voice, facial expression or gesture in the live communication video
### 5.16.6 Potential New Requirements needed to support the use case
[PR 5.16.6-1] Subject to user consent, the 5G system shall support mechanisms
to securely register, store and update the digital assets for a user.
NOTE 1: The user could be a human user using a UE with a certain subscription,
or an application running on or connecting via a UE, or a device behind a
gateway UE. The user could also be a third party, which is typically an
enterprise customer having service level agreement with the operator and
interacting with the 3GPP network via an application server.
[PR 5.16.6-2] Subject to regulatory requirements and operator's policy, the 5G
system shall provide suitable and secure means to allow trusted third-party to
authorize the use of the digital assets (that belong to the third-party
enterprise customer) by a user.
NOTE 2: In a typical example the user is an employee of the third-party
enterprise customer.
[PR 5.16.6-3] The 5G system shall be able to collect charging information per
UE for managing (e.g. register, store and update) the digital assets for an
end user (e.g. typically a human user with a certain subscription).
[PR 5.16.6-4] The 5G system shall be able to collect charging information per
application for managing (e.g. register, store and update) the digital assets
for the third party (e.g. typically an enterprise customer having service
level agreement with the operator).
[PR 5.
16.6-5] Subject to regulatory requirements and user consent, the 5G system
shall support real-time transmission, between a UE and the network, of the
body movement information (e.g. body motion or facial expressions) of a human
user in order to ensure immersive voice/audio and visual experience.
NOTE 3: The body movement information (e.g. body motion or facial expressions)
of a human user is used for rendering of the avatar of this user.
[PR 5.16.6-6] Subject to regulatory requirements, user consent and operator's
policy, the IMS shall support the capabilities of rendering the avatar based
on the body movement information (e.g. body motion or facial expression) of a
human user.
## 5.17 Use Case on Work delegation to autonomous virtual alter ego
### 5.17.1 Description
Artificial Intelligence (AI) is becoming more and more popular in many areas
where especially humans cannot handle complicated tasks well (e.g., factory,
vehicle, robot, mobile). This trend is likely to continue, and AI will be
applied to even more areas. In addition to the rapid expansion of AI, the AI
technology itself is also improving. AI that can express emotions like humans
and AI that can communicate naturally are now emerging. Given these trends, AI
could one day be used not only for industrial use cases, but also as our
personal partner and personal assistant to perform many of the tasks around
us.
This use case proposes a communication with an autonomous virtual alter ego,
which is an AI-based digital representation acting autonomously on behalf of a
user herself/himself in the mobile metaverse services. For example, user\'s
autonomous virtual alter ego autonomously sends a mail to clients on user's
behalf. Also, the alter ego can autonomously communicate with the user, other
physical users, and other alter ego by using the network capabilities based on
the user's 3GPP subscription. Therefore, the use of network by the alter ego
has to be captured correctly by the network from charging point of view.
NOTE: The term \"autonomous virtual alter ego\" means an AI-based digital
representation behaving autonomously on behalf of a user herself/himself in
the mobile metaverse services.
All the experience and knowledge performed both in physical world and
metaverse will be shared between the alter ego and its user, thus creating
more than double the opportunities to play multiple roles simultaneously. This
autonomous virtual alter ego concept aims to improve emotional well--being,
health, and life satisfaction by enabling users to perceive many opportunities
in life, such as balancing work and family and participating in many
communities simultaneously.
### 5.17.2 Pre-conditions
John has a UE which has connectivity to 5GS based on subscription to MNO and a
contract with an autonomous virtual alter ego service provider. The service
setting and parameters for this alter ego service are stored in the user's
subscription data.
John's virtual alter ego has been trained by the autonomous virtual alter ego
service provider, so he can enjoy the alter ego application via his UE.
There are two kinds of application servers. One is for alter ego application.
The other is for other applications which the alter ego application connects
for executing tasks.
The autonomous virtual alter ego service provider is trusted by MNO, and the
alter ego can use the network capabilities autonomously based on user's
subscription to MNO.
> NOTE: The autonomous virtual alter ego application server doesn't always
> have connectivity to internet (e.g., the case that alter ego service is
> operated on edge servers). Therefore, there would be the case that alter ego
> application server connects to other application servers via 5GS not via
> internet.
The MNO offers a service enabler that allows John to request that the network
limit how much resources his alter ego is able to consume on his behalf. The
service enabler also provides John with storage space that he can use to store
application specific data and information about himself in the network. John
is able to configure the enabler to give the virtual alter ego limited access
to John's information.
### 5.17.3 Service Flows
1\. John has a F2F appointment with his important client at client\'s office,
so he cannot attend an internal web meeting scheduled on the same time. Then,
he connects to the alter ego application via his UE and asks his alter ego to
show up and participate in the internal web meeting on his behalf.
2\. His virtual alter ego checks the network resources on 5GS and computing
resources on the alter ego server. Then it judges whether the requested task
is able to be process or not. The virtual alter ego checks with the enabler
server to see what information about John it is able to access.
3\. If the alter ego can complete the tasks, the information from the enabler
server is used to train the alter ego and the alter ego starts the tasks.
Otherwise, the alter ego proposes task examples it can do with current
resources to him, so that John can reconsider the request. For example, the
alter ego can only listen during the meeting and take some notes but can't say
anything. Once John and the alter ego agree on what task(s) the alter ego will
perform, John silences, or turns his off his UE so that he can focus on the
F2F meeting.
4\. Before the alter ego attends the meeting, it accesses the web meeting
server as John's alter ego via internet or 5GS by receiving the permission
from John and web meeting server. When the company internal information or
some other information is needed for the meeting, the alter ego asks the
enabler to permit access to the information or asks John to permit the access
to them.
5\. At the meeting, the alter ego explains something, makes some
questions/comments to other attendees (including physical humans and other
virtual alter egos).
6\. After the meeting, the alter ego autonomously reports to John via 5GS by
message or on call. He looks or listens to the report and returns feedback and
new requests by message or on call if any.
7\. When the alter ego communicates via 5GS, the charging information is
collected and linked to user's charging information to charge the user
subscribing the alter ego service.
{width="6.695138888888889in" height="2.8854166666666665in"}
Figure 5.17.3-1: Alter Ego Service Flow
### 5.17.4 Post-conditions
After receiving the feedback from John, the alter ego is retrained. Then, the
autonomous virtual alter ego becomes more accurate and finishes the tasks much
more immediately. As a result, the time available to him in life is more than
doubled.
### 5.17.5 Existing features partly or fully covering the use case
functionality
AIML model transfer frameworks documented in TR 23.700-80 [51] can be applied
to this use case.
IMS MMTEL services documented in TS 22.173 [3] can be applied to this use
case.
### 5.17.6 Potential New Requirements needed to support the use case
[PR 5.17.6-1] The 5G system shall be able to provide a means for a subscriber
to authorize a third party to use a subscriber's digital representation (e.g.,
avatar) and to access multimedia communication services on behalf of the
subscriber.
[PR 5.17.6-2] The 5G system shall be able to collect charging information
associated with communication involving a digital representation associated
with the subscriber.
## 5.18 Use Case on virtual meeting room in financial services
### 5.18.1 Description
Meeting rooms in banks provide a private place for customers and financial
manager to communicate, financial manager can provide customized information
on the financial products that suitable for the customers. Customers may find
a dedicated room for consulting and signing contact safer and the user
experiences are better. While meeting rooms are limited resource in bank and
customers need to go to bank by themselves for consulting, which will take
more time and resources. A virtual bank meeting room offered by a mobile
metaverse service can solve this limitation.
The virtual banking space can be designed by the consumers based on their user
preference. Consumers can be represented by their digital representations
(e.g. avatars) as they use these mobile metaverse services. Consumers can have
eye contact or observe each others\' body movements in a virtual environment,
generating a friendly face-to-face service experience. With this service
option, bank branches are freed from physical limitations of space and
location.
### 5.18.2 Pre-conditions
Each user has a unique digital representation (e.g. avatar) in the mobile
metaverse service. Bank R provides consumers a virtual bank as a mobile
metaverse service as a location agnostic service experience. This service
requires a high level of security in mobile communication as the content is
sensitive. Users have their own digital representations (e.g. avatars) that
they use to represent themselves when they use the mobile metaverse service,
and these avatars are mapped with their real identification.
### 5.18.3 Service Flows
1\. Frank is a very active user of mobile metaverse service X, he does his
daily work and entertainment making use of this mobile metaverse service X
using his avatar.
2\. Bank R has a virtual branch offered as mobile metaverse service X, in
which the bank provides financial services and can provides different
financial products to consumers based on their individual preferences. Frank
is a VIP customer of Bank R. Frank is considering to have some financial
products and he needs to consult with a professional financial manager in the
virtual bank.
3\. Frank enters the virtual bank branch using his avatar, Bank R will
identify the user Frank, represented by his digital representation (e.g.
avatar,) and authorize these by means of the 5GS to make sure the real
identification of this avatar is the same with Frank.
4\. 5GS will inform Bank R that this avatar is authenticated and authorized to
represent Frank, and this digital representations (e.g. avatar) is authorized
to represent Frank to perform financial actions. Bank R receives this
information and provides this digital representation (e.g. avatar)
representing Frank access to a customized VIP consulting room. In this room,
Bank R can provide consulting and financial services to Frank.
5\. After the authorization, the 5GS will increase automatically update the
security mechanisms (such as encryption algorithms) associated with the PDU
session to guarantee the security of the communication services used to
deliver this financial service.
### 5.18.4 Post-conditions
Frank had a safe and realistic experience using his digital representations
(e.g. avatar) in the virtual meeting room.
### 5.18.5 Existing features partly or fully covering the use case
functionality
None.
### 5.18.6 Potential New Requirements needed to support the use case
[PR 5.18.6-1] Subject to operator policy and national or regional regulation,
the 5G system shall support identification of digital representations (e.g.
avatars) associated with users, for mobile metaverse services.
[PR 5.18.6-2] Subject to operator policy and national or regional regulation,
the 5G system shall support different communication security mechanisms
according to the security requirements of different services.
## 5.19 Use Case on Privacy-Aware Dynamic Network Exposure in Immersive
Interactive Experiences
### 5.19.1 Description
With the proliferation of APIs in existing mobile applications already
creating an extensive market for application exposure, API integration in
emerging Metaverse applications and features is likely to emerge as a major
functionality for enhancing experiences across extended reality functions that
builds upon already-existing API development. Given the importance of
consistent, reliable network access and the low-latency connections necessary
to generate and maintain immersive experiences in Metaverse immersive
experiences, one could reasonably expect the development of APIs supporting
network exposure for configuring and optimizing network features for a diverse
array of emerging functions in extended reality interactions. As 5G begins to
support VR, AR, and MR interactions through the cellular network, questions
surrounding the efficiency and trustworthiness of network exposure to
application developers abound.
In particular, the exposure of network characteristics through and the
development of network-focused applications raises important questions around
the privacy of user data with respect to the use of sensitive data around
their internet usage, which could potentially reveal personally identifiable
information about their location, environment, behaviour, or specific
activities through such exposure. This concern extends beyond industry best
practices and into emerging requirements from regulations such as the GDPR
[52], CCPA [53], and other emerging national and international privacy
regulation frameworks which specify the right of individuals to privacy across
the lifecycle of data that could reveal personally identifiable information
across a broad specification of contexts. It is thus incumbent on this body to
proactively standardize the privacy features of the emerging 5GS in the
context of APIs to ensure that such network exposure in application contexts
does not expose providers or users to undue risks or liability.
### 5.19.2 Pre-conditions
The following pre-conditions and assumptions apply to this use case:
1\. Jenna is developing an application that uses potentially personally
identifiable information.
2\. Jenna is aware of the existence and relevance of tuneable network
characteristics to improve or augment an immersive experience, e.g.,
sufficient tools exist to modify characteristics like streaming bitrate in
immersive contexts.
3\. Jenna has access to exposed APIs allowing her to deploy these features in
relevant experiences for immersive interaction.
### 5.19.3 Service Flows
1\. Jenna develops an application that uses sensitive data, e.g., an
application that uses the real-time location and/or environmental features of
users' appearance and surroundings to generate a personal digital
representation (e.g. avatar) in a mobile metaverse service activity.
2\. Jenna uses an API exposing tuneable network characteristics to carry out
some function, e.g., dynamically adjust the streaming resolution of generated
mobile metaverse media (e.g. avatar/hologram,) or the streaming bitrate of the
mobile metaverse media (e.g. avatar) in motion, based on higher-level network
characteristics accessible in real time through the API.
3\. Jenna develops an application that sends user information through the
application to the network provider. Jenna does so in a way that is compliant
with existing privacy transmission, storage, and processing standards. This
means that Jenna's application considers relevant privacy-preserving features
such as informed consent to process, transmit, store, and appropriately delete
any personally identifiable information collected and ingested during the
flow.
4\. The application uses this information to optimize a network-level feature
such as streaming bitrate corresponding to a tuneable knob through the API.
The network provider also considers relevant privacy-preserving features
ingested as part of the data exchanged during this process.
5\. When ingesting potential personally identifiable information at the
network and/or application level, application provider, user, and network
provider receive transparent, verifiable guarantees that data has been
processed, stored, and transited in compliance with existing regulations
within the user's jurisdiction
### 5.19.4 Post-conditions
1\. Jenna's digital representations (e.g. avatars) and other personally
identifiable information generated through her application are able to safely
exchange information through network exposure APIs without compromising the
privacy of users or the network.
2\. Network providers remain compliant with existing privacy regulations and
best practices.
### 5.19.5 Existing feature partly or fully covering use case functionality
Not applicable.
### 5.19.6 Potential New Requirements needed to support the use case
[PR 5.19.6-1] Subject to national/regional regulations, and user consent, the
5G System shall be able to process and expose information from UEs related to
user's location, user's body, and user's environment, e.g., user's home,
user's immediate vicinity.
NOTE: This requirement does not affect the ability of regulatory services,
e.g., legal intercept service, to access such information without consent of
the user.
## 5.20 Use Case on Immersive Tele-Operated Driving in Hazardous Environment
### 5.20.1 Description
Operating vehicles, lifting devices, or machines in an industrial environment
is hazardous when achieved manually and locally by a human. Depending on the
environment, operators are exposed to dangerous material, toxic fumes, extreme
temperatures, landslide risks, radioactivity, etc.
AGVs already exist, although it is expected that human operators can take
remote control to remotely operate such moving vehicles.
In this use case, it is proposed to leverage 5G to provide an end-to-end
system in which a remote user controls a moving device (vehicle, lifting
device, robot, etc.) with an immersive cockpit displayed on a virtual reality
head-mounted display and haptic gloves for control. Furthermore, the cockpit
is complemented with information from the digital twin of the place in where
the user operates (e.g., sensors in a factory, type of material around, other
moving vehicles or persons).
The use case improves user safety and makes the operations even more accurate
by merging additional information from a digital twin.
### 5.20.2 Pre-conditions
Bob works in a seaport; he operates a lifting device. The place in which he is
operating is surrounded by cranes, machines, containers, pipes, and barrels
containing hazardous substances.
A new mobile metaverse service is available: instead of locally controlling
the device, Bob is installed in a safe remote location from which he is
working. The surrounding information is available through a digital twin of
the seaport and can come from various sources (IoT sensors, CCTV cameras,
connected machines, and other vehicles).
In order to maximize Bob's efficiency, the metaverse service experience
delivered by the system is real-time with non-noticeable latency. This use
case includes both location related and location agnostic service experience
examples.
The mobile metaverse service Bob uses for teleoperation is running on a mobile
metaverse server. In addition, Bob is equipped with a head-mounted display and
haptic gloves to remotely control the vehicle.
### 5.20.3 Service Flows
1\. This morning, Bob stayed home as his boss informed him about a potential
hazard at the factory that was identified through some sensor on a pipe.
Unfortunately, the exact nature and location of the hazard on the pipe are not
known. So, Bob decides to remotely inspect the factory before his boss and
local public authorities arrive to check.
2\. He puts on his head-mounted display on which a cockpit environment is
displayed from the mobile metaverse server: a virtual control panel appears in
front of him. He can see his hands and the control panel in the cockpit. Bob's
application is connected to the mobile metaverse server which enables him to
use the service.
3\. Bob can tell the mobile metaverse server to configure which surrounding
information from the digital twin he wants to monitor. He decides to focus on
the 3D representation of the pipe and get real-time sensor information from
it, as well as live data from the ambient temperature and gas sensors. The
mobile metaverse media displays additional predicted data that temperature is
growing, gas concentration is increasing, and that there is a high risk of
explosion in less than 10min if this continues. This surrounding information
is integrated with other display elements in the cockpit, but he can anchor it
in his FOV.
4\. While driving along the seaport by remotely controlling the lifting device
via its digital twin in the metaverse server, Bob can also see the (hidden)
content of other pipes.
### 5.20.4 Post-conditions
Thanks to the 5G mobile metaverse "Tele-operated Driving" service, Bob has
been able to drive the vehicle remotely in a reactive way avoiding dangers and
finding the leak with the help of the information provided via the digital
twins.
### 5.20.5 Existing feature partly or fully covering use case functionality
The use case related to traffic flow simulation in clause 5.2 already provides
requirements and KPIs related to the operation of a moving UE, similar to an
AGV. However, that use case does not envision the use of remote control, e.g.,
using haptic devices and HMD, which trigger new requirements.
The use case related to critical healthcare services in clause 5.10 captures
the usage of HMD and haptic devices with related requirements and KPIs, which
can be generalized to industrial operations. However, this use case does not
consider time-critical decisions based on surrounding moving objects in an
open area. Neither it relies on real-time digital twin updates to track the
characteristics of the environment (e.g., information about pipe content,
etc.)
### 5.20.6 Potential New Requirements needed to support the use case
[PR 5.20.6-1] The 5G system shall be able to provide a means to associate data
flows related to one or multiple UEs with a single digital twin maintained by
the mobile metaverse service.
[PR 5.20.6-2] The 5G system shall be able to provide a means to support data
flows from one or multiple UEs to update a digital twin maintained by the
mobile metaverse service.
[PR 5.20.6-3] Subject to regulatory requirements and operator's policy, the 5G
system shall be able to support data flows directed towards one or multiple
UEs as a result of a change in a digital twin maintained by the mobile
metaverse service, so that physical objects could be affected via actuators.
NOTE 1: How an application actually operates on physical objects upon
receiving a command via the mobile metaverse service, e.g. using actuators,
changing environmental controls configuration, etc is out of scope of the 5G
system. In addition, regulations and/or other standards could apply to remote
operations (e.g. based on a specific industry).
[PR 5.20.6-4] The 5G system shall be able to support the following KPIs for
remotely controlling physical objects via the mobile metaverse service.
**Use Cases** | **Characteristic parameter (KPI)** |  | **Influence quantity** |  |  |  |  |  |  |   
---|---|---|---|---|---|---|---|---|---|---  
| **Max allowed end-to-end latency** | **Service bit rate: user-experienced data rate** | **Reliability** | **Area Traffic capacity** | **Message Data Volume (bits)** | **Transfer interval** | **Position accuracy** | **UE speed** | **Service Area** | **Remarks**  
Metaverse-based Tele-Operated Driving | [100] ms [25] (NOTE 1) | [10~50 Mbit/s] [25] | 99% [25] | [~360 Mbit/s/km2 ] (NOTE 4) | ~8Mbps video stream. Four cameras per vehicle (one for each side): 4*8=32Mbps. Sensor data (interpreted objects). Assuming 1 kB/object/100 ms and 50 objects: 4 Mbps [25] | 20~100 ms [25] (NOTE 2) | [10] cm [25] | [10-50] km/h (vehicle) [25] Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 3) | UL (NOTE 5)  
| [20] ms [25] | [0.1~0.4 Mbit/s] [25] | 99,999% [25] | [~4 Mbit/s/km2 ] (NOTE 4) | Up to 8Kb per message [25] | 20 ms [25] (NOTE 2) | [10] cm [25] | [10-50] km/h (vehicle) [25] Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 3) | DL (NOTE 5)  
| 1-20ms (NOTE 6) | 16 kbit/s -2 Mbit/s (without haptic compression encoding); 0.8 - 200 kbit/s (with haptic compression encoding) (NOTE 6) | 99.999% (NOTE 6) | [~20 Mbit/s/km2 ] (NOTE 4) | 2-8 (1 DoF) (NOTE 6) |  |  | Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 3) | Haptic feedback  
NOTE 1: The end-to-end latency refers to the transmission delay between a UE and the mobile metaverse server or vice-versa, not including sensor acquisition or actuator control on the vehicle side, processing, and rendering on the user side (estimated additional 100ms total). Target e2e user experienced max delay depends on reaction time of the remote driver (e.g. at 50km/h, 20ms means 27cm of remote vehicle movement). NOTE 2: UL data transfer interval around 20ms (video) to 100ms (sensor), DL data transfer interval (commands) around 20ms. NOTE 3: The service area for teleoperation depends on the actual deployment; for example, it can be deployed for a warehouse, a factory, a transportation hub (seaport, airport etc.), or even a city district or city. In some cases, a local approach (e.g., the application servers are hosted at the network edge) is preferred to satisfy low latency and high-reliability requirements. NOTE 4: The area traffic capacity is calculated for one 5G network, considering 4 cameras + sensors on each vehicle. Density is estimated to 10 vehicles/km2, each of the vehicles with one user controlling them. [25] NOTE 5: Based on [25]. UL is real-time vehicle data (video streaming and/or sensor data), DL is control traffic (commands from the remote driver) NOTE 6: KPI comes from [5] cl 7.11 “remote control robot” use case |  |  |  |  |  |  |  |  |  |   
Table-5.20.6-1: Key Performance Indicator (KPI) for mobile metaverse Tele-
Operated Driving
## 5.21 Use Case on Virtual Emergency Drill over 5G Metaverse
### 5.21.1 Description
An Emergency Drill is a crucial activity for governments, local
municipalities, and citizens to prepare for potential disasters such as
earthquakes, fires, and floods. To make the drills more effective, it is
important for a wide range of people, organizations, and government entities
to participate and create simulations that are as close to real-life disaster
scenarios as possible. The use of a metaverse environment is expected to
significantly enhance the value of these drills. With the ability to provide a
more realistic experience, the Emergency Drill in the metaverse is expected to
not only improve response to direct damage from emergencies, but also provide
valuable data on human thoughts, decisions, and actions in actual crisis
situations.
It is also important for mobile operators to anticipate traffic patterns
related to confirming people\'s safety or evacuation actions during an
emergency, and take measures to address potential data traffic congestion,
overload, or failure of base stations or network equipment. The mobile network
operator should be prepared not only for disasters but also large-scale
network failures. They has to be able to quickly and accurately assess the
extent of damage and impact and take timely action to recover their networks.
### 5.21.2 Pre-conditions
City A, known for its beautiful beaches, attracts many visitors each year.
However, it is located near the sea and is at risk of suffering significant
tsunami damage in the event of a major earthquake. With the challenges of
providing rapid evacuation guidance for residents, saving lives, and restoring
infrastructure, City A holds an annual comprehensive emergency drill. Although
the drill is typically held on a holiday, the number of participants has been
decreasing in recent years due to work, leisure, or COVID-19. This year, City
A has decided to conduct the emergency drill in the metaverse environment to
address this issue.
### 5.21.3 Service Flows
{width="6.658558617672791in" height="3.619063867016623in"}
1\. City A is planning a virtual emergency drill that participants can access
from any location, such as their office, home, or even the beach.
2\. Mobile operator B will provide the 5G system and anticipated operational
and maintenance data for the emergency drill in the metaverse environment.
3\. In the metaverse environment, a virtual disaster, such as an explosion of
Mt. Fuji, is simulated, and participants, including citizens, organizations,
and governments, will immediately respond by assessing the damage, conducting
evacuation and rescue activities, and taking other necessary actions in the
virtual space.
4\. City A and designated organizations will collect various types of data
during the emergency drill.
5\. Additionally, Mobile operator A will collect data in the virtual network
environment, taking into account actual operational and maintenance data from
the real environment, such as UE mobility, overload, and out of coverage, to
evaluate the impact of the network in the event of a disaster and implement
necessary countermeasures in the virtual environment.
### 5.21.4 Post-conditions
By participating in emergency drills, citizens and organizations learn how to
respond to disaster scenarios, such as evacuations and rescues, and this
information can be incorporated into local government disaster preparedness
plans. Additionally, mobile operators can take effective measures to
counteract potential network failures and other adverse impacts.
### 5.21.5 Existing features partly or fully covering the use case
functionality
No existing features are identified.
### 5.21.6 Potential New Requirements needed to support the use case
No potential new requirements have been identified.
## 5.22 Use case of Mobile Metaverse Live Concert
### 5.22.1 Description
Mobile metaverse services allow people to enjoy an online digital concert with
their avatars beyond the limitation of time and space. In order to provide
immersive interactive location agnostic service experience to mobile metaverse
service customers, large amount of computing resouces is needed to perform
real-time processing for audio, video, and interactive data, etc. The thing to
realise here is that different customers will use terminals e.g. XR glasess
with different brands and different processing capabilities, some of the
glasses will not have enough computing resources to perform the real-time
rendering. Through split rendering, most of the computing work task can
offload to the network, the high speed and low latency transmission provided
by 5G system can cooperate with the edge cloud side for real-time rendering,
and combine with the local optimized rendering of the XR terminal side to
provide the immersive and unbounded XR experience. In addition to this,
similar to the real world, people are more likely to watch a concert together
with their friends, further, the mobile metaverse live concert service is also
provides private boxs for group of avatars to enjoy the concert privately, and
different types of social authority can be provided in the private box on
demand.
### 5.22.2 Pre-conditions
1) Alex, Bob and Carey are good friends live in different cities, they agree
to watch the mobile metaverse live concert together.
2) Alex, Bob and Carey are equipment with XR glasses and tactile, the
equipment can capture their voice, facial expression, pose information to
generate avatars and interact with the whole live concert.
3) Enough computing resources can be provided to the mobile metaverse live
concert service and the 5G network is capable of providing sufficiently high
throughput and low latency network transmission.
### 5.22.3 Service Flows
1\. Alex, Bob and Carey's subscribe to the mobile metaverse live concert
services and order a "private box" which can only be used by themselves. Alex,
Bob and Carey will be represented at the virtual concert event by their
avatars.
2\. Due to extensive computing resource requirement for image rendering in the
interactive live concert, and Alex and Carey's glasses are not strong enough
to perform this processing, the UEs negotiate with the mobile network operator
to offload the rendering service to the edge cloud. Carey\'s glasses can only
receive the rendered image and show it in the field of view. Bob's glass is
more advanced, which can render the image itself.
3\. The concert begins and the three friends access the mobile metaverse
service. The live singer is presented to the audience as her own avatar.
During the show, the singer and all the audience will be represented by their
own avatars in the virtual space. Alex, Bob and Carey can adjust their own
visual perspective, such as panoramic view, close range or even backstage. At
the same time, they can also view the singer\'s voice and movement, and
immerse themselves in the concert.
4\. At the same time, extra VIP services can be provided in the "private box",
e.g. to chat with other audience members in the private box without being
overheard. The virtual singer may also enter the \"private box\" to hold a
personal meeting with her selected fans.
### 5.22.4 Post-conditions
The consumers in the mobile metaverse live concert service enjoy a great
immersive experience and socialize with their friends.
The 5G system is capable of supporting the communication required by the
immersive mobile metaverse live concert service. Some extra edge computing
services are also provided to some consumers whose equipment has insufficient
computing capacity.
### 5.22.5 Existing feature partly or fully covering use case functionality
The functional and performance requirements for AR/VR services have been
captured in TS 22.261 clause 7.6.
### 5.22.6 Potential New Requirements needed to support the use case
[PR 5.22.6-1] Subject to operator policy, the 5G system shall be able to
support avatar-based multiparty communication in mobile metaverse service.
## 5.23 Use Case on cooperation between metaverse and network using
interactive XR
### 5.23.1 Description
The mobile metaverse allows users to access an endless virtual world at
anytime and anywhere through their terminals. The mobile metaverse are
expected to behave as the real world, which means in addition to rendering a
virtual environment like the physical world, the perceived spatial-temporal
consistency is also the key point to achieve an immersive location agnostic
service experience.
In mobile metaverse, spatial-temporal consistency for single user could mean,
for example, dropping a virtual pen and seeing this pen fall subsequently.
While for multiple players, this consistency could mean, for example, that one
person cuts down a tree and other people see the tree falling down. This user
experience requires the motion-to-photon latency in the range of 7 ms to 15ms
[5] at least for a single user viewing the consequence of her own actions.
Immersive VR requires the delivery of massive amount of data (in the order of
Gigabyte) at ultra-low latency (less than 20 ms) [54].
NOTE: For location agnostic service experience involving multiple users who
are not in the same location, the requirements above do not apply, since the
service can impose ordering and timing of representations of virtual events in
an arbitrary manner.
It should be noted that the computation resources for rendering involved in
the mobile metaverse is different from the cloud gaming and traditional VR.
For example, running a typical massively multiplayer online game today
requires multiple tera FLOPS of graphics horsepower, and the demand is
expected to grow by two orders of magnitude to create fully immersive mobile
metaverse experiences. [55]
For the mobile metaverse world, distributed computation is an inevitable
processing mode, so the selection of proper servers and data centers should
consider the requirements of network delay, processing delay, storage and
computation resource. The goal is to minimize the user\'s perception of delay.
Therefore, in order to obtain consistent experience in mobile metaverse
service anytime and anywhere, deep collaboration between mobile metaverse and
5G network is needed. The potential collaboration aspects may include caching
location, computation location, communication path, traffic scheduling and
resource allocation in network. For example, when a service request emerges,
the network control policy needs to coordinate the selection of (i) caching
locations to provide digital objects, (ii) computation locations to execute
service functions, and (iii) communication paths to route all associated data
streams, jointly optimized with dynamic decisions on (iv) traffic scheduling
and (v) resource allocation at all network locations. [55]
### 5.23.2 Pre-conditions
There are about 12,000 players sign up for a popular game and appear
simultaneously in a specific setting, such as Eve Online in 2021. Due to the
limitations of the existing server processing, it is not possible to support
such a large number of high concurrency, so the network and application server
need to cooperate to support the distribution of visitors to other servers
while ensuring low latency requirement by XR applications.
### 5.23.3 Service Flows
1\. Bob is a player who attends a popular AR interactive game and gathers with
others in a shared environment, they are aware of each other's action so that
they need high synchronization.
2\. The service provider will provide deployment information of each server to
the 5G network, and request the Bob's physical location and transmission delay
in 5G network.
3\. According to the cooperation agreement with application, 5G network will
expose information to service providers, including the physical location and
network delay of specific terminals or a group of terminals. The network delay
includes the delay inside 5G system (UE to PSA UPF) and the latency
information between PSA UPF and some potential servers.
4\. The new server is selected by the service provider according to the UE
location, network delay, business requirements, computation resource and
storage resource of application servers. The decision result will be sent back
to 5G network. Then the 5G network can then formulate corresponding policies
for the service flows.
5\. The content information will be synchronized to the new server in real
time. 5G network should support the ultra-low latency data transmission,
potentially among multiple operators.
### 5.23.4 Post-conditions
Bob will have a good experience in this interactive AR game.
### 5.23.5 Existing features partly or fully covering the use case
functionality
3GPP started the work of edge computing from R15 to R18. In R15, AF influence
mechanism is introduced to inform the 5G network of the application deployment
information to assist UPF selection. In R16, 5G system supports QoS monitoring
mechanism for end-to-end delay monitoring for URLLC services. In R17, 5GS
supports to solve the problem of edge DNS selection and service migration
between different edge platforms. In R18, the work focuses on the edge
computing platform access from other operator network, and the distribution of
network policies for a group of local UEs.
### 5.23.6 Potential New Requirements needed to support the use case
No potential new requirements have been identified.
## 5.24 Use Case on Authorization of Avatar Usage rights
### 5.24.1 Description
In metaverse, digital humans (avatars) are widely used in business activities,
such as advertising, news reporting, live shows. With the maturity of digital
human technologies and the continuous growth of market demands, lifelike
avatars have become reality in recent years. In the future, more and more
people are expected to use their own avatar to participate in business
activities in the virtual world. Especially, celebrities, famous professors
and other people with special social positions have influences also in the
virtual world. In some scenarios, authorization of avatar usage rights is
needed for commercial or other purposes. If there were no proper management of
avatar usage rights, it could cause the spread of false information, even
result in chaos in virtual world.
Therefore, the 5G system needs to support management and authorization of
avatar usage rights. The owner of the avatar is expected to be responsible for
the speech and behavior of his/her avatar. An individual or an enterprise has
to be authorized by the owner of an avatar before using the avatar especially
in business activities.
### 5.24.2 Pre-conditions
Singer J has her own lifelike avatar, which is used for her live concerts in
metaverse. Her touching voice has won hundreds of millions of fans. Company A
is a clothing manufacturer. Seeing the commercial value of Singer J, the
company invited her to be the company's brand ambassador, who helps to
increase brand awareness and attends product promotion activities. Singer J
has signed one-year business contract with Company A. Subject to the contract,
Singer J's avatar is the brand ambassador for Company A in metaverse.
MNO B provides management services (including authorizing and deauthorizing)
for the use of avatars in the mobile metaverse services. Each avatar has been
assigned a unique identification code in MNO B's management system. MNO B also
provides avatar storage services.
Singer J is one of the subscribers of MNO B, and Company A is also served by
MNO B.
### 5.24.3 Service Flows
1\. Company A registers with the MNO B as an enterprise customer, while Singer
J registers as an individual customer of MNO B. MNO B assigns IDs for Company
A and Singer J respectively.
2\. Singer J registers her personal avatar with MNO B. The avatar is lifelike
and mapped to Singer J's ID in real world. So MNO B identifies and stores the
avatar and its ID, also associates with the avatar's owner, Singer J's ID.
3\. Company A sends a request for the usage rights of Singer J's avatar that
is managed by MNO B.
4\. MNO B sends a request to Singer J to ask for authorization of the avatar
to be used in the mobile metaverse services.
5\. After being confirmed by Singer J, Company A's usage rights of avatar has
been authorized. MNO B updates the system with the information that Company A
has the usage rights of Sing J's avatar.
### 5.24.4 Post-conditions
Upon authorization, Singer J has granted the usage rights of her avatar to
Company A for one year to be used in mobile metaverse services. During this
year, Company A is authorized to use Singer J's avatar for business purposes.
When Company A wants to use Singer J's avatar in a mobile metaverse service,
the 5GS searches the avatar by its ID, and pushes the requested avatar to
company A. At the end of the year, Company A's usage rights of Singer J's
avatar will be duly terminated.
### 5.24.5 Existing features partly or fully covering the use case
functionality
None.
### 5.24.6 Potential New Requirements needed to support the use case
[PR 5.24.6-1] Subject to regulatory requirements, user consent and operator's
policy, the 5G system shall support mechanisms to identify an avatar and
associate the avatar with a subscriber (i.e. the owner of the avatar).
[PR 5.24.6-2] Subject to regulatory requirements, user consent and operator's
policy, the 5G system shall be able to authorize the avatar to be used in
mobile metaverse services.
[PR 5.24.6-3] Subject to regulatory requirements, user consent and operator's
policy, the 5G system shall provide time-bound authorization services for an
avatar to be used in mobile metaverse services.
[PR 5.24.6-4] Subject to regulatory requirements, user consent and operator's
policy, the 5G system shall be able to support mechanisms to manage the
authorization information about the use of an avatar in mobile metaverse
services (e.g. the applied time-bound authorization services, the authorized
users).
[PR 5.24.6-5] Subject to regulatory requirements, user consent and operator's
policy, the 5G system shall be able to identify the subscriber who has the
right to use an avatar in mobile metaverse services.
## 5.25 Use Case on Enabling Metaverse services to users via multiple access
connections
### 5.25.1 Description
The metaverse enables immersive virtual media, 3D avatar and holographic
communications for realizing use cases such as interactive gaming, virtualized
shared workspaces, and immersive conference rooms for remote collaboration,
etc. The goal is to create a virtual world we can work in, interact with, and
even escape to. Many mobile metaverse use cases are applicable to indoor
and/or localized areas such as home, offices, stadiums, shopping malls, movie
theatres, theme parks, hospitals, universities, concert halls, etc. Even
though metaverse services go beyond virtual reality media presenting virtual
worlds that seem to be distant, the scenarios that this use case focusses on
are tied to a single physical location which is mostly indoors and serving a
localized area. Such physical locations may prefer non-3GPP (trusted,
untrusted or wireline) access.
Some mobile metaverse services require more bandwidth and lower latencies
which can be challenging to meet. Major improvements to satisfy these
requirements of uninterrupted, lag-free, immersive mobile metaverse service
experience using non-3GPP access have been made such as:
\- incorporation of 1200 MHz of new spectrum in the 6 GHz band with Wi-Fi 6E
enabling bigger channel sizes up to 160 MHz
\- support up to 1024 QAM with Wi-Fi 6 and 6E and Wi-Fi 7 aiming to support up
to 4096 QAM
\- doubling maximum channel bandwidth available to each device to 320MHz in
the 6GHz band with Wi-Fi 7
\- incorporation of High Band Simultaneous (HBS) Multi-Link Operation (MLO) in
802.11be that aggregates two simultaneous 160 MHz channels (four streams) in 5
GHz and 6 GHz bands reducing latency to \ 99.9% | [~39.6] Tbit/s/km2 (NOTE 5) | - | 20~100 ms (NOTE 3) | - |  99.9%] [14] | [1.55] Tbit/s/km2 (NOTE 8) | Video: 1500 Audio: 100 [14] | - | - | Stationary or Pedestrian | typically  99.9%] (without compression) [> 99.999%] (with compression (NOTE 10)) [26] | [2.25] Tbit/s/km2 (NOTE 8) | 1 DoF: 2-8 3 DoFs: 6-24 6 DoFs: 12-48 [14] | 0.25-10 ms [14] |  |  |  | UL and DL haptic feedback  
Metaverse-based Tele-Operated Driving (NOTE 16) | [100] ms [25] (NOTE 11) | [10~50] Mbit/s [25] | 99% [25] | [~360] Mbit/s/km2 (NOTE 14) | - | 20~100 ms [25] (NOTE 12) | [10] cm [25] | [10-50] km/h (vehicle) [25] Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 13) | UL real-time vehicle data (video streaming and/or sensor data) [25]  
| [20] ms [25] | [0.1~0.4] Mbit/s [25] | 99,999% [25] | [~4] Mbit/s/km2 (NOTE 14) | Up to 8Kb [25] | 20 ms [25] (NOTE 12) | [10] cm [25] | [10-50] km/h (vehicle) [25] Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 13) | DL control traffic (commands from the remote driver) [25].  
| 1-20 ms (NOTE 15) | 16 kbit/s -2 Mbit/s (without haptic compression encoding); 0.8 - 200 kbit/s (with haptic compression encoding) (NOTE 15) | 99.999% (NOTE 15) | [~20] Mbit/s/km2 (NOTE 14) | 2-8 (1 DoF) (NOTE 15) |  |  | Stationary/Pedestrian (user) | Up to 10km radius [25] (NOTE 13) | Haptic feedback  
Viewports streaming from rendering device to AR glasses through direct device
connection  
(tethered/relaying case)  
(NOTE 17) | 10 ms (i.e., UL+DL between AR Glasses display and the rendering UE) (NOTE 18) | [200-2000] Mbit/s | 99.9 %  
(NOTE 18) | - | - | - | - | Stationary or pedestrian (between rendering device and AR glasses) | Up to direct device connection ranging | Immersive AR interactive experience: tethered link  
Pose information from AR glasses to rendering device through direct device
connection  
(tethered/relaying case)  
(NOTE 17) | 5 ms  
(NOTE 18) | [100-400] Kbit/s  
(NOTE 18) | 99.9 %  
(NOTE 18) | - | - | - | - | Stationary or pedestrian (between rendering device and AR glasses) | Up to direct device connection ranging |   
Movie streaming from metaverse server to the rendering device  
(NOTE 20) | Only relevant for live streaming. [1-5] s in case of live streaming | [0.1-50] Mbit/s (i.e., covering a complete OTT ladder from low resolution to 3D-8K)  
(NOTE 19) | 99.9 % | - | - | - | - | [up to 500 km/h] | - | Immersive AR interactive experience: NG-RAN multimodal communication link  
Avatar information streaming between remote UEs (end to end) | 10 ms (i.e., 20ms between both UEs excluding metaverse server processing time)  
(NOTE 22) | [0.1-30] Mbit/s  
(NOTE 21) | 99.9 % | - | - | - | - | [up to 500 km/h] | - |   
Interactive data exchange: voice and text between remote UEs (end to end)  
(NOTE 22) | 10 ms (i.e., 20ms between both UEs excluding metaverse server processing time) | [0.1-0.5] Mbit/s | 99.9 % | - | - | - | - | [up to 500 km/h] | - |   
NOTE 1: The mobile metaverse server receives the data from various sensors, performs data processing, rendering and provide feedback to the vehicles and users. NOTE 2: Examples of typical data volume including 1) camera: 10 Mbit/s per sensor (unstructured), 2) LiDAR: 90 Mbit/s per sensor (unstructured), 3) radar: 10 Mbit/s per sensor (unstructured), and 4) real-time Status information including Telemetry data: [, accessed 24.10.22.
The \"toolbox\" defining the APIs and data schemas should be finalized by the
end of 2022. The architecture of the technical solutions, such as the
centralized or decentralized orientation, are not defined to date.
[B.1] Quote from Ursula von der Leyen, President of the European Commission,
in her State of the Union address, 16 September 2020,
\, accessed 24.10.22.
[B.2] https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-
digital-age/shaping-europe-digital-future_en
[B.3] The figure is from
\, accessed 24.10.22.
###### ### Annex C (Informative): Traffic Characteristics of Metaverse Media
Communication
+----------------+----------------+----------------+----------------+ | Use Cases | D | Data Rate | Traffic | | | evice/Terminal | | C | | | Type Example | | haracteristics | +================+================+================+================+ | Localized | AR capable | - | - Data | | Mobile | glasses | | | | Metaverse | tethered to a | | transmission | | Service Use | UE | | in short | | Case | | | duration | +----------------+----------------+----------------+----------------+ | Mobile | UE (different | [ | - Data | | Metaverse for | types, e.g., | 10-100Mbit/s] | | | 5G-enabled | pedestrians, | | transmission | | Traffic Flow | sensors) | | in long | | Simulation and | | | duration | | Situational | | | | | Awareness | | | - This use | | | | | case | | | | | motivates | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE, | | | | | especially | | | | | for | | | | | | | | | | pedestrians | | | | | by using | | | | | mobile | | | | | phone | +----------------+----------------+----------------+----------------+ | Collaborative | XR devices, | \ | - Data | | and Concurrent | mobile phones, | [1-100Mbit/s] | | | Engineering in | computers | | transmission | | Product Design | | | in long | | using | | | duration | | Metaverse | | | | | Services | | | - This use | | | | | case | | | | | motivates | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | +----------------+----------------+----------------+----------------+ | Spatial Anchor | AR glasses | - | - Data | | Enabler Use | | | | | Case | | | transmission | | | | | in short | | | | | duration | +----------------+----------------+----------------+----------------+ | Spatial | UE | - | - Data | | Mapping and | | | | | Localization | | | transmission | | Service | | | in short | | Enabler Use | | | duration | | Case | | | | +----------------+----------------+----------------+----------------+ | Mobile | VR/AR/MR/Cloud | [ | - Data | | Metaverse for | Gaming mobile | 1-1000Mbit/s] | | | Immersive | devices, such | | transmission | | Gaming and | as mobile | | in long | | Live Shows | headsets or | | duration | | | other haptic | | | | | mobile | | - This use | | | devices, | | case | | | | | motivates | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | +----------------+----------------+----------------+----------------+ | AR Enabled | AR glasses | [20 | - Data | | Immersive | | 0-2000Mbit/s] | | | Experience | | | transmission | | | | | in long | | | | | duration | | | | | | | | | | - This use | | | | | case | | | | | motivates | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | | | | | | | | | | - Detailed | | | | | discussion | | | | | on energy | | | | | | | | | | utilization | | | | | may be | | | | | needed | +----------------+----------------+----------------+----------------+ | Supporting | VR glasses, | - | - Sustained | | Multi-service | Tactile gloves | | diverse | | Coordination | | | data | | in One | | | | | Metaverse | | | transmission | | | | | in long | | | | | duration | | | | | | | | | | - This use | | | | | case may | | | | | motivate | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | support | | | | | depending | | | | | on the | | | | | data | | | | | | | | | | transmission | | | | | (uplink | | | | | and | | | | | downlink). | +----------------+----------------+----------------+----------------+ | Synchronized | Metaverse | - | - Data | | predictive | devices | | | | avatars | | | transmission | | | | | in long | | | | | duration | | | | | | | | | | - This use | | | | | case may | | | | | motivate | | | | | energy | | | | | efficiency | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | +----------------+----------------+----------------+----------------+ | Use Case on | Head mount | \ | - No | | Metaverse for | device, | [1-100Mbit/s] | | | Critical | tactile glove | | requirement | | HealthCare | | | because it | | Services | | | is life | | | | | critical, | | | | | it is | | | | | assumed | | | | | that a | | | | | sufficient | | | | | power | | | | | supply | | | | | exists to | | | | | support an | | | | | adequately | | | | | long | | | | | service | | | | | life. | +----------------+----------------+----------------+----------------+ | IMS-based 3D | UE | - | - Data | | Avatar | | | | | Communication | | | transmission | | | | | in long | | | | | duration | | | | | with low | | | | | data | | | | | volume. | +----------------+----------------+----------------+----------------+ | Virtual humans | Head mount | - | - Data | | in metaverse | device, | | | | | tactile glove | | transmission | | | | | in long | | | | | duration | | | | | with low | | | | | data | | | | | volume. | +----------------+----------------+----------------+----------------+ | Work | UE | - | - Data | | delegation to | | | | | autonomous | | | transmission | | virtual alter | | | in short | | ego | | | duration. | +----------------+----------------+----------------+----------------+ | Immersive | Head mount | [10\~50 | - Data | | Tele-Operated | device | Mbit/s] | | | Driving in | | | transmission | | Hazardous | | | in long | | Environment | | | duration. | | | | | | | | | | - This use | | | | | case may | | | | | motivate | | | | | energy | | | | | efficiency | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | +----------------+----------------+----------------+----------------+ | Virtual | - | - | - Data | | Emergency | | | | | Drill over 5G | | | transmission | | Metaverse | | | in short | | | | | duration. | +----------------+----------------+----------------+----------------+ | Mobile | Head mount | - | - Data | | Metaverse Live | device, | | | | Concert | tactile glove | | transmission | | | | | in long | | | | | duration | | | | | | | | | | - This use | | | | | case | | | | | motivates | | | | | energy | | | | | efficient | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | | | | | | | | | | - Detailed | | | | | discussion | | | | | on energy | | | | | | | | | | utilization | | | | | may be | | | | | needed | +----------------+----------------+----------------+----------------+ | IMS-based 3D | UE | - | - Data | | Avatar Call | | | | | Support for | | | transmission | | Accessibility | | | in long | | Use Case | | | duration | | | | | with low | | | | | data | | | | | volume. | +----------------+----------------+----------------+----------------+ | Localized | - | - | - Data | | Mobile | | | | | Metaverse | | | transmission | | Overload | | | in long | | | | | duration. | | | | | | | | | | - This use | | | | | case may | | | | | motivate | | | | | energy | | | | | efficiency | | | | | content | | | | | delivery | | | | | to and | | | | | from the | | | | | UE | +----------------+----------------+----------------+----------------+
Table-C-1: Analysis of energy efficiency of content delivery in metaverse
services
#