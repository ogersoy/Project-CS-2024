# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
5G systems will extend mobile communication services beyond mobile telephony,
mobile broadband, and massive machine-type communication into new application
domains, so-called vertical domains, with special requirements toward
communication services. Communication for automation in vertical domains comes
with demanding requirements―high availability, high reliability, low latency,
and, in some cases, high-accuracy positioning.
Communication for automation in vertical domains has to support the
applications for production in the corresponding vertical domain, for
instance, industrial automation and energy automation, but also
transportation. This focus―together with regulations specific for vertical
domains―have led to tailored communication concepts in vertical domains such
as dependable communication as well as specific security standards and
mechanisms. The present document provides an overview on these concepts, in
order to foster a common understanding used in communication for automation in
vertical domains and 5G communication services, as well as the inference of a
common terminology.
Many vertical use cases have been analysed by 3GPP and resulted in several
vertical communication requirements that are already part of TS 22.261 [3].
Besides the already specified KPIs for latency, jitter, reliability,
communication service availability, and data rate, other general vertical
communication requirements need to be transposed into potential service
requirements for 5G systems.
Communication for automation in vertical domains may take place in type-a
and/or type-b networks. Network monitoring interfaces are necessary in order
to assure network operation (SLAs). Multiple verticals and users might be
using the same 5G communication network (multi-tenancy). Furthermore, vertical
domains have their own security standards, implementations, and vertical-
domain specific regulations, leading to stage-1 potential security
requirements. Finally, integration between 5G communication systems and
already existing communication networks of vertical domains is required.
Several missing representative vertical use cases for communication in
automation in vertical domains are described in the present document and used
for the derivation of further stage-1 potential requirements. These use cases
focus on low latency, high reliability, and high communication service
availability. Examples are automated guided vehicles and rail-bound mass
transit (subways and suburban rail).
The present document provides new use cases and stage 1 potential requirements
that have not yet been covered in previous stage 1 specifications. It is based
on input from relevant stakeholders of the respective vertical domains.
# 1 Scope
The present document focuses on 5G communication for automation in vertical
domains. This is communication that is involved in the production of and
working on work pieces and goods, and/or the delivery of services in the
physical world. Such communication often necessitates low latency, high
reliability, and high communication service availability. Nevertheless, other
types of communication are also possible in this area. Moreover,
communications with low latency, high reliability, and high communication
service availability, and other, not so demanding communication services, may
run in parallel on the same 5G infrastructure.
The present document identifies stage 1 potential requirements for 5G
communication for automation in vertical domains. The potential requirements
are derived from different sources:
> \- existing work on dependable communication as used in vertical domains;
> see, for instance, IEC 61907 [2];
>
> \- use cases describing network operation in vertical domains with, for
> instance, common usage of the network (multi-tenancy) and network monitoring
> for assurance of service level agreements;
>
> \- security mechanisms already used in vertical domains; supporting the
> specific security requirements of vertical domains;
>
> \- new (additional to already existing stage-1 work), representative use
> cases in different vertical domains based on input from relevant vertical
> interest organisations and other stakeholders.
Furthermore, the present document provides an overview of relevant
communication concepts for automation in vertical domains from the point of
view of 5G systems. This overview is provided in order to facilitate the
mapping between communication for automation in vertical domains and
communication in 5G systems.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
> \- References are either specific (identified by date of publication,
> edition number, version number, etc.) or non‑specific.
>
> \- For a specific reference, subsequent revisions do not apply.
>
> \- For a non-specific reference, the latest version applies. In the case of
> a reference to a 3GPP document (including a GSM document), a non-specific
> reference implicitly refers to the latest version of that document _in the
> same Release as the present document_.
>
> [1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
>
> [2] IEC 61907, \"Communication network dependability engineering\", 2009.
>
> [3] 3GPP TS 22.261: \"Service requirements for the 5G system\".
>
> [4] **IEC 62290-1: \"** Railway applications - Urban guided transport
> management and command/control systems - Part 1: System principles and
> fundamental concepts\".
>
> [5] TTA TTAK KO-06.0-369: \"Functional Requirements for LTE-Based
> Communication System\", Oct. 2014.
>
> [6] J. Kim, S. W. Choi, Y.-S. Song, and Y.-K. Kim, \"Automatic train control
> over LTE: Design and performance evaluation\", IEEE Comm. Mag., Vol. 53, No.
> 10, pp. 102--109, Oct. 2015.
>
> [7] IEEE 1474.1-2004: \"IEEE Standard for Communications-Based Train Control
> (CBTC) Performance and Functional Requirements\".
>
> [8] (Void)
>
> [9] Deterministic Networking, bas-usecase-detnet, IETF, October 2015
>
> [10] Richard C. Dorf and Robert H. Bishop, \"Modern Control Systems\",
> Pearson, Harlow, 13th Edition, 2017.
>
> [11] Ernie Hayden, Michael Assante, and Tim Conway, \"An Abbreviated History
> of Automation & Industrial Controls Systems and Cybersecurity\", SANS
> Institute, [https://ics.sans.org/media/An-Abbreviated-History-of-Automation-
> and-ICS-Cybersecurity.pdf]{.underline} {accessed: 2017-05-23}, 2014.
>
> [12] Gartner, \"Gartner IT
[65] http://www.humanesociety.org/issues/poaching/
> [66] Reliable Low Latency Wireless Communication Enabling Industrial Mobile
> Control and Safety Applications, Vodafone Chair Mobile Communications
> Systems, Technische Universität Dresden.
> https://arxiv.org/pdf/1804.07553.pdf
>
> [67] Network-based communication for Industrie 4.0; Federal Ministry for
> Economic Affairs and Energy (BMWi)\
> https://www.plattform-i40.de/I40/Redaktion/EN/Downloads/Publikation/network-
> based-communication-for-i40.pdf?__blob=publicationFile&v=6
>
> [68] 2015 International Conference on Indoor Positioning and Indoor
> Navigation (IPIN), 13-16 October 2015, Banff, Alberta, Canada, Sensor Fusion
> for Indoor Navigation and Tracking of Automated Guided Vehicles, Risang
> Gatot Yudanto, Frederik Petré\
> https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346941
>
> [69] 2018 International Conference on Indoor Positioning and Indoor
> Navigation (IPIN), 24-27 September 2018, Nantes, France, Location Awareness
> and Context Detection for hand-held Tools in Production Processes,
> Fraunhofer IIS, Paper is expected to be published via IEEE Digital Xplore in
> October 2018
>
> [70] IEC 61512: "Batch control - Part 1: Models and terminology"
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**aggregator:** Service provider managing a system of electric generation
units, storage systems, and load (consumers), with independent control and
customer support in its own coverage area.
**automation:** the automatic operation or control of a process, device, or
system.
NOTE 1: This definition is based on [10].
**characteristic parameter:** numerical value that can be used for
characterising the dynamic behaviour of communication functionality from an
application point of view.
**clock synchronisation service:** the service to align otherwise independent
UE clocks.
**clock synchronicity:** the maximum allowed time offset within the fully
synchronised system between UE clocks.
NOTE 2: Clock synchronicity (or synchronicity) is used as KPI of clock
synchronisation services.
**communication service availability** : percentage value of the amount of
time the end-to-end communication service is delivered according to an agreed
QoS, divided by the amount of time the system is expected to deliver the end-
to-end service according to the specification in a specific area.
NOTE 3: The end point in \"end-to-end\" is assumed to be the communication
service interface.
NOTE 4: The communication service is considered unavailable if it does not
meet the pertinent QoS requirements. If availability is one of these
requirements, the following rule applies: the system is considered unavailable
in case an expected message is not received within a specified time, which, at
minimum, is the sum of end-to-end latency, jitter, and survival time.
NOTE 5: This definition was taken from clause 3.1 in [3].
**communication service reliability:** ability of the communication service to
perform as required for a given time interval, under given conditions.
NOTE 6: Given conditions would include aspects that affect reliability, such
as: mode of operation, stress levels, and environmental conditions.
NOTE 7: Reliability may be quantified using appropriate measures such as
meantime to failure, or the probability of no failure within a specified
period of time.
NOTE 8: This definition is based on [2].
**end-to-end latency:** the time that takes to transfer a given piece of
information from a source to a destination, measured at the communication
interface, from the moment it is transmitted by the source to the moment it is
successfully received at the destination.
NOTE 9: This definition was taken from clause 3.1 in [3].
**factory automation:** automation application in industrial automation
branches typically with discrete characteristics of the application to be
automated with specific requirements for determinism, low latency,
reliability, redundancy, cyber security, and functional safety.
NOTE 9A: Low latency typically means below 10 ms delivery time.
NOTE 9B: This definition is taken from [19].
**IoT device:** a type of UE which is dedicated for a set of specific use
cases or services and which is allowed to make use of certain features
restricted to this type of UEs.
NOTE 10: An IoT device may be optimised for the specific needs of services and
application being executed (e.g., smart home/city, smart utilities, e-Health
and smart wearables). Some IoT devices are not intended for human type
communications.
NOTE 11: This definition was taken from clause 3.1 in [3].
**isochronous** : the time characteristic of an event or signal that is
recurring at known, periodic time intervals.
NOTE 12: Isochronous data transmission is a form of [synchronous data
transmission](http://ecss.nl/
NOTE 13: Isochronous data transmission ensures that data between the source
and the sink of the A/V application flows continuously and at a steady rate.
**influence quantity:** quantity not essential for the performance of an item
but affecting its performance.
NOTE 14: This definition is taken from IEV 151-16-31 in [45].
**jitter:** the maximum deviation of a time parameter relative to a reference
or target value
NOTE 15: In this document, jitter is used for describing the variation of end-
to-end latency and update time.
**microgrid:** Local grid, with own energy generation and power consumption;
limited geographical area, typical example: power network for a university
campus.
**private slice:** a dedicated network slice deployment for the sole use by a
specific tenant.
**process automation:** automation application in industrial automation
branches typically with continuous characteristics of the application to be
automated with specific requirements for determinism, reliability, redundancy,
cyber security, and functional safety.
NOTE 15A: This definition is taken from [19].
**renewable generators:** photovoltaic panels or wind turbines; energy
generation unit.
**survival time:** the time that an application consuming a communication
service may continue without an anticipated message.
NOTE 16: This definition was taken from clause 3.1 in [3].
**transmission time:** the interval from a start event at the reference
interface of a source until a stop event of the same transmission at the
reference interface of a target.
NOTE 17: Depending on the type of reference interface, the start event can be
the transfer of the first bit of user data, the first byte, or a trigger event
at a process interface. Respectively, the stop event can be the last bit of
user data, the last byte or a trigger event of a process interface.
NOTE 18: This definition is based on [19].
**type-a network:** a 3GPP network that is not for public use and for which
service continuity and roaming with a PLMN is possible.
NOTE 19: The properties of type-a networks are summarised in Annex G.
**type-b network:** an isolated 3GPP network that does not interact with a
PLMN.
NOTE 20: The properties of type-b networks are summarised in Annex G.
**update time:** the interval from a start event at the reference interface of
a target until a following stop event at the same reference interface.
NOTE 21: Depending on the type of reference interface, the start event can be
the transfer of the last bit of user data, the last byte, or a trigger event
at the process interface of a consumer.
NOTE 22: The stop event can be the last bit of user data, the last byte, or a
trigger event of a process interface that can be referred to the following
successful transmission of the same source
NOTE 23: This definition is based on [19].
**up state:** state of being able to perform as required.
NOTE 24: This definition is based on entry IEV 192-02-01 in [45].
**up time:** time interval for which the item is in an up state.
NOTE 25: This definition is based on entry IEV 192-02-02 in [45].
**user equipment:** An equipment that allows a user access to network services
via 3GPP and/or non-3GPP accesses.
NOTE 26: This definition was taken from Clause 3.1 in [3].
**user experienced data rate:** the minimum data rate required to achieve a
sufficient quality experience, with the exception of scenario for broadcast
like services where the given value is the maximum that is needed.
NOTE 27: This definition was taken from clause 3.1 in [3].
**vertical domain:** a particular industry or group of enterprises in which
similar products or services are developed, produced, and provided.
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
_T_ ~cycle~ Cycle time of a cyclic data communication service
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
> 5G Fifth Generation
>
> AC Alternating Current
>
> AGV Automated Guided Vehicle
>
> AR Augmented Reality
>
> A/V Audio/Video
>
> C2C Control-to-Control
>
> CAN Controller Area Network
>
> CCTV Closed Circuit Television
>
> CRC Cyclic Redundancy Check
>
> DL Down Link
>
> DMS Distribution Management System
>
> DP Differential Protection
>
> DSO Distribution System Operator
>
> DTU Distribution Termination Units
>
> EAP Extensible Authentication Protocol
>
> ECU Engine Control Unit
>
> EMS Energy Management System
>
> ERP Enterprise Resource Planning
>
> FIFO First in, First out
>
> FR Foundational Requirement
>
> GoA Grade of Automation
>
> GNSS Global Navigation Satellite System
>
> HD High definition
>
> HGV Heavy Good Vehicle
>
> HMI Human-Machine Interface
>
> HVAC High-Voltage Alternating Current
>
> HVDC High-Voltage Direct Current
>
> IEC International Electrotechnical Commission
>
> IEEE Institute of Electrical and Electronics Engineers
>
> IEM In-Ear Monitor
>
> INV Inverter, electronic power converter, with co-located communications and
> data processing unit
>
> IOPS Isolated E-UTRAN Operation for Public Safety
>
> IoT Internet of Things
>
> IPsec IP Security
>
> IT Information Technology
>
> LAA Licensed-Assisted Access
>
> LOS Line of Sight
>
> µDC Micro Data Centre
>
> MEC Multi-Access Edge Computing
>
> MES Manufacturing Execution System
>
> ML Machine Learning
>
> MNO Mobile Network Operator
>
> MPSC Manufactured Product as a Smart Client
>
> MTTC Mass Transit Train Control
>
> OT Operational Technology
>
> OTT Over the Top
>
> PA Public Address
>
> PDU Protocol Data Unit
>
> PER Packet Error Ratio
>
> PLC Programmable Logic Controller
>
> PMSE Programme Making and Special Events
>
> PMU Phasor Measurement Unit
>
> PS Power Station
>
> PTP Precision Time Protocol
>
> RE Requirement Enhancement
>
> RES Renewable Energy Sources
>
> SCADA Supervisory Control and Data Acquisition
>
> SL Security Level
>
> SR Security Requirement
>
> TSO Transmission System Operator
>
> UL Up Link
>
> VLAN Virtual LAN
>
> VR Virtual Reality
>
> WSN Wireless Sensor Network
>
> WWAN Wireless Wide Area Network
# 4 Overview
## 4.1 Background
Clause 4 serves two main purposes. One, in Clause 4.2, it discusses the
concept of a vertical domain and it provides an overview of the vertical
domains addressed in the present document. Two, in Clause 4.3, it provides a
short overview of salient concepts developed in automation research and
standardisation. In Clause 4.3.1 automation control paradigms and related data
flows are discussed. In Clause 4.3.2, models for describing communication in
automation are introduced, and the introduced concepts and parameters are
mapped onto 5G concepts. In Clause 4.3.3, the paramount automation system
attribute---i.e. dependability---is discussed and implications for 5G
communication systems are identified. In Clause 4.3.4 these dependability
attributes are described from a 5G communication service perspective and
potential parameters for describing dependable communication services,
monitoring services, location services, and clock synchronisation services are
introduced.
## 4.2 Vertical Domains
A vertical domain is a particular industry or group of enterprises in which
similar products or services are developed, produced, and provided.
The vertical domains addressed in the present document are
\- rail-bound mass transit;
\- building automation;
\- factory of the future;
\- eHealth;
\- smart city;
\- electric-power distribution;
\- central power generation;
\- programme making and special events; and
\- smart agriculture.
For pertinent use cases see Clause 5.
## 4.3 Automation
### 4.3.1 Data flows in automation
#### 4.3.1.1 Introduction
The term automation stands for the control of processes, devices, or systems
in vertical domains by automatic means [10]. Note that a process always
includes physical entities and their attributes. By providing particular input
to a process, one tries to generate a particular output (see Figure
4.3.1.1-1).
{width="3.5833333333333335in" height="0.8930555555555556in"}
Figure 4.3.1.1-1: Process to be controlled [10]. Note that the process always
has a physical component
NOTE: Example: input = heat; process = chemical reaction in a gas. In this
example, the output is the chemical reaction products.
Examples for such processes are chemical processes in the chemical industry,
the control of subways, and factory automation with industrial robots. Example
(1): the process in question is a chemical reaction and the input is heat; the
yield of the chemical reaction, i.e., the output, varies with said heat.
Example (2): for a subway, the input can be electrical energy, the process the
acceleration of the subway, and the output reaching the cruising velocity of
the subway.
The technology related to automation is referred to as operational technology,
which \"is hardware and software that detects or causes a change through the
direct monitoring and/or control of physical devices, processes and events in
the enterprise.\" [12]. An overview of operational-technology devices can be
found elsewhere in the literature **[11]. The last decade has seen an
increased integration of operational and information technology [14].**
Automation technology can be used in private settings such as factories, but
it is also used in critical infrastructure such as the electricity grid, civil
aviation, public transport, etc. [13].
Clause 4.3.1.2 introduces the main type of systems used in automation, i.e.
control systems. Clause 4.3.1.3 introduces the most common activity patterns
of control systems. Clause 4.3.1.4 discusses the communication attributes of
an automation system. Finally, Clause 4.3.1.5 presents the communication
patterns entailed by automation systems.
#### 4.3.1.2 Control systems
As outlined in Clause 4.3.1.1, automation is about controlling processes by
aid of automated means. This objective is accomplished by the use of control
systems. \"A **control system** is an interconnection of components forming a
system configuration that will provide a desired [process] response.\" [10].
Four main control functions can be distinguished [11]:
\- measure: obtain values from sensors and feed these values as input to a
process and/or provide these values as output, for instance to a human user;
\- compare: evaluate measured values and compare to process design values;
\- compute: calculate, for instance, current error, historic error, future
error etc.;
\- correct or control: adjust the process.
The four functions above are typically performed by four elements [11]:
\- sensor: device capable of measuring various physical properties;
\- transmitter: device that converts measurements from a sensor and sends the
signal;
\- controller: provides the logic and control instructions for the process;
\- actuator: changes the state of the environment; here the process.
NOTE: Frequently, the combination of sensor and transmitter is referred to as
a sensor. This is the style we are adhering to in the remainder of the present
document.
There are four common patterns of automation. One is open-loop control, the
second is feedback or closed-loop control, the third is sequence control
[10][11], and the fourth is batch control [70]. These patterns are discussed
in more detail in Clause 4.3.1.3.
#### 4.3.1.3 Activity patterns in automation
##### 4.3.1.3.1 Open-loop control
The salient aspect of open-loop control is the lack of output control; when
providing desired output responses to an actuator, it is assumed that the
output of the influenced process is predetermined and within an acceptable
range. Figure 4.3.1.3.1-1 depicts an open-loop control system.
{width="4.797916666666667in" height="0.7979166666666667in"}
Figure 4.3.1.3.1-1: Open-loop control system [10]
This kind of control loop works if the influences of the environment on
process and actuator are negligible. Also, this kind of control is applied in
case unwanted output can be tolerated. For instance, the damage done by an
electric toaster (open-loop control system) by slightly burning a slice of
bread is usually assumed to be negligible. In this example, the desired output
response is the crispiness of the toasted bread. The response can, for
instance, be chosen by turning a dial on the toaster. The actuator is the
heater in the toaster. The dial sets a certain energy level and/or toasting
time. When activated, the heater generates heat, which increases the
crispiness of the inserted slice of bread over time. This is the process. The
output is the toast itself.
##### 4.3.1.3.2 Closed-Loop Control
Closed-loop control enables the manipulation of processes even if the
environment influences the process or the performance of the actuator changes
over time. This type of control is realised by sensing the process output and
by feeding these measurements back into a controller. **Figure 4.3.1.3.2-1
depicts such a system.**
{width="5.273611111111111in" height="1.75in"}
Figure 4.3.1.3.2-1: Multi-variable closed-loop control system [10]
\"In contrast to an open-loop control system, a closed-loop control system
utilizes [measurements] of the actual output to compare the actual output with
the desired output response.\" [10]. \"An example of a closed-loop control
system is a person steering an automobile (assuming his or her eyes are open)
by looking at the auto's location on the road and making the appropriate
adjustments.\" [10].
##### 4.3.1.3.3 Sequence Control
Sequence control may either step through a fixed sequence or it employs logic
that performs different actions based on various system states and system
input [10][11]. Sequence control can be seen as an extension of both open-loop
and closed-loop control, but instead of achieving only one output instance, an
entire sequence of output instances can be produced. An example of sequence
control is controlling an elevator. Based on at what floor it currently
resides, to what floor it is summoned, and to what floor it is directed,
different kinds of control actions and actuations are taken. Note that
although the name \"sequence control\" seems to imply a pre-programmed
sequence of desired output responses, the elevator example shows that event-
based control can also be realised with sequence control.
##### 4.3.1.3.3A Batch control
Batch processes lead to the production of finite quantities of material
(batches) by subjecting input materials to a defined order of processing
actions by use of one or more pieces of equipment [70]. The product produced
by a batch process is called a batch. Batch processes are discontinuous
processes. Batch processes are neither discrete nor continuous; however, they
have characteristics of both.
#### 4.3.1.4 Communication attributes
##### 4.3.1.4.1 Introduction
**Communication in automation can be characterised by two main attributes:
periodicity and determinism.**
##### 4.3.1.4.2 Periodicity
**In terms of periodicity, it is possible to send messages periodically or
aperiodically.**
**Periodically means that a transmission interval is repeated. For example, a
transmission occurs every 15 ms. Reasons for a periodical transmission can be
the periodic update of a position or the repeated monitoring of a
characteristic parameter. Note that a transmission of a temperature every 15
minutes is a periodical transmission. However, most periodic intervals in
communication for automation are rather short. The transmission is started
once and continuous unless a stop command is provided.**
**An a-periodical transmission is, for example, a transmission which is
triggered instantaneously by an event, i.e. events are the trigger of the
transmission. Events are defined by the control system or by the user. Example
events are:**
\- Process events: events that come from the process when thresholds are
exceeded or fallen below, e.g., temperature, pressure, level, etc.
\- Diagnostic events: events that indicate malfunctions of an automation
device or module, e.g. power supply defective; short circuit; too high
temperature; etc.
\- Maintenance events: events based on information that indicates necessary
maintenance work to prevent the failure of an automation device.
**Most events, and especially alarms, are confirmed. In this context, alarms
are messages that inform a controller or operator that an event has occurred,
e.g. an equipment malfunction, process deviation, or other abnormal condition
requiring a response. The receipt of the alarm is acknowledged by the
application that received the alarm. The receipt of the event is usually
confirmed within a short time period. If no acknowledgment is received from
the target application after a pre-set time---the so-called monitoring time---
has elapsed, the alarm is sent again after a preset time.**
##### 4.3.1.4.3 Determinism
**Determinism refers to whether the delay between transmission of a message
and receipt of the message at the destination address is stable (within
bounds). Usually, communication is called deterministic if it is bounded by a
given threshold for the latency/transmission time. In case of a periodic
transmission, the variation of the interval is bounded.**
##### 4.3.1.4.4 Control systems and related communication patterns
**There is no straight-forward, one-to-one mapping between the type of control
and the communication pattern. However, there are preferences. Open-loop
control is characterised by one or many messages sent to an actuator. These
can be sent in a periodic or an aperiodic pattern. However, the communication
means used need to be deterministic since typically an activity response from
the receiver and/or the receiving application is expected. For instance, for
the toaster example in Clause 4.3.1.3.1, the waiting time between activating
the toaster and commencement of heating the slice of bread should not be
arbitrary.**
**Closed-loop control produces both periodic and aperiodic communication
patterns. If, for instance, sensor output is only generated when a threshold,
e.g. a preset room temperature is exceeded, the timing of the message
transmitted to the controller is regulated by the process and not a preset
timer. Closed-loop control is often used for the control of continuous
processes with tight time-control limits, e.g., the control of a printing
press. In this case, one typically relies on periodic communication patterns.
Note that in both the aperiodic and periodic case, the communication needs to
be deterministic. For instance, for periodic communication, measurements from
adjacent measurement cycles could otherwise arrive at the controller out of
order and not within the time needed to guarantee a stable operation of the
controller.**
**The communication attributes required by sequence control generally depend
on whether the underlying control paradigm is open or closed.**
**Logging of device states, measurements, etc. for maintenance purposes and
such typically entails aperiodic communication patterns. In case the
transmitted logging information can be time-stamped by the respective
function, determinism is often not mandatory.**
### 4.3.2 Communication in automation
#### 4.3.2.1 Modelling of communication in automation
##### 4.3.2.1.1 Area of consideration
For our discussion of the communication in automation we apply a definition of
the area of consideration for industrial radio communication that is found
elsewhere in the literature [17]. This definition is depicted in Figure
4.3.2.1.1-1.
{width="5.86875in" height="3.36875in"}
Figure 4.3.2.1.1-1: Abstract diagram of the area of consideration for
industrial radio communication
NOTE: Blue objects: communication system; other objects: automation
application system.
Here, a distributed automation application system is depicted. This system
includes a distributed automation application, which is the aggregation of a
number of automation functions. These can be functions in sensors, measurement
devices, drives, switches, I/O devices, encoders etc. Field bus systems,
industrial Ethernet systems, or wireless communication systems can be used for
connecting the distributed functions. The essential function of these
communication systems is the distribution of messages among the distributed
automation functions. Depending on the objectives, the dependability of the
entire communication system and/or of its devices or its links may be of
interest (more on dependability in Clause 4.3.3). Communication functions are
realised by the respective hardware and software implementation.
In order for the automation application system to operate, messages need to be
exchanged between spatially distributed application functions. For that
process, messages are exchanged at an interface between the automation
application system and the communication system. This interface is termed the
reference interface. Required and guaranteed values for characteristic
parameters which describe the behavioural properties of the radio
communication system refer to that interface (see Clause 4.3.4.4 and
[19][62]).
These characteristic parameters include dependability parameters of industrial
radio communication, which are defined in [17] and [62].
The conditions that influence the behaviour of wireless communication are
framed by the communication requirements of the application (e.g., length of
the message), the characteristics of the communication system (e.g., output
power of a transmitter), and the transmission conditions of the media (e.g.,
signal fluctuations caused by multipath propagation).
If a dependability assessment is to be performed, it is necessary---in
accordance with the definition of the concept of dependability---to specify an
asset, its function, and the conditions under which the function is to be
performed. In this context, an asset is for instance a logical link (see
Clause 4.3.2.1.2).
General requirements from the application point of view for the time and
failure behaviour of a communication system are mostly related to an end-to-
end link. It is assumed in this connection that the behaviour of the link is
representative of the communication system as a whole and of the entire scope
of the application.
##### 4.3.2.1.2 Logical link
###### 4.3.2.1.2.1 Nature and function
Starting with the general approach mentioned above, the logical link can be
regarded as a possible asset within the area of consideration (see Figure
4.3.2.1.2.1-1). The conditions under which its functions are to be performed
are vital for the dependability of the automation application system.
{width="6.690277777777778in" height="3.61875in"}
Figure 4.3.2.1.2.1-1: The concept of a logical link
This is the link between a logical end point in a source device and the
logical end point in a target device. Logical end points are elements of the
reference interface, which may group several logical end points together.
The intended function of the logical link is the transmission of a sequence of
messages from a logical source end point to the correct logical target end
point. This is achieved by transforming each message into a form that fosters
error-free transmission. The transmission process includes certain processes,
e.g. repetitions, in order to fulfil the intended function. After
transmission, the message is converted back into a form which is usable by the
application. The message is to be available and correct at the target within a
defined time. The sequence of messages at the target is to be the same as the
sequence at the source.
The functional units which are necessary to fulfil this function are shown in
Figure 4.3.2.1.2.1-2.
{width="6.273611111111111in" height="1.8215277777777779in"}
Figure 4.3.2.1.2.1-2: The asset \"logical link\"
The required function can be impaired by various influences, which can lead to
communication errors. Such errors are described elsewhere in the literature
[17][18]. A summary of these errors is provided in Annex B. The occurrence of
one of these errors influences the values of the relevant dependability
parameters of the logical link.
###### 4.3.2.1.2.2 Message transformation
From an implementation point of view, it is hardly possible to identify
communication layers and interfaces in devices in a unified manner, e.g. with
reference to the Open System Interconnection (OSI) model [15]. However, the
implementation of communication functions is mostly split between a higher
communication layer (HCL) and a lower communication layer (LCL), which may
contain different parts of the OSI reference model from implementation to
implementation. Our further discussion is therefore based on a generic
implementation view with HCL and LCL.
The messages to be transmitted for the intended function of a logical link are
defined by strings of characters with a certain semantic. Such a character
string is handed over as user data at the reference interface for
transmission. If the number of characters in a message is too great for it to
be transmitted as a unit, the message can be divided for transmission into
several packets (fragmentation). Figure 4.3.2.1.2.2-2 uses repeated sending as
a hedging method for packet loss (example of an unconfirmed service). The
packets are then passed from a higher communication layer (HCL) to a lower
communication layer (LCL) [Figure 4.3.2.1.2.2-1]. There, a bit stream is
created and handed over to the physical layer (PL). A signal stream
corresponding to the bit stream is transmitted from the physical layer of the
source device to the target device. In the target device, the signal stream
received is converted by physical layer into a bit stream, which is passed to
the lower communication layer. There, packets are formed, handed over by the
lower communication layer to the higher communication layer and grouped
together into a message. Suitable mechanisms (acknowledgement, parallel
transmission through different communication channels/media, multiple
transmissions of identical packets, etc.) can increase the probability of the
message reaching the application correctly when a packet is lost. The loss of
a packet is therefore not to be equated in all cases with the loss of a
message.
Figure 4.3.2.1.2.2-1 shows the transmission of a message that is broken into
two packets. The transmission includes acknowledgements. If no acknowledgement
is received within the required period (packet 2), the packet is transmitted
again (bit stream 2). This is the main difference to, for example, Figure
4.3.2.1.2.2-2, where the packets are repeated from the beginning to protect
against loss or error directly. In Figure 4.3.2.1.2.2-2, a confirmation is not
sent.
{width="6.547916666666667in" height="6.464583333333334in"}
Figure 4.3.2.1.2.2-1: Illustration of message, packet and bit stream
transmission for the example of repeated packet transmission
NOTE: HCL: higher communication layer; LCL: lower communication layer; PL:
physical layer.
{width="6.428472222222222in" height="5.916666666666667in"}
Figure 4.3.2.1.2.2-2: Illustration of message, packet and bit stream for the
example of unacknowledged repeated transmission
NOTE: HCL: higher communication layer; LCL: lower communication layer; PL:
physical layer.
##### 4.3.2.1.3 Communication device
The communication devices---together with the physical link---determine the
function and thus the dependability of the logical link (see Figure
4.3.2.1.3-1). The function of the communication devices is the correct sending
and correct receipt of sequences of messages. The methods and algorithms
implemented in the communication devices should take the best possible account
of the transmission conditions during message transmission, and fulfil the
requirements for message transmission as well as possible.
{width="6.809722222222222in" height="2.0479166666666666in"}
Figure 4.3.2.1.3-1: Asset \"communication device\"
Apart from the methods and algorithms themselves, their implementation in
hardware and software is also of importance. The errors listed in Annex C can
have an impact on dependability.
##### 4.3.2.1.4 Communication system
The communication system as an asset represents a quantity of logical links
whose message transmissions are implemented by a number of wireless devices
via one or more media. The communication system function to be provided
consists in transmitting messages for all the logical links in the distributed
application. This function is to be performed for a defined period, the
operating time of the automation application.
In an automation application system it is paramount that requirements
pertaining to logical links are fulfilled. These requirements and the
conditions can be very different from one case and implementation to the
other. The functions (services and protocols) for individual logical links can
therefore also be different. In spite of these differences, some of the
logical links share communication devices and media. Consequently, the
communication system as a whole is an asset for dependability assessment in
the examination of system and application aspects.
### 4.3.3 Dependable communication
#### 4.3.3.1 Introduction
According to ISO, dependability (of an item) is the \"ability to perform as
and when required\" [20]. This is a paramount property of any automation
system. Automation systems that are not dependable can, for instance, be
unsafe or they can exhibit low productivity. Clause 4.3.3.2 discusses system
dependability in further detail, and this information is used to analyse
communication dependability and its implication for 5G systems in Clause
4.3.3.3.
#### 4.3.3.2 System dependability
Dependability can be broken down into five system properties: reliability,
availability, maintainability, safety, and integrity (see Figure 4.3.3.2-1)
[44].
{width="5.595138888888889in" height="1.13125in"}
Figure 4.3.3.2-1: The five facets of system dependability: reliability,
availability, maintainability, safety, and integrity [44]
Definitions for each system property are provided in Table 4.3.3.2-1.
Table 4.3.3.2-1: Definitions of the five system properties into which system
dependability can be broken down (see Figure 4.3.3.2-1) [44].
**System property** **Definition**
* * *
Reliability Continuity of correct operation Availability Readiness for correct
operation Maintainability Ability to undergo modifications and repairs Safety
Absence of catastrophic consequences on user(s) and environment Integrity
Absence of improper system alterations
Availability indicates whether the system is ready for use at a given time.
This system property is typically quantified by the percentage of time during
which a system operates correctly. Reliability indicates how long correct
operation continues. This system property is typically defined as the (mean)
time between failures. Both properties are illustrated with and example. In
this example the system has an availability of 99,99%. This implies that its
unavailability is 0,01%, or 53 min on average per year. If the system fails on
average thrice a year then, reliability, quantified as the mean time between
failures, is four months.
Availability and reliability are closely related to the productivity of a
system. A system featuring a low availability is rarely ready for operation
and is thus characterised by low productivity. If the system reliability is
low, i.e. the time between failures is short, the system comes often to a
halt, which contravenes continuous productivity.
#### 4.3.3.3 Definition of communication dependability and its implications
##### 4.3.3.3.1 Introduction
A composite system, where every subsystem is instrumental to the operation of
the composite system, is not dependable if any of the subsystems is
undependable. This has the following implications for the use of communication
systems in general, and 5G systems in particular. Figure 4.3.3.3.1-1 depicts a
generic, distributed automation system, where the automation functions
interact via a communication system.
{width="3.654861111111111in" height="1.9166666666666667in"}
Figure 4.3.3.3.1-1: Example of a distributed automation system consisting of
automation functions and a communication network
In this example, all three subsystems, i.e. the automation functions and the
communication network need to be dependable for the automation system to be
dependable.
Communication dependability is the property of a dependable communication
system. According to IEC 61907, network dependability is the \"ability to
perform as and when required to meet specified communication and operational
requirements\" [2]. This definition largely agrees with 3GPP's own definition:
\"A performance criterion that describes the degree of certainty (or surety)
with which a function is performed regardless of speed or accuracy, but within
a given observational interval\" [1]. What does communication dependability
imply in praxis from the vantage point of the automation functions? We address
this for each of the five system properties in Figure 4.3.3.2-1.
##### 4.3.3.3.2 Reliability
According to IEC 61907, network reliability is the \"ability to perform as
required for a given time interval, under given conditions\" [2].
NOTE: Given conditions \"would include aspects that affect reliability, such
as: mode of operation, stress levels, environmental conditions\" [2].
Automation functions need highly reliable communication. As a rule of thumb
the more infrequent communication is unavailable the better.
Note that reliability in the context of dependability has a different meaning
than employed in TS 22.261, which defines it as the \"percentage value of the
amount of sent network layer packets successfully delivered to a given node
within the time constraint required by the targeted service, divided by the
total number of sent network layer packets\" [3]. This definition is more akin
to the definition of network availability (see Clause 4.3.3.3.3) and it
focuses on the inner working of the network rather than the end-to-end
experience of functions consuming the network\'s communication capabilities.
In order to avoid confusion, reliability of a communication system is
henceforth referred to as communication service reliability. We discuss this
in more detail in Clause 4.3.4.
##### 4.3.3.3.3 Availability
According to IEC 61907, network availability is the \"ability to be in a state
to perform as and when required, under given conditions, assuming that the
necessary external resources are provided\" [2]. Note that given conditions
\"would include aspects that affect reliability, maintainability and
maintenance support performance\" [2]. It is important to point out that a
communication network that does not meet the communication requirements of the
automation functions, e.g. a maximum end-to-end latency, are considered to be
unavailable.
##### 4.3.3.3.4 Maintainability
According to IEC 61907, network maintainability is the \"ability to be
retained in, or restored to, a state in which it can perform as required under
given conditions of use and maintenance\" [2]. Note that given conditions of
maintenance \"include the procedures and resources to be used\" [2].
\"Maintainability may be quantified using such measures as, mean time to
restoration, or the probability of restoration within a specified period of
time\" [2]. Clause 4.3.4 discusses what maintainability implies for a
dependable communication service.
##### 4.3.3.3.5 Safety
As introduced in Clause 4.3.3.2, safety stands for the absence of catastrophic
consequences on user(s) and environment. For a distributed automation system
this implies that neither the automation functions including their physical
embodiment, nor the processes, nor the environment should be damaged by the
communication system. This communication system property is---for instance---
addressed through regulations such as directive 2014/53/EU [21]. Note that in
most automation implementations the safety of the communication systems can be
treated separately and that the overall safety of the distributed automation
system is addressed by automation functions such as functional-safety
mechanisms.
##### 4.3.3.3.6 Integrity
According to IEC 61907, network integrity is the \"ability to ensure that the
data throughput contents are not contaminated, corrupted, lost or altered
between transmission and reception\" [2]. Note that this communication system
property is---in the communication network community---seen as an atomic
property of information security in communication systems. More on this in
Clause 6.1.
##### 4.3.3.3.7 Implications for 5G systems
In order to be suitable for automation in vertical domains, 5G systems need to
be dependable, i.e. they need to come with the system properties in Clause
4.3.3.3.2 to Clause 4.3.3.3.6. What particular requirements each property
needs to meet depends on the particularities of the domain and the use case.
More on this in Clause 5. Clause 4.3.4 addresses what the request for
communication dependability implies for communication services provided by 5G
systems.
It is important to understand that the relationship between communication
service availability, communication service continuity, communication service
reliability, and the probability of an erroneous message transmission anything
but trivial. Understanding this relationship is also important since
communication service in this document is not defined according to TL 9000
[56], for example.
According to ISO/IEC [54], service continuity is \"capability to manage risks
and events that could have serious impact on a service or services in order to
continually deliver services at agreed levels\". According to TS 22.261 [3],
the service continuity is "the uninterrupted user experience of a service that
is using an active communication when a UE undergoes an access change without,
as far as possible, the user noticing the change". The concept of service
continuity in TS 22.261 is very narrow and limited to a "capability" of
confronting only an event of "UE undergoing an access change". The
communication service continuity can be impacted by many other events either
in the control plane or the user plane, or both, such as intervening
exceptions or anomalies, whether scheduled or unscheduled, malicious,
intentional or unintentional [55]. For a reliable system, any event which
might impact to the communication service continuity is needed to be
considered.
A maximum tolerable communication service unavailability of, for instance,
10^-6^ does not always imply a maximum tolerable probability of erroneous and
prohibitively delayed end-to-end message transmission of 10^-6^ .One of the
main reasons for why this generally is not the case is a non-zero survival
time. This is illustrated with the example below.
_Survival time revisited_
According to TS 22.261, the survival time is \"the time that an application
consuming a communication service may continue without an anticipated
message\" [3]. Anticipation implies following aspects: timeliness and
correctness. The communication service continuity implies the following three
conditions: firstly, the message needs to arrive in time (timeliness);
secondly, only uncorrupted messages are accepted by the receiver; and thirdly,
the received messages need to be processed and sent out from 3GPP 5G system to
the target automation function. So, if at least one of these conditions is not
fulfilled, a timer is started by the automation function. Upon expiration of
the timer, the communication service for that application is declared
\"unavailable\" (service discontinuity; also see Clause A.2). The expiration
time is referred to as the survival time.
_Influence of survival time on the acceptable probability of untimely message
transmission_
To simplify the discussion, it is assumed that none of the transmitted
messages is corrupted or lost because of an anomaly in the communication
system and that thus the only cause of unavailability of a communication
service is that the end-to-end latency of a message lies outside the interval
specified by the jitter (see Appendix A.3 for more details on jitter and
timeliness).
Example: the update time is 50 ms. A survival time of 0 ms implies that any
untimely arrival of a message (e.g. the update time of that message delivery
is outside of the interval specified by the jitter）triggers the communication
to be declared as \"down\" by the automation function. Thus, if the aggregate
communication service unavailability is specified as 10^-6^ and lower, an
untimely arrival of messages shall only occur up to 1 in one million cycles
for periodic communication.
The situation changes markedly for non-zero survival times. For a survival
time of, e.g., 100 ms (see table 7.2.2-1 in [3]), the target automation
function waits two more cycles after a delayed message before it declares the
communication service as unavailable. If the likelihood of a single untimely
arrival is _p_ , and if the sequential untimely arrivals are independent of
each other, the likelihood of three untimely arrivals in a row is _p_ ^3^,
which is the likelihood for the communication service to be unavailable. For a
target unavailability of 10^-6^, the acceptable likelihood of a single
untimely arrival can thus be as high as 0,01.
The implications for the likelihood of a single untimely arrival are even more
relaxed for longer survival times. For automated commuter-train control, the
survival time is up to five times longer than the cycle time (see Clause5.1
_._ 1.2). Thus, in this case, six untimely arrivals have to occur in a row
before the communication service is declared unavailable. The likelihood of
such an event is _p_ ^6^, which, for a target unavailability of 10^-6^ for
this commuter-train control, implies that _p_ ≤ 0,1. In other words 1 out of
10 messages (!) may arrive outside the time interval specified by the receiver
while keeping the automation function consuming this communication service
operational.
_Influence of communication service reliability on the probability of
erroneous message transmission_
The above examples are simplistic since the influence of communication service
reliability has not been taken into account. This influence is also discussed
for a concrete example. In this example, it is assumed that the user of the
aforementioned communication service expects the downtime of the system to be
contingent and to only occur once a year. An unavailability of 10^-6^
translates thus into a maximum continuous unavailability of \~ 30 s per year.
The implication for the tolerable erroneous transmission probability is not
trivial, since many scenarios can result in such an excellent performance. One
possible extreme is that no erroneous transmission occurs during the most of
the year, but then the communication times out for 30 s in a row. Another
possible extreme is that the every cycle is erroneous, but that a correct
message is delivered before the survival timer runs out. In such a case, the
communication service unavailability is actually zero. In praxis, a
comprehensive stochastic analysis is needed in order to understand the
implications of communication service reliability on the tolerable probability
of erroneous message transmission, and a range of measurements may need to be
defined and carried out in order to infer comprehensive performance
requirements that guarantee high communication fidelity. An example for such a
performance requirement is related to the maximum service unavailability time.
_Implications of data packet fragmentation for reliability_
According to TS 22.261, reliability is defined as percentage value of the
amount of sent network layer packets successfully delivered to a given node
within the time constraint required by the targeted service, divided by the
total number of sent network layer packets [3]. If the messages to be
transported by the communication service are so short that they are not broken
into several packets, then the above discussion can also be applied to
reliability. However, in case messages are broken into several packets then
the implications of a communication service unavailability for the reliability
of a 5G system is contingent on implementation details such as how messages
are constructed from packets and whether timeliness issues in one packet
influence that of an adjacent packet.
### 4.3.4 Dependable communication service
#### 4.3.4.1 Introduction
Clause 4.3.4 discusses important criteria that are used for evaluating
dependable 5G communication services from an end-to-end perspective.
Dependability and its attributes are addressed in Clause 4.3.3.
#### 4.3.4.2 Network dependability
Network dependability can be classified as follows [2]:
> \- needed dependability: the end-users' network dependability requirements;
>
> \- offered dependability: the service provider's offerings of network
> dependability (or planned/targeted network dependability);
>
> \- achieved dependability: the dependability achieved or delivered by the
> service provider;
>
> \- perceived dependability: the dependability perceived/experienced by the
> end-users.
The end-users' \"dependability needs are the primary source of information for
establishing dependability requirements." [2]. Note that in this framework one
differentiates between needed, offered, achieved, and perceived/experienced
dependability. This is in line with concepts developed by the ITU-T for
quality in of service [22]. The ITU-T differentiates between the customer\'s
QoS requirements and the offered, achieved and perceived QoS.
#### 4.3.4.3 Network serviceability
When communication functionalities are offered as services, dependability is
contingent on what is referred to as serviceability. \"Serviceability reflects
the delivery of network dependability of service to the end-users. Higher
serviceability improves availability, provides integrity of service without
excessive impairments, and reduces service costs.
Serviceability can be described using the following performance criteria.
a) Service accessibility
Service accessibility is the ability of a network service to be accessed by
the user, under given conditions, for a given period of time. For connection-
oriented services, it refers to the ability to establish connection.
Accessibility can be measured in terms of service access delay, network access
capability, and service access control capability. (...)
b) Service retainability
Service retainability is the ability of a network service, once obtained, to
continue to be provided under given conditions for a requested duration. It
reflects the reliability of [the] network. (...) Retainability requires
network dependability support to maintain stable operation. (...)
c) Service integrity
Service integrity is the delivery of information and data by the network
without excessive impairment. Service integrity relates to the transfer of
information and data known as throughput. (...)
d) Disengagement
Disengagement concerns the network devices and links involved in the end-to-
end communication of a user as well as network resources (including bandwidth,
channel or resources related to upper-layer protocols) to be released when the
communication connection or session is closed. (...) Disengagement is a
characteristic affecting service accessibility and service retainability in
network serviceability.\" [2].
NOTE 1: Disengagement is not so much a concern of the end user, rather of the
network operator. This aspect fosters service accessibility and retainability
by keeping the share of committed but unused communication resources low.
Also, disengagement lowers the risk of security attacks in which an
unauthorised user attains network access by re-using identifiers of dormant
communication services.
Serviceability has an additional flavour, which is operability. \"From the
user's perspective, operability refers to the ability of a service to be
successfully and easily operated by a user.\" [2]. This flavour is especially
important in dynamic communication scenarios and for machine-to-machine
communication.
NOTE 2: Operability is used to characterise the four serviceability criteria
above. When applying the operability criterion one can, for instance, ask how
easy it is to gain access to the communication service, while service
accessibility focuses on how access is gained.
Table 4.3.4.3-1 provides a mapping between serviceability criteria and
dependability attributes. For the latter see Clause 4.3.3.2.
Table 4.3.4.3-1: Serviceability criteria and corresponding dependability
attributes
**Serviceability criterion** **Corresponding dependability attribute**
* * *
Accessibility Availability, reliability Retainability Availability,
reliability, maintainability, safety Integrity Integrity, safety Disengagement
Availability, reliability
A comparison of serviceability criteria stipulated by IEC 61907 (see Table
4.3.4.3-1) and those stipulated by 3GPP (see the definition of QoS in [1]) is
provided in Table 4.3.4.3-2.
Table 4.3.4.3-2: Serviceability criteria according to IEC 61907 [2] and 3GPP
[1]
**Serviceability criterion** **IEC 61907** **3GPP**
* * *
Accessibility X X Retainability X X Integrity X X Disengagement X --
#### 4.3.4.4 Describing dependable communication services
In order to deliver a dependable communication service, one needs to assure
the \"continuity of service against failures and denial of service access and
disengagement, and the transfer of user information against loss or
interruption." [2]. The dependability of the service from an end-user
perspective is ensured at the service access point (interfaces in Figure
4.3.3.3.1-1). The end-user dependability requirements determine the required
network performance, which in turn determine the required network parameters.
As outlined in Clause 4.3.4.3, dependability requirements can be sorted under
four serviceability criteria. However, as discussed in Clause 4.3.4.2, the
dependability mind set also necessitates the differentiation of requested,
offered, achieved and experienced dependability. The implied assurance is
another paramount facet of a communication service. Assurance is a praxis that
produces assurance judgments, i.e. statements that inspire confidence of, for
instance, the user of the communication service. Ideally, assurance judgments
are based on evidence. The evidence on which an assurance judgement is based
can be obtained by the means of service monitoring. More on assurance,
assurance methodology, and assurance frameworks can be found in appendix A.3
of [23]. More details on communication assurance and monitoring for assurance
can be found elsewhere in the literature [24]. Such monitoring could, for
instance, include the communication service availability. If the service
monitoring shows that the service has been available for more than 99,92% over
a year, the following assurance statement can be made: \"The continuous
monitoring of communication service availability over a year shows that the
unavailability of the communication service is less than 0,08%. This implies
that the communication service availability requested by the user, i.e. 99,9%,
is met.\" Assurance takes place at the service interface, which is the single
point of interaction between the communication network and the users, i.e. the
automation functions. The set of facets of a dependable communication service
is summarised in Figure 4.3.4.4-1.
{width="4.071527777777778in" height="1.5833333333333333in"}
Figure 4.3.4.4-1: Facets of a dependable communication service
So far only automation functions have been addressed, but other types of
applications requiring dependable communication services, e.g. PMSE (see
Clause 7.8) can be served by the same conceptual framework.
So what about the related service description, in particular the service
interface? Table 4.3.4.4-1, 4.3.4.4-2 and 4.3.4.4-3 summarise candidate
interface parameters. The lists are grouped according to whether the parameter
stands for automation function requirements (Table 4.3.4.4-1), influence
quantities (Table 4.3.4.4-2), or management/control parameters (Table
4.3.4.4-3). The meaning of the columns is explained after each table.
NOTE 1: Some of the terminology in the following tables is defined in Clause
3.1.
NOTE 2: The end-to-end connectivity either takes place on layer 3 (IP traffic)
or layer 2 (Ethernet) for the use cases described in the present document. The
parameters in Table 4.5.4.4-1 to 4.3.4.4-3 apply to both types of services.
NOTE 3: Parameters described in Table 4.3.4.4-1 to 4.3.4.4-3 are applicable to
the use cases in the following Clauses: 5.1, 5.3, 5.3.1. to 5.3.14, 5.3.18,
5.6.4, 5.6.5, 5.8.2 to 5.8.4.
NOTE 4: Not all of the parameters in Table 4.3.4.4-1 and Table 4.3.4.4.-2
would be used in a service call.
NOTE 5: Ingress and egress in this Clause are in reference to the
communication service interface between the source application and the
communication service interface (ingress) and the communication service and
the target application (egress).
Table 4.3.4.4-1: Candidate characteristic parameters for the dependable
communication service interface
**Parameter name** **Typical metric (unit)** **Traffic class (note 6)**
* * *
                                                                                                                                                            **Deterministic periodic communication**   **Deterministic a-periodic communication**   **Non-deterministic communication**
Communication service availability Minimum availability (dimensionless) X X X
End-to-end latency Target value and jitter (s) X X X Time between failures
Mean time between failures (s) X X X Time-triggered transmission Time schedule
{time, window size} (s) X X -- Service bit rate Target value (bit/s); user
experienced data rate (bit/s); time window (s) -- X X Update time Target value
and jitter (s) X -- -- NOTE 6: -- application requirements (KPIs). X: applies;
--: does not apply.
**Parameter description**
_Communication service availability_
This parameter indicates if the communication system works as contracted
(\"available\"/\"unavailable\" state). The communication system is in the
\"available\" state as long as the availability criteria for transmitted
packets are met. The service is unavailable if the packets received at the
target are impaired and/or untimely (e.g. update time > stipulated maximum).
If the survival time is larger than zero, consecutive impairments and/or
delays are ignored until the respective time has expired (see Figure A.2-1).
_End-to-end latency_
This parameter indicates the time allotted to the communication system for
transmitting a message (see Clause 3.1) and the permitted jitter.
_Time between failures_
This parameter is an indicator for communication service reliability (see
Clause 4.3.3.3.2). This parameters states how long the communication service
has to be available before it becomes unavailable. For instance, a mean time
between failures of one day indicates that a communication service on average
has to run error-free for one day before an error / errors make the
communication service unavailable (see above). The default value is zero.
_Time-triggered transmission_
With time triggered transmission, the messages are forwarded according to a
configurable time schedule, which is based on a reference time. The time
schedule defines the time windows when the message shall be transmitted. Time-
triggered transmission is contingent on a global network time and suits
periodic communication of very low update time. This type of service is
suitable when the target application does not keep the time.
_Service bit rate_
_a) deterministic communication_
The target value indicates committed data rate in bit/s sought from the
communication service. This is the minimum data rate the communication system
guarantees to provide at any time, i.e. in this case target value = user
experienced data rate.
_b) non-deterministic communication_
The target value indicates the target data rate in bit/s. This is the
information rate the communication system aims at providing on average during
a given (moving) time window (unit: s). The user experienced data rate the
lower data rate threshold for any of the time windows.
_Update time_
Applicable only to periodic communication, the update time indicates the time
interval between any two consecutive messages delivered from the egress (of
the communication system) to the application (see Clause A.4).
**Traffic classes**
In practice, vertical communication networks serve a large number of
applications exhibiting a wide range of communication requirements. In order
to facilitate efficient modelling of the communication network during
engineering and for reducing the complexity of network optimisation, disjoint
QoS sets have been identified. These sets are referred to as traffic classes
[28]. Typically only three traffic classes are needed in industrial
environments [28], i.e.
\- deterministic periodic communication;
\- deterministic aperiodic communication; and
\- non-deterministic communication.
For a discussion of determinism and periodicity see Clause 4.3.1.4.
Deterministic periodic communication stands for periodic communication with
stringent requirements on timeliness of the transmission.
Deterministic aperiodic communication stands for communication without a
preset sending time. Typical activity patterns for which this kind of
communication is suitable are event-driven actions.
Non-deterministic communication subsumes all other types of traffic. Periodic
non-real time and aperiodic non-real time traffic are subsumed by the non-
deterministic traffic class, since periodicity is irrelevant in case the
communication is not time-critical.
**Usage of the parameters in Table 4.3.4.4-1**
**Control service request and response; monitoring service response and
indication.**
Table 4.3.4.4-2: Candidate application influencing parameters for the
dependable communication service interface
**Parameter name** **Typical metric (unit)** **Traffic class (note 7)**
**Usage of this parameter**
* * *
                                                                                                                                                 **Deterministic periodic communication**   **Deterministic a-periodic communication**   **Non-deterministic communication**
Burst Maximum user data length (byte) and minimum transfer interval (s) -- X -
Control service request and response; monitoring service response and
indication message length (byte) and line rate of the service interface
(Mbit/s) -- -- X  
Survival time Maximum (s) X X -- Control service request and response Message
size Maximum or current value (byte) X (X) (X) Control service request and
response; non-deterministic data transmission; deterministic aperiodic data
transmission Transfer interval Target value and jitter (s) X -- -- Control
service request and response NOTE 7: X: applies; (X): usually does not apply;
--: does not apply.
**Parameter description**
_Burst_
The transmission of, for instance, program code and configuration data may be
handed to the 3GPP system as data burst. In this case, the ingress data rate
exceeds the capacity of the network, which implies that some of the data has
to be stored within the ingress node of the communication system before it can
be transmitted to the egress interface(s). However, the data of a burst needs
to be transmitted completely. This is in contrast to periodic data
transmission, where new messages overwrite old ones.
Typical metrices for bursts:
\- aperiodic data transmission: maximum user data length and minimum transfer
interval;
\- non-deterministic transmission: message length and line rate of the service
interface.
_Survival time_
The maximum survival time indicates the time period the communication service
may not meet the application\'s requirement before it is deemed to be in an
unavailable state.
NOTE 6: The survival time indicates to the communication service the time
available to recover from failure. This parameter is thus tightly related to
maintainability (see Clause 4.3.3.3.4) and the serviceability facet \"service
retainability\" (see Clause 4.3.4.3).
_Transfer interval_
Applicable only to periodic communication, the transfer interval indicates the
time elapsed between any two consecutive message delivered by the automation
application to the ingress of the communication system (see Clause A.4).
_Message size_
The user data length indicates the (maximum) size of the user data packet
delivered from the application to the ingress of the communication system and
from the egress of the communication system to the application. For periodic
communication this parameter can be used for calculating the requested user-
experienced data rate (see Clause A.4). If this parameter is not provided, the
default is the maximum value supported by the PDU type (e.g. Ethernet PDU:
maximum frame length is 1522 octets, IP PDU: maximum packet length is 65535
octets).
Table 4.3.4.4-3: Candidate control and management parameters for the
dependable communication service interface
**Parameter name** **Parameter description** **Service using this parameter**
* * *
Application identifier Indicates the user\'s (application\'s) service
interface and service instance, which is to be notified of communication
related events (e.g. contracted forwarding behaviour can no longer be met) All
service primitives Communication service identifier / traffic flow identifier
Indicates the data flow which shall be forwarded according to the mutually
agreed (contracted) parameters. The traffic flow identifier is associated with
a set of traffic filters, which filter traffic according to protocol data
understood by the automation and the communication function. All service
primitives Quality class identifier Indicates the traffic class of the service
flow. Control service requests and responses
#### 4.3.4.5 Other services
##### 4.3.4.5.1 Monitoring
As pointed out in Clause 4.3.4.5 and as becomes apparent when consulting the
merged requirements in Clause 8, communication service monitoring is an
important enabler of communication for automation.
Table 4.3.4.5.1-1: Metric for monitoring parameter
**Parameter name** **Typical metric (unit)**
* * *
Monitoring Parameter to be monitored {field; includes parameter identifiers
(string), measurement rate (s^-1^), output filter {e.g., a moving average},
time stamping (Y/N), etc.}
##### 4.3.4.5.2 Clock synchronisation
As outlined in Clause 7.5, clock synchronisation is needed in many
\"vertical\" use cases. The table below proposes related service parameters,
i.e. the requirements the user expresses in a service call.
Table 4.3.4.5.2-1: Metric for clock parameter
**Parameter name** **Typical metric (unit)**
* * *
Clock Update rate (s^-1^); time zone (string); combined type-A and -B
uncertainty (s) [53].
##### 4.3.4.5.3 Localisation
As outlined in in clause 7.3 in [3], \"vertical\" automation functions might
request knowledge about the position of UEs from the 5G system. The table
below proposes related service parameters, i.e. the requirements the user
expresses in a service call.
Table 4.3.4.5.3-1: Metric for position parameter
**Parameter name** **Typical metric (unit)**
* * *
Position Absolute position (Y/N); position relative to base station (Y/N);
update rate (s^-1^); time to first fix (s); combined type-A and -B uncertainty
[53]; encryption protocol for position
# 5 Use cases
## 5.1 Rail-bound mass transit
### 5.1 _._ 1 Description of vertical
In order to keep the attractiveness of public transport high, some of the key
challenges mass transit operators are facing are
\- growing traffic, both in terms of passenger flow and the number of and
frequency of mass transit vehicles;
\- the need to ensure passenger safety and security;
\- improvement of travel comfort, including delivery of real-time multimedia
information and access to the internet (social networks, etc.), both in
stations and on trains.
To reach this goal, investments are not only needed in rolling stock and
infrastructure, but also in communication networks, communication
technologies, and communication end devices.
#### 5.1 _._ 1.1 Communication services in rail-bound mass transit
There are two main drivers behind the growing importance of communication
services in mass transit: (1) passenger information and internet access and
(2) train automation. The latter can be divided into control and operations.
Both types of train automation consist of distributed applications that rely
on dependable communication. Typically, control applications are of higher
priority than operational applications, and the latter are typically of higher
priority than passenger services. One of the main challenges is to guarantee
the premium priority of control-related communication over other types of
communication, especially since the data bandwidth consumed for control is
typically dwarfed by data traffic stemming from the other two application
areas. Another challenge is to guarantee the super priority of operational
data communication over passenger-related communication. Automation in public
transport, especially in mass transit, has reached one of the highest levels
of automation among infrastructure applications. Nowadays, driverless metro
systems are not science fiction, rather they become increasingly pervasive.
The number of driverless metro lines worldwide already exceeds 100, and this
number is expected to grow. Note that driverless trains are not the only
source of automation in mass transit, and thus not the only source of
dependable machine-type communication. For instance, mass transit train
control assisted by rail-to-rail-side wireless communication (MTTC), which is
at the core of driverless trains, is also used for trains exhibiting lower
grades of automation. Examples for MTTC are communication-based train control
(CBTC) [7] and Korea Radio-Based Train Control System (KRTCS) [5][6]. The
grades of automation of trains are specified in IEC 62290-1 [4] and are
summarised in Figure 5.1.1.1-1.
{width="6.047916666666667in" height="3.1902777777777778in"}
Figure 5.1.1.1-1: The different grades of automation (GoA) in rail systems
Caused by increased safety awareness, onboard video surveillance/CCTV and
emergency calls play an increasing role in rolling-stock equipment. In GoA 4
systems (see Figure 5.1.1.1-1), both applications are mandatory in order to
setup communication with passenger during emergencies, e.g., when trains have
to stop inside tunnels. Onboard video surveillance/CCTV system generates also
excessive amount of video recordings, which need to be archived and ultimately
offloaded/ transferred from onboard into the ground storage. Additional
operational applications are train diagnostics and voice communication for
operational, service and maintenance purposes. Other common applications for
increasing passenger satisfaction are passenger information (PIS), online
advertisement, and online internet access. Figure 5.1.1.1-2 provides an
overview of the common data services in contemporary mass transit trains.
{width="6.297916666666667in" height="4.559722222222222in"}
Figure 5.1.1.1-2: Common communication-based services in rail-bound mass
transit, (not all services listed are shown in the diagram)
#### 5.1 _._ 1.2 Characteristics of main mass-transit data services
In a first level of consideration, mass-transit data services differ in
directionality and bandwidth. For instance, MTTC is a bidirectional data
service, while CCTV traffic mainly flows from train to track-side destinations
(uplink). In contrast, the PIS communication pattern is almost reverse
(dominance of downlink traffic).
In Table 5.1.1.2-1, a selection of mass-transit data services is characterised
by different attributes such as data rate, directionality, and priority. The
latter, which are not standardised, are provided for guidance.
Table 5.1.1.2-1: Characteristics of the data services in rail-bound mass
transit,
**Service** **Main direction** **Data rate in Mbit/s per application**
**Priority** **End-to-end latency** **Communication service availability**
**Security** **Data integrity**
* * *
MTTC DL, UL \ 99,999% Highest Mandatory Real-time
CCTV UL > 4 High (= 2) \ 99,99% High Recommended PIS DL \ 99,99% High Not required Passenger internet access DL ≥ 0,2 Low (=
4) \ 0,1 Medium (=
3) \ 99,99% High Recommended
As Table 5.1.1.2-1 shows, MTTC is a very demanding data service, since every
related attribute―except data rate―is at the highest requirement level. Other
important services, from an operational point of view, are CCTV and emergency
voice connections. Internet access, on the other hand, requires high aggregate
data rates while it represents the least important operation-related service
of all. The maximum bit-error ratio for control messages, e.g., MTTC messages,
is 10^-6^.
Table 5.1.1.2-2 translates the above qualitative services into typical data
rates for a commuter train.
Table 5.1.1.2-2: Typical data-rate requirements per commuter train and service
* * *
**Service** **# of networked devices\** Data rate per device\ **Overall data
rate per train\ (typically)** [Mbit/s]**[Mbit/s] (note)**
* * *
MTTC 2 0,1 0,2
Real-time CCTV 20 4 80
PIS 6 0,5 3
Emergency voice 8 0,2 1,6
Passenger internet access 500 0,2 100
Train diagnostics 50 0,1 5
CCTV offload / archiving 1 ≥ 1000 ≥ 1000
NOTE: The overall data rate is calculated under the assumption of one
application per device.
* * *
MTTC only consumes some hundred kbit/s in contrast to around 100 Mbit/s for
passenger internet access. Also, the number of networked devices per train
varies considerably from one service to the other. MTTC is at the very low end
of the list. The big number for passenger internet access stems from the fact
that passenger end devices, e.g., smart phones, significantly outnumber
automation data sources and data sinks and that many automation devices
consume comparably low data rates. Due to constantly increasing minimum
resolutions for CCTV cameras, currently reaching up to approximately 4 Mbit/s
per camera, the data rate requirements of live real-time CCTV reaches close to
that of overall data rate of Passenger internet Access. Furthermore, the CCTV
offload, which means the transfer of CCTV archives from train to ground when
the train stops at the stations or at the depot, requires relatively high
overall data rate, similar to that required by long haul trains, i.e., minimum
of 1 Gb/s. The transfer CCTV offload is even more challenging in commuter
trains than in long haul trains due to shorter stop times.
Note that communication service reliability is also an important parameter for
mass-transit communication. However, commonly agreed upon values do not exist.
As a rule of thumb, the time between communication service failures needs to
be maximised.
Also note that some mass-transit applications are comparably immune to
intermittent communication service dropouts. In case the unavailability of the
communication service is shorter than a pre-defined time (for MTTC typically
around 500 ms), the communicating applications do not experience the service
as unavailable. However, if the dropout exceeds this time limit, it counts as
a communication service failure. As a general rule, the time to repair the
communication service should be kept as short as feasible.
### 5.1.2 Coexistence of MTTC service and CCTV
#### 5.1.2 _._ 1 Description
This use case considers the behaviour of a high-priority MTTC service in
presence of a CCTV high-priority data service. The former is characterised by
rather low data rates and the latter by rather high data rates (see Table
5.1.1.2-1). In this use case CCTV is streamed in real time to the rail-side,
for instance to a traffic control centre.
#### 5.1.2.2 Preconditions
The high-priority service MTTC is switched on and communication between train
and ground-based train control units ensues. The priority level of MTTC is 1
(highest level). Train-borne and ground MTTC instances are exchanging data.
The exchange fulfils the required data rate and other relevant service
parameters (see Table 5.1.1.2-1). All other data services are switched off.
#### 5.1.2.3 Service flows
The CCTV service is switched on. The priority of this data service is 2 (one
level lower than MTTC). All other data services are switched off.
The CCTV service is switched on. The desired data rate of the CCTV service is
negotiated (typically at least 100 Mbit/s). CCTV data connections between
track and train are established (CCTV cameras are switched on and video
streams relayed to a track-side video management system, a.k.a. the CCTV data
sink).
#### 5.1.2.4 Post-conditions
> \- The MTTC QoS parameters stay in the desired range while the CCTV service
> is running;
>
> \- the CCTV QoS parameters stay in the desired range.
#### 5.1.2.5 Challenges to the 5G system
> \- Isolation of communication flows;
>
> \- guarantee of user experienced data rates;
>
> \- high communication service availability;
>
> \- high communication service reliability.
#### 5.1.2.6 Potential requirements
+----------------+----------------+----------------+----------------+ | **Reference |** Requirement | **Application |** comments**| | number** | text**| / transport** | | +================+================+================+================+ | _Mass Transit | The MTTC | A | CCTV data | | 1.1_ | service has | | rates may | | | the highest | | reach 500 | | | priority and | | Mbit/s per | | | shall not be | | train. | | | affected by | | | | | the CCTV | | [This | | | service. | | requirement is | | | End-to-end | | not covered | | | latency and | | yet by | | | availability | | existing 3GPP | | | of MTTC are | | requirements] | | | not affected | | | | | when running | | | | | the CCTV | | | | | service in | | | | | parallel. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The CCTV | A | [This | | 1.2_ | service shall | | requirement is | | | not be | | not covered | | | affected by | | yet by | | | the MTTC | | existing 3GPP | | | service, which | | requirements] | | | has a higher | | | | | priority but | | | | | lower data | | | | | rate. | | | | | Especially, | | | | | delay and | | | | | packet loss of | | | | | CCTV is not | | | | | affected by | | | | | MTTC service | | | | | that runs in | | | | | parallel. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The user | T | | | 1.3_ | experienced | | | | | data rate for | | | | | MTTC services | | | | | shall be at | | | | | least 200 | | | | | kbit/s. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The end-to-end | T | | | 1.4_ | latency for | | | | | MTTC services | | | | | shall be below | | | | | 100 ms. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The | T | | | 1.5_ | communication | | | | | service | | | | | availability | | | | | for MTTC | | | | | services shall | | | | | be higher than | | | | | 99,999%. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The use | T | | | 1.6_ | experienced | | | | | data rate for | | | | | CCTV services | | | | | shall not be | | | | | lower than 2 | | | | | Mbit/s per | | | | | CCTV | | | | | application. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The end-to-end | T | | | 1.7_ | latency for | | | | | CCTV | | | | | applications | | | | | shall be below | | | | | 500 ms. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | The | T | | | 1.8_ | communication | | | | | service | | | | | availability | | | | | for CCTV | | | | | services shall | | | | | be higher than | | | | | 99,99%. | | | +----------------+----------------+----------------+----------------+
### 5.1.3 Coexistence of MTTC service and a high data rate service with low
priority
#### 5.1.3 _._ 1 Description
This use case considers the behaviour of MTTC in presence of another data
service with low priority but very high data rates. An example for this other
service is passenger internet access.
#### 5.1.3.2 Preconditions
MTTC is switched on and communication between train and ground is maintained.
The priority of the MTTC service is set to the highest value. Train-borne and
ground MTTC instances are exchanging data with required data rate and other
specified service parameters. All other data services are switched off.
#### 5.1.3.3 Service flows
Another data service with different service parameters is switched on. The
priority of this data service is the lowest. An example service is passenger
internet access service.
The additional service is switched on. The desired data rate of the additional
service is set to, e.g., 100 Mbit/s. A pursuant data connection between track
and train is established. After successful initialisation of the additional
service, the QoS of the MTTC is unaffected, even if the data rate of the
additional service is increased up to 500 Mbit/s.
#### 5.1.3.4 Post-conditions
The MTTC QoS parameters are in the specified range, even when the other data
service is running at the highest permissible data rate.
#### 5.1.3.5 Challenges to the 5G system
\- Isolation of communication flows.
#### 5.1.3.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 2.1_ The MTTC service shall not be affected by a low priority
data service. End-to-end latency, packet loss, and availability of MTTC are
not affected by the other data service. A [This requirement is not covered yet
by existing 3GPP requirements]
### 5.1.4 Coexistence of MTTC service and high data rate service with low
priority
#### 5.1.4 _._ 1 Description
This use case considers the behaviour of the start up of the MTTC service in
case of other already running data services.
#### 5.1.4.2 Preconditions
MTTC is switched off. Data services that consume significant bandwidth are
running. Examples for such services are
> \- CCTV;
>
> \- PIS;
>
> \- emergency voice;
>
> \- passenger internet access;
>
> \- train diagnostics.
The communication service for each application is characterised by an
individual set of QoS parameters. The resulting data rate per train is 500
Mbit/s and lower.
#### 5.1.4.3 Service flows
The MTTC service is switched on.
Trackside and train-borne MTTC instances boot and communication between both
commence.
After the MTTC start-up procedure, the MTTC service is entering the data
exchange mode.
#### 5.1.4.4 Post-conditions
The MTTC service is running properly, fulfilling specified QoS parameters.
#### 5.1.4.5 Challenges to the 5G system
> Isolation of communication flows.
#### 5.1.4.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 3.1_ The MTTC service start-up shall not be affected by already
running services with different priorities. A [This requirement is not covered
yet by existing 3GPP requirements] _Mass Transit 3.2_ Already running
communication services shall not be affected by booting the MTTC service. A
[This requirement is not covered yet by existing 3GPP requirements]
### 5.1.5 Set-up of emergency call
#### 5.1.5 _._ 1 Description
This use case considers setting up emergency calls while other data services
are running.
NOTE: This use case deals with emergency calls using the train and rail
infrastructure and not public emergency services. An example for a train
emergency system is microphone/speaker boxes that are integrated into the
walls of the passenger area.
#### 5.1.5.2 Preconditions
The high priority emergency voice service is in standby mode. The
corresponding voice call devices are switched on.
Examples for such services are
> \- Real-time CCTV;
>
> \- PIS;
>
> \- emergency voice;
>
> \- passenger internet access;
>
> \- diagnostics.
The communication service, for each application, is characterised by an
individual set of QoS parameters. The resulting data rate per train is 500
Mbit/s and lower.
#### 5.1.5.3 Service flows
An emergency voice call is initiated. Trackside and train-borne voice
communication between them commences.
#### 5.1.5.4 Post-conditions
A successful voice call between train and track side devices is established.
#### 5.1.5.5 Challenges to the 5G system
> \- Isolation of communication flows;
>
> \- guarantee of user experienced data rates;
>
> \- high communication service availability;
>
> \- high communication service reliability.
#### 5.1.5.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 4.1_ The set-up of an emergency voice call shall not be affected
by already running services with different priorities. A The compound data
rate of other services is ≤ 500 Mbit/s per train. [This requirement is not
covered yet by existing 3GPP requirements] _Mass Transit 4.2_ The user
experienced data rate for emergency voice calls shall be at least 200 kbit/s.
A  
_Mass Transit 4.3_ The end-to-end latency for emergency voice calls shall be
below 200 ms. T  
_Mass Transit 4.4_ The communication service availability for emergency voice
calls shall be higher than 99,99%. T
### 5.1.6 Emergency call during a sudden rise of CCTV data rate
#### 5.1.6 _._ 1 Description
This use case considers setting up emergency calls while the data rate of
other data services rises.
#### 5.1.6.2 Preconditions
An emergency voice call is established. The MTTC service is running.
#### 5.1.6.3 Service flows
Train-borne CCTV devices start streaming videos to the trackside video system
at an overall data rate of up to 500 Mbit/s.
#### 5.1.6.4 Post-conditions
The voice call between train and track side devices is not interrupted.
5.1.6.5 Challenges to the 5G system
> Isolation of communication flows.
#### 5.1.6.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 5.1_ An emergency voice call shall not be interrupted, even when
a sudden rise of data rate of other lower-priority service such as CCTV
occurs. A The maximum compound data rate of the lower-priority services is 500
Mbit/s. [This requirement is not covered yet by existing 3GPP requirements]
### 5.1.7 Use Case: CCTV offload / transfer of CCTV archives from commuter
train to ground
#### 5.1.7.1 Description
This use case describes the CCTV offload, i.e., the transfer of CCTV archives
from the on-board system to the ground system. This use case assumes the
following:
> \- The retention time for the recordings in the on-board system is seven
> days.
>
> \- The minimum retention time for the CCTV recordings in the ground system
> is 31 days.
>
> \- CCTV offload/transfer of CCTV archives is performed only when train
> approaches stations/stops in order to stop and at the depot.
{width="6.059722222222222in" height="4.143055555555556in"}
Figure.5.1.7.1-1. Onboard CCTV storage sizes with different offload rates
#### 5.1.7.2 Pre-conditions
> \- CCTV offload/transfer of CCTV archives is performed only when the
> commuter train stops at the stations.
>
> \- Mobile communication infrastructure between commuter train and ground
> system enables CCTV offload/ transfer of CCTV archives from commuter train
> to the ground system while the train stops at the stations.
>
> \- The ground system supports sufficient archiving system for the
> transferred recordings.
#### 5.1.7.3 Service flows
> 1\. The commuter train approaches the station/stop.
>
> 2\. Mobile communication system in commuter train establishes connection
> dedicated for the CCTV offload/ transfer of CCTV archives with the ground
> system at a priority level allowing critical communication to continue in
> parallel
>
> 3\. The CCTV offload/ transfer of CCTV archives is started upon successful
> connection with the ground system.
>
> 4\. The CCTV offload/transfer of CCTV archives is stopped when the
> connection is no longer available.
#### 5.1.7.4 Post-conditions
> \- The on-board CCTV system may re-write over the seven days and older
> recordings that have been transferred.
>
> \- The on-board mobile communication system remains monitoring the next
> approach of station/stop.
#### 5.1.7.5 Challenges to the 5G system
None.
#### 5.1.7.6 Potential requirements and gap analysis
+----------------+----------------+----------------+----------------+ | **Reference |** Requirement | **Application |** Comment**| | number** | text**| / transport** | | +================+================+================+================+ | _Mass Transit | The onboard | A/T | See: | | 6.1_ | System shall | | [http://w | | | be able to | | ww.3gpp.org/te | | | support that | | chnologies/key | | | CCTV archives | | words-acronyms | | | can be | | /97-lte-advanc | | | transferred | | ed]{.underline | | | into the | | } | | | with a minimum | | | | | of 1 Gbit/s in | | A: Not covered | | | dedicated | | T: Data rate | | | places such as | | covered by LTE | | | stations/stops | | and NR | | | or train | | transport | | | depots. | | | +----------------+----------------+----------------+----------------+ | _Mass Transit | CCTV | A/T | A: Not covered | | 6.2_ | offloa | | | | | d/Transferring | | T: Covered by | | | CCTV archives | | basic 3GPP and | | | shall not | | suitable QoS | | | affect | | | | | mi | | | | | ssion-critical | | | | | communication | | | | | (note). | | | +----------------+----------------+----------------+----------------+ | NOTE: | | | | | Transferring | | | | | CCTV archives | | | | | is not | | | | | considered to | | | | | be a mission | | | | | critical | | | | | service. | | | | +----------------+----------------+----------------+----------------+
### 5.1.8 Wireless communication between mechanically coupled train segments
#### 5.1.8 _._ 1 Description
Two mass transit coaches or trains are mechanically coupled together. The
communication between both (control, operational, passenger services) is going
to be provided via a 5G RAN link. For the sake of simplicity we henceforth
refer to coaches as trains.
#### 5.1.8.2 Preconditions
Services are running in both trains. Typically, these services are of an
operational nature, but they can also include control and passenger services.
The two trains are coupled together mechanically, but communication between
both trains is not established yet.
#### 5.1.8.3 Service flows
A wireless 5G link is established between the two trains.
#### 5.1.8.4 Post-conditions
Communication services between both trains are established via the 5G wireless
link and run dependably.
#### 5.1.8.5 Challenges to the 5G system
\- Guarantee of very high user experienced data rates over a short air gap.
\- Very high communication service availability;
\- Very high communication service reliability.
#### 5.1.8.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 7.1_ The user experienced data rate between the two trains shall
be 1 Gbit/s. T  
_Mass Transit 7.2_ The end-to-end latency for MTTC services in both trains
shall be below 100 ms. T  
_Mass Transit 7.3_ The communication service availability between applications
situated in different trains shall be higher than 99,999%. T  
_Mass Transit 7.4_ Communication services between shall be possible for angles
of up to 0,52 rad between the two trains. T
### 5.1.9 Wireless communication between virtually coupled trains
#### 5.1.9.1 Description
One of the important missions that the future railway service should achieve
is to increase its transport capacity and operational efficiency. A straight-
forward solution is to minimise the distance between successive mass transit
trains so that mass transit interval is reduced. It is difficult to do so in a
legacy mass transit system, because the distance between two successive mass
transit trains should be larger than the safety braking distance.
The distance between successive trains can be shortened if the following mass
transit train immediately triggers braking as soon as the leading mass transit
train starts braking. This is the fundamental principle of the virtual
coupling. Figure 5.1.9.1-1 shows the basic concept of the virtual coupling.
Multiple mass transit trains in proximity move together _as if they are
mechanically coupled_. As two trains get closer, trains need to communicate
each other more frequently through a very-low-latency off-network
communication link for control, operational, and passenger services. The
communication between the virtually coupled trains is provided through both
on-network and off-network communication links simultaneously for
complementary use of both links.
{width="6.75in" height="2.0118055555555556in"}
Figure 5.1.9.1-1: The concept of virtual coupling
#### 5.1.9.2 Pre-conditions
> 1\. Control, operational, and passenger services are running in both leading
> and following mass transit trains. Both trains are configured to be coupled
> virtually.
>
> 2\. Two mass transit trains are connected through an on-network (e.g., Uu
> interface). Each mass transit train is capable of off-network communications
> (e.g., PC5 interface).
#### 5.1.9.3 Service flows
As the following train approaches the leading train, a very-low-latency off-
network communication link is established between two trains. Two mass transit
trains are still connected through the on-network communication link for
complementary use of both links.
#### 5.1.9.4 Post-conditions
Control, operational, and passenger communication services between virtually
coupled trains are established through both on-network and off-network
communication links simultaneously.
#### 5.1.9.5 Challenges to the 5G system
\- Guarantee of very long communication range for off-network communication
link
\- Very low end-to-end latency for off-network communication link
\- Service continuity in the application layer between on-network and off-
network
#### 5.1.9.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 8.1_ The 3GPP system shall support off-network communication up
to 3 km in the line of sight (LOS) channel environment. T  
_Mass Transit 8.2_ The end-to-end latency through off-network communication
shall be less than or equal to 10 ms. T  
_Mass Transit 8.3_ The 3GPP system shall support off-network communication,
where the UEs' relative speed is less than 50 km/h. T  
_Mass Transit 8.4_ The 3GPP system shall support simultaneous use of on-
network and off-network communication when distance between the virtually
coupled trains is less than or equal to 3 km in the LOS channel. T  
_Mass Transit 8.5_ The 3GPP system shall support service continuity in the
application layer between on-network based connection and off-network based
connection when distance between the virtually coupled trains is less than or
equal to 3 km in the LOS channel. A/T
### 5.1.10 Anticipatory train control
#### 5.1.10 _._ 1 Description
MTTC relies on continuous run-time connectivity between train and track side.
This connectivity is currently typically realised by aid of wireless
technologies such as Wireless LAN and LTE (R11). In case the connectivity
between an \"MTTC train\" and the track side becomes unavailable, the train
stops. Rather tedious procedures have to be executed before the train may
continue its journey. These procedures can take up to several minutes. Also---
if more than one train shares the same line---one train\'s emergency stop will
bring all trains on this line to a halt. For these reasons, solutions are
sought in which the loss of connectivity between track and track side is
alleviated.
#### 5.1.10.2 Preconditions
A \"MTTC train\" is connected to a 5G system that provides the mandatory
connectivity to the track side. The \"MTTC train\" provides passenger service
along a mass transit line.
#### 5.1.10.3 Service flows
The 5G system informs the \"MTTC train\" of degraded connectivity in a radio
cell of the 5G system down the train line. This information includes the
(approximate) size and intersection of the radio cell with the train line. The
degraded connectivity will result in the loss of connectivity between the
train control and the track side. The train control utilises this information
for compensatory changes to its travel profile and/or the consumed radio
connectivity.
_Changes to the travel profile_
The train reacts to the degradation of the radio connectivity down the train
line through anticipatory changes to the travel profile. For instance, the
train (progressively) lowers the train speed. In case connectivity in the
affected radio cell is recovered the train can (progressively) increase its
speed again. By so doing, emergency brakes and lengthy stand stills are
avoided. Also, since the ratio of released power vs. usable mechanical power
(used for propelling and controlling the train) decreases concavely with train
speed, substantial amounts of power can be saved in case the train partially
decelerates instead of coming to a complete halt. Partial deceleration is in
essence more readily and easily reversible than a \"go-no-go\" pattern. Thus,
a slightly decelerating train will be more energy efficient than one coming to
intermittent full stops. The change in velocity and thus the predicted route
can also be exchanged with other trains on the same line (and the same
direction of travel), so that they can adapt their travel profile in a
suitable way. This exchange can, for instance, take place via a central
instance or between adjacent trains.
_Adaptation of the radio connectivity_
In case the radio channel degrades gracefully, and in case more than one
communication relationship exists between track-side and train, the train can
evaluate whether an intermittent discontinuation of a lower-priority service
is possible, so that the degraded channel capacity is reserved for mission-
critical communication. For instance, if the train currently offloads archived
video footage, the train control can decide to intermittently discontinue the
related communication service.
Another potential adaptation is the exploitation of receiver diversity. Trains
are equipped with a least two UEs (at opposite ends of the train), and for
long trains the link budget for these UEs can be quite different. In that
case, the UEs with the better link budget can be chosen as primary receiver.
#### 5.1.10.4 Post-conditions
Connectivity is restored in the radio cell in question and the train control
is appraised of this change. The train control takes appropriate actions.
#### 5.1.10.5 Challenges to the 5G system
> \- Inference of communication service availability in radio cells.
>
> \- Signalling of communication service availability to users and a dedicated
> interface for so doing.
>
> \- Mapping of (dynamic) radio cells and the availability of communication
> services onto spatial artefacts such as train lines.
>
> \- Prediction of communication service availability.
#### 5.1.10.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Mass Transit 9.1_ The 5G system shall know the geographic location and extend
of its radio cell sector coverage and be able to expose this information to
authorised users. T  
_Mass Transit 9.2_ The 5G system shall provide a user, upon request, with
information of the current availability for a specific communication service,
for this user, in a specific radio cell sector. T This will help the train
control to decide whether, for instance, it should reduce its speed. _Mass
Transit 9.3_ The 5G system shall expose a user interface through which the
interaction and information exchange in _Mass Transit 9.2_ can be facilitated
for an authorised user. A
## 5.2 Building automation
### 5.2 _._ 1 Description of vertical
Building automation [9] refers to the management of equipment in buildings
such as heaters, coolers, and ventilators. Automation of such systems brings
several benefits, including the reduction of energy consumption, the
improvement of comfort level for people using the building, and the handling
of failure and emergency situations. Sensors installed in the building perform
measurements of the environment and report these measurements to Local
Controllers. Local Controllers (LC), in turn, report these results to a
Building Management System.
A Building Management System (BMS) may then execute different operations:
\- Store the information into a database (e.g., for histogram purpose);
\- Send an alarm to a (third-party) Building Management System;
\- The Building Management System sends a command to an actuator (e.g.,
command to increase room temperature, turn on a light).
{width="5.75in" height="4.63125in"}
Figure 5.2.1-1: Building automation system - Local Controller in Mobile Edge
System and Building Management System outside 3GPP domain
{width="5.964583333333334in" height="4.035416666666666in"}
Figure 5.2.1.-2: Building automation system with Building Management System as
part of Mobile Edge Computing system
NOTE: In existing building automation systems there are typically two layers
of network: the upper layer is called management network and the lower one is
called field network. In management networks, an IP-based communication
protocol is used. In field network, non-IP based communication protocols
(a.k.a., field protocols) are mainly used. There are many field protocols used
in today\'s deployment in which some medium access control and physical layers
protocols are standards-based and others are proprietary based. The impact of
such protocols in 3GPP requirements is FFS.
### 5.2.2 Environmental monitoring
#### 5.2.2 _._ 1 Description
In this use case, several sensors are installed in a building and each sensor
performs measurements following a pre-defined measurement interval. The
measurement data might be used for drawing a histogram with as detailed as 1 s
granularity and a 10 times sampling rate, i.e., 10 times per second. A Local
Controller collects the measurement data from its sensors and may transmit it
to the Building Management System at a certain interval. The latency in this
use case is not a concern, but it is important that the transmission is
reliable and all sensor values are collected within the measurement interval.
#### 5.2.2.2 Preconditions
There are several Local Controllers installed in the building, each connected
with many sensors (up to 100 sensors).
#### 5.2.2.3 Service flows
At the measurement interval, which might be as low as 1 second, and with the
needed sampling rate (e.g., 10/s), the Local Controller sends a request to all
its sensors in the building to report their measurements.
#### 5.2.2.4 Post-conditions
Every sensor reports their measurements and measurements are received with
99,999% reliability. The Local Controller collects these measurements and may
transmit them to the building management System.
#### 5.2.2.5 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Building Automation 1.1_ The 3GPP system shall support 99,999% communication
service availability for data transmission every second. T
### 5.2.3 Fire detection
#### 5.2.3 _._ 1 Description
In this use case, when fire is detected, the system triggers several actions,
such as closing fire shutters and turning on fire sprinklers.
#### 5.2.3.2 Preconditions
There are 10 connected sensors and one Local Controller installed in the
building.
#### 5.2.3.3 Service flows
1) Fire is detected by the building sensors.
2) Building sensors send an alarm to the Local Controller.
3) Local controller sends information to Building Management System
4) Building Management System sends commands to the actuators in the building.
#### 5.2.3.4 Post-conditions
Fire shutters are closed and fire sprinklers are turned on within 1 to 2
seconds from the time the fire is detected.
#### 5.2.3.5 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Building Automation 2.1_ The 3GPP system shall support an end-to-end latency
of 10 ms with a [99,9999%] communication service availability for data
transmission. T
### 5.2.4 Feedback control
#### 5.2.4 _._ 1 Description
In this use case, a (device) state is controlled. For example, a room
temperature is kept at a certain value. Low latency and jitter are required in
this use case in order to provide high quality of feedback control.
#### 5.2.4.2 Preconditions
There are 10 sensors and one Local Controller installed in the building. The
Local Controllers is configured with a target temperature for a connected
sensor and thus the room in which the sensor is installed.
#### 5.2.4.3 Service flows
1\. The Local Controller requests measurements from a target sensor to
establish the state of the sensor.
2\. The Local Controller calculates a control value based on the measured
target sensor state.
3\. The Local Controller sends the control value to a target actuator.
#### 5.2.4.4 Post-conditions
The target actuator receives the command and adjusts the temperature based on
the control value and the temperature reaches the target temperature.
#### 5.2.4.5 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Building Automation 3.1_ The 3GPP system shall support and end-to-end latency
of 10 ms with a communication service availability of [99,9999%] for data
transmission. T  
_Building Automation 3.2_ The 3GPP system shall support a jitter of up to 1
ms. T
## 5.3 Factories of the Future
### 5.3.1 Description of vertical
#### 5.3.1.1 Overview
The manufacturing industry is currently subject to a fundamental change, which
is often referred to as the \"Fourth Industrial Revolution\" or simply
\"Industry 4.0\" [27]. The main goals of Industry 4.0 are ―among others― the
improvement of flexibility, versatility, resource efficiency, cost efficiency,
worker support, and quality of industrial production and logistics. These
improvements are important for addressing the needs of increasingly volatile
and globalised markets. A major enabler for all this are cyber-physical
production systems based on a ubiquitous and powerful connectivity and
computing infrastructure, which interconnects people, machines, products, and
all kinds of other devices in a flexible, secure and consistent manner.
Instead of static sequential production systems, future smart factories will
be characterised by flexible, modular production systems. This includes more
mobile and versatile production assets, which require powerful and efficient
wireless communication and localisation services.
Today, the vast majority of communication technologies used in industry is
still wire-bound. This includes a variety of dedicated Industrial Ethernet
technologies (e.g., Sercos®, PROFINET® and EtherCAT®) and fieldbuses (e.g.,
PROFIBUS®, CC-Link® and CAN®) [28][29][30]. These communication technologies
are used, for example, for interconnecting sensors, actuators and controllers
in an automation system. Nowadays, wireless communication is primarily used
for special applications and scenarios, for example in the process industry,
or for connecting standard IT hardware to a production network and similar
rather non-critical applications. On the one hand, this is because there was
no need for wireless connectivity in the past, due to relatively static and
long-lasting production facilities. On the other hand, this is because most
existing wireless technologies fall short of the demanding requirements of
industrial applications, especially with respect to end-to-end latency,
communication service availability, jitter, and determinism. With the advent
of Industry 4.0 and 5G, however, this may change fundamentally, since only
wireless connectivity can provide the degree of flexibility, mobility,
versatility, and ergonomics that is required for the Factories of the Future.
Thus, 5G may significantly contribute to revolutionising the way how goods are
produced, shipped, and serviced throughout their whole lifecycle.
In this respect, several different application areas can be distinguished, as
shown in Figure 5.3.1.1-1.
{width="3.7618055555555556in" height="2.75in"}
Figure 5.3.1.1-1: Overview of the different application areas of the vertical
\"Factories of the Future\"
These areas can be briefly characterised as follows:
> **1) Factory automation:** Factory automation deals with the automated
> control, monitoring and optimisation of processes and workflows within a
> factory. This includes aspects like closed-loop control applications (e.g.,
> based on programmable logic or motion controllers), robotics, as well as
> aspects of computer-integrated manufacturing. Factory automation generally
> represents a key enabler for industrial mass production with high quality
> and cost-efficiency and corresponding applications are often characterised
> by highest requirements on the underlying connectivity infrastructure,
> especially in terms of latency, communication service availability and
> determinism. In the Factories of the Future, static sequential production
> systems will be more and more replaced by novel modular production systems
> offering a high flexibility and versatility. This involves a large number of
> increasingly mobile production assets, for which powerful wireless
> communication and localisation services are required.
>
> **2) Process automation:** Process automation refers to the control of
> production and handling of substances like chemicals, food & beverage, etc.
> Process automation improves the efficiency of production processes, energy
> consumption and safety of the facilities. Sensors measuring process values,
> such as pressures or temperatures, are working in a closed loop via
> centralised and decentralised controllers with actuators, e.g., valves,
> pumps, heaters. Also monitoring of attributes such as the filling levels of
> tanks, quality of material or environmental data are important, as well as
> safety warnings or plant shut downs. Workers in the plant are supported by
> mobile devices. A process automation facility may range from a few 100 m² to
> km² or may be geographically distributed over a certain geographic region.
> Depending on the size, a production plant may have several 10 000
> measurement points and actuators. Autarkic device power supply for years is
> needed in order to stay flexible and to keep the total costs of ownership
> low.
>
> **3) HMIs and Production IT:** Human-machine interfaces (HMIs) include all
> sorts of devices for the interaction between people and production
> facilities, such as panels attached to a machine or production line, but
> also standard IT devices, such as laptops, tablet PCs, smartphones, etc. In
> addition to that, also augmented and virtual reality (AR/VR) applications
> are expected to play an increasingly important role in future, which may be
> enabled by special AR/VR glasses, but also by more standard devices, such as
> tablet PCs or the like.
Production IT, on the other hand, encompasses IT-based applications, such as
manufacturing execution systems (MES) as well as enterprise resource planning
(ERP) systems. The overall goal of an MES system, for example, is to monitor
and document how raw materials and/or basic components are transformed into
finished goods, whereas an ERP system generally provide an integrated and
continuously updated view of important business processes. Both systems rely
on the timely availability of large amounts of data from the production
process.\ \ Since both HMIs and Production IT are more related to traditional
IT systems than to factory-specific operational technology (OT) systems, they
are bundled in one application area.
> **\- Logistics and warehousing:** Logistics and warehousing refers to the
> organisation and control of the flow and storage of materials and goods in
> the context of industrial production. In this respect, intra-logistics is
> dealing with logistics within a certain property (e.g., within a factory),
> for example by ensuring the uninterrupted supply of raw materials on the
> shop floor level using automated guided vehicles (AGVs), fork lifts, etc.
> This is to be seen in contrast to logistics between different sites, for
> example for the transport of goods from a supplier to a factory or from a
> factory to the end customer. Warehousing particularly refers to the storage
> of materials and goods, which is also getting more and more automated, for
> example based on conveyors, cranes and automated storage and retrieval
> systems. For all kinds of logistics applications, generally also the
> localisation, tracking and monitoring of assets is of high importance.
>
> **\- Monitoring and maintenance:** Monitoring and maintenance refers to the
> monitoring of certain processes and/or assets without an immediate impact on
> the processes themselves (in contrast to a typical closed-loop control
> system in factory automation, for example). This particularly includes
> applications such as condition monitoring and predictive maintenance based
> on sensor data, but also big data analytics for optimising future parameter
> sets of a certain process, for instance. For these use cases, the data
> acquisition process is typically not latency-critical, but a large number of
> sensors may have to be efficiently interconnected, especially since many of
> these sensors may only be battery-driven.
For each of these application areas, a multitude of potential use cases
exists, some of which are outlined in the following Clauses. These use cases
can be mapped to the given application areas as shown in Table 5.3.1.1-1.
Table 5.3.1.1-1: Mapping of the considered use cases (columns)\ to application
areas (rows)
                               **Motion control**   **Control-to-control**   **Mobile control panels with safety**   **Mobile robots**   **Massive wireless sensor networks**   **Remote access and maintenance**   **Augmented reality**   **Closed-loop process control**   **Process monitoring**   **Plant asset management**
* * *
Factory automation X X X X  
Process automation X X X X X HMIs and Production IT X X  
Logistics and warehousing X X  
Monitoring and maintenance X X
#### 5.3.1.2 Major challenges and particularities
Major general challenges and particularities of the Factories of the Future
include the following aspects:
> 1) Industrial-grade quality of service is required for many applications,
> with stringent requirements in terms of end-to-end latency, communication
> service availability, jitter, and determinism.
>
> 2) There is not only a single class of use cases, but there are many
> different use cases with a wide variety of different requirements, thus
> resulting in the need for a high adaptability and scalability of the 5G
> system.
>
> 3) Many applications have stringent requirements on safety, security (esp.
> availability, data integrity, and confidentiality), and privacy.
>
> 4) The 5G system has to support a seamless integration into the existing
> (primarily wire-bound) connectivity infrastructure. For example, the 5G
> shall allow to flexibly combine the 5G system with other (wire-bound)
> technologies in the same machine or production line.
>
> 5) Production facilities usually have a rather long lifetime, which may be
> 20 years or even longer. Therefore, long-term availability of 5G
> communication services and components are essential.
>
> 6) 5G systems shall support non-public operation by deploying type-a network
> or type-b network within a factory or plant, which are isolated from PLMNs.
> This is required by many factory/plant owners for security, liability,
> availability and business reasons. Nevertheless, in the case of a type-a
> network is deployed standardised and flexible interfaces shall be supported
> for seamless interoperability and seamless handovers between 5G PLMNs and
> non-public 5G systems.
>
> 7) The radio propagation environment in a factory or plant can be quite
> different from the situation in other application areas of the 5G system. It
> is typically characterised by very rich multipath, caused by a large number
> of---often metallic---objects in the immediate surroundings of transmitter
> and receiver, as well as potentially high interference caused by electric
> machines, arc welding, and the like.
>
> 8) The 5G system shall be able to support continuous monitoring of the
> current network state in real-time, to take quick and automated actions in
> case of problems and to do efficient root-cause analyses in order to avoid
> any undesired interruption of the production processes, which may incur huge
> financial damage. Particularly if a third-party network operator is
> involved, accurate SLA monitoring is needed as the basis for possible
> liability disputes in case of SLA violations.
#### 5.3.1.3 Deployment aspects
Separate communication services may need to be provided for different
application areas. Note that \"separate\" communication service can be
physically, logically or virtually separate.
The application area \"factory automation\" consists of the use cases \"motion
control\", \"control-to-control\", \"mobile robots\" and \"massive wireless
sensor networks\". Communication services for factory automation meet
stringent requirements; and operation is limited to a relatively small service
area where no interaction with the public network (e.g., service continuity,
roaming) is required.
The application area \"process automation\" consists of the use cases \"mobile
robots\", \"massive wireless sensor networks\", \"closed-loop process
control\", \"process monitoring\" and \"plant asset management\".
Communication services for process automation meet stringent requirements.
Interaction with the public network (e.g., service continuity, roaming) is
required.
The application area \"HMIs and production IT\" consists of the use cases
\"mobile control panels with safety\" and \"augmented reality\". Communication
services for HMIs and Production IT meet stringent requirements; and are
limited to a local service area where no interaction with the public network
(e.g., service continuity, roaming) is required.
The application area \"logistics and warehousing\" consists of the use cases
\"control-to-control\" and \"mobile robots\". Communication service for
logistics and warehousing meet very stringent requirements; and are limited to
a local service area (both indoor and outdoor). Interaction with the public
network (e.g., service continuity, roaming) is required.
The application area \"monitoring and maintenance\" consists of the use cases
\"massive wireless sensor networks\" and \"remote access and maintenance\".
Communication services for monitoring and maintenance meet stringent
requirements; and are limited to a local service area (both indoor and
outdoor). Interaction with the public network (e.g., service continuity,
roaming) is required.
### 5.3.2 Motion control
#### 5.3.2 _._ 1 Description
Motion control is among the most challenging and demanding closed-loop control
applications in industry. A motion control system is responsible for
controlling moving and/or rotating parts of machines in a well-defined manner,
for example in printing machines, machine tools or packaging machines. Due to
the movements/rotations of components, wireless communications based on
powerful 5G systems constitutes a promising approach. On the one hand this is
because with wirelessly connected devices, slip rings, cable carriers,
etc.―which are typically used for these applications today―can be avoided,
thus reducing abrasion, maintenance effort and costs. On the other hand, this
is because machines and production lines may be built with less restrictions,
allowing for novel (and potentially much more compact and modular) setups.
A schematic representation of a motion control system is depicted in Figure
5.3.2.1-1. A motion controller periodically sends desired set points to one or
several actuators (e.g., a linear actuator or a servo drive) which thereupon
perform a corresponding action on one or several processes (in this case
usually a movement or rotation of a certain component). At the same time,
sensors determine the current state of the process(es) (in this case for
example the current position and/or rotation of one or multiple components)
and send the actual values back to the motion controller. This is done in a
strictly cyclic and deterministic manner, such that during one communication
cycle time _T_ ~cycle~ the motion controller sends updated set points to all
actuators, and all sensors send their actual values back to the motion
controller. Nowadays, typically Industrial Ethernet technologies are used for
motion control systems. Examples for such technologies are Sercos®, PROFINET®
IRT or EtherCAT®, which support cycle times below 50 µs. In general, lower
cycle times allow for faster and more accurate movements/rotations.
{width="6.047916666666667in" height="1.523611111111111in"}
Figure 5.3.2.1-1: Schematic representation of a motion control system
While it might be possible to move away from the strictly cyclic communication
pattern for motion control systems in the long-term, it is hard to do so in
the short-term since the whole ecosystem (tools, machines, communication
technologies, servo drives, etc.) is based on the cyclic communication
paradigm. In order to support a seamless migration path, the 5G system
therefore should support such a highly deterministic cyclic data communication
service.
Furthermore, there are many scenarios where some devices (e.g., sensors or
actuators) are added / activated or removed / deactivated while the overall
control system keeps on running. In order to support such cases, hot-plugging
support is required without any (observable) impact on the rest of the system.
Table 5.3.2.1-1 shows some typical values for the number of nodes, cycle times
and payload sizes for some of the most important application areas of motion
control systems. However, it should be noted that these values may vary widely
in practice and that not all sensors and/or actuators in a motion control
system may have to be connected using a 5G system. Instead, it is expected
that there will be a seamless coexistence between Industrial Ethernet and the
5G system in the future.
Table 5.3.2.1-1: Typical characteristics of motion control systems for three
major applications
**Application** **# of sensors / actuators** **Typical message size** **Cycle
time _T_ ~cycle~** **Service area**
* * *
Printing Machine > 100 20 byte \ 1) The motion controller sends set points to all actuators.
>
> 2) The actuators take these set points and put them into an internal buffer.
>
> 3) All sensors transmit their current actual values from their internal
> buffer to the motion controller.
>
> 4) At a well-defined time instant within the current cycle, which is
> commonly referred to as the \"global sampling point\", the actuators
> retrieve the latest set points received from the motion controller from
> their internal buffer and act accordingly on the process(es) (see Figure
> 5.3.2.1-1). At exactly the same time, the sensors determine the current
> state of the process(es) and put them as new actual values in their internal
> buffer, ready to be transmitted to the motion controller. It is important
> that there is a very high synchronicity in the order of 1 µs between all
> involved devices (motion controller, sensors, actuators) with respect to
> this global sampling point.
All messages exchanged have to be properly secured (especially in terms of
data integrity and authenticity) and the probability of two consecutive packet
errors shall be negligible. This is because a single packet error may be
tolerable, but two consecutive packet errors may damage a machine and may lead
to a production downtime with possibly huge financial damage.
Some of the sensors/actuators may be moving and/or rotating, with typical
maximum speeds up to about 20 m/s.
#### 5.3.2.4 Post-conditions
The components controlled by the motion control system move/rotate as
requested by the motion controller.
#### 5.3.2.5 Challenges to the 5G system
> Special challenges to the 5G system associated with this use case include
> the following aspects:
>
> Very stringent requirements on latency, communication service availability,
> and determinism.
>
> Very stringent requirements on clock synchronicity between different nodes.
>
> Transmission of rather small chunks of data, resulting in potentially
> significant relative overhead due to signalling, security, etc.
>
> Potentially high density of UEs (sensors/actuators)
#### 5.3.2.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 2.1_ The 5G system shall support cyclic traffic with
cycle times in the order of 1 ms for a communication group of about 50 UEs and
payload sizes of about 40 byte. T  
_Factories of the Future 2.2_ The 5G system shall support cyclic traffic with
cycle times in the order of 0,5 ms for a communication group of about 20 UEs
and payload sizes of about 50 byte. T  
_Factories of the Future 2.3_ The 5G system shall support cyclic traffic with
cycle times in the order of 2 ms for a communication group of about 100 UEs
and payload sizes of about 20 byte. T  
_Factories of the Future 2.4_ The 5G system shall support a very high
synchronicity between a communication group of 50 UEs to 100 UEs in the order
of 1 µs or below. T  
_Factories of the Future 2.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
ultra-low latency and ultra-high reliability requirements T  
_Factories of the Future 2.6_ The 5G system shall support communication
service availability exceeding at least 99,9999%, ideally even 99,999999%. T  
_Factories of the Future 2.7_ The 5G system shall support hot-plugging in the
sense that new devices may be dynamically added to and removed from a motion
control application, without any observable impact on the other nodes. T  
_Factories of the Future 2.8_ The 5G system shall support UE speeds up to 20
m/s, even for communication services with ultra-low latency and ultra-high
reliability. T  
_Factories of the Future 2.9_ The cyclic data communication service of the 5G
system shall be able to support satisfy the safety requirements according to
[25] for safety integrity level 3 (SIL-3). T  
_Factories of the Future 2.10_ The 5G system shall ensure error-free
transmission of a second message within the survival time if the transmission
of the previous message failed. T
### 5.3.3 Motion control -- transmission of non-real-time data
#### 5.3.3 _._ 1 Description
In this use case, some additional non-real-time (NRT) data are transmitted
from the motion controller to one or several nodes (cf. Figure 5.3.2.1-1).
This is done in parallel to the regular cyclic data transmission service as
described in the previous clause. Examples for this NRT data are
software/firmware updates or maintenance information.
#### 5.3.3.2 Preconditions
All sensors, actuators and the motion controller are switched on and connected
to the 5G system. The strictly cyclic data communication service between the
motion controller and the sensors/actuators has been successfully set up (see
previous use case) and is running.
#### 5.3.3.3 Service flows
The cyclic data communication service as described in the previous clause is
running without any interruptions. In addition to that, the following service
flow occurs, assuming a downstream NRT data transmission from the motion
controller to one (or several) sensor(s) and/or actuator(s):
> 1) The motion controller initiates a NRT data transmission service to one or
> several sensor(s)/actuator(s).
>
> 2) The motion controller transmits the NRT data to the respective
> sensor(s)/actuator(s) with a data rate of at least 1 Mbit/s.
>
> 3) The recipient(s) confirm(s) the successful reception of the NRT data.
>
> 4) The NRT data transmission service is disengaged.
An alternative flow with an upstream NRT data transmission service may look as
follows:
> 1) A sensor/actuator requests a NRT data transmission service to the motion
> controller
>
> 2) The motion controller approves the request to establish a NRT data
> transmission service from the requesting sensor/actuator to the motion
> controller.
>
> 3) The respective sensor/actuator initiates a NRT data transmission service
> to the motion controller.
>
> 4) The respective sensor/actuator transmits the NRT data to the motion
> controller with a data rate of at least 1 Mbit/s.
>
> 5) The motion controller confirms the successful reception of the NRT data.
>
> 6) The NRT data transmission service is disengaged.
#### 5.3.3.4 Post-conditions
The NRT data has been successfully transmitted.
The strictly cyclic data communication service between the motion controller
and the sensors/ actuators has not been affected.
#### 5.3.3.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> Simultaneous transmission of non-critical NRT data and highly-critical
> motion control data with highest requirements in terms of latency and
> communication service availability over the same link and to the same device
>
> Dynamic and efficient establishment and disengagement of NRT data
> transmission services
#### 5.3.3.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 3.1_ The strictly cyclic data communication service
between a motion controller and several sensors/actuators and data service
between the motion controller and a subset of the sensors/actuators or between
one of the sensors/actuators and the motion controller can be simultaneously
supported. T  
_Factories of the Future 3.2_ The low-priority NRT data service supports a
data rate of at least 1 Mbit/s (in addition to the cyclic data communication
service). T
### 5.3.4 Motion control -- seamless integration with Industrial Ethernet
#### 5.3.4 _._ 1 Description
In this use case, not all sensors and actuators in a motion control system are
connected using a 5G system. Instead, a single motion control system could
integrate components of a wire-bound Industrial Ethernet system and components
of a 5G system. Therefore the 5G system must support the seamless integration
and interplay with Industrial Ethernet.
{width="6.13125in" height="3.7979166666666666in"}
Figure 5.3.4.1-1: Example for an industrial Ethernet network including 5G
links for motion control
#### 5.3.4.2 Preconditions
Sensors, actuators and motion controller are switched on and some of them are
connected using a 5G system and the others are connected using Industrial
Ethernet. The interconnection between Industrial Ethernet and 5G is realised
using gateway UEs connected to Ethernet switches or a device is connected
directly to a PDN using an Ethernet adapter.
#### 5.3.4.3 Service flows
The conditions for cyclic and NRT communication flows apply to this use case,
with the addition that communication flows are initiated on devices connected
via (Industrial) Ethernet and directed to devices connected via 5G or vice
versa.
As Industrial Ethernet typically operates on the Ethernet Data Link layer
(Layer 2), the communication flow establishment can only be successful if the
5G network is able to forward frames from Ethernet sources towards 5G
destinations and vice versa. In general this also includes the handling of
broadcast packets.
The Ethernet devices of a single motion control could be separated from others
on the same physical Ethernet network using Virtual LAN (IEEE 802.1Q).
Therefore the 5G system must be aware of the Virtual LAN associations when
forwarding Ethernet frames. Furthermore, the Virtual LANs Priority Code Point
assignment could be utilised to determine the 5G traffic priority.
The precise time synchronisation between multiple motions controllers
connected to Industrial Ethernet and 5G may be realised using the Precise Time
Protocol (IEEE 1588). This protocol enables the estimation of clock offsets
between network end-points and in this case the motion controllers. The
support of IEEE 1588 in a 5G system introduces the expedited processing and
transmission of certain messages at any of the intermediate devices in the
communication path.
A single motion control might share the available network resources with other
applications. Current and future Industrial Ethernet protocols offer the
reservation of network resources to overcome communication bottlenecks. Since
resource reservation considers the complete communication path, the flow of
resource reservation is introduced to the 5G system. The 5G system could be
aware of the standard Ethernet protocols as e.g., Time-Aware Scheduling
defined in IEEE 802.1Qbv.
#### 5.3.4.4 Post-conditions
The components controlled by the motion control system move/rotate as
requested by the motion controller.
#### 5.3.4.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> Seamless integration with (Industrial) Ethernet systems.
>
> Support of certain mechanisms of the IEEE 802.1 protocol family, including
> IEEE 802.1Qbv (time-aware scheduling) and IEEE 802.1Q (VLANs).
>
> Support of time synchronisation based on IEEE 1588.
#### 5.3.4.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 4.1_ The 5G system shall support the basic Ethernet
Layer-2 bridge functions as bridge learning and broadcast handling. T  
_Factories of the Future 4.2_ The 5G system shall support and be aware of
VLANs (IEEE 802.1Q) T  
_Factories of the Future 4.3_ The 5G system shall support the expedited
processing and transmission of IEEE1588 / Precise Time Protocol messages T  
_Factories of the Future 4.4_ The 5G system shall support IEEE 802.1Qbv (time-
aware scheduling) T
### 5.3.5 Control-to-control communication (motion subsystems)
#### 5.3.5 _._ 1 Description
Control-to-control (C2C) communication, i.e., the communication between
different industrial controllers (e.g., programmable logic controllers or
motion controllers) is already used today for a number of different use cases,
such as the following ones:
> Large machines (e.g., newspaper printing machines), where several controls
> are used to cluster machine functions, which need to communicate with each
> other. These controls typically need to be synchronised and exchange real-
> time data.
>
> Individual machines that are used for fulfilling a common task (e.g.,
> machines in an assembly line) often need to communicate, for example for
> controlling and coordinating the handover of work pieces from one machine to
> another.
Typically, a C2C network has no fixed configuration of certain controls that
need to be present. The control nodes present in the network often vary with
the status of machines and the manufacturing plant as a whole. Therefore, hot-
plugging support for different control nodes is important and often used.
Protocols that are used for C2C communications today include Industrial
Ethernet standards, such as Sercos, PROFINET®, and EtherCAT®, as well as OPC
UA®-based communication and other protocols, which are often based on Fast
Ethernet.
With the introduction of \"Connected Industries\" or \"Industrial IoT\"
scenarios, the amount of networking between controls is assumed to rise.
Especially the number of controls participating and the amount of data being
exchanged is assumed to rise significantly. In this respect, wireless
communication using a 5G system may pave the way for highly modular and
flexible production modules that efficiently and flexibly interact with each
other.
In the following, the main focus is on control-to-control communication
between different motion (control) subsystems, as outlined in Clause 5.3.2. An
exemplary application for that are large printing machines, where it is not
possible or desired to control all actuators and sensors by one motion
controller only. Such C2C systems typically have the most demanding
requirements on the underlying connectivity infrastructure. For other C2C
applications, the corresponding requirements (e.g., in terms of clock
synchronicity) become often more relaxed.
In general, this use case has very stringent requirements in terms of latency
and service availability. The required service area is usually bigger than for
\"motion control\" (see Clause 5.3.2), and interaction with the public network
(e.g., service continuity, roaming) is not required.
#### 5.3.5.2 Preconditions
At least a subset of the controls are switched on and connected to the 5G
network. The remaining controls may be interconnected with the other controls
using state-of-the-art wire-bound communication technologies, such as the ones
mentioned above.
#### 5.3.5.3 Service flows
Data transmission in control-to-control networks typically consists of cyclic
and non-cyclic data transfers. Both types may have real-time requirements.
However, the real-time requirements typically are lower than in the case of
communication between controls and sensors or actuators (for example in case
of motion control applications). Typical cycle times for C2C communication are
in the order of 4 to 10 ms. However, the amounts of cyclic data typically are
higher compared to the communication between controls and sensors / actuators
and may be more than 1 kB per cycle.
Many C2C networks make heavy use of hot-plug features, so that additional
control nodes may be added to a running network as well as being removed
without affecting the data transfer between other nodes.
#### 5.3.5.4 Post-conditions
The different types of data transfers between the controls function as
required. The different controls act in a coordinated and tightly coupled
manner.
#### 5.3.5.5 Challenges to the 5G system
> Special challenges to the 5G system associated with this use case include
> the following aspects:
>
> Stringent requirements on end-to-end latency, communication service
> availability, and determinism.
>
> Very stringent requirements on synchronicity between different nodes.
>
> Transmission of possibly large amounts of data per cyclic data transmission.
>
> Potentially high density of UEs in the future
#### 5.3.5.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 5.1_ The 5G system shall support strictly
deterministic cyclic traffic with cycle times down to at least 4 ms for a
communication group of 5 controls to 10 controls (in the future up to 100) and
payload sizes up to 1 kbyte. T  
_Factories of the Future 5.2_ The 5G system shall support strictly
deterministic acyclic traffic with response times of less than 10 ms, i.e.,
any non-cyclic (bi-directional) message transfer shall be successfully
completed in less than 10 ms. T  
_Factories of the Future 5.3_ The 5G system shall support a very high
synchronicity between a communication group of 5 controls to10 controls (in
the future up to 100) in the order of 1 µs or below. T  
_Factories of the Future 5.4_ The 5G system shall be able to support non-real-
time traffic, both cyclic and non-cyclic. T  
_Factories of the Future 5.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
low end-to-end latency and ultra-high availability requirements T  
_Factories of the Future 5.6_ The 5G system shall support communication
service availability exceeding at least 99,9999%, ideally even 99,999999%. T  
_Factories of the Future 5.7_ The cyclic data communication service of the 5G
system shall be able support to satisfy the safety requirements according to
[25] for safety integrity level 3 (SIL-3). T
### 5.3.6 Mobile control panels with safety functions
#### 5.3.6 _._ 1 Description
Control panels are crucial devices for the interaction between people and
production machinery as well as for the interaction with moving devices. These
panels are mainly used for configuring, monitoring, debugging, controlling and
maintaining machines, robots, cranes or complete production lines. In addition
to that, (safety) control panels are typically equipped with an emergency stop
button and an enabling device, which an operator can use in case of a safety
event in order to avoid damage to humans or machinery. When the emergency stop
button is pushed, the controlled equipment immediately has to come to a safe
stationary position. Likewise, if a machine, robot, etc. is operated in the
so-called special 'enabling device mode', the operator has to manually keep
the enabling device switch in a special stationary position. If the operator
pushes this switch too much or releases it, the controlled equipment
immediately has to come to a safe stationary position as well. This way, it
can be ensured that the hand(s) of the operator are on the panel (and not
under a moulding press, for example) and that the operator does―for
instance―not suffer from any electric shock or the like. A common use case for
this 'enabling device mode' is the installation, testing or maintenance of a
machine, during which other safety mechanisms (such as a safety fence) have to
be deactivated.
Due to the criticality of these safety functions, safety control panels
currently have mostly a wire-bound connection to the equipment they control.
In consequence, there tend to be many such panels for the many machines and
production units that typically can be found in a factory. With an ultra-
reliable low-latency wireless link, it would be possible to connect such
mobile control panels with safety functions wirelessly. This would lead to a
higher usability and would allow for the flexible and easy re-use of panels
for controlling different machines.
One way to realise the safety functions is to make use of a special safety
protocol in conjunction with the \"black channel\" principle [31]. These
safety protocols can ensure a certain safety level as specified in [32] with
no or only minor requirements on the communication channel between the mobile
control panel and the controlled equipment. To this end, a strictly cyclic
data communication service is required between both ends. If the connectivity
is interrupted, an emergency stop is triggered, even if no real safety event
has occurred. That means the mobile control panel and the safety controller
(e.g., a safety programmable logic controller [PLC]) it is attached to
cyclically exchange messages and the machine stops if either the connection is
lost or if the exchanged messages explicitly indicate that a safety event has
been triggered (e.g., that the emergency stop button has been pushed). Thus,
guaranteeing the required safety level is not difficult, but achieving at the
same time a high availability of the controlled equipment/production machinery
is. To that end, an ultra-reliable ultra-low-latency link is required.
The cycle times for the safety traffic always depend on the
process/machinery/equipment whose safety has to be ensured. For a fast-moving
robot, for example, the cycle times are lower than for a slowly moving linear
actuator.
Some of positioning related service requirements for this use case can be
found in [66].
In general, this use case has very stringent requirements in terms of latency
and service availability. The required service area is usually bigger than for
\"motion control\" (see Clause 5.3.2), and interaction with the public network
(e.g., service continuity, roaming) is not required.
#### 5.3.6.2 Preconditions
The mobile control panel with safety functions is connected to a safety PLC of
the machine/equipment it is supposed to control. The mobile control panel is
in a predefined geographical position related to the controlled device
(defined area, free field of view). A cyclic data communication service
matching the cycle time requirements of the used safety protocol has
successfully been set up. The emergency stop button is not pushed. The panel
is not operated in enabling device mode, i.e., the operator does not have to
keep the enabling device switch in a dedicated stationary position for proper
operation of the controlled equipment.
#### 5.3.6.3 Service flows
A typical service flow may look as follows:
> 1) The mobile control panel and safety PLC periodically exchange safety
> messages in intervals of _T_ ~cycle~ with a payload size of 40 bytes to 250
> bytes, indicating absence of any safety event. The value of the cycle time
> _T_ ~cycle~ depends on the controlled equipment and some examples can be
> found in the requirements given below.
>
> 2) In parallel, a non-cyclic bi-directional data communication service with
> a data rate of at least 5 Mbit/s in each direction is set up between the
> mobile control panel and the safety PLC for facilitating human interaction
> with the machine for configuring, monitoring, maintaining, etc. the machine.
>
> 3) Various bursts of non-cyclic data traffic are exchanged between the
> mobile control panel and the safety PLC in parallel to the cyclic data
> communication service for the safety traffic.
>
> 4) The operator pushes the emergency stop button on the control panel,
> triggering the transmission of corresponding safety messages to the safety
> PLC.
>
> 5) The controlled machine stops within a pre-defined time.
An alternative service flow covering the case of a broken link (which should
be avoided) may look as follows:
> 1) The mobile control panel and safety PLC periodically exchange safety
> messages in intervals of _T_ ~cycle~ with a payload size of 40 bytes to 250
> bytes, indicating absence of any safety event. The value of the cycle time
> _T_ ~cycle~ depends on the controlled equipment and some examples can be
> found in the requirements given below.
>
> 2) In parallel, a non-cyclic bi-directional data communication service with
> a data rate of at least 5 Mbit/s in each direction is set up between the
> mobile control panel and the safety PLC for facilitating human interaction
> with the machine for configuring, monitoring, maintaining, etc. the machine.
>
> 3) Various bursts of non-cyclic messages are exchanged between the mobile
> control panel and the safety PLC in parallel to the enduring cyclic data
> communication service for the safety traffic.
>
> 4) The cyclic data communication service between the mobile control panel
> and the safety PLC is interrupted or disturbed (in the sense that the cycle
> times requirement cannot be met anymore, for example), triggering a timeout
> at the safety PLC.
>
> 5) The controlled machine stops within a pre-defined time.
All messages exchanged have to be properly secured (especially data integrity
and authenticity) and the probability of two consecutive packet errors shall
be negligible. This is because a single packet error may be tolerable, but two
consecutive packet errors may lead to a false safety alarm and thus may lead
to a lengthy production downtime.
#### 5.3.6.4 Post-conditions
Machines can be controlled in a safe way while meeting the requirements. The
controlled machine has stopped within a pre-defined time after the emergency
button has been pushed or the communication link was disturbed. Nobody got
hurt.
#### 5.3.6.5 Challenges to the 5G system
> Special challenges to the 5G system associated with this use case include
> the following aspects:
>
> \- Stringent requirements on end-to-end latency and jitter along with very
> stringent requirements on communication service availability.
>
> \- Simultaneous transmission of non-critical (bi-directional) data and
> highly-critical safety traffic with stringent requirements in terms of
> latency and communication service availability to the same device.
>
> \- The need for seamless mobility support (see the first item above).
#### 5.3.6.6 Potential requirements
+-----------------+-----------------+-----------------+-------------+ | **Reference |** Requirement | **Application / |** Comment**| | number** | text**| transport** | | +=================+=================+=================+=============+ | _Factories of | The 5G system | T | | | the Future 6.1_ | shall support a | | | | | bidirectional, | | | | | cyclic data | | | | | communication | | | | | service | | | | | characterised | | | | | by at least the | | | | | following | | | | | parameters | | | | | (e.g., for | | | | | assembly robots | | | | | or milling | | | | | machines): | | | | | | | | | | Cycle time of | | | | | _T_ ~cycle~ = 4 | | | | | ms to 8 ms | | | | | | | | | | Jitter \ 5 | | | | | Mbit/s | | | | | | | | | | Target | | | | | end-to-end | | | | | latency \ \- processes for handling goods and materials, especially incoming and
> outgoing goods, in warehousing and commissioning, in transportation as well
> as transfer and provision of goods;
>
> \- followed by information flows, namely the communication of inventory and
> movement reports, the outstanding order situation, throughput times and
> availability forecasts, presenting data to support tracking, monitoring and
> if needed to make decisions on measures to be taken, as well as the
> selection and implementation of means of data transfer;
>
> \- the use of means of transportation (cranes, lifters, conveyors,
> industrial trucks, etc.), as well as monitoring and control elements
> (sensory and actuating equipment);
>
> \- and finally the use of techniques (for active/passive security, data
> management, goods and wares recognition/identification, image processing,
> goods transfer, namely provision, sorting commissioning, palletising,
> packaging).
Mobile robot systems can be divided in operation in indoor, outdoor and both
indoor and outdoor areas. These environmental conditions have an impact on the
requirements of the communication system, e.g., the handover process, to
guarantee the required cycle times. Some examples are given in Table
5.3.7.1-1.
Table 5.3.7.1-1: Overview of different operational areas including\
corresponding examples for mobile robots / AGVs
+----------------+----------------+----------------+----------------+ | * | **Indoor** | **Outdoor** | **Indoor and | | *Environment** | | | outdoor**| +================+================+================+================+ |** Area of | Several meters | Several meters | Combination of | | operation**| up to several | up to several | both, factory | | | hundreds of | kilometres in | buildings and | | | meters, | container | open areas, as | | | possibly | terminals or | described at | | | divided in | in open pit | the indoor and | | | different | mines. | outdoor areas | | | factory | | of operation | | | buildings. | | | +----------------+----------------+----------------+----------------+ |** Examples** | Light load - | Unit load | Towing | | | transport with | vehicles for | vehicles that | | | work-in | container | operate | | | process | transport in | between stock | | | movement of | automated | and production | | | goods | ports | | | | | | Cow/animal | | | Unit load | Mining trucks | feeding robots | | | vehicles | | on farms | | | (e.g., car | Combinations | | | | body) movement | of vehicles | Automated fork | | | with | for platooning | lifters for | | | w | and convoy | use in truck | | | ork-in-process | | loading | | | in a | Heavy-load | | | | production and | transport | | | | decoupled | crosses the | | | | operating | outside area | | | | times as an | | | | | advantage over | | | | | conveyor belts | | | | | | | | | | • Pallet | | | | | trucks | | | +----------------+----------------+----------------+----------------+
Depending on the operation area, this use case may have different
requirements:
\- in a limited service area (likely indoor), ultra-low latency is required
and not interaction with the public network (e.g., service continuity,
roaming);
\- in a wide service area (several kilometers, likely outdoors), the latency
requirement is relatively relaxed and interaction with the public network
(e.g., service continuity, roaming) may be required.
#### 5.3.7.2 Preconditions
All mobile robots and the guidance control system are switched on and
connected to the 5G network. The communication between the guidance control
system and mobile robots has been successfully set up and is running.
#### 5.3.7.3 Service flows
**_In the following, we distinguish three different cases, depending on who is
communicating with whom:_**
> a) Communication between mobile robot and guidance control system
>
> A number of mobile robots (up to 100, with a potential enrolment of up to
> 1000) are commonly in use, always guided by a guidance control system. The
> mostly centralised guidance control system communicates bidirectional with
> each mobile robot. In this respect, the following data is typically
> exchanged:
>
> \- Communication direction from guidance control system to mobile robot:
>
> \- Process data for control and management of mobile robots
>
> \- Emergency stop
>
> \- Communication direction from mobile robot to guidance control system:
>
> \- Process data (control and management data)
>
> \- Video or image data
>
> b) Communication between mobile robots
>
> The mobile robots can exchange real-time control data with each other and
> provide a collision-free operation of autonomous mobile robots and
> synchronised actions between multiple mobile robots. For this purpose, the
> mobile robots exchange real-time control data.
>
> c) Communication between mobile robots and peripheral facilities
>
> The mobile robots communicate with the peripheral facilities. For example,
> mobile robots are able to open and close doors or gates. For this purpose,
> the mobile robots transmit the control data to the door or gate control.
> Furthermore, mobile robots can be working together with fixed installations
> like cranes or manufacturing machines. To this end, the mobile robots
> exchange real-time control data with cranes or manufacturing machines.
#### 5.3.7.4 Post-conditions
Mobile robots and AGVs can be controlled in a safe way while satisfying the
requirements.
#### 5.3.7.5 Challenges to the 5G system
> Special challenges to the 5G system associated with this use case include
> the following aspects:
>
> Very stringent requirements on latency, communication service availability,
> and determinism.
>
> Very stringent requirements on clock synchronicity between different mobile
> robots.
>
> Simultaneous transmission of non-real time data, real-time streaming data
> (video) and highly-critical, real-time control data with highest
> requirements in terms of latency and communication service availability over
> the same link and to the same mobile robot.
>
> Potentially high density of mobile robots
>
> Good 5G coverage in indoor (from basement to roof), outdoor (plant/factory
> wide) and indoor/ outdoor environment is needed due to mobility of the
> robots.
>
> Seamless mobility support such that there is no impairment of the
> application in case of movements of a mobile robot within a factory or
> plant.
#### 5.3.7.6 Potential requirements
+----------------+----------------+----------------+----------------+ | **Reference |** Requirement | **Application |** Comment**| | number** | text**| / transport** | | +================+================+================+================+ | _Factories of | The 5G system | A | The size of | | the Future | shall support | | messages at | | 7.1_ | a cyclic data | | the | | | communication | | application | | | service, | | layer are | | | characterised | | 15kbyte to 150 | | | by at least | | kbyte for | | | the following | | video frames | | | parameters: | | in | | | | | video-operated | | | Cycle time of | | remote | | | | | control. The | | | 1 ms for | | size of all | | | precise | | other messages | | | cooperative | | in all use | | | robotic motion | | cases, e.g., | | | control | | control | | | | | messages to an | | | 1 ms to 10 ms | | actuator, is | | | for machine | | 40 byte to 250 | | | control | | byte. | | | | | | | | 10 ms to 50 ms | | | | | for | | | | | cooperative | | | | | driving | | | | | | | | | | 10 ms to 100 | | | | | ms for video | | | | | operated | | | | | remote control | | | | | | | | | | 40 ms to 500 | | | | | ms for | | | | | standard | | | | | mobile robot | | | | | operation and | | | | | traffic | | | | | management | | | | | | | | | | Jitter \ 99,9999% | | | | | | | | | | Max. number of | | | | | mobile robots: | | | | | 100 | | | +----------------+----------------+----------------+----------------+ | _Factories of | For certain | A | | | the Future | applications, | | | | 7.2_ | the 5G system | | | | | shall support | | | | | real-time | | | | | streaming data | | | | | transmission | | | | | (video data) | | | | | from each | | | | | mobile robot | | | | | to the | | | | | guidance | | | | | control system | | | | | by at least | | | | | the following | | | | | parameter: | | | | | | | | | | Data | | | | | transmission | | | | | rate per | | | | | mobile robot: | | | | | > 10 Mbit/s | | | | | | | | | | Number of | | | | | mobile robots: | | | | | 100 | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 7.3_ | seamless | | | | | mobility such | | | | | that there is | | | | | no impairment | | | | | of the | | | | | application in | | | | | case of | | | | | movements of a | | | | | mobile robot | | | | | within a | | | | | factory or | | | | | plant. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 7.4_ | user equipment | | | | | ground speeds | | | | | of up to 50 | | | | | km/h. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 7.5_ | uniform and | | | | | unequivocal | | | | | parameters for | | | | | interfaces to | | | | | allow | | | | | dependability | | | | | monitoring | | | | | (see Clause | | | | | 4.3.4). | | | +----------------+----------------+----------------+----------------+ | _Factories of | Communication | T | | | the Future | complying with | | | | 7.6_ | the above | | | | | requirements | | | | | shall be | | | | | available over | | | | | a service area | | | | | of 1 km^2^ and | | | | | less. | | | +----------------+----------------+----------------+----------------+
### 5.3.8 Massive wireless sensor networks
#### 5.3.8 _._ 1 Description
Sensor networks aim at monitoring the state or behaviour of a particular
environment. In the context of the Factory of the Future, wireless sensor
networks (WSN) are targeting the monitoring of a process and the corresponding
parameters in an industrial environment. This environment is typically
monitored using various types of sensors such as microphones, CO~2~ sensors,
pressure sensors, humidity sensors, and thermometers. In particular, these
sensors usually form a distributed monitoring system. The monitored data, from
such a system, is used to detect anomalies in the data, i.e., by leveraging
machine learning (ML) algorithms. These algorithms usually require a training
phase before a trained ML algorithm can later work on a subset of the
available measured data. However, the training as well as the analysis of the
data may be realised in a centralised or distributed manner.
The placement of the monitoring function can be dynamic and thus, may vary
over time to enable dynamic up- and down-scaling of computing resources. In
particular, the placement may also be constrained by the available WSN
hardware. Given rather simple sensing devices, the functionality needs to be
placed into a centralised computing infrastructure such as a mobile data or
data centre cloud. Opposed to that, functionality may be placed inside the
sensor network, i.e., the sensing devices, with additional external
computational resources. The computation is referred to as fog computing,
multi-access edge computing (MEC), and cloud computing, see Figure 5.3.8.1-1,
when sensor devices and gateways, gateways and edge cloud, and edge cloud and
data centre resources are involved, respectively. A more local approach, e.g.,
fog computing or multi-access edge computing, is preferred over a more
centralised approach in order to keep sensitive data in a fabrication site and
keep the automated process independent of an internet connection.
{width="6.726388888888889in" height="3.2618055555555556in"}
Figure 5.3.8.1-1: High level component view of a scalable massive sensor
network
> NOTE: It may comprise a set of heterogeneous measurement-units, wirelessly
> connected to gateways, which in turn are connected to a computing
> infrastructure such as a micro data centre (µDC). Other setups that contain
> grouped sensor devices are possible and may assist in reducing load on
> central instances.
Sensor networks facilitate the complex task of monitoring an industrial
environment to detect malfunctioning and broken elements in the surrounding
environment. An appropriate detection approach along with a classification of
the anomaly can help choosing a countermeasure or proper action to take in
case of predictive maintenance. Such actions can aid in improving the safety
by automatically triggering a machine's emergency stop in case of the
detection of a critical problem. At the same time, production efficiency can
be increased as machines can continue running in case the detected problem is
not safety relevant and only disrupts service of some elements.
Measuring the environment and propagating events may be realised in different
scenarios. In the simplest scenario, which is the least scalable, the sensors
propagate each newly measured value without any pre-processing, i.e., in a
solely proactive scenario. A more advanced approach is to solely react to the
environmental changes to reduce traffic, e.g., by only propagating events
under certain circumstances whenever a value exceeds a certain threshold [33].
In such a case, each sensor keeps measuring data but only propagates it
whenever it detects a relevant change in the environment. Depending on the
hardware, the sensing device may also be able to pause any active handling
(i.e., polling data from sensor) as long as the given threshold is not
exceeded, e.g., by receiving an interrupt when the sensor itself measures a
sudden significant change of the environment. Usually, active handling in such
a case is triggered by a call back from the sensor or a (remote) control unit.
This optimisation enhances the sensing devices durability by reducing its
power consumption. The power consumption reduction is an important task in
WSNs since sensors are often just equipped with a battery. Thus, the power
consumption reduction has gained a significant momentum in the WSN research.
Moreover, a lot of effort is put on specifying new messaging protocols to
reduce overhead of messaging protocols while still maintaining a high
reliability and low latency [34]. Additionally, the reduction of message
generation, e.g., by analysing measurements in local groups has thoroughly
been investigated.
The traffic patterns generated by the sensor network vary with the type of
measurement and the aforementioned setup. Traffic patterns may arise in the
form of self-similar and/or periodic patterns, i.e., the latter is usually the
case in proactive setups. Moreover, low-bandwidth and high-bandwidth streams
might be transmitted. Depending on the computational resources of the
gateway(s), some pre-processing of the sensor data may reduce the network
load, and with that, the uplink requirements.
{width="4.36875in" height="3.0236111111111112in"}
Figure 5.3.8.1-2: Sensor devices in star topology are connected to a local
gateway (i.e., small cell), which provides connectivity to a base station
Figure 5.3.8.1-2 depicts a massive sensor network deployment. A number of
sensor devices are connected to a local gateway (small cell) which connects to
a base station at the edge to the cloud network. Hence, the local gateway
aggregates and forwards monitoring data. Also, while aggregating, the local
gateway may pre-process the incoming data to reduce traffic load to the cloud
and computational resource requirements on the cloud. A local gateway needs to
dynamically handle the attachment requests and detachment events of sensor
devices without disruption of the monitoring service.
{width="4.452083333333333in" height="3.0833333333333335in"}
Figure 5.3.8.1-3: Sensor devices in a mesh topology realising multi-hop
connectivity to gateway (via edge devices)
Another topology for a massive sensor network is shown in Figure 5.3.8.1-3.
Here, a set of sensor nodes is directly interconnected as a mesh, where one
sensor nodes provides an uplink to the serving gateway (small cell). This
reduces the number of locally deployed cells. In such a topology the sensor
nodes may communicate just locally to reduce the load on more central
instances such as gateways and cloud resources.
In both topologies, star and mesh, a sensor node needs to perform a proper
bootstrapping to connect to the network. It needs to be able to attach itself
to the network automatically by attaching itself to a local cell or
neighbouring mesh devices. Additionally, time synchronisation of sensor nodes,
base stations, and gateways can enhance and ease monitoring. Time
synchronisation in a massive sensor network may be realised in local groups,
using the gateways or base stations. However, the bootstrapping of sensor
nodes and the time synchronisation is out of the scope of this document.
Table 5.3.8.1-1: Typical monitoring service requirements
+-------+-------+-------+-------+-------+-------+-------+-------+-------+ | ** | ** | ** | * | **Com |** C | **Ne | * |** Com | | Scena | End-t | Prior | _Data | munic | onnec | twork |_ Node | munic | | rio**| o-end | ity** | U | ation | tions | sca | dens | ation | | | la | | pdate | se | per | labil | ity**| range | | | tency | | T | rvice | gate | ity** | | per | | | (note | | ime**| avai | way** | | | n | | | 1)**| | | labil | | | | ode** | | | | | | ity** | | | | | +=======+=======+=======+=======+=======+=======+=======+=======+=======+ | Cond | 5 ms | Hi | Up to | > | 10 to | > | 0,05 | \ | 10 to | > | 0,05 | \ | 10 to | > | 0,05 | \ \- Wireless devices are attached to local gateways
>
> \- Local gateways are connected to Cloud/MEC (via base station/wired)
#### 5.3.8.3 Service flows
> 1\. Sensing devices continuously send current sensor status to centralised
> computing instance for learning of the environment
>
> 2\. After completion of learning, devices send
>
> \- less data to centralised instance. A reduced data set is either pre-
> processed data or raw data which sent less frequently;
>
> \- data to neighbouring sensor devices or local gateway for group/mesh
> processing of environmental state. Further processing may also be realised
> on MEC/Cloud.
>
> 3\. An anomaly in the measurements, e.g., a rapidly rising temperature or an
> unusual scraping sound, is detected on either of
>
> \- Fog;
>
> \- MEC;
>
> \- Cloud.
>
> 4\. Detected event is propagated to factory's controlling instance.
>
> 5\. Action is taken by controlling instance.
#### 5.3.8.4 Post-conditions
The proper action was executed that is optimum for safety and productivity (no
operation/ machine stop/ warning) for given anomaly.
#### 5.3.8.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- large number of UEs per radio cell;
>
> \- high aggregate user experienced data rate;
>
> \- local groups need to be formed; mesh topologies need to be realised;
>
> \- low-latency requirements combined with high reliability;
>
> \- automated attachment of UEs without services disruption for connected
> UEs;
>
> \- interfaces to allow programmability of gateways;
>
> \- packet prioritisation techniques to meet constraints for critical
> messages.
#### 5.3.8.6 Potential requirements
+----------------+----------------+----------------+----------------+ | **Reference |** Requirement | **Application |** Comment**| | number** | text**| / transport** | | +================+================+================+================+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.1_ | \"bursty\" and | | | | | possibly | | | | | internet-like | | | | | self-similar | | | | | traffic | | | | | patterns from | | | | | a massive set | | | | | of devices | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.2_ | high-bandwidth | | | | | streams from a | | | | | massive set of | | | | | devices with a | | | | | user | | | | | experienced | | | | | data rate of | | | | | up to 100 | | | | | Mbit/s | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | In order to | | the Future | shall support | | build local | | 8.3_ | user equipment | | sensor groups | | | (UE) mesh | | | | | networks with | | | | | multi-hop | | | | | functionality | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.4_ | the | | | | | combination of | | | | | the | | | | | requirements | | | | | _Factories of | | | | | the Future | | | | | 8.1_ and | | | | | _Factories of | | | | | the Future | | | | | 8.2_. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 8.5_ | gateways with | | | | | additional | | | | | p | | | | | rogrammability | | | | | to support | | | | | multi-access | | | | | edge computing | | | | | (MEC) | | | | | functionality | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.6_ | automatic | | | | | attachment | | | | | ( | | | | | authentication | | | | | and | | | | | association) | | | | | of previously | | | | | unattached UE | | | | | devices whilst | | | | | providing | | | | | service | | | | | continuity for | | | | | other UE | | | | | devices in the | | | | | network | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | A/T | | | the Future | shall optimise | | | | 8.7_ | the energy | | | | | consumption | | | | | per bit sent | | | | | on a UE device | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.8_ | prioritisation | | | | | of critical | | | | | messages | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.9_ | a maximum | | | | | end-to-end | | | | | latency of 10 | | | | | ms for | | | | | critical | | | | | messages | | | | | (i.e., message | | | | | from source to | | | | | final | | | | | destination, | | | | | possibly | | | | | multi-hop) | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.10_ | a very high | | | | | communication | | | | | service | | | | | availability | | | | | (> 99,9999%) | | | | | for critical | | | | | messages | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 8.11_ | a very high | | | | | connection | | | | | density of up | | | | | to 10^6^ | | | | | connections | | | | | per km^2^ | | | | | (i.e., 1 per | | | | | m^2^). | | | | | | | | | | NOTE: | | | | | Connection | | | | | density is the | | | | | total number | | | | | of connected | | | | | and/or | | | | | accessible | | | | | devices per | | | | | unit area (per | | | | | km^2^). | | | | | Normally, all | | | | | connected | | | | | devices are | | | | | not sending or | | | | | receiving | | | | | messages at | | | | | the same time. | | | +----------------+----------------+----------------+----------------+
### 5.3.9 Remote access and maintenance
#### 5.3.9 _._ 1 Description
Remote Access and Maintenance is a key motivation for looking beyond
conventionally wired device networks in automation. Typical industrial
networks are isolated from the internet and often based on very specific
protocols. In such industrial networks (peer-to-peer communication links
between just two devices, fieldbus with multiple devices and controllers, LAN
or WLANs), a remote access is often possible already today, but requires
gateway functionality at any transition of the automation pyramid between bus
systems, as shown in Figure 5.3.9.1-1. Mapping of data formats, addresses,
coding, units, and status is required for a remote access to device data going
down the automation pyramid, which comes along with a huge engineering effort.
This data mapping implemented in the gateway(s) is quite static and is not
suitable for a flexible access on event-based conditions rather than a
permanent connection and data exchange, e.g., reading the device revision in
case of a diagnostic event.
{width="6.702083333333333in" height="3.4166666666666665in"}
Figure 5.3.9.1-1: Remote access to field devices in existing systems through
controllers\ via gateways at each transition point of the automation pyramid
Remote Access and Maintenance scenarios apply to devices which
> \- already have a \"cyclic\" connection through a communication service for
> transmitting data regularly. This ad-hoc communication might be requested by
> the same end device and just runs in parallel from time to time;
>
> \- act almost autonomously, i.e., they have local computing power to run
> algorithms like measurement and data analytics, but they do not have a
> regular, cyclic connected communication service;
>
> \- have local personnel to interact with, such as a machine or even a car. A
> remote connection could be requested during a service case or if the local
> operator needs remote assistance;
>
> \- sleep most of the time and which should be woken up by establishing a
> connection via a dedicated wireless network.
A special sub-set of remote accesses is when the partner is a mobile device
(geographically) near the device instead of a service far away from the
device. The device might not differentiate if it is accessed remotely or
\"locally\" via the 5G network. Use cases described here would be the same
from the device's point of view. A remote user would expect to parse a list
with devices and have them organised in trees. A local operator has the
intention to \"talk with the device in front of her/him\" instead of walking
through a list of 10 000 devices installed at this plant. There should be
means to automatically discover which 5G devices belonging to a certain group
(e.g., all 5G devices installed in a process plant) are in the immediate
vicinity of a local operator and to provide the user with a list of all those
5G devices.
Another use case is the inventory of devices and periodic readouts of
configuration data, event logs, revision data, and predictive maintenance
information. This is often called \"asset management\", and tools for
collecting and displaying data from many connected devices are called \"asset
monitors\". Such a system may work autonomously (set of configured periodical
checks) or it is interacting with a user (\"show me status of this device\").
A remote diagnostic system might be connected to many or all devices of a
certain plant or location. A remote diagnostic system might be connected to
many or all devices the operator is responsible for. A remote diagnostic
system could also run as a device vendor's service to maintain all devices
independent of the device owner (plant, location).
There exist many protocols for reading and writing data to smart devices. Some
of these protocols are standardised and many others proprietary. Assuming an
IP-capability of a 5G system, it should be possible to either use
> \- standardised IP-based protocols (such as OPC UA®, ModbusTCP®, HARTip®);
>
> \- pack non-IP-based protocols into IP-frames in a proprietary way. This can
> be inside standardised protocols such as http (port 80) or proprietary IP-
> based protocols (device specific ports);
>
> \- proprietary IP-based protocols (device specific ports).
Communicated data could be of any kind from single bytes, longer telegrams of
a few kilobytes to continuous streams. Volume applications are expected with
power and memory limited devices. The full range from power-optimised
applications on the one end to high performance real-time data on the other
end should be covered.
Remote access to a device may happen at any time (in case the user does
authorise). So the remote access shall be non-reactive to other communication
in the 5G network and operation and performance to other devices. Remote
access should have impact only on the contacted device. Prioritised traffic
mechanisms may solve the problem to not violate configured real time
conditions when upgrading a firmware of a device in the same network.
The trigger to remotely access a device may come from the device itself, based
on a certain event or condition. In this case, a device initiates a connection
to another (known) device and submits data which alerts a service to read /
write specific data from / to that device.
A major concern associated with remote access and maintenance is the potential
vulnerability of devices in terms of cyber security. Most classical wired
communication protocols in industry do not consider any cyber security
relevant scenarios. Physical access to devices and networks is almost
restricted to skilled and authorised personnel. Furthermore, protocols are not
widely used outside these specific industries and so the knowledge is kept to
a limited community of specialists.
Users might want to block a device for any remote access, others might
restrict access to only read data or just parts of data, such as operating
hour meter, but no configuration data. Some restriction levels might be
specific to the device and as such not standardised. Those restrictions should
be fully transparent to the lower communication layers (5G in this case). A
basic set of restriction levels are expected from 5G, what could be an
adoption of existing mobile communication standards,
Electronic industrial devices do have a typical life cycle between 5 and 25
years. Customers expect old devices to remain accessible during this time with
(at least) the same functionality that was provided when they were installed.
Installations in process industry and factory automation are continuously
extended and changed, so new devices are operated in parallel to old ones.
The operation for this use case can be in a wide service area, and interaction
with the public network (e.g., service continuity, roaming) may be required.
#### 5.3.9.2 Preconditions
> \- Device is connected to the 5G network;
>
> \- service tool (asset management system or asset monitor) is connected to
> the 5G network via a gateway
#### 5.3.9.3 Service flows
An example service flow may look as follows:
> 1) device detects a condition that \"call for maintenance\" is required and
> sends a message to a predefined address, i.e., that of the asset management
> tool OR an asset management tool opens a connection to the device;
>
> 2) the tool checks device type and identity (starting with 5G information
> elements, going further down to device, customer, location specific
> information);
>
> 3) the tool reads / writes data from / to the device. This could be a few
> bytes, new firmware, or read real-time monitoring data. This is done for a
> limited time; and
>
> 4) the tool closes the connection to the device.
#### 5.3.9.4 Post-conditions
The maintenance case has been successfully completed.
#### 5.3.9.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- \"Un-configured\", sporadic network load, i.e., remote access injects
> traffic at a time not known during network setup.
>
> \- Sporadic network traffic with a particular device must not disturb
> configured and already running communication
>
> \- Low power operation of devices, i.e., long time periods without any
> communication but always ready to receive a request (or to open a session,
> link etc.)
>
> \- Compatibility for > 25 years. Compatible means here that a device can be
> used in a 5G network for 25 years or more with the same or more
> functionality as during its initial setup (installation).
#### 5.3.9.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 9.1_ Spontaneous connection to a device in a 5G
network initiated by a remote service shall be supported. T  
_Factories of the Future 9.2_ Spontaneous connection to a service connected to
a 5G network initiated by a 5G device shall be supported. T  
_Factories of the Future 9.3_ Spontaneous connection and associated data
traffic must not disturb other communication running through the 5G network T  
_Factories of the Future 9.4_ The 5G system shall be able to classify data as
RT (real-time) and non-RT. T RT data transmission (either regular / cyclic or
sporadic/burst) might need prior configuration. _Factories of the Future 9.5_
The 5G system should support the automated discovery of 5G devices belonging
to a certain group (for example all 5G devices installed in a process plant)
in the immediate vicinity of a user (radius of about 20 m around the user) and
provide the user with a list of all detected 5G devices. A This is required in
case a person (e.g., a service technician) without prior knowledge of the
plant wants to connect to a certain device he/she is seeing (e.g., a valve).
One could think of using a label as a simple alternative, but since the device
to connect to may not be directly accessible due to pipes, etc. around it and
since it may not be possible to read a label anymore after some time due to
dirt, de-saturation, etc., the described automated discovery service would be
a nice feature for the given use case. _Factories of the Future 9.6_ A 5G
system shall support IP addressable devices. T  
_Factories of the Future 9.7_ A 5G system shall allow to use standardised and
proprietary IP-based protocols A/T Not supported standardised protocols shall
be explicitly listed. _Factories of the Future 9.8_ 5G networks shall provide
backward compatibility for > 25 years at the user equipment level. T Installed
devices shall be accessible through the 3GPP network for a long time, even
when the network standard further evolves.
### 5.3.10 Augmented reality
#### 5.3.10 _._ 1 Description
It is envisioned that in future smart factories and production facilities,
people will continue to play an important and substantial role. However, due
to the envisaged high flexibility and versatility of the Factories of the
Future, shop floor workers should be optimally supported in getting quickly
prepared for new tasks and activities and in ensuring smooth operations in an
efficient and ergonomic manner. To this end, augmented reality (AR) may play a
crucial role, for example for the following applications:
> Monitoring of processes and production flows
>
> Step-by-step instructions for specific tasks, for example in manual assembly
> workplaces
>
> Ad-hoc support from a remote expert, for example for maintenance or service
> tasks
In this respect, especially head-mounted AR devices with see-through display
are very attractive since they allow for a maximum degree of ergonomics,
flexibility and mobility and leave the hands of workers free for other tasks.
However, if such AR devices are worn for a longer period of time (e.g., one
work shift), these devices have to be lightweight and highly energy-efficient
while at the same time they should not become very warm. A very promising
approach is to offload complex (e.g., video) processing tasks to the network
(e.g., an edge cloud) and to reduce the AR head-mounted device's
functionality. This has the additional benefit that the AR application may
have easy access to different context information (e.g., information about the
environment, production machinery, the current link state, etc.) if executed
in the network. A possible processing chain for such a setup is depicted in
Figure 5.3.10.1-1.
{width="6.143055555555556in" height="2.25in"}
Figure 5.3.10.1-1: Possible processing chain for an augmented reality\ system
with offloaded tracking and rendering
Here, the AR tracking algorithm determines the current viewpoint of the AR
device and places the desired augmentations at the right positions in the
current image. One of the main challenges with such a setup is that the
displayed augmentations have to timely follow any movements of the camera in
the AR device (which may be caused by any movements of the person wearing the
AR device) since otherwise the AR user may get sick after some time and a
reasonable usage would not be possible. Therefore, also a compression of the
video stream from the AR device to the image processing server and back should
be avoided if possible in order to reduce the overall processing latency and
requirements.
Some of positioning related service requirements for this use case can be
found in [66] [67].
This use case has very stringent requirements in terms of latency and service
availability. The required service area is usually bigger than for \"motion
control\" (see Clause 5.3.2). Interaction with the public network (e.g.,
service continuity, roaming) is not required.
#### 5.3.10.2 Preconditions
A user is wearing a head-mounted AR device, which is connected to an image
processing server in a (local) edge cloud via a 5G system. Some augmentations
have already been registered in the field of view of the user and shall be
tracked and rendered by the image processing server.
#### 5.3.10.3 Service flows
A possible service flow is as follows:
> 1\. A camera integrated into the AR device permanently takes new images,
> with a frame rate ≥ 60 Hz and at least HD (1280 x 720) or Full HD (1920 x
> 1080) resolution;
>
> 2\. The AR device continuously transmits all images via the 5G system to the
> image processing server, which may run in a (local) edge cloud, for example;
>
> 3\. The image processing server determines the current field of view of the
> camera integrated into the AR device based on the received image(s);
>
> 4\. The image processing server determines the optimal placements of the
> previously registered augmentations in the current image based on the
> updated viewpoint and potentially places additional augmentations in the
> current image;
>
> 5\. The image processing server renders the augmented image and sends it
> back to the AR device via the 5G system;
>
> 6\. The AR device displays the augmented image.
#### 5.3.10.4 Post-conditions
The augmentations in the view field of the user smoothly follow any movements
of the AR device in an appropriate way, offering an excellent user experience
and preventing the user to get sick after some time.
#### 5.3.10.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- very high data rate requirements along with low latency requirements.
>
> \- the need for seamless mobility support.
#### 5.3.10.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 10.1_ The 5G system shall support the bi-directional
transmission of video streams with a frame rate ≥ 60 Hz, HD (1280 x 720) or
Full HD (1920 x 1080) resolution T  
_Factories of the Future 10.2_ The end-to-end latency between capturing a new
image and displaying The end-to-end latency between capturing a new image and
displaying the augmented image based on the newly captured image shall be
smaller than 50 ms, in order to avoid cyber-sickness. The one-way end-to-end
latency of the 5G system shall be 10 ms or less. A  
_Factories of the Future 10.3_ The communication service availability shall be
higher than 99,9% with respect to successfully delivered video frames. That
means 99,9% of all video frames shall be successfully delivered within the
given latency constraints. A  
_Factories of the Future 10.4_ The 5G system shall support seamless mobility
in such a way that a handover from one base station to another one does not
have any observable impact on the application. T  
_Factories of the Future 10.5_ The 5G system shall support the simultaneous
usage of at least 3 AR devices per base station. T  
_Factories of the Future 10.6_ The (bi-directional) video stream between the
AR device and the image processing server shall be encrypted and authenticated
by the 5G system. T  
_Factories of the future 10.7_ The 5G system shall support offloading of
complex (video) tasks from the UE to allow the AR equipped UEs' to minimise
complexity and power consumption.  
_Factories of the future 10.8_ The 5G system shall support an indoor
positioning service with horizontal positioning accuracy better than 1 m, 99%
availability, heading \ \- stringent requirements on latency, communication service availability,
> and determinism (small jitter);
>
> \- potentially high density of UEs in future.
#### 5.3.11.6 Potential requirements
* * *
**Reference\** Requirement text**** Application / transport****
Comment**Number**
* * *
_Factories of the Future 11.1_ The 5G system shall support strictly
deterministic cyclic traffic with cycle times down to 10 ms and a maximum
jitter of less than 10% of the cycle time. T
_Factories of the Future 11.2_ The 5G system shall support communication
service availability exceeding at least 99,9999%, ideally even 99,999999%. T
* * *
### 5.3.12 Process automation -- process monitoring
#### 5.3.12 _._ 1 Description
Several sensors are installed in the plant to give insight into process or
environmental conditions or inventory of material. The data are transported to
displays for observation and/or to databases for registration and trending.
The operation for this use case can be in a wide service area, and interaction
with the public network (e.g., service continuity, roaming) may be required.
#### 5.3.12.2 Preconditions
Multiple sensors and observation points are distributed over the plant. All of
them are connected to the 5G system.
#### 5.3.12.3 Service flows
The sensors measure in defined time intervals and send the measurement data to
storage. Intermediate data logging within the sensor and non-cyclic data
transmission is sometimes used in order to reduce power consumption.
#### 5.3.12.4 Post-conditions
Measurement data are available at the places where needed and can be
processed.
#### 5.3.12.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- potentially harsh propagation environments with many metallic parts
> (pipes, tanks, supports);
>
> \- potentially high density of UEs;
>
> \- potentially large communication distance over multiple kilometres;
>
> \- high energy-efficiency required in case of battery-driven sensors.
#### 5.3.12.6 Potential requirements
* * *
**Reference\** Requirement text**** Application / transport****
Comment**Number**
* * *
_Factories of the Future 12.1_ The 5G system shall support a communication
service availability of about 99,99% with a data transmission in intervals
between 50 ms up to several seconds. T
_Factories of the Future 12.2_ The 5G system shall support a very high user
equipment density with up to 10 000 UEs per km². T
* * *
### 5.3.13 Process automation -- plant asset management
#### 5.3.13 _._ 1 Description
To keep a plant running, it is essential that the assets, such as pumps,
valves, heaters, instruments, etc., are maintained. Timely recognition of any
degradation and continuous self-diagnosis of components are used to support
and plan maintenance. Remote software updates enhance and adapt the components
to changing conditions and advances in technology.
The positioning requirements for this use case are mianly based on the typical
scenarios where IoT devices (e.g., sensors) are giving insight into process or
environmental conditions or inventory of material, asset management or
maintenance.
The operation for this use case can be in a wide service area, and interaction
with the public network (e.g., service continuity, roaming) may be required.
#### 5.3.13.2 Preconditions
Smart assets including self-diagnosis and sensors providing relevant data for
asset condition are distributed over the plant. All nodes are connected to the
5G system.
#### 5.3.13.3 Service flows
Data from smart assets and sensors are transmitted to storage within a defined
time interval. In the case of an actual failure, an event is transmitted
immediately.
In the case of a software update of smart devices, block data are transferred
to the devices. Multiple devices may be updated at the same time.
#### 5.3.13.4 Post-conditions
Data and event information are available where needed for processing and
displaying. Assets are maintained in an optimal manner.
#### 5.3.13.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- potentially harsh propagation environments with many metallic parts
> (pipes, tanks, supports);
>
> \- potentially high density of UEs;
>
> \- potentially large communication distance over multiple kilometres;
>
> \- high energy-efficiency required in case of battery-driven sensors.
#### 5.3.13.6 Potential requirements
* * *
**Reference\** Requirement text**** Application / transport****
Comment**number**
* * *
_Factories of the Future 13.1_ The 5G system shall support a communication
service availability of about 99.99% with a data transmission in intervals in
the order of several seconds. T
_Factories of the Future 13.2_ The 5G system shall support a very high user
equipment density with up to 10 000 UEs per km². T
_Factories of the Future 13.3_ The 5G system shall support an indoor
positioning service of assets in production environment with horizontal
positioning accuracy better than 1 m, 90% availability, latency of less than 2
s for positioning estimation for a moving UE at the speed up to 30 km/h.
* * *
### 5.3.14 Connectivity for the factory floor
#### 5.3.14.1 Description
A factory floor has adopted 5G networks for wireless automation, where a
variety of sensors, devices, machines, robots, actuators, and terminals are
communicating to coordinate and share data. Some of these devices may be
directly connected to a local type-b network and some may be connected via
gateway(s).
An example of the deployment scenario is in Figure 5.3.14.1-1.
Figure 5.3.14.1-1: Factory of the future using a dedicated local type-b
network for industrial automation
#### 5.3.14.2 Pre-conditions
An industrial factory has been provisioned with a dedicated RAN based on local
dedicated cells and a local dedicated core network.
Factory floor equipment like sensors, controllers, operator terminals, and
actuators have been provisioned with 5G connectivity modules. These modules
have subscription information to access the local network.
Operators, technicians, and engineers have 5G enabled devices. These devices
have subscription information to access the local network. These devices may
also have subscription information for other networks.
#### 5.3.14.3 Service flows
Factory equipment and human operators and technicians have sufficient
connectivity and credentials to connect to the local network, authenticating
with the local core network. Typical closed-loop control applications run over
this network with extremely low latency and high reliability. Due to the
dedicated nature of the network there is also high availability and
consistency of latency and throughput.
In the case of a device which does not have subscription information for the
network, the local core network will reject the attempt resulting in the local
RAN refusing access to the device.
Technicians can access the network on site and ensure high availability due to
pre-emptive maintenance for the local network. Network optimisation can also
be performed with a higher level of aptitude due to tighter integration with
the process control. In the case of catastrophic failure, technicians can
repair the network on-site.
Devices can be on-boarded directly by the factory owner.
#### 5.3.14.4 Post-conditions
Typical closed-loop control applications operate with consistent and
appropriate performance.
#### 5.3.14.5 Challenges to the 5G system
> \- Integration and connectivity with factory LANs, in particular real-time
> Ethernet.
>
> \- Support of local non-public deployment with KPIs that meet specific
> industrial requirements.
>
> \- Providing isolation between machines involved in specific production
> processes and other parts of a factory network. For example, in Figure
> 5.3.14.1-1, there could be some firewall functions in the dedicated local
> core to allow for isolation.
#### 5.3.14.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 14.1_ The 5G system shall support type-b network
deployments, e.g., within a factory or plant. A  
_Factories of the Future 14.2_ The 5G system shall support isolation of type-b
network with public network e.g., within a factory or plant. A  
_Factories of the Future 14.3_ The 5G system shall support interworking with
wired devices and legacy end-user equipment (e.g., devices supporting
Ethernet). A
### 5.3.15 Inbound logistics for manufacturing
#### 5.3.15.1 Description
In this example, a supply-chain company moves products between multiple
different independent manufacturing, distribution, and retail centres. The
container traverses the networks of various port operators, liner ships,
trucking companies, and warehouses. The heavy good vehicles (HGVs) operated by
the supply-chain company are wirelessly connected to the PLMN to enable real
time tracking and telematics. In these cases, the use case often changes when
the actor is in range of one or another network. What these use cases have in
common is that the actor needs to be able to connect to the correct network at
any given time to operate correctly. The pallets carrying the materials for
delivery have also been connected via 5G UEs or IoT devices for tracking and
inventory control purposes.
Some of positioning related service requirements for this use case can be
found in [68].
This use case describes the scenario where a HGV arrives at the receiving area
of a factory and delivers a pallet of materials which is subsequently
incorporated into the local factory inventory management system in an
automated manner.
#### 5.3.15.2 Pre-conditions
A factory has been provisioned with a type-b local 5G network. A supply-chain
company which delivers palletised goods supplies this factory (amongst
others).
The distribution vehicle has subscription credentials for the public macro
network.
The pallet(s) for delivery to the factory have subscription credentials for
the public macro network and either (a) subscription credentials for the
factory local type-b networks or (b) facility to obtain and use subscription
credentials for the factory local type-b network.
The supply-chain company tracks the pallets and vehicles via MNO network when
in transit.
#### 5.3.15.3 Service Flows
PLMN as each item has a separate subscription. This connection is used to
report the HGVs location and telematics, and the location of the pallets.
The HGV arrives at the industrial factory. The HGV remains connected to the
macro network but the pallet is expected to connect to the local type-b
network to track its arrival and integrate with the stock control systems of
the factory. The pallet detects the presence of the local network of the
destination factory, obtains the credentials to attach to the network (if not
already provisioned), and connects to the local type-b network. For example,
the home operator may remotely provision the credentials via a secure
mechanism, or the local type-b network may provision the credentials via a
secure mechanism.
No other pallets on the HGV (which are destined for different locations)
attempt to attach to the local type-b network.
The pallet identifies itself to the factory's stock control system and is now
tracked by the factory processes.
#### 5.3.15.4 Post-conditions
The HGV leaves the industrial premises whilst still connected to the macro MNO
network and the pallet remains, connected to the type-b network of the
factory.
#### 5.3.15.5 Challenges to the 5G system
##### 5.3.15.5.1 Identification of type-b networks and method of connection
As the use cases for local type-b networks are required to be scalable for
massive numbers of deployments, including by non-traditional MNO actors,
identification of the local type-b network need to avoid bottleneck into PLMN
ID allocation ( the PLMN IDs with limited number space might quickly exhausted
in those countries with high uptake of local type-b networks).
In this scenario, when the 5G or IoT device attached to the pallet detects the
presence of a local type-b network, there are some options on how the 5G UE or
IoT device connects to the local type-b network.
> \- Dual subscription: the 5G device has independent subscriptions for public
> and type-b 5G networks.
>
> \- Dual registration: the 5G UE or IoT Device remains registered on and
> connected to the PLMN and establishes a second registration and connection
> to the local type-b network.
>
> \- Manual PLMN selection: where the 5G UE or IoT Device performs a manual
> PLMN selection procedure (although this process may be initiated by an
> automatic procedure outside of 3GPP scope). As a consequence, the network
> should have a human readable name to enable manual selection of the type-b
> network.
#### 5.3.15.6 Potential requirements
+-----------------+-----------------+-----------------+-------------+ | **Reference |** Requirement | **Application / |** Comment**| | number** | text**| transport** | | +=================+=================+=================+=============+ | _Factories of | The 5G system | T | | | the Future | shall support | | | | 15.1_ | unique | | | | | identifiers for | | | | | type-b | | | | | networks. | | | | | | | | | | See note 1 | | | +-----------------+-----------------+-----------------+-------------+ | _Factories of | A UE shall be | T | | | the Future | able to detect | | | | 15.2_ | the identity of | | | | | a type-b | | | | | network before | | | | | attempting to | | | | | attach. | | | +-----------------+-----------------+-----------------+-------------+ | _Factories of | A UE shall be | | | | the Future | required to | | | | 15.3_ | have a | | | | | subscription | | | | | for each | | | | | particular | | | | | type-b network | | | | | from which it | | | | | can receive | | | | | communication | | | | | services. | | | +-----------------+-----------------+-----------------+-------------+ | _Factories of | The 5G system | A | | | the Future | shall support | | | | 15.4_ | devices that | | | | | can access | | | | | independently | | | | | both public 5G | | | | | network and | | | | | type-b 5G | | | | | network, | | | | | potentially at | | | | | the same time. | | | +-----------------+-----------------+-----------------+-------------+ | _Factories of | The 5G system | | | | the Future15.5_ | shall support | | | | | an indoor | | | | | positioning | | | | | service for | | | | | driving | | | | | trajectories | | | | | (if supported | | | | | by further | | | | | sensors like | | | | | camera, GNSS, | | | | | IMU) of | | | | | autonomous | | | | | driving systems | | | | | with horizontal | | | | | positioning | | | | | accuracy better | | | | | than 30 cm, | | | | | 99.9% | | | | | availability, | | | | | latency of 10 | | | | | ms for | | | | | positioning | | | | | estimation of a | | | | | moving UE at | | | | | the speed up to | | | | | 30 km/h. | | | +-----------------+-----------------+-----------------+-------------+ | _Factories of | The 5G system | | | | the Future15.6_ | shall support | | | | | an indoor | | | | | positioning | | | | | service for | | | | | storage of | | | | | goods with | | | | | horizontal | | | | | positioning | | | | | accuracy better | | | | | than 20 cm, 99% | | | | | availability, | | | | | latency of less | | | | | than 1 s for | | | | | positioning | | | | | estimation of a | | | | | moving UE at | | | | | the speed up to | | | | | 30 km/h. | | | +-----------------+-----------------+-----------------+-------------+ | Note 1: | | | | | Identification | | | | | of a type-b | | | | | network needs | | | | | to be possible, | | | | | even for a | | | | | potentially | | | | | very large | | | | | number of | | | | | type-b networks | | | | | that are needed | | | | | for numerous | | | | | enterprises. | | | | | This might | | | | | require | | | | | identification | | | | | schemes that | | | | | extend or | | | | | replace current | | | | | network | | | | | identification | | | | | based on PLMN | | | | | IDs. | | | | +-----------------+-----------------+-----------------+-------------+
### 5.3.16 Wide-area connectivity for fleet maintenance
#### 5.3.16.1 Description
A scenario where this form of deployment scenario would be applicable is in
the automatic wide-area data collection and tuning of an automotive fleet. In
the following example, a Heavy Goods Vehicle (HGV) manufacturer has an ongoing
contract with a haulage company to constantly track the performance of a fleet
of vehicles and automatically remap their Electronic Control Unit over-the-air
to ensure efficient performance based on haulage load. In this scenario, wide
area coverage is essential and the use case is highly latency tolerant.
An example of the deployment scenario is in Figure 5.3.16.1-1.
{width="1.1006944444444444in" height="0.73125in"}{width="1.1006944444444444in"
height="0.73125in"}{width="1.1006944444444444in"
height="0.73125in"}{width="1.1006944444444444in" height="0.73125in"}
#### 5.3.16.2 Pre-conditions
A HGV fleet manager has a contract for the HGV manufacturer to provide ongoing
engine control unit (ECU) updates to ensure optimal fuel efficiency.
The HGV fleet has been provisioned with connectivity modules connected to the
ECU of their engines. These modules have a subscription to a nationwide MNO
for data connectivity.
#### 5.3.16.3 Service flows
The fleet of HGVs periodically upload telematics data via the MNO-connected
connectivity modules. This upload is very delay tolerant (>30 min). The data
is routed via MNO network to the HGV manufacturer's enterprise server for
analytics and storage.
After analysis, a new ECU remapping is generated for an individual HGV and is
transmitted down to the vehicle for installation at a convenient time (e.g.,
when the vehicle is parked). This installation may also be scheduled to happen
live with an OTA connection (again, at a convenient time).
The vehicle continues periodic telematic upload throughout.
#### 5.3.16.4 Post-conditions
Data upload and download operates with use-case appropriate performance.
#### 5.3.16.5 Challenges to the 5G system
None.
#### 5.3.16.6 Potential requirements
None.
### 5.3.17 Variable message reliability
#### 5.3.17 _._ 1 Description
Many control systems operate synchronously based on a periodic control cycle.
Measurements (sent by the sensors) are usually periodic. This allows the
controller to request the actuators to perform small adjustments in order to
maintain the desired output response and a stable system. A stable system will
be operating within a desired output response during a predefined time window.
The reliability of the transmissions has to be very high: the measurements
need to be received successfully and any commands sent to the actuator must
also be received successfully, all within tight latency bounds.
There are some processes (or plants) that may already be in a stable
situation. As an example, a robotic arm which performs a repetitive task
(e.g., pick up a package), will require the arm to be positioned within a
target area A at a given time window T. As long as the arm is within the
target area A within window T, the system is considered stable. When the
system is stable, commands to adjust the position of the arm may not be
necessary. On the other hand, if the arm is outside the area at the target
time window, the controller will request the actuator to move the arm toward
the target area A, and such actions will have a high priority or urgency, and
therefore the command sent requires a higher reliability from the network.
Therefore, the information regarding system state or the \"urgency\" of the
message, or the desired reliability of the message transmission, can be used
by the network to manage resources more efficiently by scheduling resources
for each of the transmissions. Similarly, if the process or plant under
control is stable, the data may not be as urgent, and the network can give
fewer resources to the sensor device to send the measurements to the
controller or to a command to keep the actuator at the same position. That
means that the desired reliability for the message can vary dynamically, and
if this information can be provided from the controller to the network, the
network can use that information to optimise resource allocation.
The alternative to adjusting the reliability dynamically is provisioning the
system for the highest possible reliability required, which would require more
resources and therefore reduce the network efficiency.
#### 5.3.17.2 Preconditions
A robotic arm performs a repetitive task to pick up a package.
The system is considered stable if the robot's arm is positioned within a
target area A at a given time window T.
The control system operates synchronously, based on a periodic control cycle.
#### 5.3.17.3 Service flows
The robot's sensors report measurements to the application function
(controller) in an application server outside the 3GPP domain.
The application function recognises that the system is stable and the robot's
arm is within the target area A within time window T.
The application function sends a command to the actuator in the next cycle. As
the system is stable the application function informs the 3GPP network that
the message reliability can be relaxed.
#### 5.3.17.4 Post-conditions
The message is sent with lower reliability.
#### 5.3.17.5 Challenges to the 5G system
New interface definition needed between 3GPP network and application function.
#### 5.3.17.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 17.1_ The 3GPP system shall provide means for an
application function outside the 3GPP domain to request specific reliability
for each (set of) message(s) transmitted  
_Factories of the Future 17.2_ The 3GPP system shall provide means for 3^rd^
party application servers to provide information about the required
transmission QoS desired for different granularity of data transmitted by the
3^rd^ party application server. The data granularity can be each packet or a
set of packets. This information may be used by the 3GPP system to optimise
resource usage.
### 5.3.18 Flexible, modular assembly area
#### 5.3 _._ 18 _._ 1 Description
In the Factories of the Future, static sequential production systems will
increasingly be replaced by novel, modular production systems offering high
production flexibility and versatility. The concept of modular production
systems encompasses a large number of increasingly mobile production assets,
for which powerful wireless communication and localisation services are
required. An example quantity structure for car manufacturing is provided in
Annex E.
NOTE: The communication streams in this use case will usually have to coexist
with those of other applications, e.g., massive wireless sensor networks
(Clause 5.3.8) and remote access and maintenance (Clause 5.3.9).
#### 5.3 _._ 18 _._ 2 Preconditions
Life cycle chain status: a modular production environment adaptable to the
different variants of product orders is up and running. Modular assembly
stations can be added or moved inside the production area.
Communication infrastructure: a non-public, local network inside the
manufacturing hall is installed, and the deployment is adapted to the flexible
production layout. The local, type-b network or type-a network is realised by
a 5G system.
Mobile assets that are in a ready state, i.e., they are ready for factory
production. These mobile assets typically include
\- lift and lowering AGV (moveable assembly platforms);
\- convey and lift AGV;
\- mobile robots with video support for unstructured goods in stock;
\- root trains;
\- worker assistance support by aid of video up and download;
\- portable assembly tools (power screwdriver, riveting tools, staple guns
...);
\- portal cranes.
#### 5.3 _._ 18 _._ 3 Service flows
Communication flows in the assembly area:
\- vertical: ERP to and from MES (order and resource management);
\- vertical flows to and from MES: centralised orchestration of assets,
material and quality status;
\- MES (decentralised production): in many cases, communication will be
initiated by the assembled part itself. In this document this kind of
assembled part is called MPSC (manufactured product as a smart client). This
MPSC is often moved by an AGV. Associated bi-directional communication
patterns are
\- MPSC -- MES;
\- MPSC -- asset, worker (e.g., request for processing);
\- asset -- MES (e.g., status);
\- asset -- asset (e.g., collaborating robots);
\- asset -- material (e.g., ID, localisation);
\- worker -- MES (e.g., request for assistance);
\- worker -- asset (e.g., request for localisation);
\- worker -- material (e.g., ID, localisation);
\- ERP, MES and MPSC communication:
\- status information exchange between the MPSC, MES and ERP;
\- MES or MPSC resource request for mobile assets, workers, and tools
(identification, localisation, status, commands);
\- material request based on order management: mobile robots, AGVs, and root
trains are instructed to bring parts and material to the assembly station just
in time. Sending of status information;
\- At an assembly station:
\- workers get the task of assembling parts according instructions with real-
time video assistance;
\- mobile robots get the task to assemble parts according to instructions from
the MES or MPSC. Particular collaboration with a second robot or workers is
requested;
\- information reporting: during and after the assembly process; status and
quality information---combined with position info---are contributed.
#### 5.3 _._ 18 _._ 4 Post-conditions
The planned assembly step is finalised MPSC is moved to the next process step
& assembly station by an AGV.
#### 5.3 _._ 18 _._ 5 Challenges to the 5G system
> \- Ultra-reliable wireless communication for a variety communication
> services adhering to negotiated and guaranteed QoS parameters.
\- Communication bursts when several parallel actions occur (e.g., parallel
actions supported by real-time video assistance).
\- High density of mobile assets.
\- Changing the 5G network configuration in the production when the layout of
the assembly area is altered. The 5G network configuration change is carried
out by the production staff, supported by self optimising algorithm. In some
cases this only involves the UEs. In other cases, base stations might have to
be relocated or more base stations might have to be added to the 5G network.
\- 5G network maintenance and communication service assurance:
\- monitoring the production environment and processes related to
communication; detect potential communication bottlenecks in the production
area;
\- the diagnosis of network error, faults, underperformance, etc. contains
recommendations for what to do for fulfilling the requested QoS.
#### 5\. 3 _._ 18.6 Potential requirements
##### 5.3.18.6.1 Reconfiguration of the non-public 5G network for flexible
production
Changing the configuration should not compromise QoS guarantees of operating
communication services. But a change in the physical production layout could
imply changes in the communication infrastructure. Example: dynamic size
adaption of radio cells. This requires adding geographic information in QoS
communication service requests.
+----------------------+----------------------+----------------------+ | **Reference number** | **Requirement** | **Comment** | +======================+======================+======================+ | _Factories of the | The 5G system shall | The geographic | | Future 18.1_ | be able to expose | information can, for | | | information to | instance, be the | | | authorised users | geographic location | | | about allocated and | of the type-b | | | free network service | network\'s base | | | resources in a | stations within a | | | type-b and a type-a | factory hall. | | | network. The | | | | exposure shall take | | | | place via an API. | | | | Said information | | | | shall, if requested, | | | | disclose the | | | | geographic | | | | distribution of said | | | | resources. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | | | Future 18.2_ | be able to deliver | | | | the response message | | | | (described in | | | | requirement 18.1) | | | | within 1s. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | The 3s include the | | Future 18.3_ | support fast | reconfiguration time | | | configura | of the entire | | | tion/reconfiguration | device. | | | of a radio access | | | | point in a type-a | | | | network or a type-b | | | | network within 3s | | | | after being power on | | | | till the radio | | | | access point is | | | | ready for service. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | This time budget | | Future 18.4_ | support the | includes initiating | | | capability to notify | and reconfiguring | | | the network status | for basic | | | change to the | communication | | | authorised user in a | services. | | | type-a network or a | | | | type-b network | The network status | | | within 1s when the | shall reflect | | | associated | changes in the | | | notification trigger | network such as: | | | event occurs in the | power down of base | | | type-a network or | station; power up of | | | the type-b network, | base station; base | | | such as hardware was | station going | | | added or removed | through a firmware | | | (after rebooting the | update. | | | new pertinent | | | | network part), | Another examples for | | | communication | changes in the | | | service disruptions. | network is that One | | | | or several | | | | (communication) | | | | services within a | | | | certain geographic | | | | area are not longer | | | | available. | | | | | | | | By aid of this | | | | information, | | | | production processes | | | | can be changed, for | | | | instance the route | | | | of AGVs in the | | | | factory. | | | | | | | | The 1s time line is | | | | stems from the | | | | desire of factory | | | | owners to use HMIs | | | | for network status | | | | monitoring, and | | | | experience shows | | | | that any longer | | | | refresh times of the | | | | status will not be | | | | accepted by the | | | | factory workers. | +----------------------+----------------------+----------------------+
##### 5.3.18.6.2 Maintaining and operating 5G communication in flexible
production scenarios
Running a highly available and reliable type-b network or type-a network for
production environments requires different maintenance than for a typical IT
office network. Prohibitive capital damage can result from the violation of
the requested communication service quality. Therefore, the maintenance of
type-b networks or type-a networks in factories requires knowledge of the
production process. This is the reason for why the type-b network or the
type-a network in production is the responsibility of the infrastructure
operator (see Clause 6.2). Management tools and rules to run and maintain a 5G
network have to be adapted to the different qualification and view of the
staff in a production environment. Furthermore, after damages with high
economical impact, analysis based on trusted, logged network monitoring
information is needed for identifying the failure origin and cause.
+----------------------+----------------------+----------------------+ | **Reference number** | **Requirement** | **Comment** | +======================+======================+======================+ | _Factories of the | The 5G system shall | This includes QoS | | Future 18.5_ | support an open | monitoring | | | interface for | | | | communication | | | | service monitoring | | | | to authorised users | | | | in a type-a network | | | | or a type-b network. | | +----------------------+----------------------+----------------------+ | _Factories of the | The update rate of | | | Future 18.6_ | service monitoring | | | | information as per | | | | requirement 18.5 | | | | shall be larger than | | | | 1 s^-1^. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | | | Future 18.7_ | be able to log the | | | | communication | | | | history in a type-a | | | | network or a type-b | | | | network. This log | | | | includes information | | | | about what parts of | | | | the SLA are not met, | | | | and expose it to the | | | | authorised type-a | | | | network users or the | | | | authorised type-b | | | | network users. The | | | | 5G system shall be | | | | able to time-stamp | | | | the events reported | | | | in the log with the | | | | network time and to | | | | relate the positions | | | | of the involved UEs | | | | and the radio access | | | | points in the type-a | | | | network or the | | | | type-b network to | | | | these events. | | +----------------------+----------------------+----------------------+ | _Factories of the | The type-a network | An example for | | Future 18.8_ | or the type-b | external clocks is | | | network shall offer | national standard | | | an interface and the | clocks (NIST, PTB | | | related | etc.) | | | functionality for | | | | synchronising UEs | | | | with external clocks | | | | through the 5G | | | | system. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | Preferred | | Future 18.9_ | be able to provide | standardised | | | the connectivity | localisation formats | | | status and | are: JT (ISO 14306) | | | geographic position | and STL | | | of all UEs and radio | ( | | | access points in a | stereo-lithography). | | | type-a or type-b | | | | network. This | A status example is: | | | includes the UEs\' | \"in sleep mode\". | | | access point | | | | connection | | | | information, i.e., | | | | what access point | | | | maintains a physical | | | | or mere logical | | | | connection with the | | | | UEs in question. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | | | Future 18.10_ | provide an API to | | | | authorised users for | | | | monitoring the | | | | resource utilisation | | | | of the network | | | | service in a type-a | | | | network or a type-b | | | | network (radio | | | | access point and the | | | | transport network | | | | (front, backhaul). | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | One influencing | | Future 18.11_ | be able to respond | parameter included | | | to communication | in the service call | | | service requests | is the description | | | that are tied to a | of the geographic | | | pre-defined | area in which the | | | geographic area in a | communication | | | type-a network or a | service is to be | | | type-b network. The | delivered. | | | 5G service shall be | | | | able to negotiate | | | | the related | | | | communication | | | | service QoS with the | | | | UE. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | The response needs | | Future 18.12_ | be able to respond | to include the | | | to a communication | service offering. | | | service request in a | The service offering | | | type-a network or a | can be void. | | | type-b network | | | | within 100 ms. | | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | | | Future 18.14_ | be able to create | | | | and store | | | | communication | | | | service monitoring | | | | logs in a secure | | | | manner for type-a | | | | networks or type-b | | | | networks. The | | | | integrity of | | | | monitoring logs | | | | shall be protected. | | +----------------------+----------------------+----------------------+
##### 5.3.18.6.3 Public networks in flexible production scenarios
The supply chain in the factory can include deliveries by external companies
that use automated HGVs or AGVs (see also Clause 5.3.15). Also, intelligent
tags can be attached to incoming and outcome goods and these tags will require
continuous services (e.g., localisation; clock; communication for low-bit-
rate, non-deterministic data upload). The majority of all communication
services will still run in the type-a network, but some would be run in the
PLMN. Here it is assumed that the MNO operates both a PLMN and the type-a
network in the factory. The type-a network could be realised as a private
slice of the PLMN network.
+----------------------+----------------------+----------------------+ | **Reference Number** | **Requirement** | **Comments** | +======================+======================+======================+ | _Factories of the | the 5G system shall | This assumes, of | | Future 18.15_ | be able to respond | course, that the | | | to an authorized | user is connected to | | | user request to | the 5G system, e.g., | | | provide real-time | via a UE and that | | | QoS monitoring and | the monitoring | | | logging data within | service request is | | | 5s, regardless of | granted. | | | whether a UE is | | | | connected to the | This scenario is | | | type-a network or a | interesting from a | | | the PLMN of the MNO | coverage point of | | | that operates the | view. Instead of | | | type-a network. | extending the | | | | coverage of the | | | | specialised network | | | | so that it also | | | | covers the outdoor | | | | areas of a factory, | | | | the PLMN can decide | | | | to provide coverage | | | | in that area via the | | | | PLMN. | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | | | Future 18.16_ | be able to limit | | | | authorised | | | | communication | | | | services to defined | | | | areas, network | | | | segments, automation | | | | devices, or | | | | applications. The | | | | related access | | | | rights can be | | | | organised by user | | | | groups. | | +----------------------+----------------------+----------------------+ | _Factories of Future | The 5G system shall | The private slice is | | 18.17_ | be able to also | part of the PLMN. | | | offer PLMN services | | | | for authorised UEs | Here, access to the | | | that are connected | private slice of the | | | to the private slice | PLMN is conditioned | | | of the PLMN that | on authorisation by | | | serves this UE. | the lists provided | | | | by the factory owner | | | | or their delegates. | | | | Such lists can | | | | include the | | | | exclusion of UEs | | | | that are | | | | black-listed in the | | | | private slice. | | | | Communication | | | | between the PLMN and | | | | the private slice | | | | takes places via a | | | | secure interface. | | | | | | | | Rationale: if, for | | | | instance, a laptop | | | | on the shop floor is | | | | connected to the | | | | private slice, | | | | parallel connections | | | | to the public 5G | | | | network through a | | | | dedicated slice can | | | | be established. | +----------------------+----------------------+----------------------+ | _Factories of the | The 5G system shall | Examples: a UE | | Future 18.18_ | be able to | leaves the coverage | | | synchronize the time | area of the type-a | | | clock of the UEs | network and attains | | | that are distributed | access and | | | across several 5G | subsequently a time | | | deployments. The 5G | signal from the | | | system shall expose | PLMN. | | | related clock | | | | interfaces to other | | | | trusted 5G | | | | deployments. | | +----------------------+----------------------+----------------------+
##### 5.3.18.6.4 Positioning requirements in flexible production scenarios
Modular production environments are adaptable to variants of production
orders. Modular assembly stations can be added or moved inside the production
area. Mobile assets include e.g.: AGV, moveable assembly platforms, portable
assembly tools (like screwdriver) and material. Some of positioning related
service requirements for this use case can be found in [69].
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future18.19_ The 5G system shall support an indoor
positioning service of autonomous vehicles (only for monitoring proposes) with
horizontal positioning accuracy better than 50 cm, 99% availability, latency
of 1 s for positioning estimation of a moving UE at a speed up to 30 km/h .  
_Factories of the Future18.20_ The 5G system shall support an indoor
positioning service for tracking of tools at the work-place location with
horizontal positioning accuracy better than 1 m (relative positioning), 99%
availability and latency of 1 s for positioning estimation of a moving UE at a
speed up to 30 km/h.
### 5.3.19 Plug and produce for field devices
#### 5.3.19 _._ 1 Description
This use case covers the realisation of plug-and-produce for intelligent field
devices that utilise 5G communication services. This use case is based on the
plug-and-produce use case described elsewhere in the literature [52].
\"Plug and produce\" addresses the automated integration and configuration of
a (new) field device into an existing production system. The plug-and-produce
use case is applicable to discrete manufacturing as well as continuous and
batch processing. The goal of plug and produce is to increase the flexibility
and adaptability of production systems and to speed up the commissioning
process of field devices by reducing manual overhead. The field device in
question may be an individual sensor or actuator, or a more complex production
unit.
One of the main concerns when allowing this kind of dynamic integration of
field devices into a production system is to ensure that the automation system
is always complying with the automation system security requirements. This
requires controls to provide access to a production system only to
authenticated, authorised field devices. For unconstrained field devices, the
plug & produce integration of field devices within a production system can be
protected through appropriate application layer security mechanisms. In those
cases, 5G security and security of the production system can be handled rather
independently. However, some field devices are constrained (for instance
battery-driven field devices), and some real-time automation applications
might run in an (edge) cloud environment that do not allow for application
layer security mechanisms. As "over the top security" (OTT) is not possible
here, the security of the production system has to rely on the security of the
5G system. Thus, as a rule of thumb, factory operators strive to offer
security already at the network layer. Also, in many cases, the factory
operator also manages the communication network, and in that role she is
interested in securing the network itself. There is thus a strong interest in
securing wired and wireless communication in factories. That is one of the
reasons why factory operators have a strong interest in type-b 5G network
deployments so that factory operators are in control on the security of the
production system and the underlying type-b 5G network.
A 3GPP-relevant feature of such field devices is that they often offer more
than one communication port (e.g., IEEE 802.11 WLAN and IEEE 802.3 Ethernet),
and that some field devices may support only some communication technologies
(e.g., only 5G or only WLAN). So, the (type-b) 5G communication system has to
be integrated in a production system using heterogeneous communication
technologies. It is also noteworthy that factory operators prefer a unified
authentication of field devices, i.e., that the same communication
authentication credential can be used with different communication
technologies and ports. After being connected and being discovered by the
production system, the field device automatically obtains the configuration
required to participate in the production process.
One of the main differences between industrial field devices and consumer
products is the longevity of the former. Typical life times of field devices
are one decade or longer. As many production systems are subject to regulatory
approvals (e.g., safety certification), changes to a running production system
have often to be avoided. This has implication for mobile-network connectivity
since the field device (UE) will typically be integrated in a larger
production system and cannot be replaced simply.
Note that security for the field device application, i.e., the automation
function, can---in principle---be realised via OTT security (as with 3G and 5G
systems). However, so doing is not feasible for the following special cases:
> \- battery-powered field devices;
>
> \- closed-loop control requiring low-latency communication (example: mobile
> control panel; see Clause 5.3.6);
>
> \- hosting of automation functions in 5G (edge) clouds.
The plug-and-produce use case is divided into 6 steps/scenarios, starting from
the physical connection of the field device until the field device is fully
integrated into the production process (see Figure 5.3.19.1-1).
{width="6.273611111111111in" height="3.6069444444444443in"}
Figure 5.3.19.1-1: Integration of field devices into a production network
(based on [45])
> 1\. Connection: the commissioning engineer establishes physical network
> connection; the field device is switched on, the wired network connection is
> plugged in, or the field device is within reach of the 5G base station.
>
> 2\. Discovery: the field device obtains an IP address and advertises its
> services in the network.
>
> 3\. Establishment of basic communication: the field device certificate is
> validated and the connection towards the production system field device
> management server is authenticated. From the vantage point of the 5G system,
> this and the next steps take place at the application layer.
>
> 4\. Capabilities assessment: the field device management server knows the
> automation functions offered by the field device and is able to configure
> the field device\'s automation functions.
>
> 5\. Configuration: the field device is configured and set up for performing
> the desired task within the production system.
>
> 6\. Integration into production: the field device contributes to the
> production process.
Plug-and-produce may be realised using either wired or wireless networking
infrastructure. The present use case considers wireless network connectivity
using a type-b 5G network for plug and produce.
NOTE 1: The communication network and the application of the production system
need to be distinguished. On the one hand, there is the communication network
providing connectivity between the individual network nodes (OSI network
layers 1-3/4, steps 1 and 2 above). The topology of this network is determined
by the wiring of the communication nodes and the wireless connectivity of the
field devices. This functionality is assumed to be provided by the wireless 5G
network. On the other hand, there is the automation application of the
production system (OSI application layers 5 to7, steps 3 to 6 above). The
communication needs of the automation application is determined by the tasks
to be performed by the production system, e.g., the collaborations required
between individual automation field devices in order to complete a
manufacturing step.
NOTE 2: The EAP framework is increasingly been used for authentication in
factory automation networks.
Classic automation systems are structured hierarchically. Exchange of data
between hierarchies or zones is only allowed between well-defined groups of
field devices (zones and conduits). A zone is a well-defined (physical)
substructure of the automation network containing devices needed for a
production step that sha­re the same security requirements (e.g.,
confidentiality level). An example for a zone is a production line.
Data/information can only be exchanged between devices belonging to different
zones by the use of dedicated conduits. A conduit defines which data is
allowed to be exchanged between two zones. This approach is an important
building block for the security concept for industrial control and automation
systems. 5G needs to provide an equivalent way to realise restricted data flow
and implement (network) access control, etc. for industrial control and
automation systems.
#### 5.3.19.2 Preconditions
##### 5.3.19.2.1 The field device
The field device possesses a set of authentication credentials. The
authentication credentials were issued by either the plant operator or the
field device manufacturer. Different types of credentials are possible, e.g.,
> X.509 credentials (e.g., for a certificate-based EAP-TLS Authentication);
>
> SIM/AKA.
The selection of credentials depends on the deployed factory automation
system, i.e., field devices that are added before or after commission of the
automation system need to use the same type of credentials. The authentication
credential chosen is recognised by the 5G network.
##### 5.3.19.2.2 The network
The factory communication networks have been set up. In the case of 5G, a
type-b 5G network has been set up. Only UEs with valid authentication
certificates according to Clause 5.3.19.2.1 are authorised to connect to this
type-b network. UEs with other types of credentials, i.e., for a PLMN or
another type-b 5G network, are barred from connecting to the type-b network in
question. The type-b 5G network identifier reveals that this is a type-b
network and not a PLMN.
The following services have been set up:
> \- network configuration service: used to provide network (e.g., DHCP)
> configuration.
>
> \- validation authority: this service provides information about the
> validity of a digital certificate (e.g., X.509) within the scope of the
> factory automation system.
>
> \- authentication server: verifies the authentication information sent by
> the clients. An authentication server may support different types of
> authentication, depending on the authentication mechanisms and credentials
> available on the field device. The Authentication server can provide
> additional information on user/client access rights.
#### 5.3.19.3 Service flows
The following tables describe the individual service flows for steps (1) and
(2) in Figure 5.3.19.1-1 (for more details on the service flows see [45]). The
other steps are out of scope for 3GPP. As a courtesy, they are summarised in
Annex D for interested readers. Here we assume that the UE is integrated in
the field device. We explicitly state \"UE\" in situations where the
differences matters.
**A. Connection - connect field device physically**
This step includes the preparation and physical installation of the field
device in the automation system. The communication solution chosen is a 5G
system. This step has no particular implications for the type-b 5G network.
**Sub-step** **Event** **Name of process/activity** **Description of
process/activity**
* * *
1 Field device mounting request Prepare field device for connection Bring the
field device---which is a UE from the 5G system vantage point---to the
location it shall be put in operation; unpack the field device; plug in power
cable into power supply (if not battery-powered). 2 Field device prepared Plug
in field device Bring the field device into the proximity of the 5G radio
access point. 3 Field device plugged Turn on field device Switch power button
on.
NOTE: The field device may have multiple, different communication ports, both
wireless and wired.
**B. Discovery - discover field device**
This step includes the establishment of a network connection to the type-b 5G
network.
**Sub-step** **Event** **Name of process/activity** **Description of
process/activity**
* * *
1 Field device switched on Recognise newly connected field device Field device
detects connectivity at lower network layer. Here: type-b 5G connectivity at
OSI layer three or two. The UE compares the network ID with what is stored in
the field device. If PLMNs are accessible the field device does not join them
since their network ID does not match the stored value. 2 Field device
recognised Assign network address Network access authentication: mutual
authentication of field device and type-b 5G network using available,
persistent credentials for type-b network access. 3 Field device is
addressable via the network Notify field device management Announce the field
device to the field device management server to start configuring it. This
can, for instance be done by the field device itself.
#### 5.3.19.4 Post-conditions
The intelligent field device is able to communicate in the 5G network. The
communication might be constrained according to the needs of the actual
production scenario (e.g., QoS settings; limited subset of communication
partners [automation system zone]).
#### 5.3.19.5 Challenges to the 5G system
> Network access authentication for field devices in type-b networks with
> credentials (e.g., device certificate for EAP-TLS).
>
> Backward compatibility of future Releases to the EAP framework.
>
> Support of different types of authentication credentials depending on the
> credentials available in a specific industrial site/application.
>
> Allow automation system functions (e.g., MES/SCADA) to control network
> access properties in type-b networks, e.g., limiting the range of network
> addresses accessible to a plug-and-produce field device.
NOTE: Field device communication is typically not an all-to-all or many-to-
many communication scenario. Wireless field networks need to support the
enforcement of predefined communication paths based on engineering and
resource planning information (\"virtual zones and conduits\").
> Integration of 5G communication links within a local automation network
> zone, i.e., enforcement of network isolation for type-b networks.
>
> Barring of authentication of UEs in PLMNs in case the UE uses type-b network
> credentials.
>
> Barring of authentication of UEs in type-b networks in case the UE uses PLMN
> network credentials.
#### 5.3.19.6 Potential requirements
+----------------+----------------+----------------+----------------+ | **Reference\ |** Requirement | **Application |** Comment**| | number** | text**| / transport** | | +================+================+================+================+ | _Factories of | Network access | | | | the Future | authentication | | | | 19.1_ | credentials | | | | | for type-b 5G | | | | | networks shall | | | | | not be valid | | | | | for | | | | | authentication | | | | | in PLMN | | | | | networks. | | | +----------------+----------------+----------------+----------------+ | _Factories of | A type-b 5G | | One of the | | the Future | network shall | | motivations | | 19.2_ | be able to | | for this | | | deny network | | requirement is | | | access | | that small | | | authentication | | type-b | | | of UEs | | networks might | | | offering PLMN | | be vulnerable | | | authentication | | to dedicated | | | credentials | | \"au | | | (note 1). | | thentication\" | | | | | attacks, in | | | | | which a large | | | | | number of | | | | | (emulated) UEs | | | | | tries to sign | | | | | up to the | | | | | type-b | | | | | network, and | | | | | the entailed | | | | | signalling | | | | | traffic | | | | | impairs normal | | | | | operation of | | | | | the type-b | | | | | network. | +----------------+----------------+----------------+----------------+ | _Factories of | UEs that are | | | | the Future | only | | | | 19.3_ | subscribed to | | | | | a type-b | | | | | network shall | | | | | not attempt to | | | | | join any | | | | | public | | | | | network. | | | +----------------+----------------+----------------+----------------+ | _Factories of | UEs that are | | | | the Future | only | | | | 19.4_ | subscribed to | | | | | a public | | | | | network shall | | | | | not attempt to | | | | | join any | | | | | type-a or | | | | | type-b | | | | | network. | | | +----------------+----------------+----------------+----------------+ | _Factories of | In type-b | | | | the Future | networks, | | | | 19.5_ | backward | | | | | compatibility | | | | | is not | | | | | required for | | | | | network access | | | | | authentication | | | | | of UEs build | | | | | according to | | | | | R15 and | | | | | earlier. | | | +----------------+----------------+----------------+----------------+ | _Factories of | UEs that only | | | | the Future | provide type-b | | | | 19.6_ | network | | | | | credentials | | | | | when trying to | | | | | join a PLMN | | | | | shall be | | | | | barred from | | | | | joining said | | | | | PLMN. | | | +----------------+----------------+----------------+----------------+ | _Factories of | IDs of type-b | | | | the Future | 5G networks | | | | 19.7_ | shall readily | | | | | be | | | | | d | | | | | istinguishable | | | | | from PLMN IDs. | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | This is in | | the Future | shall support | | accordance | | 19.8_ | a suitable | | with TS | | | framework for | | 33.501, clause | | | subscriber | | 6.1.12 and | | | network access | | annex B | | | a | | [51]. | | | uthentication, | | | | | e.g., EAP | | | | | (note 2, note | | | | | 3). | | | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | T | An example for | | the Future | shall expose | | such a | | 19.9_ | an interface | | r | | | that allows | | econfiguration | | | automation | | is the control | | | functions in a | | of which UEs | | | type-b network | | of the type-b | | | to define and | | network are | | | reconfigure | | provided with | | | the properties | | layer2/layer3 | | | of offered 5G | | connectivity | | | communication | | with other UEs | | | services of | | in the type-b | | | the type-b | | network. | | | network. | | | | | | | This | | | This shall | | requirement is | | | include the | | related to the | | | possibility of | | study item on | | | enabling---via | | providing a | | | the exposed | | LAN service by | | | communication | | the 5G system. | | | service | | | | | interfaces---a | | This | | | direct layer 2 | | requirement | | | and/or layer 3 | | does not | | | communication | | stipulate a | | | service | | direct 5G | | | between UEs of | | radio link | | | the type-b 5G | | between UEs of | | | network via | | the type-b 5G | | | the type-b 5G | | network. | | | network | | | | | infrastructure | | | | | (similar to an | | | | | Ethernet | | | | | connection | | | | | today). | | | +----------------+----------------+----------------+----------------+ | NOTE 1: It may | | | | | be possible | | | | | that | | | | | credentials | | | | | for PLMN | | | | | networks of | | | | | some | | | | | subscribers | | | | | are valid also | | | | | for | | | | | authentication | | | | | in a specific | | | | | type-b 5G | | | | | network. | | | | | | | | | | NOTE 2: This | | | | | does not imply | | | | | that all other | | | | | authentication | | | | | methods need | | | | | to be | | | | | supported, | | | | | rather that | | | | | the EAP | | | | | framework is | | | | | in place. | | | | | | | | | | NOTE 3: | | | | | Backward | | | | | compatibility | | | | | of future | | | | | Releases to | | | | | the selected | | | | | framework | | | | | (e.g., EAP) | | | | | for network | | | | | access is | | | | | important for | | | | | industrial use | | | | | cases. | | | | +----------------+----------------+----------------+----------------+
### 5.3.20 Type-a network ‒ PLMN interaction
#### 5.3.20.1 Description
An enterprise deploys a 5G type-a network within its factory complex. The
network supports robotic controls for the manufacturing equipment;
communications between the equipment and the UEs of the responsible employees;
and communications between employee UEs. The manufacturing equipment is
limited to receiving service only on the type-a network. The employee UEs are
capable of being used both on the type-a network---for communicating with
other devices on that network---and on the PLMN, for communicating to other
UEs outside of the type-a network.
The robotic controls require URLLC capabilities that ensure appropriate
actions are taken by the robots. As these requirements can be quite different
from those for supporting the employee UEs, the network resources providing
URLLC capabilities may be reserved for use by the robotic controllers. This
separation of resources is needed to prevent an employee UE from using the
URLLC radio or network resources, and potentially interfering with a robotic
controller's ability to precisely control a factory robot.
UEs that do not belong to the factory are not allowed to access the type-a
network, to avoid any use of type-a network resources by non-authorised UEs.
Since the type-a network may overlap the coverage are of one or more PLMNs,
there is a risk of excessive resource usage and churn if a UE that does not
belong the factory finds a stronger signal from the type-a network and
attempts to attach to the type-a network, only to be rejected later based on
authentication or authorisation validation. This is of particular concern for
a URLLC type capability where the churn may impact the ability of a time
sensitive factory UE (e.g., robotic controller) in receiving access to the
network.
Since some UEs also need to be able to communicate over the PLMN as well as
within the type-a network, the enterprise provides employees with a selection
of UEs and service providers/MNOs for their PLMN usage. These UEs need to be
able to support simultaneous service on both the type-a network and PLMN, as
in the case where the employee is receiving communication from a piece of
manufacturing equipment regarding a processing problem and simultaneously
consulting with a remote colleague for the appropriate corrective action.
The system needs to support service continuity for UEs that cross between the
PLMN and the type-a network while actively engaged in a communication. For
example, an employee may call a colleague while driving to work. When the
employee reaches work and enters the building, the UE is handed over to the
type-a network, at which time the communication continues with no disruption
in service. The type-a network may provide better coverage within the building
as well as support additional functionality related to the factory business
that is not available in the PLMN.
#### 5.3.20.2 Pre-conditions
The factory equipment is only able to access the type-a network.
The employee UEs may access both the type-a network and a PLMN.
The employee UEs may simultaneously be active on both the type-a network and a
PLMN.
The employee UEs may handover between the type-a network and PLMN when
entering or leaving the factory complex.
Non-authorised UEs are not allowed access to the type-a network.
#### 5.3.20.3 Service Flows
A piece of factory equipment detects an error condition and reports the
problem to the human supervisor. This communication may take place in a
variety of data formats, from a short text message to streaming video.
A robotic controller responds in a timely and efficient manner to an alert
from one of the factory robots, providing corrective instructions that prevent
an accident.
The supervisor needs to consult a colleague to address the issue. The
colleague is on the way to the factory, but currently outside the range of the
type-a network, so the remote colleague is currently receiving service from
the PLMN. If dual coverage is available by both the type-a network and PLMN,
the supervisor can select which to use for the external call.
As the two colleagues are talking, the remote colleague reaches the factory
and the UE is handed over to receive service from the type-a network where
better coverage is available.
A person walking past the factory attempts to make a call using their personal
UE. The UE is only able to access the PLMN.
#### 5.3.20.4 Post-conditions
There is no disruption of service when the supervisor uses the type-a network
for communication with the factory equipment and the PLMN for communication
with the remote colleague.
There is no resource conflict between the robotic controller and the employee
UEs using the type-a network.
There is no disruption of service when the remote colleague reaches the
factory and is switched from PLMN to type-a network service.
There is no use of type-a network resources by the non-authorised UE.
#### 5.3.20.5 Potential Requirements
+----------------+----------------+----------------+----------------+ | **Reference |** Requirement | **Application |** Comment**| | number** | text**| / transport** | | +================+================+================+================+ | _Factories of | The 5G system | | An type-a | | the Future | shall support | | network may be | | 20.1_ | the deployment | | realised as | | | of type-a | | e.g., private | | | networks. | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | private slice | | | | | | | | | | This is a | | | | | deployment | | | | | requirement | | | | | rather than a | | | | | technical | | | | | requirement. | +----------------+----------------+----------------+----------------+ | _Factories of | The 5G system | | This | | the Future | shall support | | requirement | | 20.2_ | a mechanism | | may be met in | | | for a UE to | | different | | | identify a | | ways, | | | type-a | | depending on | | | network. | | how the type-b | | | | | network is | | | | | realised | | | | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | private | | | | | slice). | +----------------+----------------+----------------+----------------+ | _Factories of | The 3GPP | | This | | the Future | system shall | | requirement | | 20.3_ | support a | | may be met in | | | mechanism to | | different | | | allow a UE to | | ways, | | | select a | | depending on | | | type-a network | | how the type-a | | | it is | | network is | | | authorised to | | realised | | | access. | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | private | | | | | slice). | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall be | | This | | the Future | able to detect | | requirement | | 20.4_ | the | | may be met in | | | availability | | different | | | of an type-a | | ways, | | | network | | depending on | | | supported by a | | how the type-a | | | cell before | | network is | | | attempting to | | realised | | | access the | | (e.g., private | | | cell. | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | private | | | | | slice). | +----------------+----------------+----------------+----------------+ | _Factories of | The 3GPP | | This | | the Future | system shall | | requirement | | 20.5_ | support a | | may be met in | | | mechanism to | | different | | | prevent a UE | | ways, | | | from accessing | | depending on | | | an type-a | | how the type-a | | | network it is | | network is | | | not authorised | | realised | | | to select. | | (e.g., private | | | | | equipment, | | | | | contracted | | | | | with an MNO, | | | | | private | | | | | slice). | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall | | | | the | support | | | | Future20.6_ | multiple | | | | | simultaneously | | | | | active | | | | | subscriptions. | | | +----------------+----------------+----------------+----------------+ | _Factories of | A UE shall | | | | the Future | support a | | | | 20.7_ | mechanism to | | | | | simultaneously | | | | | receive | | | | | services using | | | | | multiple | | | | | subscriptions | | | | | and | | | | | connections to | | | | | multiple | | | | | type-a | | | | | networks | | | | | and/or PLMNs. | | | +----------------+----------------+----------------+----------------+ | _Factories of | Subject to an | | Supporting | | the Future | agreement | | intersystem | | 20.8_ | between the | | mobility | | | ope | | between a | | | rators/service | | type-a network | | | providers, | | and PLMN | | | operator | | depends on | | | policies and | | several | | | the regional | | factors e.g., | | | or national | | having the | | | regulatory | | appropriate | | | requirements, | | business | | | the 5G system | | relationship | | | shall support | | in place | | | intersystem | | between the | | | mobility | | network | | | between a | | operators. | | | type-a network | | | | | and a PLMN. | | Using common | | | | | identifiers | | | | | | | | | | Using common | | | | | authentication | +----------------+----------------+----------------+----------------+ | _Factories of | A type-a | | This | | the Future | network shall | | requirement | | 20.9_ | be able to | | allows the | | | provide | | case where the | | | service for | | type-a network | | | UEs with | | provides | | | subscriptions | | service for | | | to different | | both UE1 and | | | type-a network | | UE2 where UE1 | | | and/or PLMN | | also has a | | | operators. | | subscription | | | | | with MNO-A and | | | | | UE2 also has a | | | | | subscription | | | | | with MNO-B. | +----------------+----------------+----------------+----------------+ | _Factories of | A type-a | | | | the Future | network shall | | | | 20.10_ | be able to | | | | | operate in | | | | | either | | | | | licensed or | | | | | unlicensed | | | | | bands. | | | +----------------+----------------+----------------+----------------+
### 5.3.21 Communication monitoring, diagnosis, and error analysis
#### 5.3.21.1 Description
An industrial automation application is a complex system that encompasses many
hardware and software products of different types and from different vendors.
Industrial automation systems are locally distributed and are typically served
by wired and wireless communication networks of different types and with
different characteristics. If the production process---or one of its sub-
processes---does not work properly, there is the need to quickly find and
eliminate the related error or fault in order to avoid significant production
and thus financial losses. To that end, automation devices and applications
implement diagnosis and error-analysis algorithms as well as predictive
maintenance features.
Due to their inherent challenges, wireless communication systems are usually
under suspicion in case an error occurs in a distributed automation
application. Therefore, diagnosis and fault analysis features for 5G
communication systems are required. The 5G communication system needs to
provide sufficient monitoring information as input for such diagnosis
features.
The related communication services can be provided by a locally deployed
type-b network or type-a network or by a private slice in a PLMN.
#### 5.3.21.2 Preconditions
The 5G system is installed and commissioned, and it operates appropriately.
The distributed automation application is operating. At least one
communication between automation devices is established and at least one
communication service has been requested. One of the following events occurs:
> \- The communication network is to be commissioned after installation, and
> performance information about the communication network needs to be included
> in the commissioning documents;
>
> \- A periodic report interval for the communication network expires;
>
> \- A periodic maintenance interval for the production system expires;
>
> \- The production process does not operate as required;
>
> \- The automation application reports an error;
>
> \- The production system transits into a safe state without obvious reason;
>
> \- The automation system fails, i.e. it completely seizes to operate.
#### 5.3.21.3 Service flows
The 5G system delivers one or several of the monitoring functionalities listed
below.
> \- Providing (time-resolved) information about a communication service\'s
> error behaviour;
>
> \- Providing detailed (time-resolved) QoS and error information about a
> communication service for -consecutive transmissions within a pre-defined
> time frame;
>
> \- Diagnosis of the end-to-end communication with respect to a particular
> communication layer of a communication service or connection; for instance:
> did the error occur at OSI level 2 or level 3?;
>
> \- Diagnosis of a communication service\'s transmission path including
> information on error location of a connection (\"trace function\");
#### 5.3.21.4 Post-conditions
The automation application has received information needed for, e.g.,
> \- Assessing the dependability of network operation;
>
> \- Assessing the dependability of the communication services;
>
> \- Excluding particular communication errors;
>
> \- Identifying communication errors;
>
> \- Analysing the location of an error including the geographic location of
> the involved network component (UE; front-haul component; core node);
>
> \- Activation of application-related countermeasures.
#### 5.3.21.5 Challenges to the 5G system
Special challenges to the 3GPP system associated with this use case include
the following aspects:
> \- Support mechanism for monitoring the QoS of the service in real time;
>
> \- Providing real time QoS parameters and event notifications to an
> application;
>
> \- Effective methods for managing the entailed high number of monitoring
> data and information records;
>
> \- Appropriate statistical methods for characterising time and error
> behaviour of communication services;
>
> \- Methods for locating errors and faults with an accuracy that is
> acceptable for automation applications.
#### 5.3.21.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Factories of the Future 21.1_ The 5G system shall provide a mechanism for
monitoring QoS in real time.  
_Factories of the Future 21.2_ The 5G system shall be able to provide real
time QoS parameters and events to an authorised user.  
_Factories of the Future 21.3_ The 5G system shall be able to provide run-time
statistical information of service parameters and also of the error behaviour
of communication services. T The statistical data can, for instance, be the
standard deviation of the end-to-end latency and the average down time of the
communication service. For more information see IEC 62657-2 [62]. _Factories
of the Future 21.4_ The 5G system shall be able to provide communication
service monitoring records to authorised users per pre-defined patterns. T
Examples for such patterns are: one-time request, time period; triggered by
link error. _Factories of the Future 21.5_ The 5G system shall be able to
provide information that identifies the error code and the location of a
communication error. T  
_Factories of the Future 21.6_ The 5G system shall provide information that
characterise T This inference will be based on the (QoS) requirements for
communication service instantiations.
## 5.4 Smart Living - Health Care
### 5.4 _._ 1 Description of vertical
Smart living is one of the verticals that is focused on transforming
healthcare through mobile health delivery, personalised medicine, and social
media e-health applications. Medical data is very sensitive and private and
requires a high degree of reliability in transporting the data. There is
already a lot of work done in this area, but 5G mobile will play a significant
part in advancing this area of study. Some of the information transferred is
low data readings and if they are consistent then they can be transferred with
low priority until there is exceptional data that will generate an alarm to be
raised. The use case described here can be likened to any other use cases for
monitoring data. The uniqueness here is the sensitivity and privacy that is
required.
### 5.4.2 Telecare data traffic between home and remote monitoring centre
#### 5.4.2 _._ 1 Description
eHealth provides the capability for remote monitoring and care, this
eliminates the need for frequent visit to the doctors and it allows for
efficient management of chronic diseases for both patient and the medics. This
use case is about the automated monitoring of data and suggests that such
sensitive information should be managed in a secure way and in some cases can
allow for different level of authorisation of this information. In some cases
regular monitoring of patient data can trigger an alarm to the patient
depending on the information received or in other cases it can trigger
authorisation to some other parts of the patient information received by other
medical care bodies and finally the same information with different level of
authorisation can trigger a warming from a consultant which requires a
different but maybe higher level of authorisation.
#### 5.4.2.2 Preconditions
Rules used to categorise different authorisation levels and criticality of
information are set in place using a policy server.
#### 5.4.2.3 Service flows
Most telecare data will need to be transferred to support real-time critical
alarm situations. Alarm situations normally initiate a voice call. In such
alarm situations, link availability and reliability are major considerations.
In this use case it is assumed that the patient has a number of monitoring
device (wearables) which could also be part of or connected to a 3GPP 5G
device. In this scenario there are different types of information that are
collected and transferred via the 3GPP network to a medical centre (data
measuring and policy decision centre) which could be on a 3GPP server
belonging to a service provider or network operator.
The medical centre will determine the alarm level based on a policy and based
on this level will automatically request a call or text to be initiated to
either the patient (level1 decision maker), a medical advisor (a nurse: Level2
decision maker) and or to a consultant (level 3 decision maker) informing them
that the critical level has been reached and the necessary contingency plan
should be taken depending on the severity of the alarm.
The 3GPP system will automatically set up a dedicated line of communication in
a secure manner, depending of the level of the alarm and this will allow for
the right level of confidentiality for the information recipient warning them
they need to either take their medication or prepare to come to the hospital
for further investigation.
{width="5.464583333333334in" height="3.7020833333333334in"}
Figure 5.4.2.3 Remote Monitoring and authorisation in Tele Care Management
#### 5.4.2.4 Post-conditions
The system resets itself to the default state and measuring and monitoring of
data resumes.
#### 5.4.2.5 Challenges to the 5G system
Medical devices are often wearables. Sometimes the medical devices are even
implantable (e.g., an insulin pump). Wearable devices have a small form
factor. This implies that also the battery has to be small. A small battery
has two implications:
> the battery standby time will be limited unless power saving is used;
>
> the maximum output power is limited, which limits the uplink range.
Companies that supply medical devices generally want to be as much as possible
independent of local conditions. E.g., they prefer not to be dependent on a
specific subscription or application that the patient has on his/her mobile
phone (if the patient has a mobile phone) or not dependent on specific fixed
network subscriptions for in-home coverage. Therefore, an independent mobile
connection is generally preferred. This can be implemented with power-
efficient mobile access technology (e.g., eMTC).
In cases where a direct connection to the network is not feasible, a
transparent Relay UE may be used. This combines the advantage of the
subscription for the medical device being independent from the Relay UE with
battery and power efficient connectivity.
#### 5.4.2.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Smart Living 1.1_ The 3GPP system shall support mechanisms to differentiate
between levels of authorisation required for decision T  
_Smart Living 1.2_ The 5G system shall support battery- and power-efficient
communication for constraint IoT devices. T This requirement is already
addressed by the existing CIoT optimisations _Smart Living 1.3_ The 5G system
shall support the remote UE to access to the same services whether using
indirect communication or using direct communications. T This requirement is
already covered by the existing REAR feature as specified in 22.278 (Rel-15)
_Smart Living 1.4_ The 5G system shall be able to ensure the confidentiality
and integrity of data for/from the remote UE data in indirect communications.
T This requirement is already covered by the existing REAR feature as
specified in 22.278 (Rel-15) _Smart Living 1.5_ The 5G system shall be able to
support remote UE and relays with different subscriptions from different
PLMNs. T This requirement is **not yet** covered by the existing REAR feature
as specified in 22.278 (Rel-15)
## 5.5 Smart city
### 5.5.1 Description of vertical
The smart city vertical covers data collection and processing to more
efficiently monitor and control city resources, and to provide services to
city residents. Domains include road traffic, electric and water systems,
waste management, public safety, schools, and other services.
### 5.5.2 Remote CCTV analysis
#### 5.5.2.1 Description
A video analytics company is contracted to analyse CCTV streams for enforcing
restricted zones at various related retail locations across a country. The
local cameras implement motion detection, and on detection motion in their
field of view, they live stream to the video analytics company for analysis.
The remote analysis is based on object & facial recognition and provides
informational alerts to employees at the retail location guiding security
responses.
The purpose of using a private slice in this scenario is to ensure that the
video streams have sufficient guaranteed quality of service to remain at a
high quality (i.e. no UE rate adaptation), consistent latency & throughput to
prevent buffering, and -- very importantly -- that the video stream is routed
in such a way as to avoid network video optimisation functions which would
otherwise compress the feed, making analysis more difficult.
An example of the deployment scenario is in Figure 5.5.2.1-1.
{width="0.6097222222222223in"
height="0.4638888888888889in"}{width="0.3715277777777778in"
height="0.38055555555555554in"}
Figure 5.5.2.1-1: Public wide-area network with private slices for industrial
automation
#### 5.5.2.2 Pre-conditions
A video analytics company is contracted to analyse CCTV streams for enforcing
restricted zones at various related retail locations across a country.
#### 5.5.2.3 Service flows
A camera is positioned and configured to monitor an emergency fire escape and
door and the camera is provisioned with network credentials either prior to
installation or remotely by (a) the MNO operating the public network, (b) the
CCTV operator via a platform offered by the MNO operating the public network.
The CCTV camera is also authorised to use the \"CCTV\" private slice of the
public network.
The CCTV camera detects motion and the CCTV camera is triggered by this
activity to stream its feed to a video analytics server via the 5G MNO network
covering the location.
The CCTV establishes a connection to the 5G MNO network to us the \"CCTV\"
private slice which offers sufficient GBR for 1080p\@30f/s with low jitter &
traffic routeing without a video optimisation server. The CCTV camera sets up
a stream to the configured remote server at the video analytics company and
the 5G MNO routes the traffic appropriately with the requisite QoS.
The video analytics server performs object recognition (\"person\") and then
facial recognition (\"active security employee\") to determine that this is
not a threat and alerts the on-site operators of the retail venue of the
situation, including advice to cancel any ongoing alarms in that area.
#### 5.5.2.4 Post-conditions
The remote server terminates the stream and the CCTV camera returns to motion
detection.
#### 5.5.2.5 Challenges to the 5G system
This use reuses several of the existing features of the 5G system such as
private slices supporting scalability, minimum reserved capacity, and data
isolation.
Special challenges to the 5G system associated with this use case are the
protection of the integrity of the user data.
#### 5.5.2.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Smart Cities 1.1#_ The 5G system shall enable the network operator to protect
the integrity of user data for services provided by a private slice T
## 5.6 Electric-power distribution
### 5.6.1 Description of Vertical
#### 5.6.1.1 Overview
The energy sector is currently subject to a fundamental change, which is
caused by the evolution towards renewable energy, i.e. a very large number of
power plants based on solar and wind power. These changes lead to bi-
directional electricity flows and increasing dynamics of the power system. New
sensors and actuators are being deployed in the power system to efficiently
monitor and control the volatile conditions of the grid, requiring real-time
information exchange [46] [47].
The emerging electric-power distribution grid is also referred to as Smart
Grid. The smartness enhances insight into both the grid as a power network and
the grid as a system of systems. Enhanced insight improves controllability and
predictability, both of which drive improved operation and economic
performance and both of which are prerequisites for the sustainable and
scalable integration of renewables into the grid and the potential transition
to new grid architectures. Smart Grid benefits spread across a broad spectrum
but generally include improvements in: power reliability and quality, grid
resiliency, power usage optimisation, operational insights, renewable
integration, insight into energy usage, safety and security.
Overviews of (future) electric-power distribution can be found elsewhere in
the literature [63][64].
#### 5.6.1.2 Technical challenges of future electric-power distribution
> \- Energy generation by a huge number of decentralised local units. In
> larger markets, hundreds of thousands of devices need to be connected via
> 5G.
>
> Many businesses and private homes connected to the energy network become
> prosumers, i.e., customers and producers of energy. They may also operate
> local energy storage systems. Their power system will be monitored and
> controlled by inverters, or electric power converters, which communicate
> with other parts of the electric grid.
>
> \- Up to 100% of the energy will be produced from highly volatile, renewable
> resources, mainly solar, wind and---where available---hydro power. Solar and
> wind energy do not inject mechanical inertia into the local power grid,
> which will make keeping the frequency at a constant value much more
> difficult.
>
> \- Control of voltage and frequency in distribution and transmission grids
> are the key challenges of future energy networks. These new procedures are
> still under development, and they will require many economic, legal and ICT
> changes (apart from the modifications in the electric grid).
>
> \- Especially frequency control has tight requirements in terms of reaction
> time.
The main goals of future electric-power distribution includes―among others―the
reduction of CO~2~ emissions by relying on renewable energy sources (RES),
decentralisation of energy production, continuous matching of injected and
outgoing energy levels, resource efficiency, cost efficiency, maximum
security, and reliable provisioning of services to consumers.
These improvements are important for addressing the needs of increasingly
volatile and decentralised markets. A major enabler for all this are inter-
connected communication systems and computing infrastructure, which
interconnects control centres, substation automation units, energy storage
systems, and power plants of all sizes in a flexible, secure and consistent
manner.
Today, the vast majority of communication technologies used in the energy
sector is still wire-bound. This includes a variety of dedicated Industrial
Ethernet and power line solutions. These communication technologies are used,
for example, for interconnecting sensors, actuators, and controllers in an
electric network automation system.
Nowadays, wireless communication is primarily used for connecting smart meters
for customers of the power network. These meters only monitor the energy
consumption of the connected facility. There was no need for wireless
connectivity in the past, due to relatively static and long-lasting
installation of the power grid equipment. In addition, this was because most
existing wireless technologies fell short of the demanding requirements of
industrial applications, especially with respect to end-to-end latency,
communication service availability, jitter, and reliability.
With the advent of future Smart Grids and 5G, however, this may change
fundamentally, since only wireless connectivity can provide the degree of
flexibility, mobility, versatility, and ergonomics that is required for the
energy networks of the future. Thus, 5G may significantly contribute to
revolutionising the way how electric energy is monitored, stored, and
controlled for the entire industry sector.
{width="2.1069444444444443in" height="2.3333333333333335in"}
Figure 5.6.1.2 -- 1: Overview of different application areas in electric-power
distribution
In this respect, three different application areas can be distinguished, as
shown in Figure 5.6.1.2-1.
#### 5.6.1.3 Application areas in (future) electric-power distribution
The areas identified in Clause 5.6.1.2 can be briefly characterised as
follows:
**Primary frequency control with up to 100% RES:** The focus of this
application area is on the instant monitoring and control of the frequency in
the grid. In frequency control, the grid can be a long-distance transmission
network covering countries or large parts there-of, or short-distance
distribution networks connecting local consumers and distributed producers of
energy. Primary frequency control ensures that a swift response on frequency
variations is provided, while it may not lead to returning the measured
frequency to the nominal, exact target value (e.g., 50 Hz in Europe).
Typically, primary frequency control uses decentralised or distributed control
architectures allowing taking corrective actions swiftly on a local level.
**Secondary frequency control with up to 100% RES:** The focus of this
application area is the second, less time-critical correction of the frequency
in the grid. Again, the grid can be a long-distance transmission network
covering countries or large parts there-of, or short-distance distribution
networks connecting local consumers and distributed producers of energy.
Secondary frequency control ensures that an accurate and lasting response on
frequency variations is provided, and its goal is to return the measured
frequency to the nominal, exact target value (e.g., 50 Hz in Europe).
Typically, secondary frequency control uses centralised control architectures,
allowing frequency control units to take corrective actions across all parts
of the controlled power network.
**Distributed voltage control with up to 100% RES:** The focus of this
application area is monitoring and control of the voltage levels in
distribution networks. Sensors located close to the electric inverters in the
local grid measure the impedance on the grid and forward these values to a
voltage control unit co-located with a secondary substation automation unit.
The control unit analyses the impedance values and determines the need for
corrective actions. The correction action is a target impedance value that is
sent to the electric inverters so that additional energy can be injected into
the grid, or electric inverters may throttle the energy added by power plants
or storage systems.
Other application areas are differential protection, fault location,
isolation, and service restauration.
**A high-** level overview of the communication links is provided in Figure
5.6.1.3-1.
{width="5.63125in" height="3.321527777777778in"}
Figure 5.6.1.3‑1: Communication Links in Future Energy Networks with up to
100% RES
#### 5.6.1.4 Major challenges and particularities
Major general challenges and particularities include the following aspects:
> 1) Utility-grade quality of service is required for many applications, with
> stringent requirements in terms of end-to-end latency, communication service
> availability, jitter, and determinism.
>
> 2) There is not only a single class of use cases, but there are use cases
> with a wide variety of different requirements, resulting in the need for a
> high adaptability and scalability of the 5G system.
>
> 3) Many electric-power distribution applications have stringent requirements
> on safety, security (esp. availability, data integrity, and
> confidentiality), and privacy.
>
> 4) The 5G system shall support a seamless integration into the existing
> (primarily wire-bound) connectivity infrastructure. For example, the
> 5G-based solution shall allow to flexibly combine the 5G system with other
> (wire-bound) technologies in the same power network.
>
> 5) Most types of electrical equipment usually have a rather long lifetime,
> which may be 20 years or even longer. Therefore, long-term availability of
> 5G communication services and components is essential.
>
> 6) 5G systems shall support non-public operation by deploying type-a network
> or type-b network within a local distribution grid, which are isolated from
> PLMNs. This is required by many distribution system owners (DSO) for
> security, liability, availability and business reasons. Nevertheless, in the
> case of a type-a network is deployed standardised and flexible interfaces
> shall be supported for seamless interoperability and seamless handovers
> between 5G PLMNs and dedicated 5G systems.
>
> 7) The radio propagation environment in a building with energy generation or
> storage equipment can be quite different from the situation in other
> application areas of the 5G system. Inverters may be located in the basement
> of industrial buildings, where radio connectivity can be challenging. These
> buildings can host a large number of---often metallic---objects in the
> immediate surroundings of transmitter and receiver, as well as potentially
> high interference caused by electric machines, power transformers, and the
> like.
>
> 8) The 5G system shall be able to support continuous monitoring of the
> current network state in real-time, to take quick and automated actions in
> case of problems, and to do efficient root-cause analyses in order to avoid
> any undesired interruption of the production processes, which may incur huge
> financial damage. Particularly, if a third-party network operator is
> involved, accurate SLA monitoring is needed as the basis for possible
> liability disputes in case of SLA violations.
#### 5.6.1.5 Description of this vertical's communication architecture
The communication architecture for electric-power distribution grids
(visualised in Figure 5.6.1.5-1) is mapped onto the power network it serves.
This communication architecture covers multiple network domains, where each
domain relates to a specific grid voltage and geographic area. Every network
domain instance clearly belongs to exactly one power grid stakeholder.
{width="6.428472222222222in" height="3.7979166666666666in"}
Figure 5.6.1.5-1: Smart grid communication architecture [48]
> NOTE: CC: Control centre; DER: distributed energy resource; DSO:
> distribution system operator, HV: High voltage; LV: low voltage; MV: medium
> voltage; NMS: network management system; TSO: transmission system operator,
The following communication network domains can be distinguished:
> **Backbone network:** Communication network which connects the primary-
> substation LANs amongst each other and with regional control centres (often
> co-located) and central control centres.
>
> **Primary-substation LAN:** A primary substation LAN is quite complex and
> requires its own communication infrastructure that distinguishes between a
> process bus and a station bus. It is mainly based on a Gigabit Ethernet
> infrastructure.
>
> **Backhaul network:** Communication network which connects secondary-
> substation LANs with each other and with a control centre. This network
> domain might also connect to the respective primary-substation LAN in case
> the DSO and TSO roles are linked.
>
> **Secondary-substation LAN:** Network inside the secondary substation (today
> this network is quite trivial and may consist of just one single Ethernet
> switch / IP router). The secondary-substation LAN is implemented more in a
> distributed manner in US-Style regions, whereas the secondary-substation LAN
> is very often located in an encapsulated enclosure in Europe.
>
> **Access network:** Communication network which connects the customer
> premises or e.g., low-voltage sensors to a specific secondary substation.
>
> **Customer premises LAN:** In-building communication network whereas a
> customer is characterised by consumption and production of energy
> (prosumers) and the customer can be a residential, public or industrial
> prosumer.
>
> **Intra-DER Network:** For medium-sized DERs like wind/solar parks, a
> dedicated LAN is required for control, management and supervision purposes.
>
> **Intra-Control-Centre Network:** LAN within a DSO's or TSO's control
> centre.
>
> **Public network** : Fixed or mobile public communication network which
> offers different connectivity services either via dedicated services or via
> the open Internet.
Remarks:
> \- Each of the described communication network domains usually belongs to
> one electric-power distribution grid stakeholder.
>
> \- Some stakeholders―like aggregator or metering operator―do not own private
> communication infrastructure. Usually they connect to their assets /
> customers via a public network. Thus, public networks are an important part
> of the overall grid architecture.
>
> \- Not all communication network domains are mandatory to exist: for example
> the secondary-substation LAN can alternatively connect to prosumers via a
> public network service. In this case, no DSO-owned access network is
> required.
Mobile networks are used mainly for connecting secondary substation LANs and
DERs to control centres and primary substations and for connecting devices
like smart meters and sensors at customer premises to control and data
centres. The requirements of this communication are relatively low regarding
per-device bit rate, latency and communication service
availability/reliability.
Mission critical and real time communication (like tele-protection
communication) is usually not done with mobile networks due to the relatively
high latency, missing communication reliability/availability guarantees and
quality-of-service capabilities of 2G, 3G and 4G networks. It is our
expectation that suitably URLLC-enabled 5G systems will change this, and that
grid control functions might also be connected via mobile networks.
### 5.6.2 Primary Frequency Control
#### 5.6.2.1 Description
Primary frequency control is among the most challenging and demanding control
applications in the utility sector. A primary frequency control system is
responsible for controlling the energy supply injected and withheld to ensure
that the frequency is not deviating more than 0,02% from the nominal value,
e.g., 50 Hz in Europe.
Frequency control is based on having sensors for measuring the features in all
parts of the network at all points where energy generation or storage units
are connected to the grid. At these points, electronic power converters, also
known as inverters, will be equipped with communication units to send
measurement values to other points in the grid such as a frequency control
unit, or receive control commands to inject more, or less, energy into the
local network.
With the widespread deployment of local generation units, ie solar power
units, or wind turbines, hundreds of thousands of such units, and their
inverters, may have to be connected in a larger power distribution network.
Therefore, wireless communications based on high-performance 5G systems
constitutes a promising approach. There are two key benefits of this solution:
1) Wirelessly connected devices, such as sensors, inverters, and control
units, do not need cables and wirelines connections, thus reducing
installation costs and maintenance effort.
2) Devices can be connected and disconnected from the communications network
without any effort or restrictions, or lengthy preparations, allowing for
novel and swift set-ups.
A schematic representation of a communications network for frequency control
is provided in the figure below. The operation and maintenance staff in the
distribution management system (DMS) will define rules and guidelines for
frequency control which are forwarded to the automated frequency control
units. In turn, the frequency control units return alerts, escalations and
statistics to the DMS. Frequency sensors continuously monitor the current
frequency values at their local points, they can share their values with the
frequency control unit, which analyses and compares the incoming measurements,
and takes corrective actions by instructing the inverters in the local grid to
inject more or less energy into the local network. Note that frequency sensors
and actuators (inverters) can be located on the same hardware device, and in
this case, would only require one communication point.
The format and contents of these messages will be defined for standardisation
by regulatory bodies.
{width="6.297916666666667in" height="3.0236111111111112in"}
Figure 5.6.2.1-1 Schematic representation of frequency control system in a
distribution grid
Primary frequency control shall be carried out in one of three available
architecture options:
1) Centralised control, all data analysis and corrective actions are
determined by a central frequency control unit
2) Decentralised control, the automatic routine frequency control shall be
performed by the individual local inverter based on local frequency values.
Statistics and other information shall be communicated to the frequency
control unit, though.
3) Distributed control, the automatic routine frequency control shall be
performed by the individual local inverter based on local and neighbouring
frequency values. Statistics and other information shall be communicated to
the frequency control unit, though.
Furthermore, there are many scenarios where specific devices (e.g., sensors or
actuators) are added, activated, reconfigured, removed or deactivated while
the overall control system keeps on running. The overall availability of the
communications network, from an end-to-end point of view must be at least
_five Nines_ , and utility companies will demand that the total amount of
planned and unplanned downtime is not more than **5,26 minutes** per year.
In order to increase the availability level of the system, elements of the
communications network must have full redundancy and back-up units during
times of upgrade or update procedures.
#### 5.6.2.2 Preconditions
All frequency sensors, actuators/inverters, the frequency control units, and
the DMS are switched on and connected to the 5G system. Frequency control
units can be implemented locally depending upon the power system network
topology. The number of sensors, actuators/inverter, and other devices depends
upon the power system network topology.
#### 5.6.2.3 Service flows
At regular, frequent intervals, the following steps are performed. For primary
frequency control, the measurements are taken several times per second.
1) The frequency control unit requests measurements from all sensors in its
areas.
2) All sensors send the measurement values to the frequency control unit.
3) The frequency control unit analyses and compares the incoming measurements,
and determines if corrective actions are necessary.
4) In that case, the frequency control unit instructs the inverters in the
local grid to inject more or less energy into the local network.
5) The inverters will return an acknowledgement signal after executing the
changes required by the frequency control unit.
All messages exchanged have to be properly secured (especially in terms of
data integrity and authenticity) and the probability of two consecutive packet
errors shall be negligible. This is because a single packet error may be
tolerable, but two consecutive packet errors may lead to erroneous frequency
correction commands, which may cause outages of parts of the local grid. This
may cause significant financial damage.
#### 5.6.2.4 Post-conditions
The components controlled by the primary frequency control system have ensured
that the frequency value is back to acceptable values. Note that this value
may not yet be the exact nominal value yet. _Secondary frequency control_ is
used to ensure that the entire grid is back to this nominal value.
#### 5.6.2.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
\- Very stringent requirements on latency, communication service availability,
and reliability.
\- Very stringent requirements on clock synchronicity between different nodes.
\- Transmission of rather small chunks of data, resulting in potentially
significant relative overhead due to signalling, security, etc.
\- Potentially high density of end devices, including sensors and actuators.
#### 5.6.2.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Electric-Power Distribution 1.1_ The 5G system shall support highly
performant traffic with frequency measurement intervals in the order of 50 ms
for a communication group of up to 100.000 UEs and payload sizes of
approximately \~100 byte. T  
_Electric-Power Distribution 1.2_ The 5G system shall support highly
performant traffic with transmitting measurements and control commands with an
end-to-end latency in the order of \~50 ms. T  
_Electric-Power Distribution 1.3_ The 5G system shall support data processing
and frequency control procedures for local, decentralised grids with minimum
end-to-end latency, and maximum reliability and privacy. T  
_Electric-Power Distribution 1.4_ The 5G system shall ensure that critical
data traffic for power utilities is not disturbed by other traffic even at
peak times of load on the user plane. T  
_Electric-Power Distribution 1.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
ultra-low latency and ultra-high reliability requirements T  
_Electric-Power Distribution 1.6_ The 5G system shall support hot-plugging in
the sense that new devices may be dynamically added to and removed from a
frequency control application, without any observable impact on the other
nodes. T  
_Electric-Power Distribution 1.7_ The 5G system shall not violate valid,
general privacy principles applicable for electrical networks in general, and
support data minimisation and user consent if any data collection such as
frequency and impedance measurements in the local smart grid are unavoidable
for providing the required services.
### 5.6.3 Distributed Voltage Control with up to 100% RES
#### 5.6.3.1 Description
In the evolution towards 100% RES, the objective of voltage control is to
balance the voltage in future low voltage distribution grids connecting local
loads and prosumers as well as energy storage facilities. The aim is to
stabilise the voltage as local as possible, so that decisions and control
commands can be issued as quickly as possible.
The inverters, or electronic power converters, will measure the voltage and
power, and will also change the amount of power injected into the grid, and
connect and disconnect end-points from the distribution network. The
communication flow from the regional voltage control centre of the DSO
terminates at the inverter.
Figure 5.6.3.1-1 shows the communications network overview of a distribution
grid, where the distributed voltage control is performed by _voltage
controllers_ co-located with the regional secondary substation units. These
controllers receive impedance and voltage measurements from the inverters,
analyse the data and take corrective actions by sending commands to the
inverter end-points.
The controllers will also communicate with the Distribution Management System
(DMS), sending statistics, reports, and alerts to the operations centre, and
receiving regulations and other updates from the centre.
In the future, microgrids and aggregators may operate parts of the
distribution grids, with independent voltage control. In this scenario, these
new sector actors will exchange measurements, control commands, and other
information with the supervising Distribution System Operator (DSO).
{width="5.143055555555556in" height="2.9881944444444444in"}
Figure 5.6.3.1-1: Decentralised Architecture for Voltage Control
Distributed voltage control is a challenging and demanding control application
in the utility sector. Consumer devices rely on having stable voltage levels
to operate successfully. When future energy networks rely on thousands of
local energy generation units relying mostly on solar and wind power, then it
is crucial to stabilise the voltage levels in all segments of the distribution
grid.
With the widespread deployment of local generation units, i.e. solar power
units, or wind turbines, hundreds of thousands of such units, and their
inverters, may have to be connected in a larger power distribution network.
Therefore, wireless communications based on high-performance 5G systems
constitutes a promising approach. There are two key benefits of this solution:
1) Wirelessly connected devices, such as sensors, inverters, and control
units, do not need cables and wire lines connections, thus reducing
installation costs and maintenance effort.
2) Devices can be connected and disconnected from the communications network
without any effort or restrictions, or lengthy preparations, allowing for
novel and swift set-ups.
Distributed control means that the automated voltage control shall be
performed by the local voltage control units based on local _and neighbouring_
voltage and impedance values. Statistics and other information shall be
communicated to the central DMS, though.
Furthermore, there are many scenarios where specific devices (e.g., sensors or
actuators) are added, activated, reconfigured, removed or deactivated while
the overall control system keeps on running. The overall availability of the
communications network, from an end-to-end point of view must be at least
_five Nines_ , and utility companies will demand that the total amount of
planned and unplanned downtime is not more than **5,26 minutes** per year.
In order to increase the availability level of the system, elements of the
communications network must have full redundancy and back-up units during
times of upgrade or update procedures.
#### 5.6.3.2 Preconditions
All voltage sensors, actuators/inverters, the voltage control units, and the
DMS are switched on and connected to the 5G system. Voltage control units can
be implemented locally depending upon the power system network topology. The
number of sensors, actuators/inverter, and other devices depends upon the
power system network topology.
#### 5.6.3.3 Service flows
At regular, frequent intervals, the following steps are performed. For primary
voltage control, the measurements are taken several times per second.
1) The voltage control unit requests measurements from all sensors in its
areas.
2) All sensors send the measurement values to the voltage control unit.
3) The voltage control unit analyses and compares the incoming measurements,
and determines if corrective actions are necessary.
4) In that case, the voltage control unit instructs the inverters in the local
grid to inject more or less energy into the local network.
5) The inverters will return an acknowledgement signal after executing the
changes required by the voltage control unit.
All messages exchanged have to be properly secured (especially in terms of
data integrity and authenticity) and the probability of two consecutive packet
errors shall be negligible. This is because a single packet error may be
tolerable, but two consecutive packet errors may lead to erroneous voltage
correction commands, which may increase the risk for outages of parts of the
local grid. This may cause significant financial damage.
#### 5.6.3.4 Post-conditions
The components controlled by the distributed voltage control system have
ensured that the voltage value is back to acceptable values.
#### 5.6.3.5 Challenges to the 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
\- Stringent requirements on latency, communication service availability, and
reliability.
\- Very stringent requirements on clock synchronicity between different nodes.
\- Transmission of rather small chunks of data, resulting in potentially
significant relative overhead due to signalling, security, etc.
\- Potentially high density of end devices, including sensors and actuators.
#### 5.6.3.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Electric Power Distribution 2.1_ The 5G system shall support highly
performant traffic with voltage measurement intervals in the order of 200 ms
for a communication group of up to 100.000 UEs and payload sizes of
approximately \~ 100 bytes. T  
_Electric Power Distribution 2.2_ The 5G system shall support highly
performant traffic with transmitting measurements and control commands with an
end-to-end latency in the order of \~ 100 ms. T  
_Electric Power Distribution 2.3_ voltage control procedures for local,
decentralised grids with minimum end-to-end latency, and maximum reliability
and privacy T  
_Electric Power Distribution 2.4_ The 5G system shall ensure that critical
data traffic for power utilities is not disturbed by other traffic even at
peak times of load on the user plane. T  
_Electric Power Distribution 2.5_ The 5G system shall support data integrity
protection and message authentication, even for communication services with
ultra-low latency and ultra-high reliability requirements T  
_Electric Power Distribution 2.6_ The 5G system shall support hot-plugging in
the sense that new devices may be dynamically added to and removed from a
voltage control application, without any observable impact on the other nodes.
T  
_Electric Power Distribution 2.7_ The 5G system shall not violate valid,
general privacy principles applicable for electrical networks in general, and
support data minimisation and user consent if any data collection such as
frequency and impedance measurements in the local smart grid are unavoidable
for providing the required services.
### 5.6.4 Power distribution grid fault and outage management: distributed
automated switching for isolation and service restoration for overhead lines
#### 5.6.4.1 Description
A power distribution grid fault is a stressful situation for operators in the
control centre. During the fault period, they have to perform a lot of tasks
in order to restore power to the \"healthy\" section of the distribution grid
as quick as possible. Among other activities, they will:
\- collect and analyse grid status information;
\- identify faulty grid sections;
\- infer appropriate measures;
\- isolate the fault;
\- restore service;
\- inform service crews;
\- coordinate service crew operation;
\- restore normal grid configuration after the fault has been repaired.
In case disruptions affect a larger area―for example, during bad weather―the
level of stress further increases, because several outages may occur at the
same time. There are self-healing solutions for automated switching, fault
isolation and, service restoration. These solutions help avoiding these
stressful situations and allow operating personnel to concentrate on repair
work and service crew coordination. Furthermore, these solutions are ideally
suited to handle outages that affect critical power consumers, such as
industrial plants or data centres. In these cases, supply interruptions must
be fixed within less than a second, and manual outage handling in a control
centre usually fails to achieve such short restoration time. Automated
solutions are able to restore power supply within a few hundred milliseconds.
{width="6.5in" height="4.023611111111111in"}
Figure 5.6.4.1-1: Depiction of a distribution ring and a failure (flash of
lighting)
> GOOSE: Generic Object-Oriented Substation Event; NOP: normal open point.
The FLISR (Fault Location, Isolation & Service Restoration) solution consists
of switch controller devices which are especially designed for feeder
automation applications that support the self-healing of power distribution
grids with overhead lines. They serve as control units for reclosers and
disconnectors in overhead line distribution grids.
The system is designed for using fully distributed, independent automated
devices. The self-healing logic resides in each individual feeder automation
controller located at the poles in the feeder level. Each feeder section has a
controller device with a programmable logic controller (PLC). The PLC can be
configured by the utility by help of a graphical engineering tool. Using peer-
to-peer communication among the controller devices, the system operates
autonomously without the need of a regional controller or control centre.
However, all self-healing steps carried out will be reported immediately to
the control centre to keep the grid status up-to-date.
Modern communication systems primarily use the international IEC 61850
standard to support this distributed application. The IEC 61850 standard
provides the required flexibility and interactivity for the implementation of
self-healing functions. Peer-to-peer communication via IEC 61850 GOOSE
(Generic Object-Oriented Substation Event) message provides analogue and
binary data as fast as possible. Each controller unit has its own
comprehensive programmable logic designed by the sequence editor of the
graphical engineering tool to configure the automation functionality according
to the feeder layout. The controllers conduct self-healing of the distribution
line in typically 500 ms by isolating the faults.
A distributed solution offers several benefits:
\- cost-efficient and future-proof solution for automatic and high speed fault
location and service restoration;
\- flexible solution supporting central and distributed configurations;
\- easy configuration and maintenance with graphical tools;
\- automated switching procedure to return to normal operation -- no manual
intervention required;
\- SCADA system connectivity to self-healing solution for monitoring and
control purposes;
\- improvement of distribution grid reliability indicators by reduction of
outages;
> \- prevention of penalties and secure power supply for critical loads like
> hospitals and data centres;
\- all logic resides on the distributed controllers; no central control entity
needed.
There are 5 ... 20 controller units per feeder ring. The geographical
dimension of feeders is usually up to several km^2^.
The peer-to-peer protection communication between controller units is done via
IEC 61850 GOOSE messages. GOOSE is basically carried in Layer 2 multicast
message s. Although GOOSE Layer 3 (IP) transport has been already specified by
IEC 61850, it is currently not widely used . GOOSE messages are sent
periodically (with changing interval time) and are not acknowledged. The GOOSE
messages are sent by each controller to all other controllers or a number of
controllers of the same feeder. The end-to-end latency requirement is less
than 5 ms. The bit rate is low (from less than 1 kb/s to several kb/s per
controller) in steady state. GOOSE bursts do occur, especially during fault
situations (with bit rates up to 1.5 Mb/s per controller). GOOSE messages are
sent by a number of controllers or all controller units of the feeder nearly
at the same point in time during the fault location, isolation and service
restoration procedure.
The control communication between the controller units and the control centre
runs via IEC 61850 MMS, IEC 60870-5-104 or DNP3 messages. These are TCP/IP
based messages. The end-to-end latency requirement is less than 500 msec
(round-robin time).
Configuration, supervision and firmware upgrade of the controller units is
performed via the mobile network from the control centre. This is TCP/IP based
communication. The latency requirement is less than 500 ms (round-robin time).
The peer-to-peer protection communication between controller units with IEC
61850 Layer 2 GOOSE messages is usually not encrypted by the devices for
performance reasons but authenticated and integrity protected (IEC 62351-6).
The control communication between the controller units and the control centre
is usually encrypted, authenticated and integrity protected (IEC 61850 MMS
according to IEC 62351-4 and IEC 62351-6, IEC 60870-5-104, DNP3 according to
IEC 62351-3 and IEC 62351-5).
There is a certificate and key management which is based on a public key
infrastructure (PKI), according to IEC 62351-9. The security servers are
usually located on the control centre side and the certificate and key
enrolment towards the controller units runs via the mobile network.
#### 5.6.4.2 Preconditions
The controller units have been mounted, connected, and configured during
commissioning and deployment. The configuration is usually not done via the
mobile network but via local wired connection.
The controller units are switched on and connected to the 5G network.
#### 5.6.4.3 Service flows
5G communication runs
\- between controller units serving the same feeder and
\- between the controller units of the whole area and the control centre.
A feeder fault occurs.
Faults at the power distribution feeder are located and isolated by the
controller units and the healthy feeder sections are re-powered. This is
executed automatically, without operator intervention, within up to 500 ms.
#### 5.6.4.4 Post-conditions
5G communication runs and electricity is distributed through the restored part
of the distribution network.
#### 5.6.4.5 Challenges to the 5G system
The major challenge is the \"bursty\", peer-to-peer, layer-2, multicast IEC
61850 GOOSE communication with end-to-end latency requirement of less than 5
msec. The highest priority and reliability has to be applied to the IEC 61850
GOOSE message transmission. Message loss and corruption shall be avoided as
much as possible. Interference with other, lower priority traffic shall be
minimised.
There are stringent requirements on communication service availability and
reliability, which can be achieved, among others, by appropriate resilience
and redundancy measures. The duration of communication service interruptions
must be minimised as much as possible (high maintainability).
Wireless communication with the controller units must not be interrupted or
disturbed in case of power grid failure in affected area. This means the 5G
base stations etc. need to provide uninterruptible power supplies (UPSs) or
similar solutions.
Some power utilities require non-public wireless communication solution either
operated by their own or by a service provider under contract. SLAs are common
when service providers are involved.
Other power utilities require virtual type-a networks over a PLMN. Traffic
separation, QoS, and SLAs are essential.
The controller units usually implement state-of-the-art, end-to-end security
measures like authentication, integrity protection and encryption. The
security features of mobile networks can provide an additional level of
security, e.g., for the unencrypted IEC 61850 GOOSE messages. The data
encryption in the mobile network shall not result in exceeding the
aforementioned end-to-end latency requirement of the time-critical IEC 61850
GOOSE communication.
Furthermore, the mobile network shall provide capabilities of denial-of-
service prevention, particularly on the air interface.
#### 5.6.4.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Electric Power Distribution 3.1_ The 5G system shall support peer-to-peer
layer-2 multicast message communication, e.g., IEC 61850 GOOSE, with an end-
to-end latency of less than 5 ms. T  
_Electric Power Distribution 3.2_ The 5G system shall support a communication
service availability of at least 99,9999%. T  
_Electric Power Distribution 3.3_ The 5G system shall provide QoS: the peer-
to-peer layer-2 multicast message communication, e.g., IEC 61850 GOOSE, shall
be carried with the highest priority applicable to user data. Interference
from lower-priority traffic shall be minimised. T  
_Electric Power Distribution 3.4_ The 5G system shall provide minimum packet
loss and corruption for the high-priority peer-to-peer layer-2 multicast
message communication, e.g., IEC 61850 GOOSE. The packet error ratio shall be
10^-6^ or less. T  
_Electric Power Distribution 3.5_ The 5G communication shall not be
interrupted or disturbed in case of power grid failure (power supply outage)
in the served area. T  
_Electric Power Distribution 3.6_ The 5G system shall support type-a networks.
T  
_Electric Power Distribution 3.7_ The 5G system shall support virtual type-a
networks over PLMNs with traffic separation between tenants and with QoS
capabilities. T  
_Electric Power Distribution 3.8_ The 5G system shall support Service Level
Agreements (SLAs) for type-a network and PLMN solutions which fulfilment can
be supervised by the involved parties. This implies, among others, QoS
monitoring by the user. T  
_Electric Power Distribution 3.9_ The 5G system shall provide state-of-the-art
information security capabilities regarding user data authenticity, integrity,
confidentiality and denial-of-service prevention. T
### 5.6.5 Smart Grid: synchronicity between the entities
#### 5.6.5 _._ 1 Description
In electric power distribution, phasor measurement units (PMUs) are deployed
along the power line. The PMUs are used for real-time measurement and
monitoring of frequency/voltage/power to reflect the state of the system. When
a fault happens, the power line generates two voltage waves at the fault
location in two directions along the power line. Two PMUs, on both sides of
the fault location, detect the waves by the change of frequency/voltage/power
and record the time of receiving the wave. Both PMUs report the time to the
server. The server can then calculate the fault location according to the time
difference of wave detection at the two PMUs. Thus, the measurement time of
the samples produced by PMUs along the same power line needs to be highly
synchronised; otherwise the calculation of fault location is imprecise.
In current PMU solutions, time synchronisation is achieved by time-stamping
the measured synchro-phasor values. For this purpose, high-precision time
sources, e.g. GNSS clocks are used at PMU locations. Based on this procedure,
there are currently no specific requirements on clock synchronisation of the
communication network.
{width="3.4881944444444444in" height="3.6902777777777778in"}
Figure 5.6.5.1-1: PMU measurement in smart grid
#### 5.6.5.2 Preconditions
All PMUs are switched on and connected to the 5G system. A UE is integrated
into the PMU. A fault of the power line occurs in some location and generates
two voltage waves in two directions along the power line.
#### 5.6.5.3 Service flows
1) Changes in electricity frequency/voltage/power is detected by (some) PMUs
along the power line.
2) All PMUs send their measurements (the change of the frequency/voltage/power
and the corresponding time) to the server. The PMU clocks should be
synchronised with each other in the order of 1μs or below so that the
timestamp for the measurements is accurate and reliable.
3) The server analyses the information provided by the PMUs and determines the
fault location.
#### 5.6.5.4 Post-conditions
The server can inform the fault location to the reparation team.
#### 5.6.5.5 Challenges to the 5G system
Special challenge to the 5G system associated with this use case includes the
following aspects:
1) Very stringent requirements on clock synchronicity between different PMUs
#### 5.6.5.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Electric Power Distribution 4.1_ The 5G system shall support a very high
clock synchronicity between a communication group of up to 100 UEs in the
order of 1 µs or below. T  
_Electric Power Distribution 4.2_ The 5G system shall support a non-cyclic
data communication service with end-to-end latency \ The difference value between DTU-1 and DTU-2 exceeds a threshold; DTU-2
> shuts down the circuit between DTU­1 and DTU-2.
>
> DTU-2 calculates the difference between the current value simultaneously
> sampled by DTU-3 and DTU-2.
>
> The difference value between DTU-3 and DTU-2 does not exceed the threshold;
> no further actions are triggered by DTU-2.
DTU-1 and DTU-3 carry out the same procedure as DTU-2.
#### 5.6.6.4 Post-conditions
The fault between DTU-1 and DTU-2 is isolated in due time.
#### 5.6.6.5 Challenges to 5G system
Special challenges to the 5G system associated with this use case include the
following aspects:
> \- Stringent requirements on end-to-end latency and jitter.
#### 5.6.6.6 Potential requirements
**Reference number** **Requirement** **Application/Transport**
* * *
_Electric Power Distribution 5.1_ The 5G system shall support a peer-to-peer
data communication service with cycle times in the order of 0,8 ms (note). A
_Electric Power Distribution 5.2_ The 5G system shall support a peer-to-peer
data communication service with payload sizes of about 250 byte (note). T
_Electric Power Distribution 5.3_ The 5G system shall support an end-to-end
latency of less than 15 ms (note). T _Electric Power Distribution 5.4_ The 5G
system shall support a peer-to-peer data communication service with a jitter
of less than 50% cycle time (note). T NOTE: peer-to-peer means DTU to another
DTU.
### 5.6.7 Smart Grid : Millisecond-level precise load control
#### 5.6 7 _._ 1 Description
Precise load control is an important basic application for power-distribution
grids. When critical HVDC (high-voltage direct current) transmission faults
happen, millisecond-level precise load control is used to quickly remove
interruptible less-important loads, such as electric vehicle charging and non-
continuous production power supplies in factories.
Millisecond-level precise load control includes a control primary station and
load control terminals. Load information (e.g., total amount of interruptible
load, load shedding control commands) is communicated between the control
primary station and the load control terminals.
Millisecond-level precise load control system poses ultra-low end-to-end
latency requirements on the communication network. From the beginning of the
fault information collection to the successful removal of loads, the end-to-
end latency of the 5G network is less than 50 ms.
For security reasons, precise load control is a service in the production area
of the power grid. It must be completely isolated from services in management
areas.
{width="3.11875in" height="2.7979166666666666in"}
Figure 5.7.1-1: Millisecond-Level Precise Load Control
#### 5.6 7 _._ 2 Preconditions
All load control terminals are switched on and connected to the 5G system. The
load control terminals include a UE each.
#### 5.6.7.3 Service flows
1) The load control terminals report the total amount of interruptible load.
2) A test instrument simulates the HVDC transmission fault and the control
primary station receives the fault analyses.
3) The control primary station decides to shed interruptible load and sends
load shedding control commands to the corresponding load control terminals.
4) Load shedding is successfully executed.
5) Load control terminals report the results to the control primary station.
#### 5.6.7.4 Post-conditions
The successful shedding of interruptible less-important load s, and HVDC
transmission fault is cleared.
#### 5.6.7.5 Challenges to the 5G system
Special challenge to the 5G system associated with this use case includes the
following aspects:
\- ultra-low end-to-end latency: milliseconds;
\- strong isolation: precise load control is a service in the production area
of the power distribution grid; its communication must be completely isolated
from that of other services;
\- high communication service availability: 99 ,9999 %.
#### 5.6.7.6 Potential requirements
+------------------+------------------+------------------+----------+ | Reference Number | Requirement text | Application / | Comments | | | | Transport | | +==================+==================+==================+==========+ | Electric Power | The 5G system | T | | | Distribution | shall support | | | | | transmission of | | | | 6.1 | load control | | | | | commands with an | | | | | end-to-end | | | | | latency of less | | | | | than 50 ms. | | | +------------------+------------------+------------------+----------+ | Electric Power | The 5G system | T | | | Distribution | shall support | | | | | mechanism for | | | | 6.2 | isolating | | | | | communication | | | | | services in | | | | | power grid: | | | | | communication | | | | | services in the | | | | | production area | | | | | of the power | | | | | grid must be | | | | | completely | | | | | isolated from | | | | | communication | | | | | services in | | | | | management | | | | | areas. | | | +------------------+------------------+------------------+----------+ | Electric Power | The 5G system | T | | | Distribution | shall support | | | | | communication | | | | 6.3 | service | | | | | availability | | | | | exceeding | | | | | 99,9999%. | | | +------------------+------------------+------------------+----------+
## 5.7 Centralised power generation
### 5.7 _._ 1 Description of vertical
This domain comprises all aspects of centralised power generation, i.e. the
centralised conversion of chemical energy and other forms of energy into
electrical energy. Typical electric-power outputs are 100 MW and more.
Examples for pertinent systems are large gas turbines, steam turbines,
combined-cycle power plants, and wind farms. The planning and installation of
respective equipment and plants as well as the operation, monitoring and
maintenance of these plants is encompassed by this vertical domain.
### 5.7.2 Run-time access to operational data and control information
#### 5.7.2 _._ 1 Description
This use case covers applications which feature (temporary) access to an
existing local plant control system via wireless connections. The access
interface on a handheld device is typically a graphical user interface fitted
to the mobile device. The graphical user interface can be similar to the
control room operator display.
Monitoring as well as control information is transported between the power
unit and the handheld device.
There are two basis application scenarios: (1) on-site mobile access, for
example for commissioning or maintenance work; (2) remote access to unmanned,
fully-automated power units for controlling them.
#### 5.7.2.2 Preconditions
A 5G communication service between mobile device and plant control system is
established.
#### 5.7.2.3 Service flows
Bi-directional communication between the plant control system and the mobile
device is carried out. The information transported to and fro the mobile
device conveys states of and changes in process parameters, control settings,
and/or power unit characteristic parameters with stringent real-time
requirements.
#### 5.7.2.4 Post-conditions
The 5G connection is terminated on the handheld device side.
#### 5.7.2.5 Challenges to the 5G system
> Low-latency confirmation of receipt of control commands from the plant
> control system.
>
> Persistent \"sign-of-life\" beacon from mobile device to control system (and
> vice versa); confirmation of persistence of connection.
#### 5.7.2.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Centralised Power Generation 1.1_ The integrity of the information exchanged
via the 5G system shall be protected. T  
_Centralised Power Generation 1.2_ The 5G system shall support concurrent,
independent connections between the power unit and two or more mobile devices.
T The communication between the power unit and the mobile devices includes
control messaging and the exchange of operational data. Control messaging can
include cyclic communication. Especially during deployment and commissioning
of a power unit, many workers need communication access to the power unit at
the same time.
### 5.7.3 Data acquisition for non-real-time plant monitoring
#### 5.7.3 _._ 1 Description
This use case covers applications which feature periodic (but not permanent)
or a-periodic (event-driven) transmission of high amounts of locally recorded
data. Examples for such data are high-resolution images (documentation of work
piece quality) or raw signals from vibration measurements (event diagnosis).
Note that this data does not entail real-time requirements. For a new plant,
wired communication infrastructures can fulfil the respective communication
needs, but there might be cases, for instance retrofitting of legacy
installations or remote installations, for which an implementation of a wired
communication infrastructure is not viable
#### 5.7.3.2 Preconditions
A 5G communication service between an on-site data storage system and a remote
recipient (e.g., a cloud storage) is established.
#### 5.7.3.3 Service flows
Locally recorded information is uploaded to the remote recipient. The uplink
data volume is high. Virtually no downlink transmission occurs (only control
commands and acknowledgments)
#### 5.7.3.4 Post-conditions
The 5G connection between the on-site data storage system and the remote
recipient is terminated.
#### 5.7.3.5 Challenges to the 5G system
No extraordinary challenges recognised.
#### 5.7.3.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Centralised Power Generation 2.1_ The 5G system shall offer communication
services that offer a time-guaranteed upload service. The source application
in question requests an upload of a specific amount of data to a target
application. The source application also specifies by when the data shall be
uploaded, and during what time intervals the upload may not take place. T
### 5.7.4 Remote support for plant maintenance
#### 5.7.4 _._ 1 Description
This use case covers applications which feature a temporary support for
augmented-reality devices such as glasses for remote support of maintenance /
retrofitting work. This use case encompasses the uplink of video streams and
the downlink of the projection of virtual-reality elements.
#### 5.7.4.2 Preconditions
A 5G communication service between a mobile augmented-reality device and a
remote support system is established.
NOTE: Remote support system can comprise databases for component design
information, component operating history information, component maintenance
instructions, etc., as well as tools/methods for visualising those data as
augmented-reality elements.
#### 5.7.4.3 Service flows
Bi-directional communication between the mobile device and the remote support
system occurs. This communication transports video data and/or locally
processed image features from the mobile device to the remote support system
and augmented-reality elements from the remote support system to the mobile
device.
#### 5.7.3.4 Post-conditions
The wireless connection between mobile device and remote support system is
terminated.
#### 5.7.4.5 Challenges to the 5G system
> \- \"Simultaneous\" uplink / downlink of high-bit-rate data streams.
>
> \- Assist in achieving low end-to-end latencies over large geographic
> distances~.~
#### 5.7.4.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_Centralised Power Generation 3.1_ The 5G system should assist an end-to-end
latency \ \- management of wind turbine equipment;
>
> \- control applications for controlling the wind turbine operation;
>
> \- surveillance applications carrying remote video/audio data used for
> supervising the area in and around the turbine. This includes video and
> audio, which is very helpful for remote support of technicians in off-shore
> turbines. For instance, the video feed is used to visually check that the
> rotors and the landing pad of an offshore wind turbine are in a safe state
> prior to hoisting technicians by helicopter. It's also used to visually
> capture the helicopter during the hoist situation, to remote control from
> the helicopter based on visual feedback, and to stream video from the
> helicopter to, e.g., the control centre;
>
> \- monitoring of parameters of the wind turbine (collection of sensor
> information). This includes reports about power production.
NOTE: During the maintenance phase the aforementioned helicopter might leave
the coverage area of the wind power plant communication network. In such
cases, it is beneficial if an adjacent PLMN can provide service continuity for
at least some of the related communication services, e.g. for supporting the
video stream to and from the helicopter to, e.g., the control centre.
The converged management, monitoring, surveillance, and control applications
all transmit over the same physical communication network. The management,
monitoring, control, and surveillance applications typically have different
functional scopes and different QoS requirements. Field-level control traffic
typically requires real-time guarantees in terms of end-to-end latency, jitter
(timeliness), and capacity (data throughput).
Examples of SCADA control applications requiring strict end-to-end latency
bounds and reliable delivery are grid regulation signalling and transmission
of grid protection signals. Although most individual actuators in a wind
turbine are controlled autonomously by a local turbine controller, the SCADA
might override the local configuration in order to globally optimise
production or to enforce production limits. Therefore, SCADA will also apply
blade pitch angle configurations, modifications to generator speed, yaw
rotation actions or enabling/disabling of individual turbines or whole wind
power plants based on the production limit thresholds.
Remote video surveillance, on the other hand, might have relaxed requirements
in terms of end-to-end delay, but it requires a relatively high bandwidth
(capacity, data throughput).
Specific security and isolation network properties need to be in place, since
not all stakeholders of the wind farm should be allowed to access the local
sensor, monitoring, and surveillance data. Temporary, restricted access to the
local sensor, monitoring, and surveillance data needs to be granted to
external stakeholders such as the helicopter company.
Furthermore, some wind turbine sensors (e.g., wind speed and direction) might
transmit information where delayed arrival of packets---or even the loss of
some packets over a small number of sample measurements---might be acceptable.
However, other sensor information, such as the temperature input for
determining thermal stability extremes or high vibration level alarms, might
require immediate action and thus timely and dependable communication. A
state-of-the-art wind turbine may host up to 3000 sensors, which provide input
to local turbine controllers. The turbine controllers often host multiple data
pre-processing modules, all communicating back to the central SCADA. Each of
these communication flows typically comes with its own QoS. Due to regulation,
reporting on power production has specific requirements on the application and
the communication.
Managing large wind power plants also means managing a large number of wind
turbines and an ever-growing number of locally hosted applications and data
sources. This results in the need for dynamic connectivity and large
bandwidths. Current wind power plants may consist of up to 650 wind turbines
on-shore and up to 200 wind turbines off-shore.
The introduction of new services with added value services will require
application-specific network configurations to reflect the stakeholder's
access rights and communication requirements. Examples for such services are
as follows.
> \- Active and proactive power production---in accordance with a dynamic
> electricity market---will require the flexible management of distributed
> energy resources and energy storage capacities based on the per-minute
> saturation of the energy market and the related pricing model.
>
> \- External stakeholders, such as grid operators or maintenance and support
> personnel, require insight into the capabilities and (diagnostic)
> information of the wind power plant in order to utilise its capabilities
> (e.g., reaction to grid errors, production and consumption of reactive
> energy when there is little wind, etc.) and to perform monitoring,
> maintenance, and repairs.
>
> \- The deployment of data mining and remote pattern matching techniques for
> early fault recognition and identification of irregularities will result in
> a wide range of additional scenarios, requiring automated setups and
> configuration of deterministic network services in order to provide sensor
> data for processing in externally or internally hosted data analytics
> appliances.
>
> \- On-demand wireless connectivity for on-site maintenance personnel would
> be advantageous. The maintenance personnel will thus be enabled to easily
> performing maintenance tasks such as reading out characteristic parameters
> and running test procedures. Strict access rules and safety regulations have
> to be followed for these kind of communication scenarios.
>
> \- Remote control of wind power plant services by a remote wind power plant
> service centre through external wide area networks.
Customised access for different stakeholders of wind power plants entails that
the access for a particular service might be
> \- dynamic: it has a limited time duration and is setup during operation
> (unscheduled or scheduled);
>
> \- limited: it contains only a constrained number of devices;
>
> \- demanding: it has strict; industrial-grade QoS requirements;
>
> \- restricted: it has access to a limited functionality only;
>
> \- separated: it is isolated from other communication services in the wind
> power plant communication network.
Today, grouping and isolation of the different communication flows of the wind
power plant applications is usually done by configuring VLANs in wind power
plant communication networks. Reconfiguration of VLANs is required for
customised access for different stakeholders for particular application
services.
A fast and flexible---ideally automated---way for the dynamic introduction and
deployment of communication services in a wind power plant network is
required. It needs to be tailored to the applications and the stakeholders.
Since stakeholders typically belong to different organisations and subsets
thereof, the wind power plant communication network needs to support multi-
tenancy. The network needs thus to consider the access level and access rights
associated with the requesting stakeholder. The wind power plant communication
network also needs to provide customised access to specific tenants and ensure
adequate isolation and service quality for different (internal and external)
applications during the network runtime. The invocation of communication
services can be quite dynamic.
#### 5.7.5.2 Preconditions
The wind power plant communication network is up and running. Static
communication services have been configured and instantiated. They satisfy
different sets of service requirements of the communication services. These
services support, for instance, deterministic control traffic, dependable
sensor information transmission, and best effort sensor data transmission. The
communication services might be running within the wind turbine only, between
the wind turbines and the internal SCADA, or between end points of the wind
power plant network and a remote service centre. The communication services
are requested by different stakeholders with different access rights (multi-
tenancy).
The wind power plant communication network supports dynamic instantiation of
communication services by exposing corresponding interfaces and implementing
the decision-making methods on where and how to position the new services.
The wind power plant operator or the end-node applications know their own
requirements on the communication services in terms of QoS parameters
requested via the communication service network interface. The service
requirements are described in accordance with the communication service
request model provided by the service interface of the communication network.
Wind power plant operator and end-node applications possess the knowledge of
their connectivity, QoS, and security requirements for correct operation by
means of service monitoring and unique service IDs.
The wind power plant communication network can limit or allow addition of new
services based on stakeholder access rights, available network resources, and
current network state.
Reachability between the communication service end points (applications,
controllers, SCADA, etc.) is supported by a suitable network design.
#### 5.7.5.3 Service flows
This clause describes a usage scenario in a wind power plant network. This
usage scenario contains several scenarios with encompasses several occurrences
of the use case "Customised access of stakeholders to wind power plant
network" in different situations with different communication service
requirements and different stakeholders.
In a first step, new permanent application functionality is added to an
existing wind power plant network (data analytics functionality). This new
functionality triggers a predictive maintenance action in a second step. The
maintenance process is done in several stages.
The different use case occurrences in the scenarios follow the same general
principle and general service flow. This general service flow is given after
the description of the different steps of the usage scenario.
##### 5.7.5.3.1 Scenario: adding new functionality
New data analytic functionality for predictive maintenance is installed at the
wind turbine controllers with a remote software update. The new functionality
is added for continuous, permanent use. In principle, there is no deadline
assigned to this software installation.
The communication network sets up communication services for the software
distribution (best effort, perhaps with a deadline; also see Clause 5.7.3.6).
For the operation of the new data analytics functions, the communication
network of the wind power plant sets up communication services for the
transmission of collected data or aggregated data analytic information between
the wind turbine controllers and the SCADA, the remote service centre, or even
an external data analytics provider. The access to the wind power plant
communication network, especially if an external stakeholder is involved,
needs to be restricted to the data relevant for the data analytics task.
##### 5.7.5.3.2 Scenario: triggering of a maintenance action
A maintenance case is triggered either due to predicted maintenance (based on
the new data analytics functionality) or scheduled maintenance.
If the maintenance case is triggered due to predicted maintenance based on the
new data analytics functionality (see Clause 5.7.5.3.1), a communication
service is required between the data analytics functionality and a local or
remote service centre. Predictive maintenance action of a certain wind turbine
is triggered only rarely, and the transmission of the predictive maintenance
request comes with relaxed delay requirements. Therefore, the communication
service can usually be setup on-demand.
##### 5.7.5.3.3 Scenario: maintenance action -- first stage: acquisition of
additional information
In the case of an impending maintenance action, the remote service centre and
the personnel on location require on-demand access to additional sensor data
based on the respective maintenance case. Communication services are set up
through the wind power plant communication network between the remote service
centre / personnel on location and the wind turbine controller for accessing
additional, case-based sensor data and information.
The additional sensor data might include video surveillance of the wind
turbine in question. The video can be used for a remote visual evaluation of
the wind turbine. Communication services for video surveillance are setup
between the video cameras on and around the wind turbine and the remote
service centre / the personnel on location. The communication service has
service requirements based on the particularities of video transmission. These
are mainly requirements toward bandwidth, communication service availability,
communication service reliability, and a typically rather relaxed end-to-end
latency. It is important that these high-bandwidth communication services must
not interfere with the existing and ongoing deterministic control traffic of
the wind turbines and the wind power plant.
Only authenticated users are granted access to the aforementioned information
and communication services on additional sensor data and video surveillance.
At this stage, the involved users are presumably internal stakeholders that
have to evaluate the technical parameters of the wind turbine and that have to
decide on the next steps for the maintenance.
##### 5.7.5.3.4 Scenario maintenance action -- second stage: preparation for
on-site maintenance action
If the decision is made for on-site maintenance by technicians, the wind
turbine in question has to be prepared for carrying out this maintenance
action. A particular focus is on the safety of the technicians.
Communication services for video surveillance of the actual situation at the
wind turbine in question are setup between the video cameras on and around the
wind turbine and the remote service centre / the personnel on location. First,
it is important to monitor the weather conditions (wind, sleet, rain ...). It
might be possible to wait for good weather when carrying out a maintenance
action triggered by predictive maintenance analytics.
Weather conditions permitting, the maintenance technicians are flown to and
hoisted to the wind turbine by helicopter. Before the technicians can land on
the landing pad, the wind turbine has to be put into a safe state, the vanes
of the wind turbine and the landing pad in particular. A communication service
is set up between the local control centre and the wind turbine controller for
stopping the wind turbine. While the service requirements for the
communication that triggers the shutdown of the wind turbine are rather
relaxed with respect to maximum latency and data throughput, they are strict
with respect to communication service availability, communication service
reliability, authentication, and authorisation. Only authorised personnel is
allowed to shut down a wind turbine. Furthermore, the control communication
service between the wind turbine controller and the actuators of the wind
turbine usually comes with strict and demanding requirements concerning
latency, communication service availability, and communication service
reliability. The shutdown of the wind turbine will be monitored with several
video feeds. Respective communication services for video surveillances have to
be set up.
A further communication service for video surveillance needs to be set up
between the wind turbine and the helicopter, so that the pilot can monitor the
hoisting and arrival of the technicians before leaving for a helicopter
platform. Furthermore, some wind turbine functions might be remote controlled
from the helicopter based on video surveillance. Surveillance video is
sometimes also streamed from the helicopter to, e.g., the control centre; it
is beneficial if this video stre am is not interrupted when the helicopter
(accidentally) leaves the coverage area of the wind power plant communication
network and the video is streamed via an adjacent PLMN.
The communication services for video surveillance have service requirements
based on the particularities of video transmission (mainly requirements
pertaining to bandwidth, communication service availability, communication
service reliability, and a typically rather relaxed end-to-end latency). These
high-bandwidth communication services must not interfere with existing and
ongoing deterministic control traffic of the wind turbines and the wind power
plant.
Furthermore, specific security and isolation network properties need to be in
place, e.g., secured and isolated connection and deployment and strict use of
authentication and authorisation mechanism. This is necessary since this
activity includes several stakeholders, both internal and external, and only
selected stakeholders should be allowed access to the local surveillance data.
Furthermore, the access needs to be restricted to a minimum of data and
devices, so that only information that is necessary for the maintenance action
is actually made available.
##### 5.7.5.3.5 Scenario maintenance action -- third stage: carrying out on-
site maintenance
Future deployments of wind power plant communication networks may allow
wireless access of maintenance personnel to the communication network and data
of a wind turbine on location. In this case, the technicians will request
access to the wind power plant communication network. For security and safety
reasons, the access has to follow very strict authentication and authorisation
rules, so that only authorised devices and authorised personnel can access the
actuators and data of a wind turbine. Furthermore, the access of the
technicians will be restricted to the actuators and data of the wind turbine
in question and to the respective data of the wind turbine in the local
control centre.
The maintenance device of the technicians requests setup of communication
services in order to access the relevant data of the wind turbine and the
local control centre (SCADA). These communication services are usually subject
to rather relaxed end-to-end latency requirements.
The technicians may perform certain test procedures at the wind turbine by
means of their maintenance device. Such a test procedure might be testing the
rotation of the wind mill vanes. The rotation will be controlled at the
maintenance device. The maintenance device needs to setup a dependable
communication service with the actuators of wind mill vanes for this
deterministic control traffic. This communication service is also
characterised by a low end-to-end latency.
After the maintenance is finished, the set up communication services for the
maintenance tasks need to be disengaged (torn down). First, the technicians
tear down the communication services between their maintenance devices and the
communication network at the wind turbine in question.
Communication services for voice communication are needed throughout the
entire maintenance action. These services are needed for voice communication
between the technicians on-site at the wind turbine and for voice
communication between the technicians on the wind turbine and the staff at the
local control centre.
After their safe return to their base location, the communication services for
the video surveillance are disengaged (torn down).
#### 5.7.5.4 Post-conditions
All communication services related to the maintenance action have been
disengaged (torn down). The wind power plant communication network is mostly
in the same configuration as before the service flows in Clause 5.7. 5.3.
Furthermore, communication services for the added continuous data analytics
functionality for predictive maintenance are running.
#### 5.7.5.5 Challenges to the 5G system
##### 5.7.5.5.1 General challenges
The 5G system needs to provide interfaces for the dynamic setup of
communication services with different service requirements, fulfilling a wide
range of service requirements.
The 5G system needs to support isolation of communication services and of
stakeholders.
The 5G system needs to enable effective authentication and authorisation or
capabilities for deploying application-specific authentication and
authorisation methods.
The 5G system needs to provide a LAN service for migration scenarios. VLAN
configurations of today's wind power plant communication networks need to be
supported by the 5G system.
The 5G system needs to provide authenticated and authorised access to remote
stakeholders such as a remote service centre. The remote stakeholder
communicates with the type-a 5G network (wind power plant communication
network) through wide area networks, possibly 5G networks of 5G network
operators.
The 5G system needs to support a potentially large number of communication
services (scalability). The wind power plant communication network might
contain half to one million of sensors and actuators, many communication
services for the control of the wind turbines, plus several other dynamically
set up communication services for specific events such as maintenance, remote
service, and monitoring access. Some communication services might be
restricted to specific areas such as a wind turbine; others might cover the
entire wind power plant communication network.
The 5G system needs to support communication services identifiers and
priorities of communication services. Communication services with high
priority such as for emergency overwrites need to preempt non-essential
communication services if necessary (insufficient network resources).
A possible solution for high communication service availability and high
reliability is multi-path connectivity. In order to use multi-path for
demanding dependable communication services, the 5G system needs to support
multi-path transmissions and their setup.
NOTE: A possible means of admission control for communication services is
multi-constrained path selection. It allows satisfying the different sets of
requirements of the dynamically set up communication services.
##### 5.7.5.5.2 KPIs for communication services in wind power plant
communication networks
The KPIs of communication services in a wind power plant communication network
are listed in Table 5.7.5.5.2-1. The KPIs of the communication service are
measured over the operation time of the particular service.
Table 5.7.5.5.2-1: KPIs for communication services in a wind power plant
communication network (based on [60] and [61])
**Communication Service** **End-to-end latency** **Communication service
availability** **Packet error ratio**
* * *
Analogue measurement 16 ms 99,99% \ Special challenges to the 5G system associated with this use case include
> the following aspects:
1) Very stringent requirements concerning on end-to-end latency, packet error
ratio and communication service availability.
2) Very stringent requirements concerning clock synchronicity at application
nodes between different nodes. This implies that the 5G system must support
isochronous packet transfer and ultra-precise time synchronisation, even in
the UE. Time-Sensitive Networking features may be exposed to the application
and pertinent interfaces implemented.
3) Transmission of rather small chunks of data, resulting in potentially
significant overhead due to signalling, routing, security, etc.
4) Multicast support of stringent requirements.
5) Support of local type-b network deployment that meet the application
performance requirements.
#### 5.8.3.6 Potential requirements
**Reference number** **Requirement text** **Application / transport**
**Comment**
* * *
_PMSE 2.1_ The 5G system shall support a clock synchronicity of 20 µs or
better at application level between a communication group of 50 UEs to 500 UEs
in multicast operation. T  
_PMSE 2.2_ The 5G system shall support data integrity and confidentiality
protection, even for communication services with ultra-low latency and ultra-
high communication service availability requirements T  
_PMSE 2.3_ The communication service availability of the 5G system shall
exceed at least 99,9999% T  
_PMSE 2.4_ The 5G system shall support hot-plugging in the sense that new
devices may be dynamically added to and removed from a local conference
application, without any observable impact on the communication to and from
the other devices. T  
_PMSE 2.5_ The 5G system shall support industry standards for precision clock
synchronisation (e.g., IEEE 1588) for IP-based A/V systems in a way that the
synchronisation requirements outlined in PMSE 2.1 can be met. T  
_PMSE 2.6_ The 5G system shall support type-b network deployments (physical
and virtual), e.g., within a private conference location. A
### 5.8.4 High data rate video streaming / professional video production
#### 5.8.4.1 Description
The use case "high data rate video streaming / professional video production"
addresses demanding applications that partially require extensive post-
processing of the material supplied by cameras. Post processing increases the
quality requirements of the source material, which entails an increased
transmission bandwidth. In particular, scenic productions (e.g., TV-shows,
theatre, award ceremonies, etc.) place high demands on the communication
network - due to their \"quasi-live\" producing character.
In this use case, high-quality video data is streamed wirelessly from one or
several cameras to the base station of a Local High Quality Network (see
Figure 5.8.4.1-1), where it can be recorded or edited. Moreover, the video
production system allows remote operation of the cameras (e.g., focus control)
by sending and receiving control data. In general, the operation is limited to
a relatively small service area (see Table 5.8.4.1).
The video stream sent from a camera is used for two purposes:
> As the camera signal for, e.g., \"quasi live\" editing and recording;
>
> For viewing by remote camera operators, e.g., crane operators and focus
> pullers (i.e. to maintain image sharpness of whatever subject or action is
> being filmed).
Currently, several dedicated wireless systems for video and control need to be
used in parallel. This is disadvantageous and potentially can be improved by
the use of 5G RAT as a common enabler.
The professional video production use case is quite demanding, as high data
rate is required in parallel with low latency. Especially, for the remote lens
control application, which is performed by the focus pullers, low latency is a
critical topic. This is because focus tracking of moving objects based on
delayed monitoring or outdated control commands inevitably results in focusing
faults.. Two scenarios are defined for this use case:
> _Portable, high-data-rate video production system_ : The location and time
> of deployment are flexible and can change from one occasion to the next. The
> system is in worldwide use. It is typically shipped in flight cases from one
> location to another. This scenario places major requirements on the wireless
> system, especially to delay, data rate , error behaviour, ease of deployment
> and dismantling.
>
> _Fixed installation, high-data-rate video production system_ : The camera
> part is the same as for the portable scenario. However, the base station(s)
> - including antennas and system wiring - can be permanently installed in a
> typical production studio.
The separation in two scenarios should mainly be considered for a later
product definition with respect to the base station side. The camera part and
the typical requirements for the performance of the wireless system is the
same for both scenarios. The latter are summarised in Table 5.8.4.1-1.
Professional video systems rely on ultra-precise time synchronisation at the
application level. Each professional video terminal shall independently refer
to an accurate time reference signal that reaches all terminal devices
simultaneously, i.e. within a jitter interval much smaller than the video
sampling period.
{width="6.690277777777778in" height="3.0833333333333335in"}
Figure 5.8.4.1-1 Logical topology of high data rate video streaming use case
Table 5.8.4.1-1: System parameters of the high data rate video streaming/
professional video production use case
+----------------------+----------------------+----------------------+ | | **Local Conference |** Comment**| | | System** | | +======================+======================+======================+ | **Application | \  \- common definitions and metrics;
>
> \- requirements concerning setup of a security organisation (related to
> information security management systems), solution suppliers, and service
> provider processes;
>
> \- technical requirements and methodology for security at a system-wide
> level;
>
> \- requirements concerning the security development lifecycle of system
> components, and security requirements for such components at a technical
> level.
{width="4.678472222222222in" height="3.8097222222222222in"}
Figure 6.3-1: IEC 62443 Overview and Status
> NOTE: Proc.: process; Func.: function; IACS: industrial automation and
> control system.
According to the methodology described in IEC 62443-3-2 [41], a complex
automation system is structured into zones that are connected by and
communicate through so-called \"conduits\" that map, for example, to the
logical network communication between two zones. Moreover, IEC 62443-3-3
defines security levels (SL) that correlate with the strength of a potential
adversary as shown in Figure 6.3-2 below [42].
{width="3.2381944444444444in" height="2.154861111111111in"}
Figure 6.3-2: Security levels according to IEC 62443-3-3 [42]
In order to reach a dedicated SL, the related requirements have to be
fulfilled. For each security level, IEC 62443-3-3 defines a set of security
requirements for an automation system. Seven foundational requirements group
specific requirements:
> \- FR 1: identification and authentication control;
>
> \- FR 2: use control;
>
> \- FR 3: system integrity;
>
> \- FR 4: data confidentiality;
>
> \- FR 5: restricted data flow;
>
> \- FR 6 timely response to events;
>
> \- FR 7: resource availability.
For each of the foundational requirements, there exist several concrete
technical security requirements (SR) and further requirement enhancements (RE)
that have to be fulfilled for a specific security level. In the context of
communication security, the security levels are specifically interesting for
the conduits connecting different zones. The following examples are taken from
IEC 62443-3-3 [42] to illustrate some of the requirements.
> \- FR1 SR 1.6 -- Wireless access management: The control system shall
> provide the capability to identify and authenticate all users (humans,
> software processes or devices) engaged in wireless communication.
>
> \- FR3, SR3.1 -- Communication integrity: \"The control system shall provide
> the capability to protect the integrity of transmitted information\".
>
> \- FR3, SR3.8 -- Session integrity: \"The control system shall provide the
> capability to protect the integrity of sessions. The control system shall
> reject any usage of invalid session IDs.\"
>
> \- FR4, SR 4.1, RE 1 -- Protection of confidentiality at rest or in transit
> via untrusted networks: \"The control system shall provide the capability to
> protect the confidentiality of information at rest and remote access
> sessions traversing an untrusted network.\"
>
> \- FR5, SR 5.2 -- Zone boundary protection: \"The control system shall
> provide the capability to monitor and control communications at zone
> boundaries to enforce the compartmentalisation defined in the risk-based
> zones and conduits model.\"
As IEC 62443 gains more acceptance in different industry domains, different
technical security solutions suitable for the application domain and coping
with the security requirements are needed.
## 6.4 5G security for automation applications
### 6.4.1 Introduction
Depending on how 5G networks and 5G technologies are used by a vertical
automation application, different requirements have to be met by the
underlying 5G security solutions. Clause 6.4.2 addresses pertinent quality
properties of the security solution, and Clause 6.4.3 provides potential
requirements that were---among others---inferred from IEC 62443 [42] and from
an application-centric study [43].
### 6.4.2 Quality properties of 5G security solutions
_Authentication of communication peers and the 5G system:_ In many vertical
use cases, security responsibilities are complex, as they are shared by
several actors and need to be managed by credential pairs or certificates from
different sources. Authentication and verification purposes are increasingly
achieved using the EAP framework. Related potential requirements can be found
in Clause 5.3.19.6.
_Flexible subscriber access management:_ Efficient management of 5G
subscriptions/permissions is important for small, medium and huge numbers of
5G UEs that provide communication among automation system entities (e.g.,
machinery on a factory floor). In this context, \"small\" is on the order of
10, and \"huge\" on the order of 10.000 or larger. In this context, management
stands for adding UEs to a 5G subscription so that they can use 5G
communication services, but also for removing UEs from the subscription base.
One of the challenges here is medium-sized installations, where manual
management is not practical, while fully automated solutions that require a
complex infrastructure may be too cumbersome.\ Subscriber access management
may involve action on the UE side, e.g., installation, or activation of pre-
installed security credentials in the UE; as well as action at the network
side, e.g., enabling network access to these UEs on the factory floor.
_Long-term security:_ Devices in many verticals operate over long usage
periods (in industrial environments typically 10 to 20 years), which makes
long-term security an important topic. Note that in many vertical environments
updates may be installed only during planned service windows. Note that such
updates may imply that a laborious safety recertification has to be repeated,
which generally is avoided.
It is important that an automation application system can be kept in service
over a long usage period without requiring regular physical access to the
devices for upgrades (e.g., replacing hardware components; redesigning the
technical solution). However, it is also critical for the distributed
automation application that UEs are upgradable or can be patched (including
firmware, security-related algorithms and long-term keys) in order to maintain
the security of the system to the state-of-the art over the life span of the
devices. For a related discussion see the note 3 in the requirement Factories
of the Future 19.6 (Clause 5.3.19.6).
_5G as communication infrastructure:_ When the security provided by the
communication system is deemed to be insufficient for a vertical automation
application, security of the industrial solution is realised on top (e.g.,
using IPsec or TLS). A non-automation example for this is online banking. In
many deployments, the 5G network is expected to provide certain dependability
guarantees independently of security. Communication dependability is discussed
in detail in Clause 4.3.3.
NOTE 1: As discussed in Clause 6.1.3, security and other patches/updates may
often only be installed during the maintenance window of the automation
system. For typical automation systems this window occurs once a year and
lasts about one week, during which the automation system can be physically
accessed.
Also---for automation traffic requiring QoS---it is important that the
required QoS can be provided irrespective of whether over-the-top (OTT)
security is used.
NOTE 2: Since the 5G system is not in control of the automation application\'s
security-related data flows, the aforementioned required communication service
dependability and QoS can perhaps not be met due to high communication
resource consumption. In this case, the automation application, including its
security functions, would need to be optimised in order to lower the
communication resource consumption and to thus increase the communication
service dependability.
NOTE 3: If security for automation is only ensured by OTT security, the
possible usages of 5G in vertical applications is limited (e.g., reduced
lifetime of battery-powered devices, limited real-time capability).
_Reliance on 5G security alone:_ 5G systems may be used for connecting IoT
devices. In such deployments, realising security on top of 5G could reduce the
life time of battery-powered IoT devices, or deteriorate experienced
dependability parameters (see above). Therefore, it may be decided to rely on
5G communication security alone. If that is the case, the vertical automation
applications need to verify that the required 5G security mechanisms are
actually active.
_Message integrity:_ As outlined in Clause 6.1.2, integrity within automation
systems---and thus information integrity and communication integrity---are of
high importance, even if no encryption is in place. Current encryption
mechanisms like authenticated encryption for protecting confidentiality
typically also come with integrity protection mechanisms. However, when
encryption mechanisms are not activated, communication integrity might not be
protected. One motivation for not activating encryption may be to reduce the
end-to-end latency of a 5G system when additional over-the-top security is in
place. Another motivation is that encryption of telecommunication may not be
permitted in certain countries.
### 6.4.3 Potential security requirements
The following potential security requirements are considered to be essential
for automation in vertical domains in addition to the potential requirements
in Clause 5.3.19.6.
+----------------------+----------------------+----------------------+ | **Reference number** | **Requirement text** | **Comment** | +======================+======================+======================+ | _Security 1_ | Type-b and type-a 5G | For more information | | | networks shall | see \"5G as | | | permit over-the-top | communication | | | end-to-end security | infrastructure\" in | | | protocols in | Clause 6.4.2. | | | general, and for | | | | \"stringent | | | | QoS\"--traffic in | | | | particular. | | +----------------------+----------------------+----------------------+ | _Security 2_ | An automation | For more information | | | application that | see \"Reliance on 5G | | | uses a 5G | security alone\" in | | | communication | Clause 6.4.2. | | | service shall be | | | | able to log and | | | | audit the 5G | | | | security mechanisms | | | | used by the | | | | communication | | | | service [43]. | | +----------------------+----------------------+----------------------+ | _Security 3_ | The 5G system shall | For more information | | | expose an interface | see \"Reliance on 5G | | | that provides to the | security alone\" in | | | automation system | Clause 6.4.2. | | | operator security | | | | logging information | | | | for UEs of the | | | | automation system | | | | (note 1). | | +----------------------+----------------------+----------------------+ | _Security 4_ | Mutual | See requirements | | | authentication of UE | SR1.6 and SR3.8 in | | | and of the 5G | [42] and \"Message | | | system---plus the | integrity\" in | | | integrity of | Clause 6.4.2. | | | transmitted messages | | | | on the user | Also see clauses | | | plane---shall always | 5.1.3 and 5.2.3 in | | | be ensured. (note 2) | [51]. | +----------------------+----------------------+----------------------+ | _Security 5_ | In case a 5G | | | | communication | | | | service in a type-a | | | | network is provided | | | | through a private | | | | slice of a PLMN, | | | | authentication | | | | should still be | | | | possible even when | | | | backhaul connection | | | | is not available | | +----------------------+----------------------+----------------------+ | NOTE 1: The provided | | | | log information | | | | allows the | | | | automation system | | | | operator to check | | | | whether the expected | | | | security features | | | | for 5G access of UEs | | | | of the automation | | | | system are in fact | | | | active. | | | | | | | | NOTE 2: This is also | | | | the case when | | | | communication | | | | confidentiality is | | | | not used, for | | | | instance due to | | | | telecom regulatory | | | | limitations. | | | +----------------------+----------------------+----------------------+
# 7\. Deployment for automation in vertical domains
## 7.1 5G deployment options
The present TR describes use cases for deployment of 5G technology in a wide
range of locations. Application areas broadly range from transportation
including railways, to shipping/logistics, factories of the future, massive
sensor networks, augmented reality, program making, fleet maintenance, smart
grid, and smart cities.
New deployment options support some of these application areas. For example,
private deployments may serve factories and other specific places.
In order to ensure the requirements for availability and reliability can be
met, various deployment options can be utilised. Restricting access to
resources to only those entities belonging to the enterprise, or even a subset
of them, can ensure that specific capabilities are available when needed. For
example, in an automated factory, radio and network resources supporting
specific KPIs for reliability and latency may be restricted for access only by
the robotic controls requiring the specific KPIs. This eliminates competition
for those resources by other UEs that do not have the same reliability and
latency requirements. Similarly, restricting access to enterprise resources by
only authorised UEs eliminates churn from unauthorised UEs (e.g., a person
with a cell phone walking past the factory) attempting access to resources
they will not be able to use.
3GPP supports a range of deployment scenarios based around macro cells and
small cells and 5G is expected to extend the supported topologies whilst
supporting both macro and small cell deployments. Specifically, short
wavelength spectrum such as mm-wave is likely to trigger new types of cell
deployment due to its radio characteristics. 3GPP also has a host of other
deployment tools such as LAA, Dedicated Core Networks, IOPS, etc. which can
offer a great number of deployment options. What is key, is matching the
appropriate network deployment to the required use case or scenario.
As discussed in the present document, 5G communication for automation in
vertical domains requires high levels of availability, reliability, and
maintainability in order to ensure high levels of service accessibility and
productivity. To meet specific end user requirements, specific network
deployments and parameters may be utilised. For example, a sub-surface mining
operation and associated surface support have very different levels of
existing deployment & coverage, propagation environment, existing deployed
communication infrastructure, and required services in comparison to a
nationwide electricity grid with communication services deployed via public
WWAN.
# 8 Merged potential service requirements
## 8.1 Network service performance requirements
### 8.1.1 Remarks
Select parameters listed in the following Clauses are explained in Clause
4.3.4.4 and Annex A. If not mentioned otherwise, the provided potential
requirements are per instantiated communication service.
For a description of the traffic classes below see Clause 4.3.4.4.
NOTE: The service performance requirements of all the use cases described in
Clause 5 can be found
### 8.1.2 Periodic deterministic communication
Characteristic parameter (KPI) Influence quantity Requirement Remark
* * *
Communication service availability End-to-end latency: target value End-to-end
latency: jitter (note) Message size [byte] Transfer interval: target value
Survival time UE speed # of UEs Service area  
> 99,999% \ Transit 1.3, 1.4, 1.5, 1.6, 1.7, 1.8 Control of automated train; 2 UEs per
> train unit 99,9999% to 99,999999% \ ms Transfer interval ≤ 20 m/s ≤ 100 _Factories of the Future 2.1, 2.2, 2.3,
> 2.8, 2.10_ Motion control and control-to-control use cases 99,9999% to
> 99,999999% \ _Factories of the Future 5.1, 5.3, 5.6_ Motion control and control-to-
> control use cases > 99,9999% \ interval 40 to 150 k 1 to 500 ms Transfer interval ≤ 14 m/s ≤ 100 ≤ 1 km2
> _Factories of the future 6.1, 6.2, 6.4, 6.6, 7.1, 7.6; Electric Power
> Distribution 5.1, 5.2, 5.4_ Mobile control panels, mobile robots, and
> differential protection NOTE 1: The jitter interval is symmetric. However,
> only late arrivals count as communication error.
NOTE 2: The time parameters and the message size in row two and three are to
be read as follows. First, a transfer interval value that lies within the
provided interval is chosen. Then the end-to-end latency and the survival time
are inferred. For instance, one chooses 10 ms in row four. In this case the
survival time is also 10 ms, and the end-to-end latency is smaller than 10 ms
and the jitter is 5 ms. Next, the message size is chosen; for instance, 250
kbyte.
### 8.1.3 A-periodic deterministic communication
Characteristic parameter (KPI) Influence quantity Related requirement Remark
* * *
Communication service availability End-to-end latency: target value End-to-end
latency: jitter (note) Service bit rate: user-experienced data rate UE speed #
of UEs  
99,9999% \ 5 Mbit/s
_Factories of the Future 6.2, 6.6_ Mobile control panels with safety
functions; bi-directional communication > 99,999% \ 99,99% \ 99,9% \ 10 Mbit/s ≤ 14 m/s ≤ 100 ≤ 1 km^2^ _Factories of the Future 7.2, 7.6_ Mobile
> robots; real-time video stream ≥ 1 Gbit/s \~ 0 m/s See remark _Mass Transit
> 6.1_ CCTV offload in train stations; typically 1 UE per train used
### 8.1.5 Mixed traffic
**Characteristic parameter (KPI)** **Influence quantity** **Related
requirement** **Remark**
* * *
**Communication service availability** **End-to-end latency: target value**
**Service bit rate: aggregate user-experienced data rate** **UE speed** **# of
UEs**  
> 99,9999% \ Communication between mechanically coupled train segments; angle between
> segments \ 8.3, 8.4, 8.5_ Virtually coupled trains; UE speed: relative speed between
> trains low; separation of UEs ≤ 3 km; 2 UEs per train unit 99,9999999% 16 ms
> _Centralised Power Generation 4.9_ Wind power park control traffic; can be
> realised with aid of wired connections; PER \ \- transfer interval;
>
> \- end-to-end latency;
>
> \- update time.
Note, that jitter is not always explicitly stated in technical documents. In
such a case, the maximum value of the characteristic time parameter needs to
be known. Sometimes, also a minimum value may be given. This time should not
be undershot. A minimum value is only used in particular use cases, for
instance, when putting labels at a specific location on moving objects.
# A.4 Periodicity - cycle time, transfer interval, and update time
In industrial automation applications, sensor data is in many cases gathered
periodically. Accordingly, actuator data is provided periodically, too.
Therefore, the transmission of this data via communication networks is in most
cases also periodic.
NOTE: Periodic communication is usually continuous. After the related
automation functions are activated---e.g. controlling, sensing, and actuating
---periodic data transmission continuous until these functions are stopped. In
extreme cases, continuous operation can extend over a calendar year or more.
Event driven data transmission might be also used. In this case, additional
periodic control messages are introduced for dependability reasons. These are
called heart beat messages or keep alive messages.
The periodic processes within industrial automation applications are often
called _cycle_ and the related interval _cycle time_. Unfortunately, the term
cycle has different meanings depending on the point of view. With respect to
the periodic communication, it is better to use the communication-related
terms transfer interval and update time instead of cycle time.
Periodic communication means that a transmission is repeated. For example, a
transmission occurs every 15 ms. Reasons for a periodical transmission can be
the periodic update of a position or the repeated sensing or monitoring of a
characteristic parameter. Although a transmission of a temperature every 15
minutes is a periodical transmission, too, most periodic transfer intervals in
communication for automation are rather short. The user experienced data rate
of periodic communication of fixed message sizes is the message size divided
by the transfer interval. For instance, for a message size of 40 byte and a
transfer interval of 1 ms, the user experienced data rate is 40 byte/1 ms =
320 kb/s.
The _update time_ is the equivalent of the transfer interval at the receiver.
A periodic transmission is started once and continuous unless a stop command
is provided. Figure A.4-1 illustrates transfer interval and update time.
{width="5.940277777777778in" height="7.428472222222222in"}
Figure A.4-1 -- Transfer interval and update time of periodic communication
NOTE: the update time may experience jitter. This can be caused by varying
waiting times until the actual transmission, for instance, varying waiting
times for the assigned timeslots in TDMA superframes. This has to be taken
into account when requirements are specified and for the assurance of quality
of service.
###### ### Annex B: Communication errors
# B.1 Introduction
IEC 61784-3-3 describes fundamental communication errors which can be
identified for applications with functional safety requirements [25]. The
description of these communication errors refers to field buses. These errors
may however also occur in other communication systems.
# B.2 Corruption
Messages may be corrupted due to errors within a bus participant, due to
errors on the transmission medium, or due to message interference.
> NOTE 1: Message error during transfer is a normal event for any standard
> communication system; such events are detected with high probability at
> receivers by use of a hash function.
>
> NOTE 2: Most communication systems include protocols for recovery from
> transmission errors, so these messages will not be classed as \'loss\' until
> recovery or repetition procedures have failed or are not used.
>
> NOTE 3: If the recovery or repetition procedures take longer than a
> specified deadline, a message is classed as \'unacceptable delay\'.
>
> NOTE 4: In the very low probability event that multiple errors result in a
> new message with correct message structure (for example addressing, length,
> hash function such as CRC, etc.), the message will be accepted and processed
> further. Evaluations based on a message sequence number or a time stamp can
> result in fault classifications such as unintended repetition, incorrect
> sequence, unacceptable delay, insertion [25].
# B.3 Unintended repetition
Due to an error, fault, or interference, not updated messages are repeated at
an incorrect point in time.
NOTE 1: Repetition by the sender is a normal procedure when an expected
acknowledgment/response is not received from a target station, or when a
receiver detects a missing message and asks for it to be resent.
In some cases, the lack of response can be detected, and the message repeated
with minimal delay and no loss of sequence, in other cases the repetition
occurs at a later time and arrives out of sequence with other messages.
NOTE 2: Some field buses use redundancy to send the same message multiple
times or via multiple alternate routes to increase the probability of good
reception [25].
# B.4 Incorrect sequence
Due to an error, fault, or interference, the predefined sequence (for example
natural numbers, time references) associated with messages from a particular
source is incorrect.
NOTE 1: Field bus systems can contain elements that store messages (for
example FIFOs in switches, bridges, routers) or use protocols that can alter
the sequence (for example by allowing messages with high priority to overtake
those with lower priority).
NOTE 2: When multiple sequences are active, such as messages from different
source entities or reports relating to different object types, these sequences
are monitored separately, and errors can be reported for each sequence [25].
# B.5 Loss
Due to an error, fault, or interference, a message or acknowledgment is not
received [25].
# B.6 Unacceptable delay
Messages may be delayed beyond their permitted arrival time window, for
example due to errors in the transmission medium, congested transmission
lines, interference, or due to bus participants sending messages in such a
manner that services are delayed or denied (for example FIFOs in switches,
bridges, routers).
NOTE: In underlying field buses using scheduled or cyclic scans, message
errors can be recovered in the following ways:
> a) immediate repetition;
>
> b) repetition using spare time at the end of the cycle;
>
> c) treating the message as lost and waiting for the next cycle to receive
> the next value.
>
> In case (a), all the following messages in that cycle are slightly delayed,
> while in case (b) only the resent message gets a delay.
>
> Cases (a) and (b) are often not classed as an unacceptable delay.
>
> Case (c) would be classed as an unacceptable delay unless the cycle
> repetition interval is short enough to ensure that delays between cycles are
> not significant and the next cyclic value can be accepted as a replacement
> for the missed previous value [25].
# B.7 Masquerade
Due to a fault or interference, a message is inserted that relates to an
apparently valid source entity, so a non-safety related message may be
received by a safety-related participant, which then treats it as safety
related.
NOTE: Communication systems used for safety-related applications can use
additional checks to detect masquerade, such as authorised source identities
and pass-phrases or cryptography [25].
# B.8 Insertion
Due to a fault or interference, a message is received that relates to an
unexpected or unknown source entity.
NOTE: These messages are additional to the expected message stream, and
because they do not have expected sources, they cannot be classified as
correct, unintended repetition, or incorrect sequence [25].
# B.9 Addressing
Due to a fault or interference, a safety-related message is delivered to the
incorrect safety related participant, which then treats reception as correct
[25].
###### ### Annex C: Communication system errors
# C.1 Hardware errors
Hardware errors encompass the failure or disturbance of the function of
electrical, electronic and programmable components. They are caused by
physical and chemical processes which take place in the environment or in the
system. If the function of a component no longer meets the specification, it
is---as a rule---assessed as unusable and therefore as failed.
Depending on the cause, the scope and the speed of occurrence, distinctions
are made between:
> \- random failure and deterministic failure;
>
> \- total failure and partial failure;
>
> \- sudden failure and degradation failure, and fatigue failure;
>
> \- hardware errors (may, for example, be caused by poor workmanship);
>
> \- components of sub-standard quality,
>
> \- ageing;
>
> \- overloading (e.g. clock frequency, voltage or current);
>
> \- high or low temperatures;
>
> \- frequent temperature changes;
>
> \- impacts, acceleration, or vibration;
>
> \- electrical, magnetic, or electromagnetic fields;
>
> \- ionising radiation.
# C.2 Software errors or program errors
Software errors or program errors encompass the malfunction of computer
programs. Distinctions are made between the following kinds of software error:
> Syntax errors: infringements of the grammatical rules of the programming
> language used. A syntax error prevents compilation of the defective program.
>
> \- Run time errors: designates all kinds of errors which occur during run
> time; example: exceeding the value range or incorrect data types for
> variables on input, when there is no verification or an incorrect version of
> the operating system.
>
> \- Logical errors: occur due to an incorrect entry or incorrect algorithm.
>
> \- Design errors: are errors in the basic concept which are caused by the
> assumption of incorrect requirements or defective software design.
>
> \- Errors as a consequence of physical operating conditions: electromagnetic
> fields, radiation, mechanical stresses, temperature fluctuations, etc. can
> lead to errors even in systems that are working within the specification.
Considerations of software and hardware errors can be found, for example, in
IEC 62439 [26], which also deals with concepts of redundancy in industrial
Ethernet networks, or in IEC 62673 [24], in which a general methodology for
dependability assessment and assurance in communication networks throughout
their life cycles is described.
# C.3 Physical link errors
In wireless transmission, the physical link presents a special challenge. The
environment in which it is used has a great influence on the dependability
parameters. A distinction is made between passive influences (where the signal
transmitted is influenced on the way to the target) and active influences
(where additional signals impair recognition of the useful signal at the
target). The passive influences include the distance (distance-related
attenuation of the signal), metallic obstacles (reflection, diffraction and
refraction of the signal), dielectric obstacles (attenuation of the signal)
and heavy rain or fog (absorption of the signal). Active influences result
from the transmission of electromagnetic waves in the vicinity of the
communication devices. The passive and active influences are referred to as
disturbances. These disturbances have an effect on the physical link and can
be the cause of transmission errors.
###### ### Annex D: Plug and produce (steps c to e)
The text below augments the set of hot-plugging steps described in Clause
5.3.19.3.
**C. Establishment of information communication of the automation application
via an application protocol**
This step includes the establishment of a connection to the automation network
by use of an application protocol, e.g. OPC UA. The sub-steps use the
communication network via the application protocol and are independent of the
underlying communication technology (5G, WLAN, etc.).
**Sub-step** **Event** **Name of process/activity** **Description of
process/activity**
* * *
1 Notification of device availability Connect to the field device Contact the
field device and create a new session context on the field device for the
following interaction. 2 Connected to field device Get field device
certificate Retrieve field device certificate from the field device. This is
usually a different certificate than the one used in Step B (see Clause
5.3.19.3). Also see note 1. 3 Field device certificate available Validate
certificate Contact validation authority and validate the certificate to
ensure valid identity of the field device. 4 Field device certificate
validated Authorise connection Contact authentication service to authorise the
field device management server for reading and writing field device
parameters. NOTE 1: The field device possesses a set of authentication
credentials. The selection of the actually used types of credentials depends
on the automation solution in use (see Clause 5.3.19.2). The Extensible
Authentication Protocol (EAP) is used as a common authentication framework in
order to foster the necessary flexibility, extensibility, and \"future-
proofness\".
**D. Integrate field device into production**
+----------+-----------+---------------------+---------------------+ | **Step** | **Event** | **Name of |** Description of | | | | process/activity**| process/activity** | +==========+===========+=====================+=====================+ | 1 | | Refine network | Configure the | | | | access/visibility | network in order to | | | | | allow communication | | | | | with other field | | | | | devices; required | | | | | to complete the | | | | | current production | | | | | task. Restrict | | | | | communication with | | | | | other field devices | | | | | accordingly. | | | | | | | | | | Adapt or establish | | | | | virtual zones and | | | | | conduits (see | | | | | Clause 5.3.19.2). | +----------+-----------+---------------------+---------------------+
**E. Identity validation fails (alternative service flow)**
This is an alternative scenario to step C (establishment of basic
communication). In contrast to step C, the outcome of the validation of the
certificate is negative.
Sub-steps 1-3 are the same as in step C (establishment of basic
communication).
**Sub-step** **Event** **Name of process/activity** **Description of
process/activity**
* * *
1-3 see C see C see C 4 Field device identity validation failed Disconnect and
discard the field device Disconnect from the field device or quarantine the
field device; mark it as not reliable, notify other systems. See note 2. NOTE
2: Sub-step 4 corresponds to the action of the automation application. Basic
network access, as established in step B (see Clause 5.3.19.3), might not be
affected by these actions. It is advantageous if the 5G network can support
the quarantining of unreliable field devices.
###### ### Annex E: Characteristic parameters for the calculation of the
required communication capacity in a flexible, modular production area
This Annex provides a quantity structure for calculating the required
communication capacity in a flexible, modular production area. The
characteristic parameters were inferred from production areas in car
manufacturing. The particular lifecycle phase addressed is engineering and
commissioning of a flexible, modular production area. Based on Table E-1 and
E-2, a rough estimate of required wireless resources can be inferred. This
calculation is contingent on the layout of the respective production hall
(fixed assets, obstructions...). The areas in the production hall are divided
into operation areas (e.g., assembly stations) and logistic areas. Some of
these areas may overlap. The horizontal hall dimension of a car production,
for instance, is typically 400 m x 250 m and less. The hall is typically
divided in a process area and a two times smaller logistic area. This
description is generic and independent of the communication technology used.
Table E-1: Characteristic parameters for assembly devices in a modular,
flexible production area in car manufacturing
+----------------+----------------+----------------+----------------+ | **Type of | ** |** Maximum # | **Maximum # | | assembly | Characteristic | of active | active devices | | device and | parameters** | devices per 10 | in a LOS space | | activity**| | m x 10 m** | (note 1)__| +================+================+================+================+ | Monorail, | Jitter: see | 2 | 50 | | e.g., flexible | requirement | | | | rail-mounted | Factories of | | | | shuttle system | the Future | | | | for logistic | 6.4. | | | | support | | | | | | During | | | | { | | | | | width="1.58333 | - cycle time | | | | 33333333333in" | 10 ms to | | | | hei | 100 ms; | | | | ght="0.3569444 | | | | | 4444444445in"} | - message | | | | | size 256 byte. | | | +----------------+----------------+----------------+----------------+ | Logistic | Jitter: see | 4 | 450 | | devices, e.g., | Clause | | | | AGVs, root | 5.3.7.6. | | | | trains, mobile | | | | | robots | During | | | | | movement: | | | | { | - cycle time | | | | width="0.85694 | 40 ms to 500 | | | | 44444444444in" | ms; | | | | hei | | | | | ght="0.4402777 | - message | | | | 7777777777in"} | size 256 byte. | | | | { | Operation, | | | | width="0.66666 | i.e., during a | | | | 66666666666in" | stop at an | | | | hei | assembly area: | | | | ght="0.4763888 | event-driven | | | | 8888888886in"} | non | | | | | -deterministic | | | | { | (messages | | | | width="1.41666 | related to | | | | 66666666667in" | order | | | | hei | management, | | | | ght="0.4881944 | asset | | | | 4444444443in"} | communication, | | | | | status | | | | | information | | | | | exchange, | | | | | etc.) | | | | | | | | | | Us | | | | | er-experienced | | | | | data rate 15 | | | | | kbit/s (note | | | | | 2). | | | +----------------+----------------+----------------+----------------+ | Logistic | Jitter: see | 4 | 100 | | devices: | Clause | | | | mobile robots | 5.3.7.6. | | | | for logistic | | | | | support | During | | | | (autonomous | movement: | | | | navigation) | | | | | | - cycle time | | | | { | ms; | | | | width="0.79791 | | | | | 66666666667in" | - message | | | | he | size 256 byte. | | | | ight="0.595138 | | | | | 8888888889in"} | Operation, | | | | | i.e., assembly | | | | | on location: | | | | | interactions | | | | | with other | | | | | assets (e.g., | | | | | loading or | | | | | off-loading | | | | | goods). | | | | | | | | | | - Periodic | | | | | traffic: | | | | | | | | | | - cycle time | | | | | 1 ms to 50 ms; | | | | | | | | | | - message | | | | | size \  99,999% \ 99,99% \ 99,99% \ 99,999% \ 5Mbit/s _Factories of the Future 6.2, 6.6_ Mobile control panels
with safety functions; bi-directional communication 5.3.6 99,9999% to
99,999999% \ 99,9999% \
99,9999% \ 99,9999% \ 99,9999% \ 99,9999% \ 10 Mbit/s ≤ 14 m/s ≤ 100 ≤ 1
km2 _Factories of the Future 7.2, 7.6_ Mobile robots; real-time video stream
5.3.8 > 99,9999% ≤ 10 ms ≤ 100 Mbit/s _Factories of the Future 8.2, 8.10,
8.11_ Massive wireless sensor networks; connection density up to 1/m2;
normally, all connected devices are not sending or receiving messages at the
same time. 5.3.10 > 99,9% \ 99,9% \ 99,999% ≤ 14 m/s _PMSE 3.3, 3.5_ Video
streaming / professional video production; support rotations up to 0,52 rad/s
NOTE: if not stated otherwise per instantiated communication service
###### ##
###### ### Annex G: Properties synopsis of type-a and type-b networks
Property PLMN Type-a network Type-b network
* * *
_Service provided for_ Public access Limited access (e.g., enterprise) Limited
access (e.g., enterprise) _Authentication method, identities, credentials_
3GPP only 3GPP only May use alternative _Roaming support to a PLMN_ In- and
out-bound In- and out-bound None _Interaction with a PLMN (i.e. service
continuity)_ Yes Yes None _Functionality_ Typical PLMN capabilities May have
specific capabilities (e.g., URLLC, TSN) May have specific capabilities (e.g.,
URLLC, TSN)
###### ### Annex H: Considerations on communication service interface
## H.1 Overview
_This section classifies requirements of industrial applications necessary for
5G communication services and provides further classification of the
requirements for different types of communications services._
_It also provides considerations for the definition of communication service
interfaces._
## H.2 Classification of application requirements
_Industrial networks support a wide range of automation functions
(applications) with various requirements on communication. These requirements
are mostly use-case centric. Regardless, there exist general requirements on
communication which may differ in the absolute threshold values of the
measured parameters but not necessarily the parameters themselves._
_The requirements on communication can be classified under four themes:_
_Real-time requirements_
_Non-real-time requirements_
_Safety requirements_
_Integrity requirement_
_Real-time and Non-real-time requirements are communication requirements based
on timeliness of delivery of messages or data between interacting applications
through the 5G network._
_Safety requirements are centred on normal operation of the industrial
application such that failure in communication does not results in
endangerment of neither the user nor result in financial loss (catastrophic
situations). These requirements are mostly focused around dependability
requirements of the communication services and the 5G network. This
requirement defines the ability of the communication service to perform as and
when required. The dependability requirements subsume communication service
availability, reliability and maintainability requirements described in [7]._
_Integrity requirements focus on the ability of the communication service or
network to ensure that information sent via the network stays uncorrupted.
This also falls partly under network security and dependability requirements._
_Under the theme real-time and non-real time, the communication is classified
into deterministic and non-deterministic communication classes._
_The deterministic class represents requirements on traffic with strict
measure on predictable service outcomes on transmission of messages whether
periodic or a-periodic. If a requirement is deterministic and at the same time
periodic, the requirement is called a deterministic periodic requirement.
Likewise, a-periodic requirements are called deterministic a-periodic
requirements. A real-time deterministic requirement therefore subsumes all
periodic and non-periodic requirements that specify stringent and predictable
bounds on the QoS measurements of the timeliness of the transmissions._
_On the other hand, the non-deterministic class subsumes all other types of
requirements that do not specify predictable bounds on the QoS measurements of
timeliness of the transmissions._
_The timeliness parameter should consider end-to-end latency (lateness or
earliness)._
### H.2.1 Characteristic parameters for identification of requirement classes
_The class of deterministic requirements maintains a constant bound on
descriptive parameters and QoS measures throughout its operation. The class of
non-deterministic requirements imposes no bounds pertaining to any descriptive
characteristic and QoS parameter throughout the service operation._
### H.2.2 Descriptive parameters (Dsp)
_Application Relation (AR) and Communication Relation (CR): Most industrial
communications happen between two or more applications [6]. The AR parameter
identifies the expected communication or traffic pattern between interacting
applications. The CR parameter describes the different types of communication
occurring in an AR instance. The CR specifies the unique QoS requirements
between interacting applications or application functions and can be
interpreted during service provisioning for optimized resource allocations
(configuration). The AR/CR is described by three main labels: 1. Unicast 2.
Multicast_One-2-M (one-to-many multicast) 3. Multicast_M-2-1 (many-to-one
multicast). Unicast models one-to-one relations, Multicast_one-2-M models one-
to-many relations, and Multicast_M-2-one models many-to-one relations. An
application may specify additional identifiers for other applications it may
wish to send messages to._
_Transfer interval: measures the time duration for which an application can
repeat a routine task._
_Message size: measures the amount of data (in bytes) that can be transmitted
at an instance of time by an application. The message size for periodic
applications constitutes the sum of all messages that are sent within a
cycle._
_Send time: Measures the time instance for which an application can begin
transmission of its data. This is only required for scheduled applications._
### H.2.3 Quality of Service Parameters (QoSp)
_These are parameters that describe the requirements of the application
relative to the communication service performance._
_End-2-end latency: The time required for a message to be transferred from the
ingress to the egress of a network (reference endpoints). This is measured
relative to the application send-time in the context of periodic and
a-periodic deterministic traffic._
_Communication service availability: The ability of a service to be in a state
to perform as and when required given that all necessary conditions (internal
or external) are provided. This is part of the dependability requirements
[7]._
_Communication service reliability: The ability of a service to perform as
required without failures for a given time interval. This falls under the
dependability safety requirements [7]._
_Update time: The time interval between a message and a follow-up message from
an application measured at a reference interface or reference point. This
applies to periodic communication._
_Survival time: The time duration for which data sent between communicating
applications (transmitting and receiving) can be lost without affecting normal
operation. In periodic communication, survival time is often given as the
number of lost messages. Any value above this threshold results in failure._
## H.3 Requirement classification
### H.3.1 Deterministic periodic
_Deterministic periodic requirements are described by 4-tuples \  where the
presence of bounds for the descriptive parameters transfer-interval, message
size per transfer interval, and QoS identifies this group._
### H.3.2 Deterministic a-periodic
_Deterministic a-periodic requirements are described by the 3-tuples \
 where the absence of descriptive parameter
transfer interval and the presence of bounds for the descriptive parameters
message size and QoS identifies this group._
### H.3.3 Non-deterministic
_This requirement class specifies no bounds on QoS and descriptive parameters
within the duration of its operation. Note, transfer interval is an irrelevant
parameter within this class. Non-deterministic is identified by the absence of
bounds on descriptive as well as QoS parameters._
### H.3.4 Real-time and non-real-time classification
_Requirements are classified as real-time and non-real-time only by the degree
of measure of QoS parameters on timeliness. While the descriptive parameters
can unambiguously identify requirements to be deterministic or non-
deterministic (periodic and/or a-periodic), it is only the QoS measurement
parameters on timeliness that can identify a requirement to be real-time
(critical) and non-real-time (non- critical). The criticality of the
requirements is not an objective fact but a subjective one (use-case or domain
centric). This is particularly the case given that QoS bounds, that are
considered critical in one network domain or use-case, may pass as non-
critical in another. It is therefore advised that real-time and non-real-time
criteria should be defined in the context of a domain or use-case. Domains
include (but are not limited to): industrial automation, data centers, IoT,
telecommunication, etc. Each domain may define a degree of measure for which
QoS measurements can be classified as belonging to real-time (critical) and
non-real-time (non-critical)._
### H.3.5 Other classifications
_Other aspects worth consideration are safety and integrity (security)
requirements. Though these are currently not considered in this work on the
communication service interface, their presence can give a complete picture of
requirements pertaining to accessibility, confidentiality, and data integrity.
Safety aspects are worth considering particularly in industry automation.
Safety classification should be based on communication service availability,
communication service reliability, and maintainability requirements [7]._
_Table H.3.5-1:_
+---------+---------+---------+--------+---------+------+---------+ | Par | | Requi | | type | | | | ameters | | rements | | | | | +=========+=========+=========+========+=========+======+=========+ | | Re | Non-re | Safety | In | | | | | al-time | al-time | | tegrity | | | +---------+---------+---------+--------+---------+------+---------+ | | deter | determ | | | | | | | minitic | inistic | | | | | | | p | a-p | | | | | | | eriodic | eriodic | | | | | +---------+---------+---------+--------+---------+------+---------+ | Message | - | - | - | - | - | num | | size | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | T | - | - | - | - | - | num | | ransfer | | | | | | | | i | | | | | | | | nterval | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | Send | - | - | - | - | - | Time | | time | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | En | - | - | - | - | - | Num | | d-to-en | | | | | | | | latency | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | Avail | - | - | - | - | - | Num | | ability | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | Reli | - | - | - | - | - | Bool | | ability | | | | | | ean/num | +---------+---------+---------+--------+---------+------+---------+ | Update | - | - | - | - | - | Num | | Time | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | S | - | - | - | - | - | Num | | urvival | | | | | | | | Time | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | | | | | | | | +---------+---------+---------+--------+---------+------+---------+ | - r | - not | - may | | | | | | elevant | r | be | | | | | | | elevant | r | | | | | | | | elevant | | | | | +---------+---------+---------+--------+---------+------+---------+
## H.4 Considerations on communication service interface
_The communication service interface contains two functional blocks: 1. Input
block 2. Output block._
### H.4.1 Input block
_This functional block represents the service request description which
includes descriptive parameters (clause H.2.2) and communication service QoS
requirements of the applications. The content of the input block is specified
by the communication service requestor._
_The following are sets of parameters that can be considered for input and
output functional blocks:_
_Reference end points of the communication service: These identify the start
(ingress) and end (egress) interfaces from which a communication service can
be provisioned. The endpoint identifies a single entity or a group of
entities. It can be a physical and/or logical endpoint. It should be known to
both the service provider and service requester(s) (application)._
_Descriptive parameters (Dsp): These are parameters that describe the inherent
attributes of the application to the 5G network. These may include as well QoS
requirements._
_QoS parameters (QoSp): These are parameters that describe the quality of
service requirements of the applications pertaining to the communication
service provided._
_Security parameters (Scp): These are parameters for e.g. authentication,
access control, confidentiality, and data integrity requirements of the
application pertaining to the communication service._
_Safety parameters (Sfp): These are parameters pertaining to safety
requirements of the application._
_Safety parameters and QoS parameters are also used to describe dependability
requirements needed by the application from the communication service._
### H.4.2 Output block
_This functional block represents the service response status and additional
information pertaining to the communication service. The content of the output
block is to be determined by the operation or objective of the service request
defined in clause H.5.1. The contents of the output block are provided by the
communication service provider._
### H.4.3 Communication service interface consideration for 5G systems
_The communication service interface definition is subject to 5G communication
services and capabilities. In scenarios where an application's requirement
does not have a direct mapping to a service parameter or capability as defined
by the 5G vertical requirements, the mapping can be made on the parameters
that subsumes a similar or analogous functionality to the definition and
purpose of the parameter as described by the 5G system._
_The interface consists of mandatory and optional artifacts. The optional
artefacts only apply in the context of the application requirement relating to
descriptive parameters and QoS parameters. The absence of one or more
parameters in the optional artifact group should not result in ambiguity of
the classification and thus the communication service outcome. The mandatory
artifacts on the other hand are always required for service identification,
mapping decisions and responses. The absence of these mandatory artifacts
should result in immediate cancellation of the request made on the
communication service. These artifacts may therefore not at any time be null.
The mandatory artifacts are marked by "m" and optional artifacts by "o"._
### H.4.4 General application and communication service interface format
> _CSIF_{{input Block}, {Output Block}}_
_This presents a highlevel format of a communication service interface._
> _CSIF_Input_Block { Objective\ ,Service Identification\, reference
> Endpoints\, Dsp\, Sfp\, QoSp\, Scp\, AR\,CS\}_
_This represent the input block format._
> _CSIF_Output_Block { ServiceIdentification\ , status\,
> (Dsp,sfp,QoSp)\}_
_This represent the output block format._
_The interface should describe an objective, service identification, service
reference points, descriptive parameters, and communication service
dependability requirements which include safety parameters, QoS parameters,
and security parameters._
_Service identification: This represents any form of identification that can
uniquely identify a communication service request amongst several other
communication services. It should be associated with a communication service
on the provider network._
_Service reference endpoint: This is a unique ID that identifies the endpoints
of a communication service. This can be any form of ID that uniquely
identifies the network ingress and egress interfaces or the user end station
equipment (UE)._
## H.5 Communication service objectives and candidate parameters
_A service interface defines a set of mandatory and optional parameters
required to achieve the desired objective of the communication service
requestor. The service objective may consist of four operations:
disengagement, setup, modification and monitoring. Clauses H.5.1, H.5.2,
H.5.3, and H.5.4 illustrate these operations and parametric consideration
under each operation._
### H.5.1 Communication service disengagement
_This operation triggers a tear down of an existing communication service.
Additional parameters required for such tear down are provided according to
the mandatory and/or optional parameters of the communication service. Upon
successful processing of this operation, applications using this communication
service should not be able any more to send and receive message via the torn
down communication service of the 5G network. Below is an example format for
communication service disengagement._
_CSIF_Disengagement{{CSIF_Input}, {CSIF_Output}}_
> _CSIF_Input{del\ , id\, Scp\, referenceEndpoints\}_
>
> _CSIF_Output{id\ , status\}_
_CSIF_Input: includes the service objective (del) and the service
identification (id). Reference endpoints may be optional. The service id
identifies univocally the communication service that is addressed by the
operation from all other communication services. It maps to a unique
communication service on the network. It should not be null. The security
parameters Scp should not be null. That is, a requester should provide valid
authentication credentials to carry out the operation._
_CSIF_Output: includes the service (id) and the status of the service
operation. The status should provide information regarding positive (success)
and negative (failure) acknowledgement of the operation._
### H.5.2 Communication service setup
_This triggers the configuration or provisioning of a new communication
service. A successful setup operation results in a communication service via
which the involved application can exchange messages. An unsuccessful setup
operation results in a scenario where the application cannot exchange
information via the network. Below is an example format for communication
service setup:_
_CSIF_Setup{{CSIF_Input}, {CSIF_Output}}_
> _CSIF_Input {add\ , id\, Scp\, referenceEndpoint\, {one or more
> (Dsp, QoSp, Sfp)\ }}_
>
> _CSIF_Output{id\ , status\, referenceEndpoints\, Dsp\}_
_CSIF_Input: includes the service objective (add) and the service
identification (id). Reference endpoints may be optional. The service id
identifies univocally the to be setup communication service from all other
communication services. It will map to a unique communication service on the
network and should not be null. Additionally, it may include one or more
parameters from H.2.3._
_CSIF_Output: includes the service (id) and the status of the service
operation. The status provides information regarding positive (success) and
negative (failure) acknowledgement of the operation. In addition, the output
may include one or more parameters from the descriptive parameters. Especially
in the case of scheduled communication, the network may be required to provide
information on the send time or the time an application can commence usage of
the communication service. It may also provide reference end points._
### H.5.3 Communication service modification
_This operation triggers a change request to modify an existing communication
service. This operation specifies a communication service id that can be used
to identify a communication service of the network. Below is an example format
for communication service modification:_
_CSIF_Setup{{CSIF_Input}, {CSIF_Output}}_
> _CSIF_Input{mod\ , id\, Scp\, referenceEndpoint\, {one or more
> (Dsp, QoSp, Sfp)\}}_
>
> _CSIF_Output{id\ , status\, referenceEndpoints\, {one or more (Dsp,
> QoSp, Sfp)\}}_
_CSIF_Input: includes the service objective (mod) and the service
identification (id). The service id identifies univocally the to be modified
communication service from all other communication services. It will map to a
unique communication service on the network and should not be null.
Additionally, it includes the parameters of the modification of the service.
Security parameters (Scp) and id should not be null._
_CSIF_Output: includes the service (id) and the status of the service
operation. The status provides information regarding positive (success) and
negative (failure) acknowledgement of the operation. In addition, the output
may include one or more parameters from the descriptive parameters (Dsp).
Especially in the case of scheduled communication, the network may be required
to provide information of the send time or the time an application can
commence usage of the communication service. It may also provide reference
points._
### H.5.4 Communication service monitoring
_This triggers a query request for statistical information on an existing
communication service. This operation should not result in any changes to the
communication service on the network. The network provides read only
information regarding the specified communication service. Below is an example
format for communication service monitoring:_
_CSIF_Monitoring{{CSIF_Input}, {CSIF_Output}}_
> _CSIF_Input{mon\ , id\, Scp\, {one or more (QoSp,Sfp)\}}_
>
> _CSIF_Output{id\ , Scp\, status\, {one or more (QoSp,Sfp)\}}_
_CSIF_Input: includes the the service objective (mon) and service
identification (id). The service id identifies univocally the communication
service. It maps to a unique communication service of the network and should
not be null. Additionally, it includes the parameters to be monitored.
Security parameters (Scp) and id should not be null. If no QoSp nor Sfp
parameters are specified, all performance metrics related to the communication
service are considered._
_CSIF_Output: includes the service (id) and the status of the service
operation. The status provides information regarding positive (success) and
negative (failure) acknowledgement of the operation. A success results in the
information regarding the communication service for the requested metrics._
#