# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
# Introduction
This document covers use cases and potential requirements for 5G system
support of **Artificial Intelligence (AI)/Machine Learning (ML) model
distribution and transfer (download, upload, updates, etc.)**. The TR on AI/ML
services includes three aspects: AI/ML operation splitting between AI/ML
endpoints, AI/ML model/data distribution and sharing over 5G system,
distributed/Federated Learning over 5G system.
# 1 Scope
**This report captures the study of the use cases and the potential**
performance requirements for 5G system support of **Artificial Intelligence
(AI)/Machine Learning (ML) model distribution and transfer (download, upload,
updates, etc.)** , and identifies traffic characteristics of AI/ML model
distribution, transfer and training for various applications, e.g.
video/speech recognition, robot control, automotive, other verticals.
The aspects addressed include:
  * AI/ML operation splitting between AI/ML endpoints;
  * AI/ML model/data distribution and sharing over 5G system;
  * Distributed/Federated Learning over 5G system.
Study of the AI/ML models themselves are not in the scope of the TR.
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or non‑specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TR 22.891, Feasibility Study on New Services and Markets Technology
Enablers
[3] 3GPP TR 22.863, Feasibility study on new services and markets technology
enablers for enhanced mobile broadband
[4] 3GPP TS 22.261, Service requirements for the 5G system
[5] 3GPP TS 22.104, Service requirements for cyber-physical control
applications in vertical domains
[6] 3GPP TS 23.273, 5G System (5GS) Location Services (LCS); Stage 2
[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "ImageNet classification
with deep convolutional neural networks", in Proc. NIPS, 2012, pp. 1097--1105.
[8] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-
scale image recognition," 2014, arXiv:1409.1556. [Online]. Available:
https://arxiv.org/abs/1409.1556
[9] C. Szegedy, et al., "Going deeper with convolutions", in Proc. CVPR, 2015,
pp. 1-9.
[10] Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, Junshan Zhang, "Edge
intelligence: Paving the last mile of artificial intelligence with edge
computing", Proceeding of the IEEE, 2019, Volume 107, Issue 8.
[11] Jiasi Chen, Xukan Ran, "Deep learning with edge computing: A review",
Proceeding of the IEEE, 2019, Volume 107, Issue 8.
[12] I. Stoica et al., "A Berkeley view of systems challenges for AI", 2017,
arXiv:1712.05855. [Online]. Available: https://arxiv.org/abs/1712.05855
[13] Y. Kang et al., "Neurosurgeon: Collaborative intelligence between the
cloud and mobile edge", ACM SIGPLAN Notices, vol. 52, no. 4, pp. 615--629,
2017.
[14] E. Li, Z. Zhou, and X. Chen, "Edge intelligence: On-demand deep learning
model co-inference with device-edge synergy", in Proc. Workshop Mobile Edge
Commun. (MECOMM), 2018, pp. 31--36.
[15] 3GPP TR 38.913, Study on Scenarios and Requirements for Next Generation
Access Technologies (Release 15)
[16] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, "A survey of research on
cloud robotics and automation," IEEE Transactions on automation science and
engineering, vol. 12, no. 2, pp. 398--409, 2015.
[17] Huaijiang Zhu, Manali Sharma, Kai Pfeiffer, Marco Mezzavilla, Jia Shen,
Sundeep Rangan, and Ludovic Righetti, "Enabling Remote Whole-body Control with
5G Edge Computing", to appear, in Proc. 2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems. Available at:
https://arxiv.org/pdf/2008.08243.pdf
[18] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image
recognition," in Proc. IEEE CVPR, Jun. 2016, pp. 770-778.
[19] A. G. Howard et al., "MobileNets: Efficient convolutional neural networks
for mobile vision applications," 2017, arXiv:1704.04861. [Online]. Available:
https://arxiv.org/abs/1704.04861
[20] B. Taylor, V. S.Marco, W. Wolff, Y. Elkhatib, and Z. Wang, "Adaptive deep
learning model selection on embedded systems," in Proc. ACM LCTES, 2018, pp.
31--43.
[21] G. Shu, W. Liu, X. Zheng, and J. Li, "IF-CNN: Image-aware inference
framework for CNN with the collaboration of mobile devices and cloud", IEEE
Access, vol. 6, pp. 621--633, 2018.
[22] D. Stamoulis et al., "Designing adaptive neural networks for energy-
constrained image classification", in Proc. ACM ICCAD, 2018, Art. no. 23.
[23] Sergey Ioffe and Christian Szegedy. "Batch normalization: Accelerating
deep network training by reducing internal covariate shift", In ICML., 2015.
[24] C.-J. Wu et al., "Machine learning at facebook: Understanding inference
at the edge," in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA),
Feb. 2019, pp. 331--344.
[25] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel S. Emer, "Efficient
processing of deep neural networks: A tutorial and survey", Proceeding of the
IEEE, 2017, Volume 105, Issue 12.
[26] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521,
no. 7553, pp. 436-444, May 2015.
[27] "An All-Neural On-Device Speech Recognizer", March 12, 2019, Posted by
Johan Schalkwyk, https://ai.googleblog.com/2019/03/an-all-neural-on-device-
speech.html
[28] Yanzhang He, etc., "Streaming End-to-end Speech Recognition for Mobile
Devices", 2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP 2019)
[29] 3GPP TS 22.243: \"Speech recognition framework for automated voice
services; Stage 1\".
[30] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Y. Arcas,
"Communication-efficient learning of deep networks from decentralized data",
Proc. of the International Confe rence on Artificial Intelligence and
Statistics, Apr. 20 17. [Online]. Available: https://arxiv.org/abs/1602.05629
[31] "Federated Learning",
https://justmachinelearning.com/2019/03/10/federated-learning/
[32] T. Nishio and R. Yonetani, "Client selection for federated learning with
heterogeneous resources in mobile edge", 2018, arXiv:1804.08333. [Online].
Available: https://arxiv.org/abs/1804.08333
[33] E. Park et al., "Big/little deep neural network for ultra low power
inference", in Proc. 10th Int. Conf. Hardw./Softw. Codesign Syst. Synth.,
2015, pp. 124--132.
[34] Nguyen H. Tran ; Wei Bao ; Albert Zomaya ; Minh N. H. Nguyen ; Choong
Seon Hong, "Federated Learning over Wireless Networks: Optimization Model
Design and Analysis", In proc. IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications
[35] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A
Horowitz, and William J Dally. "EIE: efficient inference engine on compressed
deep neural network", In 43rd International Symposium on Computer
Architecture, IEEE Press, 243--254.
[36] V. Sze, "Efficient Computing for Deep Learning, AI and Robotics," Dept
EECS, MIT, Available online at
https://lexfridman.com/files/slides/2020_01_15_vivienne_sze_efficient_computing.pdf
[37] V. Sze, Y. Chen, "Efficient Processing of Deep Neural Networks: A
Tutorial and Survey" Proc. of IEEE, 2017, Available online at:
https://www.semanticscholar.org/paper/Efficient-Processing-of-Deep-Neural-
Networks%3A-A-and-Sze-Chen/3f116042f50a499ab794bcc1255915bee507413c
[38] Stanford University, CS231n -- Lecture 5-7: CNN, Training NNs, Available
at YouTube.com
[39] S. Han, J. Pool, J. Tran, and W, J. Dally, \"Learning both weights and
connections for efficient neural networks\", NIPS, May 2015
[40] P. A. Merolla, et al., "A million spikingneuron integrated circuit with a
scalable communication network and interface",Science, vol. 345, no. 6197, pp.
668--673, Aug. 2014.
[41] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P.
Kuksa, "Natural language processing (almost) from scratch," J. Mach. Learn.
Res., vol. 12 pp. 2493--2537, Aug. 2011.
[42] T. N. Sainath, A.-R. Mohamed, B. Kingsbury, and B. Ramabhadran, "Deep
convolutionalneural networks for LVCSR", in Proc. ICASSP, 2013, pp. 8614--
8618.
[43] L. P. Kaelbling, M. L. Littman, and A. W. Moore, "Reinforcement learning:
A survey", J. Artif. Intell. Res., vol. 4, no. 1, pp. 237--285, Jan. 1996.
[44] 3 AI Trends for Enterprise Computing. [Online]. Available:
https://www.gartner.com/smarterwithgartner/3-ai-trends-for-enterprise-
computing/
[45] Shiming Ge ; Zhao Luo ; Shengwei Zhao ; Xin Jin ; Xiao-Yu Zhang,
"Compressing deep neural networks for efficient visual inference", In proc.
2017 IEEE International Conference on Multimedia and Expo (ICME)
[46] 3GPP TS 22.186, Enhancement of 3GPP support for V2X scenarios; Stage 1
(Release 16) v16.2.0
[47] "Develop Smaller Speech Recognition Models with NVIDIA's NeMo Framework",
https://developer.nvidia.com/blog/develop-smaller-speech-recognition-models-
with-nvidias-nemo-framework/
[48] 3GPP TS 23.501, System architecture for the 5G System (5GS)
[49] 3GPP TS 23.502, Procedures for the 5G System (5GS)
[50] S. Ren, K. He, R. Girshick, J. Sun, "Faster R-CNN: Towards Real-Time
Object Detection with Region Proposal Networks"
[51] J. Redmon, A. Farhadi, "YOLOv3: An Incremental Improvement"
[52] W. Sun, Z. Chen, "Learned Image Downscaling for Upscaling using Content
Adaptive Resampler"
[53] C. Ledig et al., "Photo-Realistic Single Image Super-Resolution Using a
Generative Adversarial Network"
[54] A. G. Howard et al., "MobileNets: Efficient convolutional neural networks
for mobile vision applications," 2017, arXiv:1704.04861. [Available online:
https://arxiv.org/abs/1704.04861]
[55] SSD-ResNet34 -
https://github.com/IntelAI/models/tree/master/benchmarks/object_detection/tensorflow/ssd-
resnet34
[56] MLCommons Mobile Inference Benchmark v0.7 -
https://mlcommons.org/en/inference-mobile-07/
[57] MASK C-RNN - https://arxiv.org/abs/1703.06870
[58] https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-
recommendation-model/
[59] 3GPP TR 28.809, Study on enhancement of management data analytics
[60] Mingzhe Chen, \"A Joint Learning and Communications Framework for
Federated Learning over Wireless Networks\", Oct 2020
# 3 Definitions, symbols and abbreviations
## 3.1 Definitions
For the purposes of the present document, the terms and definitions given in
3GPP TR 21.905 [1] and the following apply. A term defined in the present
document takes precedence over the definition of the same term, if any, in
3GPP TR 21.905 [1].
**communication service availability** : percentage value of the amount of
time the end-to-end communication service is delivered according to an agreed
QoS, divided by the amount of time the system is expected to deliver the end-
to-end service according to the specification in a specific area.
NOTE 1: This definition was taken from TS 22.261 [4].
**End-to-End Latency:** the time that takes to transfer a given piece of
information from a source to a destination, measured at the communication
interface, from the moment it is transmitted by the source to the moment it is
successfully received at the destination.
NOTE 2: The end point in \"end-to-end\" is assumed to be the communication
service interface.
NOTE 3: This definition was taken from TS 22.261 [4].
**reliability** : in the context of network layer packet transmissions,
percentage value of the amount of sent network layer packets successfully
delivered to a given system entity within the time constraint required by the
targeted service, divided by the total number of sent network layer packets.
NOTE 4: This definition was taken from TS 22.261 [4].
**user experienced data rate:** the minimum data rate required to achieve a
sufficient quality experience, with the exception of scenario for broadcast
like services where the given value is the maximum that is needed.
NOTE 5: This definition was taken from TS 22.261 [4].
## 3.2 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
AI Artificial Intelligence
CNN Convolution Neural Network
DNN Deep Neural Network
FL Federated Learning
GPU Graphics Processing Units
IDC Internet Data Center
ML Machine Learning
# 4 Overview
Artificial Intelligence (AI)/Machine Learning (ML) is being used in a range of
application domains across industry sectors. In mobile communications systems,
mobile devices (e.g. smartphones, automotive, robots) are increasingly
replacing conventional algorithms (e.g. speech recognition, image recognition,
video processing) with AI/ML models to enable applications. The 5G system can
at least support three types of AI/ML operations:
  * AI/ML operation splitting between AI/ML endpoints;
  * AI/ML model/data distribution and sharing over 5G system;
  * Distributed/Federated Learning over 5G system.
The scheme of split AI/ML inference can be depicted as in Figure 4-1. The
AI/ML operation/model is split into multiple parts according to the current
task and environment. The intention is to offload the computation-intensive,
energy-intensive parts to network endpoints, whereas leave the privacy-
sensitive and delay-sensitive parts at the end device. The device executes the
operation/model up to a specific part/layer and then sends the intermediate
data to the network endpoint. The network endpoint executes the remaining
parts/layers and feeds the inference results back to the device.
{width="6.7444444444444445in" height="2.698611111111111in"}
> Figure 4-1. Example of split AI/ML inference
The scheme of AI/ML model distribution can be depicted as in Figure 4-2.
M**ulti-functional mobile terminals might need to switch the AI/ML model in
response to task and environment variations.** The condition of adaptive model
selection is that the models to be selected are available for the mobile
device. However, given the fact that the AI/ML models are becoming
increasingly diverse, and with **the limited storage resource in a UE, it can
be determined to not pre-load all candidate AI/ML models on-board. Online
model distribution (i.e. new model downloading) is needed, in which an AI/ML
model can be distributed from a NW endpoint to the devices when they need it
to adapt to the changed AI/ML tasks and environments. For this purpose, the
model performance at the UE needs to be monitored constantly.**
{width="4.2625in" height="1.9777777777777779in"}
> Figure 4-2. AI/ML model downloading over 5G system
The scheme of Federated Learning (FL) can be depicted as in Figure 4-3. The
cloud server trains a global model by aggregating local models partially-
trained by each end devices. **Within each training iteration, a UE performs
the training based on the model downloaded from the AI server using the local
training data. Then the UE reports the interim training results to the cloud
server via 5G UL channels. The server aggregates the interim training results
from the UEs and updates the global model. The updated global model is then
distributed back to the UEs and the UEs can perform the training for the next
iteration.**
{width="4.384722222222222in" height="2.029166666666667in"}
> Figure 4-3. Federated Learning over 5G system
# 5 Split AI/ML operation between AI/ML endpoints
## 5.1 Split AI/ML image recognition
### 5.1.1 Description
The AI/ML-based mobile applications are increasingly computation-intensive,
memory-consuming and power-consuming. Meanwhile end devices usually have
stringent energy consumption, compute and memory limitations for running a
complete offline AI/ML inference on-board. Many AI/ML applications, e.g. image
recognition, currently intent to offload the inference processing from mobile
devices to internet datacenters (IDC). For example, photos shot by a
smartphone are often processed in a cloud AI/ML server before shown to the
user who shot them. However, the cloud-based AI/ML inference tasks need to
take into account the computation pressure at IDCs, required data rate/latency
and privacy protection requirement.
Image and video are the biggest data on today's Internet. Videos account for
over 70% of daily Internet traffic [4]. Convolutional Neural Network (CNN)
models have be widely used for image/video recognition tasks on mobile
devices, e.g. image classification, image segmentation, object localization
and detection, face authentication, action recognition, enhanced photography,
VR/AR, video games. Meanwhile, CNN model inference requires an intensive
computation and storage resource. For example, AlexNet [7], VGG-16 [8] and
GoogleNet [9] require 724M, 15.5G and 1.43G MACs (multiply-add computation)
respectively for a typical image classification task.
Many references [10-14] have shown that AI/ML inference for image processing
with device-network synergy can alleviate the pressure of computation, memory
footprint, storage, power and required data rate on devices, reduce end-to-end
latency and energy consumption, and improve the end-to-end accuracy,
efficiency and privacy when compared to the local execution approach on either
side. The scheme of split AI/ML image recognition can be depicted in Figure
5.1.1-1. The CNN is split into two parts according to the current image
recognition task and environment. The intention is to offload the computation-
intensive, energy-intensive parts to network server, whereas leave the
privacy-sensitive and delay- sensitive parts at the end device. The device
executes the inference up to a specific CNN layer and sends the intermediate
data to the network server. The network server runs through the remaining CNN
layers. While the model is developed or invocated, the split AI/ML operation
is based on the legacy model.
Due to the characteristics of some algorithms in the model training phase, a
model has a certain degree of robustness[xx-xy]. Therefore, if there are
errors in the intermediate data transmission, the model has a certain
tolerance and can still guarantee the accuracy of the inference results. Since
the inference result needs to be forwarded to the UE, the reliability of the
inference result transmission needs to be guaranteed.
{width="5.164583333333334in" height="1.91875in"}
Figure 5.1.1-1. Example of split AI/ML image recognition
The split AI/ML image recognition algorithms can be analyzed based on the
computation and data characteristics of the layers in the CNN. As shown in
figure 5.1.1-2 and 5.1.1-3 (based on figures adopted from [13]), the
intermediate data size transferred from one CNN layer to the next depends on
the location of the split point. Hence, the required UL data rate is related
to the model split point and the frame rate for the image recognition, as also
observed by [13-14]. For example, assuming images from a video stream with 30
frames per second (FPS) need to be classified, the required UL data rate for
different split points ranges from 4.8 to 65 Mbit/s (listed in Table 5.1.1-1).
The result is based on the 227×227 input images. In case of images with a
higher resolution, higher data rates would be required.
{width="6.697222222222222in" height="1.886111111111111in"}
Figure 5.1.1-2. Layer-level computation/communication resource evaluation for
an AlexNet model
Table 5.1.1-1: Required UL data rate for different split points of AlexNet
model for video recognition \@30FPS
+----------------------+----------------------+----------------------+ | **Split point** | **Approximate output |** Required UL data | | | data size (MByte)**| rate (Mbit/s)** | +----------------------+----------------------+----------------------+ | Candidate split | 0.15 | 36 | | point 0 | | | | | | | | (Cloud-based | | | | inference) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.27 | 65 | | point 1 | | | | | | | | (after pool1 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.17 | 41 | | point 2 | | | | | | | | (after pool2 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.02 | 4.8 | | point 3 | | | | | | | | (after pool5 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | N/A | N/A | | point 4 | | | | | | | | (Device-based | | | | inference) | | | +----------------------+----------------------+----------------------+
VGG-16 is another widely-used CNN model for image recognition. Still assuming
images from a video stream with 30 FPS need to be classified, the required UL
data rate for different split points ranges from 24 to 720 Mbit/s (listed in
Table 5.1.1-2).
{width="6.692361111111111in" height="1.7361111111111112in"}
Figure 5.1.1-3. Layer-level computation/communication resource evaluation for
a VGG-16 model
Table 5.1.1-2: Required user experienced UL data rate for different split
points of VGG-16 model \@30FPS
+----------------------+----------------------+----------------------+ | **Split point** | **Approximate output |** Required user | | | data size (MByte)**| experienced UL data | | | | rate (Mbit/s)** | +----------------------+----------------------+----------------------+ | Candidate split | 0.6 | 145 | | point 0 | | | | | | | | (Cloud-based | | | | inference) | | | +----------------------+----------------------+----------------------+ | Candidate split | 3 | 720 | | point 1 | | | | | | | | (after pool1 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 1.5 | 360 | | point 2 | | | | | | | | (after pool2 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.8 | 192 | | point 3 | | | | | | | | (after pool3 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.5 | 120 | | point 4 | | | | | | | | (after pool4 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | 0.1 | 24 | | point 5 | | | | | | | | (after pool5 layer) | | | +----------------------+----------------------+----------------------+ | Candidate split | N/A | N/A | | point 6 | | | | | | | | (Device-based | | | | inference) | | | +----------------------+----------------------+----------------------+
### 5.1.2 Pre-conditions
The involved AI/ML endpoints (e.g. UE, AI/ML cloud/edge server) run
applications providing the capability of AI/ML model inference for image
recognition, and support the split AI/ML image recognition operation.
The 5G system has the ability to provide 5G network related information to the
AI/ML server.
### 5.1.3 Service Flows
1) The AI/ML based image recognition application is requested by the user to
start recognizing the image/video shot by the UE.
2) Under the determined split mode and split point, the AI/ML based image
recognition application in an involved AI/ML endpoint executes the allocated
part of AI/ML model, and sends the intermediate data to the next endpoint in
the AI/ML pipeline.
3) After all the involved AI/ML endpoints finish the co-inference, the image
recognition results are fed to the user using the results.
4) The AI/ML based image recognition applications in the endpoints perform the
split image recognition until the image recognition task is terminated.
Redo Step 3) and 4) for split mode/point re-selection/switching if needed to
adapt to the changing conditions.
### 5.1.4 Post-conditions
The objects in the input images or videos are recognized and the recognition
accuracy and latency need to be guaranteed.
The image recognition task can be completed under the available computation
and energy resource of the UE. And the consumed the computation, communication
and energy resources over the AI/ML endpoints are optimized.
### 5.1.5 Existing features partly or fully covering the use case
functionality
This use case mainly requires high data rate together with low latency. The
high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of
TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and
50Mbps UL experienced data rate are required in dense urban scenario, and
1Gbps DL experienced data rate and 500Mbps UL experienced data rate are
required in indoor hotspot scenario. As in Table 7.6.1-1 of [4],
cloud/edge/split rendering- related data transmission requires up to 0.1Gbps
data rate with [5-10]ms latency countrywide.
### 5.1.6 Potential New Requirements needed to support the use case
The "image recognition latency" can be defined as the latency from the image
is captured to the recognition results of the image are output to the user
application, which was not specially addressed in [4][15]. Following the
principle of analyzing the latency and data rate requirements of split image
recognition introduced in Section 5.1.1, the image recognition latency is
related to the user application the recognition is used for.
Computer vision and image recognition have been widely used for many important
mobile applications such as object recognition, photo enhancements,
intelligent video surveillance, mobile AR, remote-controlled automotive,
industrial control and robotics. The image recognition is usually a step of
the processing pipeline of the application. And the recognition latency is a
part of the end-to-end latency, as depicted in Figure 5.1.6-1.
{width="3.7493055555555554in" height="1.2541666666666667in"}
Figure 5.1.6-1. Image recognition latency is a part of end-to-end latency
For example, if the image recognition results are just used for the object
recognition e.g. unknown object recognition for smartphone user or criminal
searching in database for intelligent security, it is acceptable that the
image recognition is finished in seconds. If the image recognition result is
used as an input to another time-sensitive application, e.g. AR
display/gaming, remote-controlled automotive, industrial control and robotics,
a much more stringent latency will be required. Based on the end-to-end
latency requirements of the applications, the image recognition latency
requirement can be derived, as listed in Table 5.1.6-1.
#### **5.1.6.1 Potential KPI Requirements**
The potential KPI requirements needed to support the use case include:
[P.R.5.1-001] The 5G system shall support intermediate data uploading for
split image recognition with a maximum latency as given in Table 5.1.6.1-1.
[P.R.5.1-002] The 5G system shall support intermediate data uploading for
split image recognition with a user experienced UL data rate as given in Table
5.1.6.1-1.
[P.R.5.1-003] The 5G system shall support intermediate data uploading for
split image recognition with communication service availability not lower than
99.999 %.
[P.R.5.1-004] The 5G system shall support inference results downloading for
split image recognition with reliability not lower than 99.999 % and
intermediate data uploading for split image recognition with reliability not
lower than 99.9%.
NOTE: Above requirements apply to intermediate data with a maximum size of
0.27MB (for AlexNet model) or 1.5 MB (for VGG-16 model).
Table 5.1.6.1-1: Image recognition latency and UL data rate for intermediate
data uploading
+----------+----------+----------+----------+----------+----------+ | **User | ** |** User | | | | | appli | Latency: | exp | | | | | cation**| m | erienced | | | | | | aximum** | UL data | | | | | | | rate**| | | | +----------+----------+----------+----------+----------+----------+ | |** En | **Image |** Inte | **A | ** | | | d-to-end | rec | rmediate | lexNet** | VGG-16**| | | l | ognition | data | | | | | atency** | l | u | **(Fig. |**(Fig. | | | | atency**| ploading | 5.1.1-1, | 5.1.1-2, | | | | | l | see note | see note | | | | | atency** | 4)**| 4)** | +----------+----------+----------+----------+----------+----------+ | One-shot | Several | \~1s | \~100ms | 1.6\~21 | 8\~2 | | object | seconds | | | .6Mbit/s | 40Mbit/s | | rec | | | | | | | ognition | | | | | | | at | | | | | | | sm | | | | | | | artphone | | | | | | +----------+----------+----------+----------+----------+----------+ | Person | Several | \~1s | \~100ms | 1.6\~21 | 8\~2 | | identi | seconds | | | .6Mbit/s | 40Mbit/s | | fication | | | | | | | in | | | | | | | security | | | | | | | surv | | | | | | | eillance | | | | | | | system | | | | | | +----------+----------+----------+----------+----------+----------+ | Photo | Several | \~1s | \~100ms | 1.6\~21 | 8\~2 | | enha | seconds | | | .6Mbit/s | 40Mbit/s | | ncements | | | | | | | at | | | | | | | sm | | | | | | | artphone | | | | | | +----------+----------+----------+----------+----------+----------+ | Video | Several | 33m | \~10ms | 16\~2 | 80M | | rec | seconds | s\@30FPS | | 16Mbit/s | bit/s\~2 | | ognition | | | | | .4Gbit/s | +----------+----------+----------+----------+----------+----------+ | AR | 7\~15ms | \ NOTE: The listed models above are examples for this use-case and is not an
> exhaustive list.
### 6.2.3 Service Flows
1) Shortly after the start of the concert, Alice, as most of the fans,
launches the camera application on her mobile phone to film the scene and to
get additional information about the band, the individual artists or the songs
or instruments.
2) She points her device's camera towards the scene.
3) The environment is very dark with strong light spots. To be fully
functional and to render the best user experience, the camera application
downloads ad-hoc ML models.
4) The proposed ML models are very performant in this environment but also
very heavy in size.
5) The ML models can be used for the whole capture especially if environment
remains stable. If the environment changes substantially, or better ML models
become available, ML models can be progressively updated accordingly with
respect of some operating rules (like a maximum number of ML update per
second). In all cases, the camera application continues working seamlessly.
6) The audio and video streams are captured, improved in quality, processed to
extract and display additional information, and either stored in real-time on
the mobile phone itself or provided as a live stream.
### 6.2.4 Post-conditions
Alice can see that even in harsh light conditions and with the noisy
background the photos and videos are great, additional information is provided
and all is correctly tagged as requested.
The post-conditions are:
1) Photos and videos are either stored on the mobile phone in the improved
high quality, ready to be uploaded and shared on social media, or directly
shared on social media.
2) Audio recording is high quality with ambient noise reduction, improved
stereo balance.
3) Additional information about band, song/lyrics, instruments, etc. are
displayed on the mobile phone and stored in media recordings' metadata.
4) Alice can visualize additional information and upload the photos and videos
on her social network(s) with the associated tags and information provided by
the models, and also store the above on her personal media server.
### 6.2.5 Existing features partly or fully covering the use case
functionality
The performance requirements for high data rate and traffic density scenarios
are found in TS 22.261 [4] clause 7.1. The scenario Broadband access in a
crowd is relevant for the use case of very dense crowds, for example at
stadiums or concerts. In addition to a very high connection density, the users
can share their experience, i.e. what they see and hear. This can put a higher
requirement on the uplink than the downlink.
This new use case has some requirements on the downlink not covered by the
existing requirements, see table 6.2.5-1.
Table 6.2.5-1. Excerpt from TS 22.261 [4] Table 7.1-1
| Scenario | Experienced data rate (DL) | Experienced data rate (UL) | Area traffic capacity (DL) | Area traffic capacity (UL) | Overall user density | Activity factor | UE speed | Coverage  
---|---|---|---|---|---|---|---|---|---  
4 | Broadband access in a crowd | 25 Mbit/s | 50 Mbit/s | [3,75] Tbit/s/km2 | [7,5] Tbit/s/km2 | [500 000]/km2 | 30% | Pedestrians | Confined area  
### 6.2.6 Potential New Requirements needed to support the use case
#### 6.2.6.1 Introduction
Potential new requirements needed to support the use case result of the
following parameters:
a) AI/ML model size.
b) Accuracy of the model.
c) latency constraint of the application or service.
d) number of concurrent downloads, i.e. number of UEs requesting AI/ML model
downloads within same time window.
The number of concurrent downloads further depends on the density of UE in the
covered area and the covered area size.
The tables 6.2.6.1-1, 6.2.6.1-2 and 6.2.6.1-3 contain KPI for different
aspects of the real-time media editing use case.
Table 6.2.6.1-1. Typical sizes of AI/ML models for the UC
* * *
**AI/ML Model** **Number of parameters (Million)** **Size of the AI/ML model
(MByte)** **Comments** MobileNet 3.2 3.2 8-bit parameters MobileNet 3.2 12.8
32-bit parameters RCAN 15.44 15.44 8-bit parameters DarkNet 20 20 8-bit
parameters Inception v4 41 41 8-bit parameters RCAN 15.44 61.78 32-bit
parameters YOLONet 64 64 8-bit parameters DarkNet 20 80 32-bit parameters
VGGNet 134 134 8-bit parameters Inception v4 41 164 32-bit parameters YOLONet
64 256 32-bit parameters VGGNet 134 536 32-bit parameters
* * *
From table 6.2.6.1-1, AI/ML models currently available to elaborate the use
case have sizes that vary from 3.2 MB to 536 MB.
As indicated in the use case in clause 6.1, it can be noted that the size of
AI/ML models can be reduced prior to transmission with dedicated model
compression techniques. On the contrary, AI/ML models with more neural network
layers and more complex architectures arise to solve more complex tasks and to
improve accuracy. This trend is expected to continue in the coming years.
Typical model sizes in the range of 3 MB to 500 MB appear to be a reasonable
compromise to consider for this use case.
In the following, two categories are considered for AI/ML model sizes:
a) AI/ML model sizes below 64 MB, which can be associated to models optimized
for fast transmission,
b) AI/ML model sizes below 500 MB, which can be associated to models optimized
for higher accuracy.
Maximum latency in function of application or service:
a) Videocall service: end-to-end latency below 200 ms,
b) Video recording, video streaming, and object recognition applications:
latency below 1 s.
User experienced DL data rate results from above AI/ML models' sizes and
maximum latency values are summarized in the table 6.2.6.1-2.
Table 6.2.6.1-2. UC AI/ML model download -- single UE -- KPIs
* * *
**UC model download** **AI/ML model size** **Latency: maximum** **User
experienced DL data rate** Single Model / Single UE [3 MB -- 64 MB] \ For an example of image size 32 x 32 x 3 (32 wide, 32 high, 3 depth/colour
> channels), the weight is 3072; for images with more respectable size 200 x
> 200 x 3 = 120,000 weights; For a simple ConvNet for CIFAR-10 classification,
> the regular Neural Network architecture is INPUTCONVRELUPOOLFC (Input
> layer, convolutional layer, pooling layer and fully-connected layer).
>
> EXAMPLE 1: 32 x 32 x 3 image and six 5 x 5 filters produce a new image of
> size 28 x 28 x 6! = 564,480
>
> EXAMPLE 2 (Language understanding): BERT_{base} with L = 12 (layers), H =
> 768 (hidden size), A = 12 (heads). The number of parameters = 110M
>
> EXAMPLE 3 (Language understanding): BERT_{large} with L = 24 (layers), H =
> 1024 (hidden size), A = 16 (heads). The number of parameters = 340M
>
> EXAMPLE 4: [39] for 8-bit VGG16 Pruned, it can reduce the original size
> (VGG-16 Ref) of 138MB by a factor of (1/13), which size will be
> approximately 10.3MB. Thus, 10.3MB / (GPU time / 2) ≒ 196MB/sec =
> 1.56Gb/sec.
>
> NOTE 2: Compared to raw data, the latency requirement for trained data is
> considered more rigorous as it belongs to the category of data that is more
> readily usable by the related machine (e.g., by UE, or by agent).
Table 7.3.6-2: Performance requirements (KPI vectors)
+-------+-------+-------+-------+-------+-------+-------+-------+ | | **D |** D | ** | | | | | | | escri | escri | Range | | | | | | | ption | ption | (see | | | | | | | in | in | note | | | | | | | Com | AI/ML | 2)**| | | | | | | munic | oper | | | | | | | | ation | ation | | | | | | | | aspe | as | | | | | | | | cts** | pects | | | | | | | | | (all | | | | | | | | | in | | | | | | | | | clusi | | | | | | | | | ve)**| | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | | ** | * |** Se | ** | * | **Se | | | | End-t |_User | rvice | End-t |_ Data | rvice | | | | o-end | e | in | o-end | r | in | | | | la | xperi | terru | late | ate** | terru | | | | tency | enced | ption | ncy**| | ption | | | | (see | data | t | | | t | | | | note | rate | ime** | | | ime**| | | | 1)** | (see | | | | | | | | | note | | | | | | | | | 1)** | | | | | | +-------+-------+-------+-------+-------+-------+-------+-------+ | Lea | \ 3^rd^ | | | with a | | party" and | | | predicted time | | rewording | | | of the event. | | | | | (e.g., alerting | | | | | about traffic | | | | | congestion or | | | | | UE moving | | | | | into/out of a | | | | | different | | | | | geographical | | | | | area). | | | | | | | | | | NOTE: A 3rd | | | | | party AI/ML | | | | | application may | | | | | use the | | | | | prediction | | | | | information to | | | | | minimize | | | | | disturbance in | | | | | the transfer of | | | | | learning data | | | | | and AI/ML model | | | | | data. | | | +-----------+-----------------+----------------+-----------------+ | CPR 8.2-6 | The 5G system | PR 7.4-001 | | | | shall be able | | | | | to support an | | | | | authorised | | | | | 3^rd^ party to | | | | | change | | | | | aggregated QoS | | | | | parameter | | | | | values | | | | | associated with | | | | | a group of UEs, | | | | | e.g. UEs of a | | | | | FL group. | | | +-----------+-----------------+----------------+-----------------+ | CPR 8.2-7 | Subject to user | PR 7.4-001 | | | | consent, | | | | | operator policy | | | | | and regulatory | | | | | requirements, | | | | | the 5G system | | | | | shall be able | | | | | to expose | | | | | information to | | | | | an authorized | | | | | 3^rd^ party to | | | | | support the | | | | | 3^rd^ party to | | | | | determine | | | | | members of a | | | | | group of UEs, | | | | | e.g. UEs of a | | | | | FL group, based | | | | | upon criteria | | | | | provided in the | | | | | request from | | | | | the 3rd party. | | | +-----------+-----------------+----------------+-----------------+ | CPR 8.2-8 | The 5G system | PR.7.4-002 | It is proposed | | | shall be able | | to adopt the | | | to expose | PR.7.4-003 | three potential | | | aggregated QoS | | requirements | | | parameter | PR.7.4-004 | into CPRs | | | values for a | | | | | group of UEs to | | | | | an authorized | | | | | service | | | | | provider. | | | +-----------+-----------------+----------------+-----------------+ | CPR 8.2-9 | The 5G system | PR 7.4-004 | The CPR is | | | shall be able | | changed from | | | to support | | the original PR | | | collection of | | to support the | | | charging | | charging for FL | | | information for | | group where the | | | a group of UEs, | | member may be | | | e.g. UEs of a | | dynamically | | | AI/ML FL group. | | changed. | +-----------+-----------------+----------------+-----------------+
# 9 Conclusion and recommendations
Regarding the Feasibility Study on traffic characteristics and performance
requirements for AI/ML model transfer in 5GS, the TR analyses use cases of
AMMT as follows:
  * Use cases on AI/ML operation splitting between AI/ML endpoints:
    * Split AI/ML image recognition;
    * Enhanced media recognition: Deep Learning Based Vision > Applications;
    * Media quality enhancement: Video streaming upgrade;
    * Split control for robotics;
    * Session-specific model transfer split computation operations.
  * Use cases on AI/ML model/data distribution and sharing over 5G > system:
    * AI/ML model distribution for image recognition;
    * Real time media editing with on-board AI inference;
    * AI/ML model distribution for speech recognition;
    * AI model management as a Service;
    * AI/ML based Automotive Networked Systems;
    * Shared AI/ML model monitoring;
    * Prediction of AI/ML model distribution.
  * Use cases on Distributed/Federated Learning over 5G system:
    * Uncompressed Federated Learning for image recognition;
    * Compressed Federated Learning for image/video processing;
    * Data Transfer Disturbance in Multi-agent multi-device ML > Operations;
    * Group Performance "Flocking" Use Case.
It is recommended to proceed with normative work, and include the potential
new requirements identified by this TR into a new TS. The consolidated
potential requirements in Clause 8 are candidates for the normative
requirements.
###### ### Annex A: Introduction to AI/ML models
## A.1 AI and ML
Artificial Intelligence (AI)/Machine Learning (ML) is being used in a range of
application domains across industry sectors, realizing significant
productivity gains. In particular, in mobile communications systems, mobile
devices (e.g. smartphones, smart vehicles, UAVs, mobile robots) are
increasingly replacing conventional algorithms (e.g. speech recognition,
machine translation, image recognition, video processing, user behaviour
prediction) with AI/ML models to enable applications like enhanced
photography, intelligent personal assistants, VR/AR, video gaming, video
analytics, personalized shopping recommendation, autonomous
driving/navigation, smart home appliances, mobile robotics, mobile medicals,
as well as mobile finance. As forecast by Gartner [44], more than 80% of
enterprise IoT projects will include an AI component by 2022, up from only 10%
today.
**_Artificial Intelligence (AI)_** is the science and engineering to build
intelligent machines capable of carrying out tasks as humans do, defined by
John McCarthy in 1956. The categorization of AI approaches can be illustrated
in figure A.1-1 [25].
{width="2.995138888888889in" height="2.0166666666666666in"}
Figure A.1-1. Categorization of AI/ML approaches (figure adopted from [25])
Within AI is a large subfield called **_machine learning (ML)_** , which was
defined in 1959 by Arthur Samuel as the field of study that gives computers
the ability to learn without being explicitly programmed. Instead of the
laborious and hit-or-miss approach of creating a distinct, custom program to
solve each individual problem in a domain, a single ML algorithm simply needs
to learn, via a process called training, to handle each new problem [25]. Many
ML methodologies as exemplified by decision tree, K-means clustering, and
Bayesian network have been developed to train the model to make
classifications and predictions, based on the data obtained from the real
world [19].
## A.2 Deep neural network
Within the ML field, there is an area that is often referred to as brain-
inspired computation, which is a program aiming to emulate some aspects of how
we understand the brain to operate. Since it is believed that the main
computational elements a human brain are 86 billion neurons, the two subareas
of brain-inspired computation are both inspired by the architecture of a
neuron [25], as shown in figure A.2-1 (a).
Compared to spiking computing approaches, e.g. [3], the more popular ML
approaches are using **_"neural network"_** as the model. Neural networks (NN)
take their inspiration from the notion that a neuron's computation involves a
weighted sum of the input values. But instead of simply outputting the
weighted sum, a NN applies a nonlinear function to generate an output only if
the inputs cross some threshold, as shown in figure A.2-1(a). Figure A.2-1(b)
shows a diagrammatic picture of a computational neural network. The neurons in
the input layer receive some values and propagate them to the neurons in the
middle layer of the network, which is also called a "hidden layer". The
weighted sums from one or more hidden layers are ultimately propagated to the
output layer, which presents the final outputs of the network [25].
{width="2.220138888888889in" height="1.3125in"} {width="2.1381944444444443in"
height="1.9673611111111111in"}
(a) (b)
Figure A.2-1. Architecture of neuron and neural network
Neural networks having more than three layers, i.e., more than one hidden
layer are called **_deep neural networks (DNN)_**. In contrast to the
conventional shallow-structured NN architectures, DNNs, also referred to as
deep learning, made amazing breakthroughs since 2010s in many essential
application areas because they can achieve human-level accuracy or even exceed
human accuracy. Deep learning techniques use supervised and/or unsupervised
strategies to automatically learn hierarchical representations in deep
architectures for classification [26]. With a large number of hidden layers,
the superior performance of DNNs comes from its ability to extract high-level
features from raw sensory data after using statistical learning over a large
amount of data to obtain an effective representation of an input space [25].
In recent years, thanks to the big data obtained from the real world, the
rapidly increased computation capacity and continuously-evolved algorithms,
DNNs have become the most popular ML models for many AI applications.
## A.3 Training and inference
**_Training_** is a process in which a AI/ML model learns to perform its given
tasks, more specifically, by optimizing the value of the weights in the DNN. A
DNN is trained by inputting a training set, which are often correctly-labelled
training samples. Taking image classification for instance, the training set
includes correctly-classified images. When training a network, the weights are
usually updated using a hill-climbing optimization process called gradient
descent. The gradient indicates how the weights should change in order to
reduce the loss (the gap between the correct outputs and the outputs computed
by the DNN based on its current weights). The training process is repeated
iteratively to continuously reduce the overall loss [25]. Until the loss is
below a predefined threshold, the DNN with high precision is obtained.
There are multiple ways to train the network for different targets. The
introduced above is supervised learning which uses the labelled training
samples to find the correct outputs for a task. Unsupervised learning uses the
unlabelled training samples to find the structure or clusters in the data.
Reinforcement learning can be used to output what action the agent should take
next to maximize expected rewards. Transfer learning is to adjust the
previously-trained weights (e.g. weights in a global model) using a new
training set, which is used for a faster or more accurate training for a
personalized model [25].
After a DNN is trained, it can perform its task by computing the output of the
network using the weights determined during the training process, which is
referred to as **_inference_**. In the model inference process, the inputs
from the real world are passed through the DNN. Then the prediction for the
task is output, as shown in Figure A.3-1. For instance, the inputs can be
pixels of an image, sampled amplitudes of an audio wave or the numerical
representation of the state of some system or game. Correspondingly, the
outputs of the network can be a probability that an image contains a
particular object, the probability that an audio sequence contains a
particular word or a bounding box in an image around an object or the proposed
action that should be taken [25].
{width="3.1875in" height="0.9048611111111111in"}
Figure A.3-1. Example of AI/ML inference
The performance of DNNs is gained at the cost of high computational
complexity. Hence more efficient compute engines are often used, e.g. graphics
processing units (GPU) and network processing units (NPU). Compared to the
inference which only involves the feedforward process, the training often
requires more computation and storage resources because it involves also the
backpropagation process [10].
## A.4 Widely-used DNN models and algorithms
Many DNN models have been developed over the past two decades. Each of these
models has a different "network architecture" in terms of number of layers,
layer types, layer shapes (i.e., filter size, number of channels and filters),
and connections between layers [25]. Figure A.4-1 presents three popular
structures of DNNs: multilayer perceptrons (MLPs), convolution neural networks
(CNNs), and recurrent neural networks (RNNs). **_Multilayer perceptrons
(MLP)_** model is the most basic DNN, which is composed of a series of fully
connected layers [41]. In a fully connected layer, all outputs are connected
to all inputs, as shown in Figure A.4-1. Hence MLP requires a significant
amount of storage and computation.
{width="3.576388888888889in" height="1.3347222222222221in"}
Figure A.4-1. MLP DNN model
An approach to limiting the number of weights that contribute to an output is
to calculate the output only using a function of a fixed-size window of
inputs. An extremely popular window-based DNN model uses a convolution
operation to structure the computation, hence is named as **_convolution
neural network (CNN)_** [25]. A CNN is composed of multiple convolutional
layers, as shown in figure A.4-2. Applying various convolutional filters, CNN
models can capture the high-level representation of the input data, making it
popular for image classification [7] and speech recognition [42] tasks. In
recent years, the modern CNN models have dramatically improved the performance
of image classification tasks (e.g., AlexNet [7], VGG network [8], GoogleNet
[9], ResNet [18], MobileNet [19]), as shown in figure A.4-3 [25].
{width="3.8715277777777777in" height="1.6888888888888889in"}
Figure A.4-2. CNN model
{width="3.4298611111111112in" height="1.3729166666666666in"}
Figure A.4-3. Image classification improvements made by CNN models (Figure
adopted from [25])
**_Recurrent neural network (RNN)_** models are another type of DNNs, which
use sequential data feeding. The input of RNN consists of the current input
and the previous samples. Each neuron in an RNN owns an internal memory that
keeps the information of the computation from the previous samples. As shown
in figure A.4-4, the basic unit of RNN is called cell, and further, each cell
consists of layers and a series of cells enables the sequential processing of
RNN models. RNN models have been widely used in the natural language
processing task on mobile devices, e.g., language modelling, machine
translation, question answering, word embedding, and document classification.
{width="3.4715277777777778in" height="1.8472222222222223in"}
Figure A.4-4. RNN model
**_Deep reinforcement learning (DRL)_** is not another DNN model. It is
composed of DNNs and reinforcement learning [43]. As illustrated in figure
A.4-5, the goal of DRL is to create an intelligent agent that can perform
efficient policies to maximize the rewards of long-term tasks with
controllable actions. The typical application of DRL is to solve various
scheduling problems, such as decision problems in games, rate selection of
video transmission, and so on.
{width="3.854861111111111in" height="1.925in"}
Figure A.4-5. Reinforcement learning
###### ### Annex B: General principle of split AI/ML operation between AI/ML
endpoints
In recent years, the AI/ML-based mobile applications are increasingly
computation-intensive, memory-consuming and power-consuming. Meanwhile end
devices usually have stringent energy consumption, compute and memory cost
limitations for running a complete offline AI/ML inference onboard. Hence many
AI/ML applications currently intent to offload the inference processing from
mobile devices to internet datacenters (IDC). Nowadays, even photos shot by a
smartphone are often processed in a cloud AI/ML server before shown to the
user who shot them. However, the cloud-based AI/ML inference tasks need to
take the following factors into account:
1) Computation pressure at IDCs
> As the estimates in [44], by 2021, nearly 850ZB data will be generated by
> end devices per year, whereas the global IDC traffic can only reach 20.6ZB.
> That means most of the data can only be left at network edge (i.e. devices
> and MEC) for AI/ML processing.
2) Required data rate and latency
> Increasing number of AI/ML applications are requiring a high data rate
> meanwhile a low latency for communications between devices and the network,
> e.g. VR/AR, automatic driving, remote-controlled robotics. According to the
> estimates in [44] on device-initiated traffic, offloading all the data to
> cloud servers for AI/ML inference would consume excessive uplink bandwidth.
> This introduces challenging requirements on mobile communications system
> capacity, including for the 5G system.
3) Privacy protection requirement
> The sensing/perception data supporting the inference in the cloud server
> often carry privacy of the end users. Different types of privacy protection
> problems need to be considered in case of either processing the data at the
> device or reporting it to the cloud/edge server. Compared to reporting it to
> the server, keeping the raw data at the device can reduce the pressure of
> privacy protection at the network side.
Hence in many cases, the split AI/ML inference over device and network are
required, to enable the AI/ML applications with conflicting requirements which
are computation-intensive, energy-intensive as well as privacy-sensitive and
delay- sensitive. Many references [10-14] have shown that processing AI/ML
inference with device-network synergy can alleviate the pressure of
computation, memory footprint, storage, power and required data rate on
devices, reduce end-to-end latency and energy consumption, and improve the
end-to-end accuracy and efficiency when compared to the local execution
approach on either side.
The scheme of split AI/ML inference can be depicted in Figure B.1-1. The AI/ML
operation/model is split into multiple parts according to the current task and
environment. The intention is to offload the computation-intensive, energy-
intensive parts to network endpoints, whereas leave the privacy-sensitive and
delay- sensitive parts at the end device. The device executes the
operation/model up to a specific part/layer and sends the intermediate data to
the network endpoint. The network endpoint executes the remaining parts/layers
and feeds the inference results back to the device. It should be noted that,
in the example in Figure B.1-1, the final inference result is output by
network AI/ML endpoint 2. According to actual use case, the inference result
can also be output by other endpoints, e.g. network AI/ML endpoint 1.
{width="6.286111111111111in" height="2.0659722222222223in"}
Figure B.1-1. Example of split AI/ML inference
The modes for split AI/ML operations between device and network are
illustrated in Figure B.1-2. The modes are in general applicable for AI/ML
training as well as inference. In this section, we focus on the inference
processing. Mode a) and b) are traditional schemes operating the AI/ML
inference wholly on one endpoint. Mode c) - g) attempt to split the AI/ML
inference or even the model into multiple parts according to the current task
and environment, to alleviate the pressure of computation, memory/storage,
power and required data rate on both device and NW endpoints, as well as to
obtain a better model inference performance on latency, accuracy and privacy
protection.
  * Mode a): Cloud/edge-based inference
> In this mode (as shown in Figure B.1-2 (a)), the AI/ML model inference is
> only carried out in a cloud or edge server. The device only reports the
> sensing/perception data to the server, and does not need to support AI/ML
> inference operations. The server returns the inference results to the
> device. The advantage of this mode is limiting the device complexity. One
> disadvantage is that the inference performance depends on communications
> data rate and latency between the device and the server. Real-time uploading
> some perception data (e.g. high-resolution video streaming) requires a
> stably-high data rate and some AI/ML services (e.g. remote-controlled
> robotics) requires a stably-low latency, which are challenging to be
> guaranteed in 5G system due to different network coverages. And due to the
> disclosure of the privacy-sensitive data to the network, corresponding
> privacy protection measurements are required.
  * Mode b): Device-based inference
> In this mode (as shown in Figure B.1-2 (b)), the AI/ML model inference is
> performed locally at the mobile device. The advantage is that, during the
> inference process, the device does not need to communicate with the
> cloud/edge server. Another motivation of this mode is preserving the privacy
> at the data source, i.e. the device, although the privacy protection problem
> needs also be considered at the device side. The disadvantage is potentially
> imposing an excessive computation/memory/storage resource to the device. And
> also pointed out by [10], we cannot assume the device always keep all the
> potentially-needed AI/ML models onboard. In some cases, the mobile device
> may need to obtain the AI/ML model from the edge cloud/server, which
> requires a corresponding downloading data rate from the 5G system, as
> introduced in Section 7.
  * Mode c): Device-cloud/edge split inference
> In this mode (as shown in Figure B.1-2 (c)), an AI/ML inference operation or
> model is firstly split into two parts between the device and the cloud/edge
> server according to the current system environmental factors such as
> communications data rate, device resource, and server workload. Then, the
> device will execute the AI/ML inference up to a specific part or the DNN
> model up to a specific layer, and send the intermediate data to the
> cloud/edge server. The server will execute the remaining part/layers and
> sends the inference results to the device. Compared to Mode a) and b), this
> mode is more flexible and more robust to the varying computation resource
> and communications condition. A key link for this mode is to properly select
> the optimum split point between device side and network side based on the
> conditions.
  * Mode d): Edge-cloud split inference
> This mode (as shown in Figure B.1-2 (d)) can be regarded as an extension of
> Mode a). The difference is that the DNN model is executed through edge-cloud
> synergy, rather than executed only on either cloud or edge server. The
> latency-sensitive part of an AI/ML inference operation or layers of an AI/ML
> model can be performed at the edge server. The computation-intensive
> parts/layers that the edge server cannot perform can be offloaded to cloud
> server. The device only reports the sensing/perception data to the server,
> and does not need to support AI/ML inference operations. The intermediate
> data are sent from the edge server to the cloud server. A proper split point
> needs to be selected for an efficient cooperation between edge server and
> cloud server.
  * Mode e): Device-edge-cloud split inference
> This mode (as shown in Figure B.1-2 (e)) is the combination of Mode c) and
> d). An AI/ML inference operation or an AI/ML model is split over the mobile
> device, the edge server and the cloud server. The computation-intensive
> parts/layers of an AI/ML operation/model can be distributed among the cloud
> and/or edge server. The latency-sensitive parts/layers can be performed on
> the device or the edge server. The privacy-sensitive data can be left at the
> device. The device sends the intermediate data outcome from its computation
> to the edge server. And the edge server sends the intermediate data outcome
> from its computation to the cloud server. Two split points need to be
> selected for an efficient cooperation between the device, the edge server
> and the cloud server.
  * Mode f): Device-device split inference
> This mode (as shown in Figure B.1-2 (f)) provides a de-centralized split
> inference. An AI/ML inference operation or model can be split over different
> mobile devices. A group of mobile devices can perform different parts of an
> AI/ML operation or different DNN layers for an inference task, and exchange
> intermediate data between each other. The computation load can be
> distributed over devices meanwhile each device preserves it private
> information locally.
  * Mode g): Device-device-cloud/edge split inference
> Mode g) can be further combined with Mode c) or e). As shown in Figure B.1-2
> (g), an AI/ML inference operation or model is firstly split into the device
> part and network part. Then the device part can be executed in a de-
> centralized manner, i.e. further split over different mobile devices. The
> intermediate data can be sent from one device to the cloud/edge server. Or
> multiple devices can send intermediate data to the cloud/edge server.
{width="0.9645833333333333in" height="1.613888888888889in"}
{width="1.1770833333333333in" height="1.613888888888889in"} {width="0.96875in"
height="1.6180555555555556in"} {width="2.08125in"
height="1.6104166666666666in"}
(a) (b) (c) (d)
{width="2.078472222222222in" height="1.7638888888888888in"} {width="1.9125in"
height="0.7243055555555555in"} {width="1.9638888888888888in"
height="1.9569444444444444in"}
(e) (f) (g)
Figure B.1-2. Split AI/ML inference modes over endpoints
###### ### Annex C: General principle of AI/ML model/data distribution and
sharing over 5G system
For the inference tasks which requires low latency and desires the privacy-
sensitive data to be preserved at the UE side, offline AI/ML inference is
desired, rather than the cloud-based inference. However, an offline AI/ML
model running on mobile devices must have a relatively low computation
complexity and a small storage size. An approach to enabling offline DNN
models on mobile devices is to compress the model to reduce its resource and
computational requirements [27-28, 35, 45]. However, DNN compression will lead
to loss of inference accuracy and adaptivity to various tasks and
environments. A solution to this challenge is to adaptively select the model
for inference from a set of trained models [10]. The model selection is
motivated by the observation that the optimum model for inference depends on
the input data and the precision requirement [20][45]. M**ulti-functional
mobile terminals usually need to switch the AI/ML model in response to task
and environment variations.**
The condition of adaptive model selection is that the models to be selected
have been available for the mobile device. However, given the fact that the
DNN models are becoming increasingly diverse, and with **the limited storage
resource in a UE, it is unfeasible to pre-load all candidate AI/ML models on-
board. Online model distribution (i.e. new model downloading) or online
transfer learning (i.e. partial model updating) is needed. As illustrated in
Figure C.1-1, an AI/ML model can be distributed from a NW endpoint to the
devices when they need it to adapt to the changed AI/ML tasks and
environments.**
{width="4.196527777777778in" height="2.0930555555555554in"}
Figure C.1-1. AI/ML model downloading over 5G system
**The model to be distributed can be determined in two ways: Requested by a
device, or controlled by a network server. The condition of the first
mechanism is that the device can make the model selection/re-selection
decision based on the understanding to the oncoming AI/ML task, environment
and the list of the models available at the network server. As shown in Figure
C.1-2, a model selector on the device is trained to select the best DNN for
different input data.**
**The model selector is trained to determine the optimum DNN model for a new,
unseen input using a set of automatically tuned features of the DNN model
input, and taking into consideration the precision constraint and the
characteristics of the input.**
{width="5.093055555555556in" height="0.6513888888888889in"}
Figure C.1-2. AI/ML model selection and downloading
**The data rate for downloading the needed models depends on the following
factors:**
  * Size of the model
> This depends on different AI/ML applications. Along with the increasing
> performance requirements to AI/ML operations, the sizes of the models also
> keep increasing, although model compression techniques are under
> improvements.
  * Required downloading latency
> This depends on how fast the model needs to be ready at the device. It is
> impacted by the extent to which the oncoming application can be predicted.
> Considering the unpredictability of user behaviour and typical waiting time
> a user can tolerate, the downloading of the AI/ML model needs to be finished
> in seconds or even in milliseconds. Different from a streamed video which
> can be played when a small portion is buffered, a DNN model can only be used
> until the whole model is completely downloaded.
In should be noted that the network-based and split AI/ML inference often
requires a high and constant uplink data rate for continuously offloading
sensing/intermediate data to the cloud/edge server. On the contrary, AI/ML
model distribution mainly requires a high downlink data rate in a burst. This
makes model distribution more suitable to the downlink-dominant (e.g.
employing a high DL-to-UL ratio) mobile communications systems or systems with
an unstable coverage. Of course the condition is that the mobile device's
computation resource can afford the on-board execution of the AI/ML model. If
the computation load is beyond the device's capability, the network-based or
split inference has to be adopted.
###### ### Annex D: General principle of Distributed/Federated Learning over
5G system
With continuously improving capability of cameras and sensors on mobile
devices, valuable training data, which are essential for AI/ML model training,
are increasingly generated on the devices. For many AI/ML tasks, the
fragmented data collected by mobile devices are essential for training a
global model. In the traditional approaches, the training data gathered by
mobile devices are centralized to the cloud datacenter for a centralized
training.
However, an AI/ML model training often requires a large data set and
significant computational resources for multiple weight-update iterations.
Nowadays, most of the AI/ML model training tasks are performed in the power
cloud datacenters since the resource consumption of the training phase
significantly overweights the inference phase. In many cases, training a DNN
model still takes several hours to multiple days. However, cloud-based
training means that the enormous amount of training data should be shipped
from devices to the cloud, incurring prohibitive communication overhead as
well as the data privacy pressure at the network side [10]. Similar to the
split AI/ML inference introduced in Annex B, AI/ML model training tasks can
also work in a cloud-device coordination manner. Distributed Learning and
Federated Learning are examples in this manner.
In Distributed Learning mode, as shown in Figure D.1-1, each computing node
trains its own DNN model locally with local data, which preserves private
information locally. To obtain the global DNN model by sharing local training
improvement, nodes in the network will communicate with each other to exchange
the local model updates. In this mode, the global DNN model can be trained
without the intervention of the cloud datacenter [10].
{width="2.11875in" height="1.6145833333333333in"}
Figure D.1-1. Distributed Learning
In Federated Learning (FL) mode, the cloud server trains a global model by
aggregating local models partially-trained by each end devices. **The most
agreeable Federated Learning algorithm so far is based on the iterative model
averaging [30]. As depicted in Figure D.1-2, within each training iteration, a
UE performs the training based on the model downloaded from the AI server
using the local training data. Then the UE reports the interim training
results (e.g., gradients for the DNN) to the cloud server via 5G UL channels.
The server aggregates the gradients from the UEs, and updates the global
model. Next, the updated global model is distributed to the UEs via 5G DL
channels. Then the UEs can perform the training for the next iteration.**
{width="4.384722222222222in" height="2.029166666666667in"}
Figure D.1-2. Federated Learning over 5G system
The performance requirements for Distributed/Federated Learning are listed
below. The requirements to 5G communication links (e.g. data rate, latency,
reliability) can be derived from the following requirements.
  * Training loss:
> Training loss is the gap between the correct outputs and the outputs
> computed by the DNN model which indicates how well the trained DNN model
> fits the training data. Aim of training task is to minimize the training
> loss. Training loss is mainly affected by the quality of the training data
> and the efficiency of the training methods, i.e. whether the meaning of
> training data can be fully and properly explored. For Federated Learning,
> only when the valuable local training data can be fully learned in the
> duration of the iteration and the local training updates can be correctly
> reported to the cloud server within the target duration, the training loss
> can be minimized.
>
> This implies that the requirements to the devices joining in the training
> process on the achievable UL data rate, latency and reliability for
> reporting the trained updates, and the achievable UL data rate, latency and
> reliability for distributing the model for training in next iteration. And
> to minimize the training loss with device heterogeneity (in computation and
> communication performance), training device selection and training
> configuration are needed before the training is performed in an iteration
> [31, 48] (will be introduced later in this section). The QoS of the relevant
> controlling messages, e.g. for training request, training resource
> reporting, training device selection, training configuration, and resource
> allocation for the training updates reporting, also needs to be guaranteed.
  * Training latency:
> Training latency is one of the most fundamental performance metrics of AI/ML
> model training task since it directly influences when the trained model is
> available for use. Nowadays, cloud-based training often takes several hours
> to multiple days. The latency of the Distributed/Federated Learning process
> would take even a longer time if the computation latency or the
> communication latency is not minimized.
>
> The latency of the Distributed/Federated Learning process is determined by
> the convergence rate (e.g. number of iterations before the training process
> converges to a consensus) and the latency of each iteration which consists
> of computation latency and communication latency. The computation latency
> depends on the computation/memory resource available on training devices.
> The computation latency depends on the DL data rate available for model
> distribution and UL data rate available for trained model updating. The
> latency of the whole training process is determined by the larger one
> between the computation latency and the communication latency. Hence the
> latencies of the computation and communication links need to be
> cooperatively minimized. If the communication latency cannot match to the
> computation latency, the communication link will become the bottleneck and
> prolong the whole training process.
>
> For synchronous Federated Learning, in each iteration, the training latency
> is determined by the last device that reports its training update because
> the federated aggregation can be finished when all needed training updates
> are correctly gathered. That means the device heterogeneity (in computation
> and communication performance) will also highly impact the overall training
> latency. Rather than requiring the UL transmission latency of a specific
> device, the overall latency required for all training devices to upload the
> training updates (device-group latency) needs to be defined. And the QoS of
> the controlling messages for minimizing the device-group latency, e.g. for
> training request, training resource reporting, training device selection,
> training configuration, and resource allocation for the training updates
> reporting, also needs to be guaranteed.
Energy efficiency:
> For Distributed/Federated Learning, both the computation and communication
> processes consume considerable energy. The Federated Learning architecture
> and protocol should also consider the power constraints on the training
> devices and the energy efficiency on device as well as the network side.
  * Privacy:
> When training the DNN model by using the data originated at a massive of end
> devices, the raw data or intermediate data should be transferred out of the
> end devices. Compared to reporting it to the cloud/edge server, preserving
> privacy at the end devices can reduce the pressure of privacy protection at
> network side. For example, Federated Learning is an agreeable approach to
> avoid uploading the raw data from device to network, as a cloud-based
> training requires.
#