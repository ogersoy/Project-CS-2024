# Foreword
This Technical Report has been produced by the 3rd Generation Partnership
Project (3GPP).
The contents of the present document are subject to continuing work within the
TSG and may change following formal TSG approval. Should the TSG modify the
contents of the present document, it will be re-released by the TSG with an
identifying change of release date and an increase in version number as
follows:
Version x.y.z
where:
x the first digit:
1 presented to TSG for information;
2 presented to TSG for approval;
3 or greater indicates TSG approved document under change control.
y the second digit is incremented for all changes of substance, i.e. technical
enhancements, corrections, updates, etc.
z the third digit is incremented when editorial only changes have been
incorporated in the document.
In the present document, modal verbs have the following meanings:
**shall** indicates a mandatory requirement to do something
**shall not** indicates an interdiction (prohibition) to do something
The constructions \"shall\" and \"shall not\" are confined to the context of
normative provisions, and do not appear in Technical Reports.
The constructions \"must\" and \"must not\" are not used as substitutes for
\"shall\" and \"shall not\". Their use is avoided insofar as possible, and
they are not used in a normative context except in a direct citation from an
external, referenced, non-3GPP document, or so as to maintain continuity of
style when extending or modifying the provisions of such a referenced
document.
**should** indicates a recommendation to do something
**should not** indicates a recommendation not to do something
**may** indicates permission to do something
**need not** indicates permission not to do something
The construction \"may not\" is ambiguous and is not used in normative
elements. The unambiguous constructions \"might not\" or \"shall not\" are
used instead, depending upon the meaning intended.
**can** indicates that something is possible
**cannot** indicates that something is impossible
The constructions \"can\" and \"cannot\" are not substitutes for \"may\" and
\"need not\".
**will** indicates that something is certain or expected to happen as a result
of action taken by an agency the behaviour of which is outside the scope of
the present document
**will not** indicates that something is certain or expected not to happen as
a result of action taken by an agency the behaviour of which is outside the
scope of the present document
**might** indicates a likelihood that something will happen as a result of
action taken by some agency the behaviour of which is outside the scope of the
present document
**might not** indicates a likelihood that something will not happen as a
result of action taken by some agency the behaviour of which is outside the
scope of the present document
In addition:
**is** (or any other verb in the indicative mood) indicates a statement of
fact
**is not** (or any other negative verb in the indicative mood) indicates a
statement of fact
The constructions \"is\" and \"is not\" do not indicate requirements.
# Introduction
For initial AR experiences, an expected prominent setup will be wirelessly
tethered of AR glasses, typically connected to a 5G UE. The tethering
technology between a UE and an AR Glasses may use different connectivity, for
example provided through WiFi or 5G sidelink. Different architectures for
tethering result in different QoS requirements, session handling properties,
and also media handling aspects. For enhanced end-to-end QoS and/or QoE, AR
glasses may need to provide functions beyond the basic tethering connectivity
function. Generally, smartly tethering AR glasses is an important aspect for
successful AR experiences using the 5G System.
Based on this, the present document introduces Smartly Tethering AR Glasses
(SmarTAR) in such user experience may be maximized using the 5G System.
# 1 Scope
The present document addresses architectures, QoS and media handling aspects
of when tethering AR Glasses to 5G UEs based on initial discussions in TR
26.998 [2]. In particular, the following aspects are in scope:
\- Definition of different tethering architectures for AR Glasses including 5G
sidelink and non-5G access based on existing 5G System functionalities
\- Documentation of the relationship between AR Glasses tethering and AR
glasses considered as PIN (Personal IoT Network) elements according to TR
22.859 [3] and the derived service requirements in TS 22.261 [4].
\- Documentation of end-to-end call flows for session setup and handling
\- Identification media handling aspects of different tethering architectures
\- Identification of end-to-end QoS-handling for different tethering
architectures and define supporting mechanisms to compensate for the non-5G
link between the UE and the AR glasses
\- Providing recommendations for suitable architectures to meet typical AR
requirements such as low power consumption, low latency, high bitrates,
security and reliability.
\- Collaboration with relevant other 3GPP groups on this matter
\- Identification of potential follow-up work on this matter
# 2 References
The following documents contain provisions which, through reference in this
text, constitute provisions of the present document.
\- References are either specific (identified by date of publication, edition
number, version number, etc.) or nonâ€‘specific.
\- For a specific reference, subsequent revisions do not apply.
\- For a non-specific reference, the latest version applies. In the case of a
reference to a 3GPP document (including a GSM document), a non-specific
reference implicitly refers to the latest version of that document _in the
same Release as the present document_.
[1] 3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".
[2] 3GPP TR 26.998: \"Support of 5G glass-type Augmented Reality / Mixed
Reality (AR/MR) devices\".
[3] 3GPP TR 22.859: \"Study on Personal Internet of Things (PIoT) networks\".
[4] 3GPP TS 22.261: \"Service requirements for the 5G system\".
[5] 3GPP TR 23.700-78: \"Study on Application layer support for Personal IoT
and Residential Networks\".
[6] 3GPP TR 23.700-88: \"Study on architecture enhancements for Personal IoT
Network (PIN)\"
[7] 3GPP TS 23.304: \"Proximity based Services (ProSe) in the 5G System
(5GS)\".
[8] 3GPP TS 23.287: \"Architecture enhancements for 5G System (5GS) to support
Vehicle-to-Everything (V2X) services\".
[9] 3GPP TS 23.501: \"System Architecture for the 5G System; Stage 2\".
[10] The Khronos Group, \"The OpenXR Specification\",
https://registry.khronos.org/OpenXR/specs/1.0/html/xrspec.html
[11] 3GPP TS26.531, \"Data Collection and Reporting; General Description and
Architecture\", V17.1.0, Sept. 2022.
[12] IETF RFC 792, Internet Control Message Protocol, 1981.
[13] IEEE-1588-2019, \"Standard for a Precision Clock Synchronization Protocol
for Networked Measurement and Control Systems\".
[14] IETF RFC 8285, A General Mechanism for RTP Header Extensions, 2017.
[15] IETF draft-mlichvar-ntp-ntpv5-07, Network Time Protocol Version 5, March
2023.
[16] IETF RFC6051, Rapid Synchronisation of RTP Flows, 2010.
[17] 3GPP TS 26.119: \"Media Capabilities for Augmented Reality\".
[18] 3GPP TS 26.522: \"5G Real-time Media Transport Protocol Configurations\".
# 3 Definitions of terms, symbols and abbreviations
## 3.1 Terms
For the purposes of the present document, the terms given in 3GPP TR 21.905
[1] and the following apply. A term defined in the present document takes
precedence over the definition of the same term, if any, in 3GPP TR 21.905
[1].
## 3.2 Symbols
For the purposes of the present document, the following symbols apply:
## 3.3 Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR
21.905 [1] and the following apply. An abbreviation defined in the present
document takes precedence over the definition of the same abbreviation, if
any, in 3GPP TR 21.905 [1].
API Application Programming Interface
AR Augmented Reality
BLE Bluetooth Low Energy
DASH Dynamic Adaptive Streaming over HTTP
FFS For Further Study
FLUS Framework for Live Uplink Streaming
GPS Global Positioning System
GPU Graphics Processing Unit
GTP GPRS Tunneling Protocol
ICMP Internet Control Message Protocol
MAF Media Access Function
MCS Modulation and Coding Scheme
MSH Media Session Handler
NAT Network Address Translation
NEF Network Exposure Function
NTP Network Time Protocol
PCF Policy Control Function
PCM Pulse Code Modulation
PDU Protocol Data Unit
PEGC PIN Element with Gateway Capability
PEMC PIN Element with Management Capability
PIN Personal IoT Network
PINAPP Personal IoT Network Application
PQI PC5 QoS Identifier
PSA PDU Session Anchor
PTP Precision Time Protocol
QoS Quality-of-Service
QFI QoS Flow Identifier
RAN Radio Access Network
RGB Red-Green-Blue
RGBA Red-Green-Blue-Alpha
RTP Real-Time Protocol
RTT Round-Trip Time
SMF Session Management Function
SRTP Secure RTP
STAR Standalone AR glasses
UDP User Datagram Protocol
UPF User Plane Function
URL Universal Resource Locator
URLLC Ultra Reliable and Low Latency Communications
WLAR WireLess tethered AR glasses
WTAR Wired Tethered AR glasses
# 4 Motivation and Background
## 4.1 Summary of TR 26.998
The 5G WireLess Tethered AR UE is introduced in TR26.998 [2] as one functional
structural device type. It is further split into two sub-types, Type 3a: 5G
Split Rendering WireLess Tethered AR UE and Type 3b: 5G Relay WireLess
Tethered AR UE. For Type 3a, the tethering 5G Phone provides both the network
connectivity and the rendering/pre-rendering assistant functionalities to the
AR glasses. For Type 3b, the tethering 5G Phone only provides the IP network
connectivity to the AR glasses.
Table 4.1.1-1 provides an overview of functionality splitting for Wireless
Tethered AR Glasses devices.
NOTE: The 5G Phone, as a tethering device, initiates the \"tether\" action to
an AR Glasses which belongs to the tethered device.
Table 4.1-1: Functionality splitting for Wireless Tethered AR Glasses device
+----------------------+----------------------+----------------------+ | Functionality | Type 3a: Split | Type 3b: Relay WLAR | | splitting | Rendering WLAR UE | UE | +======================+======================+======================+ | 5G connectivity | Tethering device | Tethering device | +----------------------+----------------------+----------------------+ | Media Access | Tethering device | AR Glass | | Function | | | | | | | | User Plane (Media | | | | Client) | | | +----------------------+----------------------+----------------------+ | Media Access | Tethering device | Tethering device | | Function | | | | | | | | Control Plane (Media | | | | Session Hander) | | | +----------------------+----------------------+----------------------+ | AR runtime | Local and uses from | Local and uses from | | | sensors, audio | sensors, audio | | | inputs or video | inputs or video | | | inputs, but may be | inputs. | | | assisted by | | | | functionalities on | | | | tethering devices. | | +----------------------+----------------------+----------------------+ | Media Processing | May be done on the | Either done on the | | | AR glasses and | Glasses device or it | | | energy intensive | is split with the | | | AR/MR media | network. | | | processing may be | | | | done on the AR/MR | | | | tethering device or | | | | split. | | +----------------------+----------------------+----------------------+
Different from other types of AR UE, the end-to-end path includes one more
wireless/wireline tethering link between AR Glasses and the tethering 5G
Phone. In order to fulfil the end-to-end QoS requirements for the AR session,
the AR UE need to acquire the tethering link status via measurement tests or
empirical values, and takes it into account when determining the QoS for the
5G system link. With the tethering link status, the Media Access Function may
communicate with AF for dynamic QoS policy adjustment accordingly.
## 4.2 Guiding Use Cases
### 4.2.1 Introduction
This clause provides several guiding use cases in order to simplify the
analysis in the course of this report.
### 4.2.2 Cycling Glasses
Assume the following use case: Thomas got new AR glasses for his 51^st^
birthday. He plays around and can connect the glasses to his mobile phone
using a dedicated WiFi connection. Thomas wants to go cycling so he uses the
AR glasses. When Thomas goes cycling, he does use four main apps in parallel:
\- A navigation app that provides directions on where to go
\- A fitness app that tracks all different kinds of environmental as well as
body data
\- A music app to play his favourite live music radio station
\- A live video sharing app such that his family and friends can track him and
cheer him
In addition, as Thomas is a nerd, he also loves to get notifications from
major apps such as on social media, work e-mails, Teams notifications from
Imed, and so on. All these applications run on the phone and the phone sends
notifications to the peripherals. The notifications create certain
A(udio)/V(isual)/H(apical) signals. Of these additional apps, you may even
then kick off and create and render additional information on the peripherals
(headset, glass). Now wearing a Glasses during cycling, including an audio
headset and some haptical notification.
Thomas's wife is scared when he wears smart glasses on his bike and she wants
to be absolutely sure that he does not get motion sick on the bike and
possibly falls off.
In addition, the navigation app uses the camera feeds from the Glasses and GPS
location of the phone to send exact navigation instructions. The camera feeds
may have to use an edge for exact detection of the environment and may make
the AR glasses provide overlays and information on the environment. But at the
same time it will need to send notifications from other apps. These
notifications may be any sense or combined senses. So the phone needs to
render eye buffers, audio output and provide haptical information to the
glasses.
## 4.3 PIN (Personal IoT Network)
4.3.1 Media share within PINs use case
\"Personal IoT networks\" (PINs) in 3GPP TR 22.859 [3] are of a type of
private network typically consisting of a user smartphone, wearables and home
automation devices. These networks are very different to commercial IoT
device, they are usually less rugged, most highly battery constrained and
lifespan of the battery typically a couple of days or weeks. User plane
traffic typically stays with a constrained environment, around the body or in
the home i.e., within the PIN. Notifications can be received on smartphones
that events have occurred within the PIN. A typical wearable PIN is depicted
in Figure 4.3.1-1.
Figure 4.3.1-1: Wearable PINs (from [3])
One use case in clause 5.3 of [3] is called \"Media share within PINs\". A
sub-use case to be noted consists of watching a movie on a smartphone and then
switching to watching the movie on AR glasses.
3GPP TR 22.859 [3] also includes Potential Consolidated Requirements to be
considered for 5GS evolution. These potential requirements include control
plane requirements such as Device and Service Discovery, Privacy and Security,
PIN Management and Charging. It also includes user plane type of potential
requirements such as Gateway capability, Direct Communications, Connectivity
and QoS.
4.3.2 Definitions and service requirements of PIN
According to Service requirements for the 5G system 3GPP TS 22.261 [4], _A
Personal IoT Network (PIN) consists of PIN Elements that communicate using PIN
Direct Connection or direct network connection and is managed locally (using a
PIN Element with Management Capability). Examples of PINs include networks of
wearables and smart home / smart office equipment. Via a PIN Element with
Gateway Capability, PIN Elements have access to the 5G network services and
can communicate with PIN Elements that are not within range to use PIN Direct
Connection. A PIN includes at least one PIN Element with Gateway Capability
and at least one PIN Element with Management Capability._
PIN Element related definitions can be found from TS 22.261 [4]:
_**PIN Element** : UE or non-3GPP device that can communicate within a PIN._
_**PIN Element with Gateway Capability:** a UE PIN Element that has the
ability to provide connectivity to and from the 5G network for other PIN
Elements._
_NOTE 5C: A PIN Element can have both PIN management capability and Gateway
Capability._
_**PIN Element with Management Capability:** A PIN Element with capability to
manage the PIN._
Service requirements for the 5G system TS 22.261 [4] include both control and
user plane requirements such as:
\- General: including user plane connectivity requirements such as \"the 5G
system shall support a data path not traversing the 5G network for intra-PIN
communications via direct connections.\" And \"The communication path between
PIN Elements may include licensed and unlicensed spectrum as well as 3GPP and
non-3GPP access.\"
\- Gateways: including e.g. \"The 5G system shall be able to support access to
the 5G network and its services via at least one gateway (i.e. PIN Element
with Gateway Capability [...]) for authorised UEs and authorised non-3GPP
devices in a PIN[...].\"
\- Operation without 5G core network connectivity: \"The 5G system shall allow
PIN Elements to communicate when there is no connectivity between a PIN
Element with Gateway Capability and a 5G network. For a Public Safety PIN
licensed spectrum may be used for PIN direct communications otherwise
unlicensed spectrum shall be used.\"
\- PIN discovery: e.g. \"The 5G system shall enable a UE or non-3GPP device in
a [...] PIN to discover other UEs or non-3GPP devices within the same [...]
PIN subject to access rights.\" and PIN element capability discovery.
NOTE: Although discovery is in the TS 22.261 [4], the agreement for normative
work is to divide discovery into two layers: if transport layer is based on
3GPP PC5, the existing procedures defined for 5G ProSe Direct Communication
are re-used. If the transport layer functionality based on non-3GPP
communication specification is outside the 3GPP scope. The application layer
for PIN and PIN Element discovery and selection is not specified by SA2.
Further updates may provided once the work in 3GPP has progressed,
\- Relay selection for PIN direct connection: \"The 5G system shall support a
mechanism for a PIN Element to select a relay for PIN direct connection that
enables access to the target PIN Element.\"
\- Authentication, Privacy and Security: \"The 5G system shall provide user
privacy; location privacy, identity protection and communication
confidentiality for non-3GPP devices and UEs that are using the PIN Element
with Gateway Capability [...].\"
\- QoS monitoring and control requirements don't apply to PINs but to UEs in
CPNs (Customer Premises Networks)
NOTE: Despite the lack of specific Stage 1 QoS requirements on PIN, Stage 2
includes a solution on QoS management for PINs: \"Solution #11: Differentiated
QoS between a PINE and 5GS when a PEGC is used for the relay\"
\- Charging: \"The 5G system shall support charging data collection for data
traffic to/from individual UEs in a [...] PIN (i.e., UEs behind the PIN
Element with Gateway Capability [...])\"
\- PIN Creation and Management: \"The 5G system shall support mechanisms for a
network operator or authorized 3rd party (e.g., a PIN User) to create, remove
and manage a PIN\"
4.3.3 PIN Architecture related study in 3GPP
There's existing study in 3GPP regarding PIN network and application
architectures can be potentially considered for AR Glasses tethering and media
delivery. The Figure 4.3.3-1 shows the application architecture for enabling
PINAPP (\"Personal IoT Network Application\") as described in 3GPP TR
23.700-78 [5].
Figure 4.3.3-1: PINAPP architecture (from [5])
Figure 4.3.3-2 shows the PIN network architecture in 5GS as described in TR
23.700-88[6].
Figure 4.3.3-2: Personal IoT Networks Architecture in 5GS
As shown in Figure 4.3.3-2, the system architecture contains the following
reference points:
**P1:** Reference point between the PINE and the PEGC. This reference point is
based on non-3GPP access (e.g. WIFI, Bluetooth).
**P2:** Reference point between the PEMC and the PEGC. This reference point is
based on non-3GPP access (e.g. WIFI, Bluetooth) or 5G ProSe Direct
Communication.
**P3:** Reference point between the PEMC and the PIN Application Server. This
reference point can be based on the direct user plane path to 5GS, relay path
via the PEGC, or other communication path via Internet.
**P4:** Reference point between the PEGC and the PIN Application Server. This
reference point is based on the user plane path between PEGC and 5GS.
When considering the _device architectures for Tethered Glasses_ in clause
4.4, and the PIN architecture aspects described in 3GPP TR 23.700-88 [6] there
is a potential solution to use PINs for AR glasses. tethering and media
delivery, when wireless tethered connectivity between 5G device and AR glasses
device is provided through non-3GPP access e.g., WiFi or BLE:
\- The _5G Phone_ is a 3GPP UE which can act as a PEGC (PIN Element with
Gateway Capability) and PEMC (PIN Element with Management Capability)
\- The _AR glasses device_ can act as a PINE (PIN element).
\- The _AR glasses device and the 5G device_ can then be considered a PIN.
## 4.4 Device Architectures for Tethered Glasses
### 4.4.1 General
Based on the guiding use case in clause 4.2.2 as well as the discussions in TR
26.998 [2], this clause identifies the architectures and media handling for
different tethered AR glasses.
Looking at existing AR Glasses, based on the study in TR 26.998 [2] and based
on information from chipset manufacturers on existing and emerging devices, an
AR Glasses designed for AR experiences does integrate complex functionalities
and many of those relate to capabilities. Figure 4.4.1-1 is a picture
providing an overview of an AR glasses.
Figure 4.4.1-1 - Overview of an AR glass
Typical functions of such a AR Glasses consists of:
\- Peripherals including
\- Displays
\- Cameras
\- Microphones
\- Sensors
\- Camera/Sensor Aggregators
\- Perception functionality: Eye Tracking, Face Tracking, etc.
\- SoC Media
\- Display Processing
\- GPU functionalities: Composition/Reprojection
\- Decoding
\- Decryption
\- Camera Front ends
\- Perception functionality: 6DoF, etc.
\- Encoding
\- Connectivity
\- Wi-Fi, Bluetooth, 5G, etc.
An interesting aspect to consider from the above is that the device consists
of different thermal islands, hence division in multiple chips in the headset
is highly desirable. This means that both minimizing the power consumption per
thermal island as well as minimizing the overall power consumption is an
essential design constraint for the device battery life. Such devices require
to partition workloads to remote devices or the cloud to some extent to
balance the power load. Based on this, media capabilities are also possibly
required on UE that acts as a hub for a tethered glasses. Architectures and
processing for this will the main subject of discussion in this Technical
Report.
It should be noted that such AR glasses are predominantly served with media
that can directly be rendered by the peripherals, or produce media captured on
the device and sent to remote processing. Initial System-on-Chip (SoC) media
will likely rely on existing hardware, for example from lower end mobile
chipsets. Some people consider XR even a hack that uses existing components in
a smart manner. However, a core aspect of XR experiences different from
traditional mobile devices is the concurrent operation of multiple encoders
and/or decoders to address different sensors, eye buffers, layers and so on,
as well as the rendering to GPU instead of directly going to the display. Only
over time, such hardware will get added specific functionalities, but not in
the near and mid-term. Expected in the future are higher render and display
resolutions, multi-layer composition, etc.
Figure 4.4.1-2 provides an 5G AR UE as a framework. In this context the XR
application is offered several functionalities on the device as well as
connectivity options to create an XR experiences as defined in TR 26.998,
clause 4.2, namely:
\- XR Runtime: The XR Runtime is a device-resident software or firmware that
implements a set of XR APIs to provide access to the underlying AR/MR
hardware, including capability discovery, session management, input and
sensors, composition, and other XR functions. An example for such APIs is
provided by OpenXR.
\- XR Scene Manager: A Scene Manager is a software component that is able to
process a scene description and renders the corresponding 3D scene. To render
the scene, the Scene Manager typically uses a Graphics Engine that may be
accessed by well-specified APIs such as defined by Vulkan, OpenGL, Metal,
DirectX, etc. Spatial audio is also handled by the Scene Manager based on a
description of the audio scene. Other media types may be added as well.
\- Media Access Functions: supports the application to access and stream
media. For this purpose, a Media Access Function includes: media processing,
codecs, content delivery protocols, content protection, QoS control, metrics
collection and reporting, etc.
\- 5G System: supports the XR application to access the network through the 5G
system, either directly or through the MAF.
Figure 4.4.1-2 -- 5G AR UE Framework
Given that many functionalities are defined through Khronos OpenXR [10],
defining capabilities for example by mandating or recommending support of
certain APIs or parameter settings on API may be relevant. In some cases it
may not even be possible to define capabilities, but for example rely on test
signals and benchmarking requirements that estimate the performance of a
device.
In the following, two different approaches for tethering AR Glasses are
identified, identifying how:
\- Tethered Standalone AR Glasses: In this case, the AR Glasses runs an XR
application that uses the capabilities of the Glasses to create a service. The
AR Glasses is tethered to a 5G device and potentially uses the capabilities of
the phone to support the application. For details refer to clause 4.4.2.
\- Display AR Glasses: In this case, the AR Glasses is tethered to a 5G device
that includes the application and the XR functions. The 5G device runs the
application that uses the capabilities of the 5G device to run an AR/MR
experience. The AR Glasses is connected to the 5G Device, but the XR runtime
API is exposed to the 5G device/phone. For details refer to clause 4.4.3.
### 4.4.2 Tethered Standalone AR Glasses
Figure 4.4.2-1 provides the technical architecture of a typical stand-alone AR
Glasses device. The AR Glasses runs an XR application that uses the
capabilities of the Glasses to create a service. The AR Glasses is tethered to
a 5G device and potentially uses the capabilities of the phone to support the
application.
Figure 4.4.2-1 -- Tethered Standalone AR glass-based device architecture
The AR/MR Application is responsible for orchestrating the various device
resources to offer the AR experience to the user. In particular, the AR/MR
Application can leverage three main internal components on the device which
are:
\- The Media Access Functions (MAF)
\- The XR Runtime
\- The XR Scene Manager
The AR/MR Application can communicate with those three components via
dedicated APIs called the MAF-API, the XR Scene Manager API and the XR Runtime
API. Among other functionalities, those APIs enables the AR/MR Application to
discover and query the media capabilities in terms of support as well as
available resources at runtime.
The XR runtime features several sensors and user controllers relevant for AR
experiences such as cameras, microphones, speakers, display and generic user
input. The XR Runtime typically also deals with the composition of primitive
buffers that are mapped to the eye buffer display taking into account device
characteristics as well as the latest pose information to apply late stage
reprojection.
The XR scene manager is typically very lightweight and with no or very limited
GPU capabilities. It maps raw media primitive buffers such as texture and
depth information
Once the XR application is running, the downlink media is accessed by the MAF
in compressed form and then from then MAF to the XR Scene Manger in
primitives. The device may also establish an uplink data flow from the XR
Runtime to the MAF wherein the data may be in an uncompressed form and then
from the MAF to the remote device, it is typically compressed the data in
order to facilitate the expected transmission over the network.
In order to analyse the use cases and tethering architectures in more details
in terms of bitrates and processing, the following assumptions may be made on
Tethered Standalone AR Glasses:
\- Video Playback and decoding: H.265 Main 10 Profile with maximum processing:
up to 8,294,400 Macroblocks per second (corresponding to 8192x4320 @ 60fps)
\- Video recording and encoding: H.265 Main 10 Profile with maximum
processing: up to 3,888,000 Macroblocks per second (corresponding to 3840x2160
@ 120fps), low-latency encoding, error-robustness, slicing, intra refresh,
long term prediction.
\- Maximum number of combined encoding and decoding instances: 16 for video,
16 for audio
\- Audio capabilities that allow to encode several audio inputs with low-
latency and to decode multiple audio streams in parallel for binaural
playback.
\- The scene manager is very lightweight and passes through primitive buffers
to be consumed by swap chains of the XR run-time. Swapchain images are
typically 2D RGB.
NOTE: For more detailed assumptions on the rendering capabilities for AR
devices, refer to TS 26.119 [17].
### 4.4.3 Tethered Display AR Glasses
Figure 4.4.3-1 provides the technical architecture of a typical tethered
display AR Glasses device. The AR Glasses is tethered to a 5G device that
includes the application and the XR functions. The 5G device runs the
application that uses the capabilities of the 5G device to run an AR/MR
experience. The AR Glasses is connected to the 5G Device, but the XR runtime
API is exposed to the 5G device/phone.
Figure 4.4.3-1 -- Tethered Display AR glass-based device architecture
In this case, the connection between the phone and Glasses is hidden to the
application and tethers the XR Runtime API on the 5G phone to the XR Runtime
core functions on the glasses. The overall function is referred to as XR Link.
In order to analyse the use cases and tethering architectures in more details,
it is assumed that the media access and rendering functions of a high-end
smart phone can be used.
NOTE: For more detailed assumptions on the capabilities for AR devices, refer
to TS 26.119 [17].
### 4.4.4 Tethered AR Glasses with 5G Relay
This architecture corresponds to the \"Type 3b: 5G Relay WireLess Tethered AR
UE\" from TR26.998 [3], but is redrawn in line with the 5G AR UE Framework
shown in Figure 4.4.1-2.
A Basic AR/MR Application runs on the AR Glasses. It performs functions such
as initiating the XR application that triggers a corresponding XR application
in the Cloud/Edge, and initiating the setup of the tethering connection.
The XR runtime is split between the AR Glasses and the Cloud/Edge. An XR
Runtime API is located in the Cloud/Edge. From the perspective of the XR
application on the Cloud/Edge, the XR Runtime including the core functions
(which are located on the AR glasses device) appears local.
The Media Access Function on the 5G Device/Phone performs QoS measuring and
reporting for the tethering connection.
Figure 4.4.4-1 -- Tethered AR glasses with 5G relay
# 5 System Architectures and Call Flows
## 5.1 System Architecture
The basic problem to solve is shown in Figure 5.1-1 below. An Application
Server, for example on the edge, uses the 5G System to distribute content
(e.g., content generated in response to the video/audio/pose input from the
user) to a 5G phone. The 5G phone is connected with a pair of tethered
glasses. The question is now how to set up connectivity and media call flows
to provide a best user experience under latency and processing constraints on
different links.
Figure 5.1-1 The system architecture.
Architectural decisions can be made based on different applications,
capabilities of the glasses and UE, link qualities and so on.
## 5.2 Call flows
### 5.2.1 Call flows for standalone AR glasses-based device architecture
For the network architecture for the standalone AR glasses-based device
architecture as shown in Figure 5.2.1-1, the call flow is provided in Figure
5.2..1-1, which is similar to the call flow in subclause 4.3.1 of TR 26.998.
Note that the 5G device serves as a relay, and may optionally be offloaded to
perform some \"phone based processing functions\".
The call flow is described below.
1\. The AR glasses device and the 5G device/phone sets up the tethering link.
2\. The application contacts the application provider to fetch the entry point
for the content. The acquisition of the entry point may be performed in
different ways and is considered out of scope. An entry point may for example
be a URL to a scene description.
3\. Session set up:
3a. In case when the entry point is a URL of a scene description, the
application initializes the Scene Manager using the acquired entry point.
3b. The Scene Manager retrieves the scene description from the scene provider
based on the entry point information.
3c. The Scene Manager parses the entry point and creates the immersive scene.
3d. The Scene Manager requests the creation of a local AR/MR session from the
XR Runtime.
3e. The XR Runtime creates a local AR/MR session and performs registration
with the local environment.
Then steps 4 and 5 run in parallel:
4: XR Media Delivery Pipeline: In case when entry point is a scene URL, a
delivery session - for accessing scenes (new scenes or scene updates) and
related media over the network is established. This can basically use the MAF
as well as the scene manager and the corresponding network functions. Details
are introduced in Figure 5.2.1-2.
NOTE: The realization of XR media delivery pipeline may vary in different
architectures.
5: XR Spatial Compute Pipeline: A pipeline that uses sensor data to provide an
understanding of the physical space surrounding the device to determine the
device's position and orientation and placement of AR objects in reference to
the real world and uses XR Spatial Description information from the network to
support this process. Details are introduced in Figure 5.2.1-3.
6: Steps 4 and 5 run independently, but the results of both pipelines (e.g.,
media organized in a scene graph and pose of the AR device) are inputs of the
AR/MR Scene Manager function. This function handles the common processing of
the two asynchronous pipelines to create an XR experience.
Figure 5.2.1-1: Call flow for the tethering architecture standalone AR
glasses-based device architecture.
The XR media delivery pipeline (step 4 of Figure 5.2.1.-1) in Figure 5.2.1-1
is provided in Figure 5.2.1-2, which is similar to the media delivery pipeline
in subclause 4.3.2 of TR 26.998.
For an XR Media Delivery Pipeline:
1\. The Scene Manager initializes XR scene delivery session.
2\. The MAF establishes XR scene delivery session.
3\. The MAF may receive updates to the scene description from the scene
provider
4\. The MAF passes the scene update to the Scene Manager.
5\. The Scene Manager updates the current scene.
6\. The Scene Manager acquires the latest pose information and the user's
actions
6\. The Scene Manager in the device shares that information with the Scene
Manager in edge/cloud
The media rendering loop consists of the following steps. Note that steps 8, 9
and 10 are running as 3 parallel loops:
8\. For each new object in the scene:
a. The Scene Manager triggers the MAF to fetch the related media.
b. The MAF creates a dedicated media pipeline to process the input.
c. The MAF establishes a transport session for each component of the media
object.
9\. For each transport session:
a. The media pipeline fetches the media data. It could be static, segmented,
or real-time media streams.
b. The media pipeline processes the media and makes it available in buffers.
10\. For each object to be rendered:
a. The Scene Manager gets processed media data from the media pipeline buffers
b. The Scene Manager reconstructs and renders the object
11\. The Scene Manager passes the rendered frame to the XR Runtime for display
on the tethered standalone AR glass-based device.
Figure 5.2.1-2: Media delivery pipeline for call flow in Figure 5.2.1.
The XR spatial compute pipeline (step 5 of Figure 5.2.1) in Figure 5.2.1 is
provided in Figure 5.2 1-3 (below), which is similar to the XR spatial compute
pipeline in subclause 4.3.3 of TR 26.998.
Figure 5.2.1-3 Functional diagram for XR Spatial Compute Pipeline for call
flow in Figure 5.2.1
For a XR Spatial Compute downlink delivery session:
1\. The XR Spatial Compute function in the XR Runtime asks the MAF to
establish a XR Spatial Compute downlink delivery session
2\. The MAF communicates with the network to establish the proper resources
and QoS
3\. The XR Spatial Compute function requests access to XR Spatial Description
information
4\. An XR Spatial Description downlink delivery session is established across
the XR Spatial Compute server, the media delivery function, the media access
function and XR Spatial Compute function on the device.
5\. XR Spatial Description information is delivered in this downlink delivery
session
For a XR Spatial Compute uplink delivery session:
6\. The XR Spatial Compute function in the XR Runtime asks the MAF to
establish a XR Spatial Compute uplink delivery session
7\. The MAF communicates with the network to establish the proper resources
and QoS
8\. The MAF established an appropriate uplink delivery pipeline
9\. An XR Spatial Description uplink delivery session is established across
the XR Spatial Compute function on the device, the media access function, the
media delivery function and the XR Spatial Compute server.
10\. Spatial compute information is upstreamed to the XR Spatial Compute
server.
11\. Data is continuously exchanged between the Scene Manager and the XR
Runtime
### 5.2.2 Call flows for display AR glasses-based device architecture without
edge rendering
For the network architecture corresponding to the display AR glasses-based
device architecture without edge rendering, as shown in Figure 5.2.-2, the
call flow is shown in Figure 5.2.1.-3. The call flow is different from the one
in Figure 5.2.1.-1 mainly in that the AR/MR Application, Scene Manager and
Media Access Function reside on the 5G Device. Additionally, an XR Runtime API
is on the 5G Device, the XR Runtime is on the AR Glasses Device, and the
interactions between the XR Runtime API and the XR Runtime are proprietary.
This simplifies the design from the point of view of the AR/MR application
because it only needs to concern about the XR Runtime API.
The call flow is similar to the one in Figure 5.2.1-3 and the main difference
is on the procedures related to XR Runtime. For example, step 3d: Establish
AR/MR session now points to XR Runtime API, instead of XR Runtime.
Figure 5.2.1-1-: Call flow for the network architecture for tethered display
AR glasses-based device without edge rendering.
The XR media delivery pipeline (step 4 of Figure 5.2.-3) in Figure 5.2-3 is
provided in Figure 5.2-4.
Figure 5.2.2-2-: Media delivery pipeline for call flow in Figure 5.2-3.
The XR spatial compute pipeline (step 5 of Figure 5.2-3) in Figure 5.2-3 is
provided in Figure 5.2-x below.
Figure 5.2.2-3 Call flow for XR spatial compute for call flow in Figure 5.2-3.
### 5.2.3 Call flows for tethered AR glasses with 5G relay with edge rendering
For the network architecture corresponding to the tethered AR glasses with 5G
relay with edge rendering, as shown in Figure 4.2.2 2, the call flow is shown
in Figure 5.2.3-1.
The call flow is different from the one in Figure 5.2-1 and Figure 5.2.3
mainly in that the XR Scene Manager, XR Runtime Functions are located within
the AR Glasses and the Cloud/Edge. An XR Runtime API is located in the
Cloud/Edge.
The call flow is similar to the one in Figure 5.2-3 and the main difference is
on the procedures related to XR Runtime. For example, step 3, the XR link
initialization and Step 4: session set-up takes place between XR Runtime and
XR Runtime API. Establish AR/MR session now points to XR Runtime API, instead
of XR Runtime. Step 5 and Step 6 follows similar procedure as the previous
architectures.
Figure 5.2.3-1 Call flow for the network architecture for Tethered AR glasses
with 5G relay.
# 6 Identified Key Issues and Potential Solutions
## 6.1 Key Issue #1: How to provide End-to-End QoS for the 5G relay
architecture
### 6.1.1 Description of the key issue
To an XR application, the QoS metrics that matter are the end-to-end QoS
metrics, including:
\- Delay
\- Packet loss rate
\- Bit rate
However, for the tethered AR glasses with 5G relay architecture, as shown in
figure 6.1-1, a typical AR/MR end-to-end path traverses both the 5G network
and non-5G networks. The non-5G networks, e.g., Wi-Fi, the Internet, do not
provide guaranteed QoS. A key challenge is how to provide end-to-end QoS with
a mix of a 5G network and non-5G networks.
### 6.1.2 Potential solution
#### 6.1.2.1 End-to-end latency
Figure 6.1-1 depicts a breakdown of the end-to-end delay and suggests a
solution: the 5G network adjusts the delay within the 5G network to compensate
for the delays incurred in non-5G networks such that the end-to-end delay
meets an end-to-end delay requirement. Note that this solution has its
limitation in that it is impossible for the 5G network to compensate for the
delay in the non-5G networks if the aggregate delay in the non-5G network
exceeds the delay imposed by the end-to-end delay requirement.
Figure 6.1-1: End-to-end delay breakdown
To meet the end-to-end latency requirement for the AR/MR session, some
entities must determine the delay on the tethering link $D_{n,1}$, and the
delay on the Internet (between the UPF and the edge application server)
$D_{n,2}$.
Additionally, the determined delay components need to be reported to the 5G
system which uses the reported delays and the target end-to-end delay to
determine the delay to be provisioned within the 5G system.
Then, Figure 6.1-1 suggests that to meet a desired end-to-end latency
requirement $D_{e2e,\ \ desired}$, the 5G network provides a delay $D_{c}^{'}\
$as follows:
$$D_{c}^{'} = D_{e2e,\ \ desired} - D_{n}$$
The above description can be extended to support delay provisioning within the
5G network in a statistical sense. For example, by taking into account the
variation of the non-5G delays, the 5G network can determine the delay in the
5G network to make the end-to-end delay below a desired delay value 99% of the
time.
#### 6.1.2.2 Other End-to-end QoS metrics
For other end-to-end QoS metrics, such as packet error rate and bit rate, the
5G network can adjust the metrics within the 5G network to achieve desired
end-to-end metrics.
First, consider the packet error rate. Assuming that the packet error rate in
the 5G network (denoted $p_{c}$) and the effective packet error rate in the
non-5G networks (denoted $p_{n}$) are independent, the end-to-end packet error
rate (denoted $p_{e2e}$) is then
$$p_{e2e} = 1 - \left( 1 - p_{c} \right)\left( 1 - p_{n} \right).$$
This suggests that if $p_{n}$ can be estimated, then the 5G network can adjust
$p_{c}\ $to achieve a desired value for $p_{e2e}.$ The above equation also
suggests a method to estimate $p_{n}$: if $p_{e2e}$ is measured, then $p_{n}$
can be solved for as
$$p_{n} = \frac{p_{e2e} - p_{c}}{1 - p_{c}}$$
Thus a method for the 5G network to set a packet loss rate to achieve a
desired end-to-end packet loss rate is as follows:
1\. The 5G network estimates the packet error rate in the 5G network $p_{c}$.
2\. The end-to-end packet error rate $p_{e2e}$ is measured.
3\. The collective packet error rate of the non-5G networks $p_{n}$ _is
computed according to_ $p_{n} = (p_{e2e} - p_{c})/(1 - p_{c}\ )$
4\. The 5G network adjusts the packet error rate within he 5G network to
$p_{c}^{'}$ to meet the desired end-to-end packet error rate $p_{e2e,\
desired}$
$$p_{e2e,\ desired} = 1 - \left( 1 - p_{c}^{'} \right)\left( 1 - p_{n}
\right).$$
Note that to measure the end-to-end packet error rate, the required number of
increases with the packet error rate. For example, if the end-to-end packet
error rate is 0.1%, there is one packet error event expected to occur in 1000
packets. This implies a slow response when the packet error rate is low. This
is different from delay measurement, where a single packet can provide a valid
measurement regardless of the range of the delay.
Next consider the bit rate. The bit rate depends on the bit rate allocation of
the network segments in the end-to-end path and rate control and congestion
control. However, there is a simple relationship among the maximum allowed bit
rates that can be exploited to determine the bit rate allocation. It is
observed that the maximum allowed end-to-end bit rate ($r_{e2e}$) is the
minimum of the maximum allowed bit rate in the 5G network ($r_{c}$) and the
maximum allowed bit rate in the non-5G networks ($r_{n})$, i.e., $r_{e2e} =
\min{(r_{c},\ r_{n})}$. Thus the 5G network only needs to provide an maximum
allowed bit rate that is higher than the desired end-to-end bit rate.
## 6.2 Key Issue #2: How to determine the non-5G delay for the 5G relay
architecture
### 6.2.1 Description of the key issue
In an end-to-end connection that includes a tethering link (e.g., Wi-Fi link),
a 5G network and the Internet, the Wi-Fi segment and the Internet segment
typically cannot guarantee latency. To achieve low end-to-end latency, one
approach is to make the latency in the 5G network very conservative such that
the end-to-end latency is below a target value. This, however, comes at a
cost, because provisioning an unnecessarily low latency in the 5G network
means excessive resource allocation (e.g., to support a more robust
modulation-and-coding scheme (MCS)) or pre-empting many other traffic flows.
An alternative approach is to dynamically adjust the delay in the 5G network
in accordance with the total delay incurred elsewhere on the end-to-end path.
The delay on a Wi-Fi link may change over time depending on the interference
generated by other nearby Wi-Fi networks operating on the same frequency.
Similarly, the delay between the UPF and the application server depends on the
location of this selected UPF and the network congestion level. Therefore,
measurements may be used to estimate these time-varying delays on the non-5G
segments.
For delay measurement, it is important that the measured delay is
representative of the delay to be experienced by the data packets. Delay
measurements based on delay measurement messages such as the ping message
(ICMP Echo and Echo Reply) may not accurately reflect the delay experienced by
the data packets for two reasons: (1) the delay measurement message uses a
protocol number (e.g., 1 for ping) that is different from the protocol number
for the data packet (e.g., 17 if the data packet is sent with RTP/UDP),
resulting in different IP 5-tuples and consequently different QoS treatment in
the communication network; (2) the packet size of a delay measurement
typically is much smaller than that of a data packet, resulting in different
transmission delays which are part of the overall delay.
There are two ways to measure the latency and they fill in the details for
step 10 in Figure 5.2-5 in clause 5.2.
### 6.2.2 Solution: Segment-by-segment delay measurement
The delay on Wi-Fi link and the delay between the UPF and the application
server are measured separately. One simple solution is to use the ICMP ping
protocol (ICMP Echo and Echo Reply, IETF RFC792 [12]). The 5G phone sends a
ping request to the AR glasses, which replies with a ping response. The 5G
phone then obtains the RTT over the Wi-Fi link. Similarly, the UPF sends a
ping request to the application server, which replies with a ping response,
and the UPF obtains the RTT between the UPF and the application server. The
respective RTTs can then be halved to get estimates of the one-way delays for
the two non-5G segments.
In step 4, the MAF reports the one-way delay estimate $D_{n,1}$ to the AF.
In step 7, the UPF reports the one-way delay estimate $D_{n,2}$ to the SMF,
which forwards the estimate to the AF.
NOTE: How UPF retrieves the RTT between the UPF and the application server and
further exposes the latency results to the AF are not supported in SA2 in
current release.
In step 9, the AF determines the desired value for the delay in the 5G network
needed to compensate for the variation in the delay in the non-5G segments in
order to meet the end-to-end latency requirement for the application, and
sends a delay request to the PCF.
Figure 6.2.2-1: segment-by-segment delay measurement
### 6.2.3 Solution: End-to-end delay measurement
The delay measurement is carried out in an end-to-end fashion. This avoids the
potential rejection of a measurement message that originates from the UPF and
reaches the application server. The AR glasses sends a ping request message to
the application server, which replies with a ping response. The AR glasses
then estimate the UL one-way end-to-end delay $D_{e2e}$ by halving the RTT.
The 5G network estimates the UL one-way delay within the 5G network $D_{c}$,
e.g., by recording the time when the ping request arrives at the phone and the
time when the ping request reaches the UPF and takes the difference. or using
the QoS monitoring mechanism to obtain the UL one-way delay within the 5G
network.
NOTE: How UPF detects and reports the arrival time of the ping test is not
supported yet in SA2.
The estimated UL one-way delay on the non-5G segments is then $D_{n} = D_{e2e}
- D_{c}$ as shownin Figure 6.2.3-1.
Figure 6.2.3-1: End-to-end delay measurement
To make the end-to-end delay measurements accurately reflect the end-to-end
delay experienced by the data packets, one potential solution is to use in-
band measurement as shown in Fig. 6.2.3-2. A timestamp message is piggybacked
to an RTP packet that carries data when needed. The payload type field in the
RTP packet header is determined by the data, and an RTP header extension is
added to instruct the receiver how to separate the timestamp message and the
data. This way, the RTP packet is treated as a data packet with packet size
substantially the same as an RTP packet that carries data only. Similarly, a
timestamp reply message is piggybacked to an RTP packet in the reverse
direction. This mechanism is payload-format independent, i.e., it can be
applied to all payload formats. The order of the Data and the Timestamp msg
(or Timestamp Reply Msg) may be switched.
Fig. 6.2.3-2 In-band end-to-end delay measurement by piggybacking a delay
measurement message to an RTP packet.
The timestamp message may be an ICMP Timestamp message (as defined in RFC792
[12]) with a timestamp field of 32 bits, which represents the number of
milliseconds with respect to midnight Universal Time (UT). However, the
achievable accuracy is limited by the 1ms granularity of the timestamp format.
Alternatively, Precision Time Protocol or PTP (IEEE 1588-2019 [13]) may be
used for better accuracy due to a finer granularity of 1 nanosecond of its
timestamp format.
The target precision for the measurement depends on the overall latency and
the use case. TR 26.998 listed 50-60ms for the pose-to-render-to-photon
latency. For better user experience, we may need to consider better target
precision. .
Instead of using RTP, using RTCP for delay measurement may be a candidate
solution (as allowed in RFC3550), but it may have the following drawbacks:
\- First, although RTCP and RTP may use the same port number as allowed by RFC
5761 which was motivated to simplify NAT and firewall management, it is not
guaranteed that they always use the same port number. When they use different
port numbers, RTCP and RTP packets will be mapped to different QoS flows (in
the 5G core network) and receive different QoS treatment.
\- Second, the payload type for RTCP packets (200 for sender report (SR) and
201 for receiver report (RR)) are different from those of RTP packets, and if
a network takes into account the payload type in provisioning QoS (e.g., in
Wi-Fi an RTP packet carrying video may be mapped to the Video access category,
while RTCP packets may be mapped to the Best Effort access category), RTCP
packets and RTP packets may receive different QoS.
Third, the overhead of RTCP packets may be greater, because a separate packet,
which includes the UDP packet header, the IP packet header and lower layer
packet headers, needs to be sent.
TS 23.501 [9] offers two measurement methods for measuring the delay in the 5G
system $\mathbf{D}_{\mathbf{c}}$, originally intended for QoS monitoring to
assist URLLC service. The first method, termed \"Per QoS Flow per UE QoS
Monitoring\", leverages the GTP-U headers to carry the timestamps, and the
second method, termed \"GTP-U Path Monitoring\", leverages the GTP-U Echo
protocol. The first method is shown in Figure 6.2.3-3.
Figure 6.2.3-3: Measuring the delay in the 5G system: Per QoS Flow per UE QoS
Monitoring in TS23.501 [9]
The PCF generates the QoS monitoring policy based on the request from the AF
(directly or via NEF) (step 2).
The SMF initiates a QoS monitoring request to the NG-RAN (step 3) and the PSA
UPF (step 4).
Step 6: Time stamp T1 is taken in the PSA UPF, indicating the time when the
PSA UPF sends a monitoring packet to the NG-RAN (i.e., gNB).
Step 7: the PSA UPF sends a monitoring packet to the NG-RAN, containing T1,
QFI and QoS Monitoring Packet (QMP) indicator in the GTP-U header.
Step 8: Time stamp T2 is taken when the monitoring packet is received by the
NG-RAN.
Step 10: Time stamp T3 is taken when the NG-RAN forwards an UL packet, or
generate a dummy UL packet, where for either case the NG-RAN puts UL/DL packet
delay results of RAN part, T1, T2, T3 and the QMP indicator in the GTP-U
header.
Step 12: Time stamp T4 is taken when the UL packet is received.
Step 13: Between the NG-RAN and the PSA UPF, if they are synchronized, then
the UL delay will be T4-T3, and the DL delay will be T2-T1. If they are not
synchronized, then the procedure computes the average one-way delay (T2-T1 +
T4-T3)/2.
Step 14: The delay on the access network (between the UE and the NG-RAN) can
be added to the results in step 13 to get the total delays in the 5G system,
i.e. UL, DL or RT latency.
Finally, the UPF reports the QoS monitoring results to SMF and SMF further
reports to PCF. The PCF then exposes the QoS monitoring results to the AF
directly or via NEF as requested and the AF eventually obtains the UL/DL or
average delay within the 5G System.
Piggybacking timestamps rather than the timestamp message or timestamp reply
message to an RTP data packet may reduce the communication overhead. For
example, the Timestamp message or Timestamp Reply message are of the following
format (RFC792 [12]) as shown in Figure 6.2.3-4.
0 1 2 3
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| **Type** \| Code \| Checksum \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| Identifier \| Sequence Number \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| **Originate Timestamp** \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| **Receive Timestamp** \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| **Transmit Timestamp** \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
Figure 6.2.3-4: The packet format of Timestamp Reply message in RFC792[12]
For the Timestamp message, the size is 12 bytes. If only the timestamp portion
-- the Originate Timestamp -- is piggybacked, the size is reduced to 4 bytes.
To let the receiver know the type of the information (i.e., how many
timestamps are contained), the Type field can be added. As a result, the total
size is 5 bytes. The savings is 7 bytes. This may not look much but can be
significant if frequent measurements are needed.
Similarly, for the Timestamp Reply message, the size is 20 bytes. If again
only the timestamps and the Type information is included in the piggybacked
RTP packet, the size is reduced from 20 bytes to 13 bytes. Assuming the number
of Timestamp messages are the same as the number of Timestamp Reply messages,
the average saving is 44%.
RTP header extensions defined in RFC8285 [14] can be used to do timestamp
piggybacking. There are two formats of RTP header extension, namely the one-
byte and the two-byte formats [14].
Figure 6.2.3-3 shows the timestamp piggybacking in the RTP packet payload that
allows for measuring the one-way delays in both directions as well as the
round-trip time. Specifically, uplink one-way delay = T2 \-- T1, downlink one-
way delay = T4 -- T3, and round-trip time = T2 -- T1 \+ T4 -- T3 = T4 -- T1 --
(T3 -- T2).
The reason for putting the timestamp(s) in the RTP packet payload rather than
in the RTP header is to accurately capture the processing delay (e.g.,
encryption of the payload in the case of SRTP) and other delays (e.g., wait
time experienced by the media in the processing pipeline) experienced by the
media.
Note that RFC6051[16] specifies an RTP header extension that carries a
timestamp, which has shortcomings compared to putting the timestamp(s) in the
RTP packet payload. Although the motivation of the technique in RFC6051, i.e.,
putting a timestamp in the RTP header extension, was to speed up the
synchronization between multiple RTP sessions, the technique has the benefit
of offering more accurate delay measurement than the RTCP approach and ICMP
approach described earlier because the latter approaches lead to a different
treatment between the measurement packets and the RTP packets in the network.
However, the technique in RFC6051 is not preferred because the timestamp fails
to capture the processing delay and other delays experienced by the media,
and, as a lesser problem, the technique currently supports only one timestamp
to be carried in the RTP header extension.
Figure 6.2.3-3 In-band end-to-end delay measurement by piggybacking timestamps
to an RTP packet.
The one-byte RTP header extension for piggypacking timestamps is shown in
Figure 6.2.3-4. The header extension consists of a single RTP header extension
element, the ID of which is set to 1, and the length L field of which is set
to 2 (which indicates a size of 3 bytes rather than 2 bytes for the 'data'
field of the RTP header extension element in the one-byte format according to
RFC8285 [14]), and the data of which is labeld \"#timestamps, start, size\"
occupying 24 bits. Specifically,
\- The \"#timestamps\" field specifies the number of timestamps. In the case
of piggybacking the originate timestamp (T1), this field is set to 1; in the
case of piggybacking the originate timestamp (T1), the receive timestamp (T2)
and the transmit timestamp (T3), this field is set to 3.
\- The \"start\" field specifies whether the timestamps are at the beginning
of the RTP payload or at the end of the RTP payload.
\- The \"size\" field specifies the size of the timestamps. The size depends
on the timestamp formats. Considering the granularity and overhead, the 32-bit
short NTP timestamp format [15] or a truncated version of it seems a good
choice.
0 1 2 3
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| 0xBE \| 0xDE \| length=1 \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| ID=1 \| L=2 \| #timestamps, start, size \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
Figure 6.2.3-4 The one-byte RTP header extension for piggybacking timestamps.
The two-byte RTP header extension for piggypacking timestamps is shown in
Figure 6.2.3-5. The difference is that the ID and the L fields together occupy
2 bytes rather than 1 byte as in the one-byte format. Note that the value in
the L field indicates the size of the data of the RTP header extension element
literally, i.e., L=2 means 2 bytes.
0 1 2 3
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| 0xBE \| 0xDE \| length=1 \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
\| ID=1 \| L=2 \| #timestamps, start, size \|
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
Figure 6.2.3-5 The two-byte RTP header extension for piggybacking timestamps.
For both the one-byte format and the two-byte format, the timestamp(s) are put
in the RTP payload according to the respective \"#timestamps, start, size\"
fields.
### 6.2.4 Time measurement protocol
The ICMP ping protocol uses two timestamps generated at the transmitter to get
an estimate of the RTT. The measured delay includes the time gap between the
reception of the ping request message and the transmission of the ping
response message at the receiver. The time gap contributes to the estimation
error, and it depends on the operating system used at the receiver and may
become significant for low latency applications.
Alternatively, ICMP timestamp approach (IETF RFC792 [12]) can be used, which,
compared to ICMP ping, provides the source two timestamps, one for the
reception of the timestamp message and the other for the transmission of the
timestamp reply message. The two timestamps are carried back to the source,
which can use the difference to calculate the time gap and get a more accurate
estimate of the RTT.
## 6.3 Key Issue #3: What and how to report the non-5G delay
### 6.3.1 Description of the key issue
In order for the 5G network to provide an appropriate delay over the 5G
segment of the end-to-end path to meet a target end-to-end delay requirement,
the non-5G delay needs to be reported to the 5G system.
The delay over a non-5G network may vary over time. A key issue is what should
be reported to the 5G network, and how.
### 6.3.2 Potential solutions
In the segment-by-segment measurement method (clause 6.2.2), the 5G device
(e.g., 5G phone) can report the delay between the AR glasses and the 5G device
via the Direct Data Collection Client as indicated in 3GPPTS26.531 [11], Other
5G functions on the 5G device is not excluded. The the UPF can report the
delay between the UPF and the application server to SMF.
In the end-to-end measurement method (clause 6.2.3), the AR glasses measures
the end-to-end delay, forwards the measurement to the 5G device. The 5G device
reports it through MAF to the 5G core network.
The reporting methodology may depend on the tethering architectures. Figure
6.3-1 shows a possible call flow for delay reporting for the end-to-end
measurement approach. The reported statistics of the end-to-end delay may
include the mean and standard deviation.
Figure 6.3-2 shows a possible call flow for delay reporting for the partial
path measurement approach, where
\- D~WM~: delay between Wi-Fi and Media Access Function (MAF) on the 5G Device
\- D~WW~: delay between Wi-Fi on the AR Glasses Device and Wi-Fi on the 5G
Device
\- D~ME~: delay between MAF and the Edge/Cloud
The end-to-end delay D~e2e~ = D~WW~+ D~WM~ + D~ME.~
In step 15, the Direct Data Collection Client reports to the Data Collection
AF a desired value for the delay within the 5G network D~c~.
Figure 6.3-1: reporting statistics of end-to-end delay based on end-to-end
measurement
Figure 6.3-2: reporting statistics of delays based on partial path measurement
Note that in Figure 6.3-2, the Media Access Function is mapped to an instance
of the UE Application in the reference architecture for data collection and
reporting in 3GPP TS26.531 [11], i.e., the reporting is via the R7 reference
point. This also applies to step 8, where an application may be used to read
the log on Wi-Fi and the application is an instance of the UE Application via
the R7 reference point.
## 6.4 Key Issue #4: Formats and Connectivity of Tethered Glass
### 6.4.1 Description
Split Rendering across a proprietary link may have limitations in terms of
formats that can be used, as well as on the supported connectivity and
associated bitrates. The knowledge of the capabilities of a tethered glasses,
accessible through the XR Runtime API on the phone, can support the operation
of the phone over the 5G Network in terms of required bitrates as well as in
terms of preferable formats. This clause discusses the workflow and provides
relevant conclusions in terms of capabilities and related signalling.
### 6.4.2 Background
The architecture of relevance for the device is shown in Figure 4.4.1-2. The
key issue is the handling of media data in the workflow. In a typical
implementation aligned with OpenXR processing, the following applies for
visual media:
\- To present images to the user, the runtime provides images organized in
swapchains for the application to render into.
\- The XR runtime may support different swapchain image formats and the
supported image formats may be provided to the application through the runtime
API. XR runtimes is expected to at least support R8G8B8A8 and R8G8B8A8 sRGB
formats. Details may depend on the graphics API specified in xrCreateSession.
Options include DirectX or OpenGL. For example, support for OpenGL ES as a
reference may be assumed, i.e. an extension equivalent to the functionalities
provided in XR_KHR_opengl_es_enable. OpenGL ES is platform independent and
suited for embedded systems.
\- Swapchain images can be 2D or 2D Array. Arrays allow to extract a subset of
the 2D images for rendering
\- The application or scene manager _can offload the composition of the final
image to a XR runtime-supplied compositor_. By this, the rendering complexity
is significantly lower since details such as frame-rate interpolation and
distortion correction are performed by the XR runtime. It is assumed that the
XR Runtime provides these functionalities.
\- A runtime on a XR device typically supports OpenXR composition, namely
Projection, Quad, Cube, Cylinder, Equirectangular.
An OpenXR application life cycle is shown in Figure 6.4.2-1. In this case,
after creating an OpenXR session, the application starts a frame loop. The
frame loop is executed for every frame. The frame loop consists of the
following steps:
1) Synchronize actions: this step consists of retrieving the action state,
e.g. the status of the controller buttons and the associated pose. During this
step, the application also establishes the location of different trackables.
The application may also send haptics feedback.
2) Start a new frame: this step starts with waiting for a frame to be provided
by the XR runtime. This step is necessary to synchronize the application frame
submission with the display. The xrWaitFrame function returns a frame state
for the requested frame that includes a predictedDisplayTime, which is a
prediction of when the corresponding composited frame will be displayed. This
information is used by the application to request the predicted pose at
display. Once the xrWaitFrame function completes, the application calls
xrBeginFrame to signal the start of the rendering process.
3) Retrieve rendering resources: the application starts by locating the views
in space and time by calling the xrLocateViews function, provided with the
predicted display time and the XR space. It then acquires the swap chain image
associated with every view of the composition layer. It waits for the swap
chain image to be made available so it can write into it.
4) Rendering: the application then performs its rendering work. It iterates
over the scene graph nodes and renders each object to the view. This step
usually uses a Graphics Framework such Vulkan, OpenGL, or Direct3D to perform
the actual graphics operations.
5) Release resources: once the rendering is done for a view, the application
releases the corresponding swap chain image. Once all views are rendered, it
sends them for display by calling the xrEndFrame function.
Figure 6.4-2-1: OpenXR application life cycle
For audio media, similar processes as video typically apply. OpenXR and OpenSL
ES aligned terminology is used as a reference, A typically possible
decomposition of steps for immersive audio rendering is as follows: An
interface to the XR runtime is available to hand over raw audio buffers to
determine how the XR application and scene manager would access a device's
audio capabilities. In order address a concrete implementation example, the
model of OpenSL ES is used as a reference for. OpenSL ES supports both file-
based and in-memory data sources, as well as buffer queues, for efficient
streaming of audio data from memory to the audio system. Buffer queues in
OpenSL may be viewed as equivalent to visual swap chains. OpenSL ES may be
viewed as companion to 3D graphic APIs such as OpenGL ES. The 3D graphics
engine will render the 3D graphics scene to a two-dimension display device,
and the OpenSL ES implementation will render the 3D audio scene to the audio
output device. In today's implementations, in addition to the functionalities
from such buffer queues, different types of audio signals may be provided, and
additional/alternative processing steps may be carried out. Audio signals
(i.e. the combination of metadata and buffer queues) may be
\- non-immersive or also known as non-diegitic, i.e. they are not rendered
according to the pose.
\- Immersive and describe a full 6DoF experience in the reference space of the
XR session. In this case, the XR runtime will create a rendered signal
according to the latest pose.
\- Immersive and pre-rendered for a specific render pose. In this case, the
signals have been prepared such that the runtime can use the audio signal and
the associated render pose and supplementary data for a pose correction to the
latest pose.
\- a mixture of such signals that are jointly presented.
\- the signals may originate from different source, for example some may be
generated locally, others may be part of a pre-rendering or a full scene
created in the network
The audio data is considered to be uncompressed.
### 6.4.3 Assumptions
Assuming now a device as documented in clause 4.4.3 as a tethered AR glasses,
then in a typical deployment, Figure 6.4.3-1 provides a summary of the
operation.
\- Swap chain images are provided by phone-based XR Scene manager to OpenXR
\- Actions and pose information are provided to the application
\- A proprietary system is operated south of the OpenXR API for split
rendering, i.e. depicted left of the OpenXR API. This is very typical in
implementations today for which the phone and the glassesare provided by the
same vendor.
\- The OpenXR API supports a set of typical formats, that are raw formats
according to the discussion in clause 6.4.2.
Figure 6.4.3-1: Tethering based on proprietary split rendering operation
In an extension to Figure 6.4.3-1, Figure 6.4.3-2 provides more details of the
typical functions carried out in the split rendering, namely converting the
raw OpenXR formats, encoding and providing a security framework, setting up a
connection together with a protocol and match the bitrate of the link. On the
receiving end, the signals sent over the tethered link are decrypted, decoded
and the provided to the XR runtime on the glassesfor final pose correction.
Similar processing happens on the uplink to provide encoded pose and actions.
Figure 6.4.3-2: Split rendering operations on proprietary link
### 6.4.4 Problem Statements
#### 6.4.4.1 General
Now a couple of scenarios exist, in case the media is not generated on device,
but received over the 5G System. In the regular operation, the formats on the
two links, i.e. the 5G system and the proprietary link, would operate
completely independently.
#### 6.4.4.2 Case 1: Restricted Raw formats
However, this misses the following aspects when the raw formats are decoded
and provided:
1) The formats on the AR Glass may be restricted. Hence, only a reduced set of
media data, for example in terms of resolution, may be usefully provided to
the AR glass
2) The bitrate on the tethered link may be restricted, or may even dynamically
change. In this case, the quality of the formats provided at the input may not
be maintained end-to-end
3) The security framework between the glassesand the phone is such that it is
unusable for certain formats.
Examples for such cases may be in Media Streaming, for which only formats and
signals are downloaded that can be processed by the raw formats supported over
the tethered link. For conversational applications, also such restrictions may
apply, for example for audio only stereo signals are supported.
#### 6.4.4.3 Case 2: Transcoding Problem
Even more severe, if the content is provided from the 5G System in a
compressed from, then the operation in the phone results in a typical
transcoding operation with the following drawbacks
1) Addition of latency because of algorithmic and processing delays
2) Additional distortion may be added in the transcoding process
3) Unnecessary power consumptions for the encoding and decoding processes
4) A trusted security point may be interrupted, put in clear and re-encoded
5) The bitrate on the tethered link may be restricted, or may even dynamically
change. In this case, the quality of the formats provided at the input may not
be maintained end-to-end
6) The security framework between the glassesand the phone is such that it is
unusable for certain formats.
Examples for such cases may be in Media Streaming, but likely to a limited
attempt. For conversational applications, also such restrictions may apply,
for example for audio only stereo signals are supported. More prominent is the
case in conversational speech cases and most prominent is the case in split
rendering context, i.e. if the buffer view is pre-rendered on the edge or the
cloud. In this case, transcoding is most impactful.
### 6.4.5 Potential Solutions
#### 6.4.5.1 Layer-2 or Layer-3 Relay
In order to avoid format or transcoding conversion on the UE in case 1 from
above, a simple relay can be done according to the architecture in clause
4.4.4. However, this has several downsides:
1) The phone is excluded in the rendering and the application. No locally
generated data can be added to the rendering. The application needs to reside
on the edge, and the UE may upload data for rendering.
2) If the connection to the cloud/edge is not available or interrupted, no
service can be provided.
3) The latency that is possibly incurred by this operation may be to high
#### 6.4.5.2 Raw Format adaptation
Raw format adaptation primarily addressed case 2 from 6.4.4. In this case, the
application on the UE not only queries the supported raw formats, but it also
gets information from the XR Runtime on information related to the resolution
of the formats, the quality of the link in a static and dynamic fashion and
other information that the phone can use in the communication between across
the 5G System as well as in the rendering in the device to properly match the
formats received and generated on the phone to match those of the one
supported on the proprietary link.
The key extensions are as follows:
\- Support in the runtime query for supported formats additional information
that include
\- the resolution of the format
\- the quality degradation incurred by the combination of the link and the
coding
\- the security framework and capabilities of the device
\- dynamically providing the metrics and information of the signal quality on
the proprietary link
\- Support of usage of this provided static and dynamic information for
example in
\- the Media streaming client to select the appropriate content codecs,
bitrates, possibly in a dynamic fashion
\- a communication client to negotiate with the network to support the
appropriate content formats, bitrates and qualities, possibly in a dynamic
fashion
\- Announcing appropriate information on the 5G System network side to the 5G
Phone in order to be able to make such selections.
The details of these extensions need to be added to for example a streaming
manifest, the session description protocol or to a scene description.
#### 6.4.5.3 Pass-through Compressed Media Formats for XR Runtimes
This case primarily addressed the case 2 from 6.4.4, namely the issue of
transcoding. In order to address this case, it is considered according to
Figure 6.4.5.3-1, the media format is passed through from the 5G network over
the tethering link, using the OpenXR or the general run-time API. This allows
to also add local data as an example that can be added on different layers.
Pass-through may include passing only the compressed media data, but may also
include the security frame work or the entire protocol. This depends on the
capability of the endpoint on the glasses.
Figure 6.4.5.3-1: Pass-through compressed media format.
The key extensions are as follows:
\- Support in the runtime query for supported formats additional information
that include
\- the resolution of the format
\- the quality degradation incurred by the combination of the link and the
coding
\- the security framework and capabilities of the device
\- dynamically providing the metrics and information of the signal quality on
the proprietary link
\- the audio and video decoding capabilities of the glass
\- the security capabilities of the glass
\- the security framework and capabilities of the device
\- statically and dynamically the bitrate and delay of the link
\- Support of usage of this provided static and dynamic information for
example in
\- the Media streaming client to select the appropriate content formats,
bitrates, codecs and qualities, possibly in a dynamic fashion
\- a communication client to negotiate with the network to support the
appropriate content formats, codecs bitrates and qualities, possibly in a
dynamic fashion
\- in split rendering for which the formats are provided accordingly as shown
in Figure 6.4.5.3-2
\- Announcing appropriate information on the 5G System network side to the 5G
Phone in order to be able to make such selections.
The details of these extensions need to be added to for example a streaming
manifest, the session description protocol or to a scene description.
Figure 6.4.5.3-2: Pass-through compressed media format.
A simplified approach is provided in Figure 6.4.5.3-3, for which compressed
formats ate exchanged via the XR runtime API.
Figure 6.4.5.3-3: Simplified architecture: Compressed formats are exchanged
via XR Runtime API
In order to address these extensions in the Swapchain Image Management of
OpenXR are provided here
https://registry.khronos.org/OpenXR/specs/1.0/html/xrspec.html#swapchain-
image-management and summarized in clause 6.4.2, the following extensions are
considered:
\- In a regular operation, the xrEnumerateSwapchainFormats functional call
enumerates the texture formats supported by the current session. The type of
formats returned are dependent on the graphics API specified in
xrCreateSession.
\- As an example, Vulkan permits compressed image formats
https://registry.khronos.org/vulkan/specs/1.3-khr-extensions/html/chap45.html
relying on compressed data formats from Khronos:
https://registry.khronos.org/DataFormat/specs/1.3/dataformat.1.3.html
\- However, none of the APIs refer to using any video compression formats for
each of the swap chain images.
\- Hence, in order to support the above operation of pass-through compressed
formats, OpenXR or any runtime is extended to allow a format that adds
compressed video formats as swap chain images for which
\- Time stamp is target display time (for example RTP time stamp)
\- the compressed format includes render pose
\- compressed texture format is handed over as part of the swap chain
management
Note that this swap chain image management with compressed data only applies
for parts of the submitted swap chain buffers, one layer may be sent over in
compressed from, whereas a locally generated layer may be sent over in raw
form. The synchronization needs to be done by the runtime.
For audio similar principles apply, for which a compressed bitstream is handed
to the glasses.
For the uplink media data, action and pose data may be passed on in compressed
form as received in the glassesto the network. The XR Runtime API may even
provide such data in compressed (for sending to network) and in raw form (for
processing on the phone)
### 6.4.6 Conclusions
In order to support tethered links and glass-based endpoints properly, it is
beneficial to provide a framework that allows to adapt to the end point
capabilities to maximize end-to-end quality in terms of signal quality,
latency, power consumption, etc. In order to support this, extensions
considered above include:
\- Support in the runtime query for supported formats additional information
that include
\- the audio and video decoding capabilities of the glass
\- the security capabilities of the glass
\- the security framework and capabilities of the device
\- statically and dynamically the bitrate and delay of the link
\- Support of usage of this provided static and dynamic information for
example in
\- the Media streaming client to select the appropriate content formats,
bitrates and qualities, possibly in a dynamic fashion
\- a communication client to negotiate with the network to support the
appropriate content formats, bitrates and qualities, possibly in a dynamic
fashion
\- in split rendering which the formats are provided from the cloud/edge
rendering
\- Announcing appropriate information on the 5G System network side to the 5G
Phone in order to be able to make such selections.
These extensions are preferably addressed in extensions OpenXR, 5G Media
Streaming, 5G Real-time communication and the related stage-3 protocols such
as DASH, ISO BMFF, SDP, RTP/RTCP, etc.
## 6.5 Key Issue #5: Compute distribution across UE and network for tethered
glasses
### 6.5.1 Introduction
In the tethered display AR Glasses context, the compute functions are
distributed across the AR Glasses, as well as possibly the UE (phone) and the
network. Even within the network the compute may be done in an edge or in the
cloud. The decision how to distribute the compute across different network
entities highly depends on the use case and application, the available
capabilities in different network entities, the available network connections,
economical reasons and possibly many other constraints. Generally, the
situation may even change over time, for example due to changes in the
application, varying network connections or load re-distribution. It is not
expected that a specification will solve the distribution of the compute
resources. However, what is essential is that the decision-making entity has
as much static and dynamic information in order to make informed decisions.
This clause provides some background on different distribution scenarios. The
main focus is the derivation of relevant static and dynamic status and
capability information to establish proper workflows.
### 6.5.2 Background
Based on TS 26.119 [17], once a session is running and in focussed state a
rendering loop is executed following Figure 6.5.2:
a) The XR Application retrieves the action state, e.g. the status of the
controllers and their associated pose. The application also establishes the
location of different trackables.
b) Before an application can begin writing to a swapchain image, it first
waits on the image to avoid writing to it before the Compositor has finished
reading from it. Then an XR application synchronizes its rendering loop to the
runtime. In the common case that an XR application has pipelined frame
submissions, the application is expected to compute the appropriate target
display time using both the predicted display time and predicted display
interval. An XR Runtime is expected to provide and operate a swapchain that
supports a specific frame rate.
c) Once the wait time completes, the application initiates the rendering
process. In order to support the application in rendering different views the
XR Runtime provides access to the viewer pose and projection parameters that
are needed to render the different views. The view and projection info is
provided for a particular display time within a specified XR space. Typically,
the target/predicted display time for a given frame.
d) the application then performs its rendering work. Rendering work may be
very simple, for example just directly copying data from the application into
the swap chain or may be complex, for example iterating over the scene graph
nodes and rendering complex objects. Once all views/layers are rendered, the
application sends them to the XR Runtime for final compositing including the
expected display time as well as the associated render pose.
e) An XR Runtime typically supports (i) planar projected images rendered from
the eye point of each eye using a perspective projection, typically used to
render the virtual world from the user's perspective, and (ii) quad layer type
describing a posable planar rectangle in the virtual world for displaying two-
dimensional content. Other projection types such as cubemaps, equirectangular
or cylindric projection may also be supported.
f) The XR application offloads the composition of the final image to an XR
Runtime-supplied compositor. By this, the rendering complexity is
significantly lower since details such as frame-rate interpolation and
distortion correction are performed by the XR Runtime. It is assumed that the
XR Runtime provides a compositor functionality for device mapping. A
Compositor in the runtime is responsible for taking all the received layers,
performing any necessary corrections such as pose correction and lens
distortion, compositing them, and then sending the final frame to the display.
An application may use multiple composition layers for its rendering.
Composition layers are drawn in a specified order, with the 0th layer drawn
first. Layers are drawn with a \"painter's algorithm,\" with each successive
layer potentially overwriting the destination layers whether or not the new
layers are virtually closer to the viewer. Composition layers are subject to
blending with other layers. Blending of layers can be controlled by layer per-
texel source alpha. Layer swapchain textures may contain an alpha channel.
Composition and blending is done in RGBA.
g) After the compositor has blended and flattened all layers, it then presents
this image to the system's display. The composited image is then blend with
the user's view of the physical world behind the displays in one of three
modes, based on the application's chosen environment blend mode:
h) Meanwhile, while the XR Runtime uses the submitted frame for compositing
and display, a new rendering process may be kicked off for a different swap
chain image.
Figure 6.5.2-1 Rendering loop for visual data
Once this loop is running, the rendering can statically or dynamically be
adjusted as long as operation northbound of the rendering loop is consistent.
### 6.5.3 Assumptions
According to TS 26.119 [17], media to be rendered and displayed by the XR
device through the XR runtime is typically not available in uncompressed form
on the device. In contrast, media is accessed using a 5G System, decoded in
the device using media capabilities, and the decoded media is rendered to then
be provided through swapchains to the XR Runtime as shown in Figure 6.5.3-1.
Figure 6.5.3-1 Media pipelines: Access, decoding and rendering
The rendering function is responsible to adapt the content to be presentable
by the by the XR Runtime by making use of a rendering loop and using
swapchains. The application configures pipeline of different processes, namely
the media access, the decoding and the rendering.
For certain applications and scenes, the rendering capabilities on the
glasses, or the phone may not be sufficient in order to address the
application requirements and pre-rendering is done in the network.
### 6.5.4 Problem Statement
In all considered architectures, the compute functions are distributed across
the AR Glasses, as well as possibly the phone and the 5G network. Even within
the network the compute may be done in an edge or in the cloud. The decision
how to distribute the compute across different network entities highly depends
on the use case and application, the available capabilities in different
network entities, the available network connections, economical reasons and
possibly many other constraints. Generally, the situation may even change over
time, for example due to changes in the application, varying network
connections or load re-distribution.
### 6.5.5 Potential Solutions
The solution is obvious, namely the distribution of processing functions
across different entities. However, it is not expected that a specification
will solve the distribution of the compute resources, but that a decision-
making entity has sufficient static and dynamic information in order to make
informed decisions.
The decision-making entity may for example be a Media Session Handler
functions that configures the workflows. In order to do such configuration,
the Media Session Handler needs to collect static and real-time information on
the capabilities as well as real-time metrics from:
\- tethering glass
\- phone running the 5G System and Media Session Handler
\- 5G Edge Server
\- Cloud Server
Collected static and dynamic information includes, but is not limited to,
\- link quality (bitrate, QoS),
\- available encoding and decoding resources,
\- available rendering capabilities,
\- operational QoE metrics and logs
Reconfiguration of such workflows needs to be possible.
### 6.5.6 Conclusions
Based on the considerations, the Split Rendering architecture is expected to
be extended to address:
\- A workflow configuration management
\- This workflow management collects static and real-time capabilities and
metrics information based on metrics in clause 6.5.5 from tethering glasses,
phones running the 5G System and Media Session Handler, 5G edge server as well
as cloud server.
\- The workflow management is able to re-configure the rendering workflow.
## 6.6 Key Issue #6: Usage of PIN for Tethered Glasses
### 6.6.1 Description
In order to use PIN (Personal IoT Network) for AR Glasses tethering, a
functional mapping between PIN architecture [6] and the device architecture
for tethered AR Glasses in clause 4.4 of this document is needed.
AR Glasses may support different connectivity solutions, e.g., it can be a
3GPP UE, or a Wi-Fi device. A key issue is to clarify the usage of PIN for AR
Glasses tethering for different scenarios.
### **6.6.2 Potential solutions**
When AR Glasses considered as a 3GPP UE, and typically in the Tethered
Standalone AR glass-based device architecture in clause 4.4.2 and assumed that
a 3GPP UE can act as PEGC and/or PEMC [6], AR Glasses can act as PEGC and/or
PEMC. It means that AR glasses can directly use the PC5 interface for
tethering. In this situation, it is not necessary to use PIN network for AR
glasses tethering.
When AR Glasses considered as a non-3GPP device, wireless tethered
connectivity is provided through non-3GPP access e.g., Wi-Fi or BLE, With the
consideration of the Tethered Display AR glass-based device architecture in
clause 4.4.3 and Tethered AR glasses with 5G relay in clause 4.4.4, a solution
for functional mapping can be found as follows, shown in Figure 6.6.2-1:
\- The 5G Device/Phone is a 3GPP UE which can act as a PEGC and PEMC
\- The Tethered AR Glasses can act as a PINE
\- The XR application server can be considered as a PIN Application server
Figure 6.6.2-1 -- Using PIN for Tethered AR glasses
Accordingly, reference point P1 between the PINE and the PEGC, which is based
on non-3GPP access, can be used for AR Glasses tethering connection.
Reference point P2 between the PEMC and the PEGC is based on non-3GPP access
(e.g. Wi-Fi or BLE) or 5G ProSe Direct Communication. Reference point P3
between the PEMC and the PIN Application Server is based on the direct user
plane path to 5GS, relay path via the PEGC, or other communication path via
Internet. PEMC can be used for PIN management
(create/modify/delete/activate/deactivate a PIN) and add/remove of PINE and
PEGC via P2 and P3, to manage the tethering connection of AR Glasses.
Reference point P4 between the PEGC and the PIN Application Server, which is
based on the user plane path between PEGC and 5GS, can be used for supporting
the relay path between PINE and 5GS for the Tethered AR glasses with 5G relay
architecture in clause 4.4.4.
# 7 Summary of Key Issues and Potential Work Topics
## 7.1 General
This clause documents and clusters potential new work and study areas
identified in the context of this Technical Report.
## 7.2 End-to-End QoS
In case of an architecture, for which the phone is predominantly acting as a
relay as shown in clause 4.4.4, the topics identified in Key Issue #1, key
issue #2 and key issue#3 require solutions to address handling of QoS
parameters and QoE monitoring for such relay systems.
The following is a list of work topics need to be addressed in normative
specifications:
\- Specification of RTP header extensions to support in-band end-to-end delay
measurements as defined in clause 6.2.3.
\- Specification of reporting mechanisms for the end-to-end delay measurements
as defined in clause 6.2.3.
The RTP header extensions are preferably addressed by extending the 5G RTP
protocol as defined in TS 26.522 [18] and the reporting mechanisms may be
addressed by some other related work items.
## 7.3 Capabilities, Formats and Connectivity of Tethered Glass
Following the conclusions for key issue #4 in clause 6.4, in order to support
tethered links and glass-based endpoints properly, it is beneficial to provide
a framework that allows to adapt to the end point capabilities to maximize
end-to-end quality in terms of signal quality, latency, power consumption,
etc. In order to support this, extensions considered above include:
\- Support in the runtime query for supported formats additional information
that include
\- the audio and video decoding capabilities of the glass
\- the security capabilities of the glass
\- the security framework and capabilities of the device
\- statically and dynamically the bitrate and delay of the link
\- Support of usage of this provided static and dynamic information for
example in
\- the Media streaming client to select the appropriate content formats,
bitrates and qualities, possibly in a dynamic fashion
\- a communication client to negotiate with the network to support the
appropriate content formats, bitrates and qualities, possibly in a dynamic
fashion
\- in split rendering which the formats are provided from the cloud/edge
rendering
\- Announcing appropriate information on the 5G System network side to the 5G
Phone in order to be able to make such selections.
These extensions are preferably addressed in extensions OpenXR, 5G Media
Streaming, 5G Real-time communication and the related stage-3 protocols such
as DASH, ISO BMFF, SDP, RTP/RTCP, etc.
## 7.4 Workflow management in distributed split rendering
Based on the considerations in Key Issue #5 as documented in clause 6.5, as
Split Rendering architecture is expected to be extended to address:
\- A workflow configuration management
\- This workflow management collects static and real-time capabilities and
metrics information based on metrics in clause 6.5.5 from tethering glasses,
phones running the 5G System and Media Session Handler, 5G edge server as well
as cloud server.
\- The workflow management is able to re-configure the rendering workflow.
## 7.5 Usage of PIN for Tethered Glasses
Clause 6.6.2 provides a functional mapping in case AR Glasses are considered
as a non-3GPP device, and the wireless tethered connectivity is provided
through non-3GPP access e.g., Wi-Fi or BLE. While the functional mapping is
complete, a more detailed analysis on potential gaps on the user plane path P4
between PEGC and 5GS as well as P1 between the PINE and the PEGC is needed to
understand whether more details would have to be specified. Additional studies
are encouraged.
# 8 Conclusions and Recommendations
The present document addressed different architectures, QoS and media handling
aspects of when tethering AR Glasses to 5G UEs based on initial discussions in
TR 26.998 [2].
Based on the details in the report, the following next steps are proposed:
\- Specify RTP header extensions for in-band end-to-end delay measurements to
support end-to-end QoS based on the discussion in clause 7.2 using the
framework of 5G RTP protocol as defined in TS 26.522 [18].
\- Support capability exchange, formats and connectivity extensions to support
adapting to the end point capabilities of tethered in order to maximize end-
to-end quality in terms of signal quality, latency, power consumption as well
as the compute distribution across different physical devices (glasses, phone,
edge, cloud) based on the detailed discussion in clause 7.3.
\- Support information collection for workflow management, including static
and real-time capabilities and metrics information from tethering glasses,
phones running the 5G System and Media Session Handler, 5G edge server as well
as cloud server, including the ability to re-configure the rendering workflows
based on the discussions in clause 7.4.
\- Continue the study of applicability and potential gaps of PIN for tethered
glasses based on the discussion in clause 7.5.
All work topics will benefit to be carried out in close coordination with
other groups in 3GPP on 5G System and radio related matters, edge computing
and rendering as well in communication with experts in MPEG on the MPEG-I
project as well as with Khronos on their work on OpenXR, glTF and
Vulkan/OpenGL.
###### ## Annex A (informative): QoS Control of Relay WLAR UE when 5G Sidelink
Used for Tethering Link
For Relay WLAR UE, the 5G sidelink communication is used for the tethering
link. The tethering 5G Phone providing the IP network connectivity can be
seemed as a 5G ProSe Layer-3 UE-to-Network Relay and the tethered AR Glasses
can be seemed as a Remote UE as specified in TS 23.304 [7]. The QoS
requirements for the end-to-end AR session can be satisfied by the
corresponding QoS control for the tethering link between AR Glasses and 5G
Relay UE (i.e. PC5 QoS control), and the QoS control through the 5G system
(i.e. the PDU Session between UE and UPF). The tethering link QoS and the 5G
System QoS are separately controlled with corresponding QoS rules and QoS
parameters (e.g. 5QI, PQI) as specified in TS 23.287 [8] and TS 23.501 [9].
As shown in figure A-1 below, the end-to-end QoS can be met only when the QoS
requirements are properly translated and satisfied over the two legs
respectively.
Figure A-1: End-to-End QoS translation for 5G Layer-3 Relay operation
To achieve this, the QoS mapping can be pre-configured or provided to the 5G
Relay UE from the 5GC. The QoS mapping includes combinations of the 5QIs for
the 5G link and the PQIs for the tethering link as entries. Both 5QIs and PQIs
have standardized values as specified in TS 23.501 [9] and TS 23.287 [8].
If the QoS setup of 5G system link is initiated by network, the 5G Core
Network can generates the QoS parameters (e.g. 5QI) and signal to the 5G Relay
UE. Then the 5G Relay UE determines the tethering link QoS parameters based
the pre-retrieved QoS mapping and then setup the tethering link between AR
glasses and the 5G Relay UE.
If the AR Glasses initiates QoS setup or modification for the tethering link,
it provides the QoS Info to the 5G Relay UE. The QoS Info (i.e. PQI, etc.) are
interpreted as the end-to-end QoS requirements by the 5G Relay UE for the
traffic transmission through the 5G system. The 5G Relay UE would check if the
end-to-end QoS requirements can be supported, and decide the 5GS QoS and
tethering link QoS parameters based on the QoS mapping.
#